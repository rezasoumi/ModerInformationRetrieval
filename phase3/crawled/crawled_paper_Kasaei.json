[
    {
        "id": "12508951ba96b7d4c0906ed95542287d3ebdfd95",
        "title": "The Eighth Visual Object Tracking VOT2020 Challenge Results",
        "abstract": "A significant novelty is introduction of a new VOT short-term tracking evaluation methodology, and introduction of segmentation ground truth in the VOT-ST2020 challenge \u2013 bounding boxes will no longer be used in theVDT challenges. The Visual Object Tracking challenge VOT2020 is the eighth annual tracker benchmarking activity organized by the VOT initiative. Results of 58 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The VOT2020 challenge was composed of five sub-challenges focusing on different tracking domains: (i) VOT-ST2020 challenge focused on short-term tracking in RGB, (ii) VOT-RT2020 challenge focused on \u201creal-time\u201d short-term tracking in RGB, (iii) VOT-LT2020 focused on long-term tracking namely coping with target disappearance and reappearance, (iv) VOT-RGBT2020 challenge focused on short-term tracking in RGB and thermal imagery and (v) VOT-RGBD2020 challenge focused on long-term tracking in RGB and depth imagery. Only the VOT-ST2020 datasets were refreshed. A significant novelty is introduction of a new VOT short-term tracking evaluation methodology, and introduction of segmentation ground truth in the VOT-ST2020 challenge \u2013 bounding boxes will no longer be used in the VOT-ST challenges. A The Eighth Visual Object Tracking VOT2020 Challenge Results 3 new VOT Python toolkit that implements all these novelites was introduced. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website.",
        "publication_year": "2020",
        "authors": [
            "M. Kristan",
            "A. Leonardis",
            "Jiri Matas",
            "M. Felsberg",
            "R. Pflugfelder",
            "Joni-Kristian K\u00e4m\u00e4r\u00e4inen",
            "Martin Danelljan",
            "L. \u010c. Zajc",
            "A. Luke\u017ei\u010d",
            "O. Drbohlav",
            "Linbo He",
            "Yushan Zhang",
            "Song Yan",
            "Jinyu Yang",
            "G. Fernandez",
            "A. Hauptmann",
            "Alireza Memarmoghadam",
            "\u00c1lvaro Garc\u00eda-Mart\u00edn",
            "Andreas Robinson",
            "A. Varfolomieiev",
            "Awet Haileslassie Gebrehiwot",
            "Bedirhan Uzun",
            "Bin Yan",
            "Bing Li",
            "C. Qian",
            "Chi-Yi Tsai",
            "C. Micheloni",
            "Dong Wang",
            "Fei Wang",
            "Fei Xie",
            "Felix J\u00e4remo Lawin",
            "F. Gustafsson",
            "G. Foresti",
            "Goutam Bhat",
            "Guang-Gui Chen",
            "Haibin Ling",
            "Haitao Zhang",
            "Hakan Cevikalp",
            "Haojie Zhao",
            "Haoran Bai",
            "Hari Chandana Kuchibhotla",
            "Hasan Saribas",
            "Heng Fan",
            "Hossein Ghanei-Yakhdan",
            "Houqiang Li",
            "Houwen Peng",
            "Huchuan Lu",
            "Hui Li",
            "Javad Khaghani",
            "Jes\u00fas Besc\u00f3s",
            "Jianhua Li",
            "Jianlong Fu",
            "Jiaqian Yu",
            "Jingtao Xu",
            "J. Kittler",
            "Jun Yin",
            "Junhyun Lee",
            "Kaicheng Yu",
            "Kaiwen Liu",
            "K. Yang",
            "Kenan Dai",
            "Li Cheng",
            "Li Zhang",
            "Lijun Wang",
            "Linyuan Wang",
            "L. Gool",
            "Luca Bertinetto",
            "Matteo Dunnhofer",
            "Miao Cheng",
            "Mohana Murali Dasari",
            "Ning Wang",
            "Pengyu Zhang",
            "Philip H. S. Torr",
            "Qiang Wang",
            "R. Timofte",
            "Rama Krishna Sai Subrahmanyam Gorthi",
            "Seokeon Choi",
            "Seyed Mojtaba Marvasti-Zadeh",
            "Shao-Chuan Zhao",
            "S. Kasaei",
            "Shoumeng Qiu",
            "Shuhao Chen",
            "Thomas Bo Sch\u00f6n",
            "Tianyang Xu",
            "W. Lu",
            "Weiming Hu",
            "Wen-gang Zhou",
            "Xi Qiu",
            "Xiao Ke",
            "Xiaojun Wu",
            "Xiaolin Zhang",
            "Xiaoyun Yang",
            "Xuefeng Zhu",
            "Yingjie Jiang",
            "Yingming Wang",
            "Yiwei Chen",
            "Yu Ye",
            "Yuezhou Li",
            "Yuncon Yao",
            "Yunsung Lee",
            "Yuzhang Gu",
            "Zezhou Wang",
            "Zhangyong Tang",
            "Zhenhua Feng",
            "Zhijun Mai",
            "Zhipeng Zhang",
            "Zhirong Wu",
            "Ziang Ma"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "144",
        "reference_count": "88",
        "references": [
            "/paper/The-Ninth-Visual-Object-Tracking-VOT2021-Challenge-Kristan-Matas/f1d53e9c301d78e0b148e2f91adfc4fde2621ee5",
            "/paper/Long-term-Visual-Tracking%3A-Review-and-Experimental-Liu-Chen/2d4713ce1df60f771b65e900fd02352989df82ef",
            "/paper/Switch-and-Refine%3A-A-Long-Term-Tracking-and-Xu-Zhao/ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
            "/paper/Multi-modal-Visual-Tracking%3A-Review-and-Comparison-Zhang-Wang/d884af3933148cef3b50fd38c810f5a7763d0fc9",
            "/paper/CoCoLoT%3A-Combining-Complementary-Trackers-in-Visual-Dunnhofer-Micheloni/23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
            "/paper/Is-First-Person-Vision-Challenging-for-Object-Dunnhofer-Furnari/ca97f741f331b5b43d0577a46c05984f0785a8fa",
            "/paper/Visual-Object-Tracking-With-Discriminative-Filters-Javed-Danelljan/0530cbeb847f5e5002d1183c482759dff5f8c439",
            "/paper/A-Large-scale-Sports-Tracking-Dataset-and-Based-Wang-Zhou/dd950231a9c5fb671a23c97a5038ba061937f6f4",
            "/paper/Visual-Object-Tracking-in-First-Person-Vision-Dunnhofer-Furnari/c89da5aa9697ab9d5366353ec29b3e9c1b610469",
            "/paper/Global-Tracking-via-Ensemble-of-Local-Trackers-Zhou-Chen/ef19859f204048cc83bed9d3eeaa74f75e2fbabc",
            "/paper/The-Seventh-Visual-Object-Tracking-VOT2019-Results-Kristan-Matas/786577081e00d69eeac8e9612eaf2dad59765e73",
            "/paper/The-Sixth-Visual-Object-Tracking-VOT2018-Challenge-Kristan-Leonardis/219e9a4527110baf1feb3df20db12064eeafdfb7",
            "/paper/The-Visual-Object-Tracking-VOT2017-Challenge-Kristan-Leonardis/350d507f5d899e4d7293b1aa951aa0f81b9fd30a",
            "/paper/The-Visual-Object-Tracking-VOT2015-Challenge-Kristan-Matas/047ea298464b041a90c4ab4e716356c019d613ab",
            "/paper/The-Visual-Object-Tracking-VOT2016-Challenge-Kristan-Leonardis/966aad492f75b17f698e981e008b73b51816c6aa",
            "/paper/The-Visual-Object-Tracking-VOT2013-Challenge-Kristan-Matas/4b1a47709d0546e5bc614bf9a521c550e6881d04",
            "/paper/Performance-Evaluation-Methodology-for-Long-Term-Luke%C5%BEi%C4%8D-Zajc/c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "/paper/A-thermal-Object-Tracking-benchmark-Berg-Ahlberg/dd45fe910a0200d43aaa77362f658542f6e175ff",
            "/paper/Long-term-Tracking-in-the-Wild%3A-A-Benchmark-Valmadre-Bertinetto/ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "/paper/D3S-%E2%80%93-A-Discriminative-Single-Shot-Segmentation-Luke%C5%BEi%C4%8D-Matas/45512d44f1205bc92775f2e880858b3f23c9f5fd"
        ]
    },
    {
        "id": "c6db34ade32b3681a92068b22a354903b2953d52",
        "title": "Benign and malignant breast tumors classification based on region growing and CNN segmentation",
        "abstract": "Semantic Scholar extracted view of \"Benign and malignant breast tumors classification based on region growing and CNN segmentation\" by R. Rouhi et al.",
        "publication_year": "2015",
        "authors": [
            "R. Rouhi",
            "M. Jafari",
            "S. Kasaei",
            "P. Keshavarzian"
        ],
        "related_topics": [
            "Computer Science",
            "Medicine"
        ],
        "citation_count": "329",
        "reference_count": "59",
        "references": [
            "/paper/Classification-of-benign-and-malignant-breast-based-Rouhi-Jafari/2415fc06de82ab41ad8b9615162247afb02974af",
            "/paper/Benign-and-malignant-breast-cancer-segmentation-Punitha-Amuthan/8133e66c9c03095ef605090e6a72b752dc774d92",
            "/paper/An-enhancement-of-mammogram-images-for-breast-using-Patel-Hadia/f3eb7b753cf2a9569a1d9fdd11618bce28b1ef46",
            "/paper/Fuzzy-C-means-and-region-growing-based-of-tumor-Sadad-Munir/301fee6989cc75f84343836cab5a25691d58dc5c",
            "/paper/Segmentation-and-classification-of-breast-cancer-Ramesh-Sasikala/8e75f635dd7578926aa7ae19ff29a8fc5c3911ae",
            "/paper/Classification-of-Mammograms-Using-Convolutional-Debelee-Amirian/156733fbf757d4e8a333537518c8961163c4fbf7",
            "/paper/Machine-learning-based-computer-aided-diagnosis-for-Singh-Sharma/6e69f7ea9f63b651ebd676d51d6cca1a483cb2ec",
            "/paper/A-Hierarchical-Classification-Method-for-Breast-Mohammadpoor-Shoeibi/4d7ef4f116a535750529e2853c181d5d3b678646",
            "/paper/Breast-Cancer-Detection-and-Classification-from-Gurudas-Shaila/a7f3cd1885a0f1548596b288eb6768d818b8463d",
            "/paper/Breast-MRI-Tumor-Automatic-Segmentation-and-Breast-Guo-Huang/b4ae16a6f020e12062f426781ef3971c82f7f455",
            "/paper/An-adaptive-region-growing-algorithm-for-breast-in-Cao-Hao/3c948ca247c6f55ef994400e713412b5f845dd40",
            "/paper/Directional-features-for-automatic-tumor-of-images-Buciu-Gacs%C3%A1di/9b5d0a48b0feb156a1270da54d90d0963a3f0404",
            "/paper/Performance-evaluation-of-a-region-growing-for-Rabottino-Mencattini/76389ebb7c1496239e66fd663b0e7e43d391bca9",
            "/paper/Building-an-ensemble-system-for-diagnosing-masses-Zhang-Tomuro/d0059280e3f69b8fd07ce036e7d2407e3ebcff9e",
            "/paper/Classification-of-benign-and-malignant-patterns-in-Verma-McLeod/2dfb1fd3adfa58a3448251e03b1a5a78239958b3",
            "/paper/Classification-of-benign-and-malignant-masses-based-Tahmasbi-Saki/46c409dd878e643271ef63f1817ded8b57abc01e",
            "/paper/Breast-Cancer-Detection-Using-Neural-Network-Models-Pawar-Patil/342da5d8633aebf27d914a8618e523579a130289",
            "/paper/Support-vector-machines-combined-with-feature-for-Akay/fe83150bc326fd62d352cb2993ac91344f195e10",
            "/paper/A-ranklet-based-image-representation-for-mass-in-Masotti/483f0f12feb8ac1c396349e5526a7552b6b067cd",
            "/paper/Breast-mass-contour-segmentation-algorithm-in-Berber-Alpkocak/a9d0b3485f3091e832f87edb469c350c90cabae1"
        ]
    },
    {
        "id": "ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
        "title": "Event Detection and Summarization in Soccer Videos Using Bayesian Network and Copula",
        "abstract": "A novel Bayesian network-based method that is capable of detecting seven different events in soccer videos; namely, goal, card, goal attempt, corner, foul, offside, and nonhighlights is proposed. Semantic video analysis and automatic concept extraction play an important role in several applications; including content-based search engines, video indexing, and video summarization. As the Bayesian network is a powerful tool for learning complex patterns, a novel Bayesian network-based method is proposed for automatic event detection and summarization in soccer videos. The proposed method includes efficient algorithms for shot boundary detection, shot view classification, mid-level visual feature extraction, and construction of the related Bayesian network. The method contains of three main stages. In the first stage, the shot boundaries are detected. Using the hidden Markov model, the video is segmented into large and meaningful semantic units, called play-break sequences. In the next stage, several features are extracted from each of these units. Finally, in the last stage, in order to achieve high level semantic features (events and concepts), the Bayesian network is used. The basic part of the method is constructing the Bayesian network, for which the structure is estimated using the Chow-Liu tree. The joint distributions of random variables of the network are modeled by applying the Farlie-Gumbel-Morgenstern family of Copulas. The performance of the proposed method is evaluated on a dataset with about 9 h of soccer videos. The method is capable of detecting seven different events in soccer videos; namely, goal, card, goal attempt, corner, foul, offside, and nonhighlights. Experimental results show the effectiveness and robustness of the proposed method on detecting these events.",
        "publication_year": "2014",
        "authors": [
            "M. Tavassolipour",
            "Mahmood Karimian",
            "S. Kasaei"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "96",
        "reference_count": "38",
        "references": [
            "/paper/Bayesian-System-And-Copula-For-Event-Detection-And-Patil/662df45afea0d80922a86a4af431eb80ede87da2",
            "/paper/Event-detection-in-soccer-videos-using-shot-focus-Zhao-Lu/0a89745435b38291f0f2f17173f33ccb5d3a826b",
            "/paper/A-survey-on-event-detection-based-video-for-cricket-Raval-Goyani/828f9424abab6fd3e48df5025bef4ff5d56c4295",
            "/paper/Replay-and-key-events-detection-for-sports-video-Javed-Irtaza/5d8aff02861984abde3ba17033ca3f07e4138b46",
            "/paper/Replay-and-key-events-detection-for-sports-video-Javed-Irtaza/b66b45ba5d12490f7101cc314d7c6c066455fe72",
            "/paper/Video-Summarization%3A-Survey-on-Event-Detection-and-Khan-Pawar/92b1bf710340ecbf5f40005d88f9005d6882325f",
            "/paper/A-decision-tree-framework-for-shot-classification-Javed-Malik/c3c70ec1636e9cbb271cdf84e6b6125894c38cfb",
            "/paper/Soccer-Video-Structure-Analysis-by-Parallel-Feature-Fani-Yazdi/0b1e432fd1852d797417b4cbd317c44ce01f5a66",
            "/paper/Structural-Approach-for-Event-Resolution-in-Cricket-Premaratne-Jayaratne/cc810ccea173885c7630e159409be23a2fdd028c",
            "/paper/Soccer-Video-Event-Detection-Based-on-Deep-Learning-Yu-Lei/168ebf77527329bc2dbd0ce82bddb9905c3aa7ee",
            "/paper/Automatic-Soccer-Video-Analysis-and-Summarization-Ekin-Tekalp/d3448a792e771d8cd49a4e0c3b08ae5b3d51e51f",
            "/paper/Semantic-analysis-of-soccer-video-using-dynamic-Huang-Shih/d71c84fbaff0d7f2cbcf2f03630e189c58939a4a",
            "/paper/Event-Detection-of-Broadcast-Baseball-Videos-Hung-Hsieh/43eb2d80721daa318cc19e4cce405122934e88f2",
            "/paper/Knowledge-Discounted-Event-Detection-in-Sports-Tjondronegoro-Chen/0cca81f3b2928f6d4005e1a8b617960e2fb14d3e",
            "/paper/A-semantic-event-detection-approach-and-its-to-in-Haering-Qian/0365c1382924394e200cb627e59cb9c21f8e75bd",
            "/paper/Integrated-Mining-of-Visual-Features%2C-Speech-and-Tseng-Su/c9edc10d5c22ae62a700c37a41bb0ea22961a0aa",
            "/paper/Using-Webcast-Text-for-Semantic-Event-Detection-in-Xu-Zhang/90fbd777cf57096e9292601dfe0dbab30198d40f",
            "/paper/Automatic-player-detection%2C-labeling-and-tracking-Liu-Tong/f7b4fcc9dbb97997dc1793d7a366cba274f53134",
            "/paper/Event-detection-in-field-sports-video-using-and-a-Sadlier-O'Connor/8e0f60b718fa19c2ed10bd93401796683af79512",
            "/paper/A-Video-Event-Detection-and-Mining-Framework-Guler-Liang/c2f8178ce89a6cc1ad0e1dd5db4bf155d5a80620"
        ]
    },
    {
        "id": "53fc0415e0d00f9691994a49b8232a1cc2dfad5f",
        "title": "An efficient PCA-based color transfer method",
        "abstract": "Semantic Scholar extracted view of \"An efficient PCA-based color transfer method\" by A. Abadpour et al.",
        "publication_year": "2007",
        "authors": [
            "A. Abadpour",
            "S. Kasaei"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "67",
        "reference_count": "45",
        "references": [
            "/paper/Color-transfer-based-on-multiscale-gradient-aware-Su-Deng/24dccf31b0347d138cdca66d71d9cb264e7498b1",
            "/paper/PRINCIPAL-COLOR-AND-ITS-APPLICATION-TO-COLOR-IMAGE-Abadpour-Kasaei/be6a3fb065281fe3f27bd99d61ef28e41635e575",
            "/paper/A-Survey-on-Color-Transfer-Methods-Alappatt-Paul/44175dc6641b3a389a81bd457d4d4deb7d5d3e08",
            "/paper/A-Computationally-Efficient-Technique-for-Image-Pipirigeanu-Botchko/f2ec186b8532ca09442886f30f33e5eb5e20b5dc",
            "/paper/Dark-Image-Enhancement-Using-Perceptual-Color-Cepeda-Negrete-S%C3%A1nchez-Y%C3%A1%C3%B1ez/a53b9a81fc16016355975215fd9d130c08c441ab",
            "/paper/Color-fusion-algorithm-for-visible-and-infrared-on-Wang-Shi/c4e047007c9ca9c1755698442bc2fe1f211a2922",
            "/paper/Optimizing-color-transfer-using-color-similarity-Chen-Huang/b98e227f6fd632c95806828116378d04782bac24",
            "/paper/An-optimized-region-based-color-transfer-method-for-Zaveri-Zaveri/e9be22d7ec8756c432729b2c21c1bec911fbdc4c",
            "/paper/Selective-color-transferring-via-ellipsoid-color-Liu-Sun/8de9fd36e1fe69466cc76a82b7a432dc2eb10861",
            "/paper/Cartoon-and-Texture-Decomposition-Based-Color-for-Han-Xu/73fe6c0223d3cae49567a20cdd4797b5b14603e5",
            "/paper/A-fast-and-efficient-fuzzy-color-transfer-method-Abadpour-Kasaei/e1dfff8cff33ede3564a35724632bddc5b8619b1",
            "/paper/New-Principle-Component-Analysis-Based-Colorizing-Abadpour-Kasaei/d93e4e77e2baba98f1af7f23d47fbf9b46be4df5",
            "/paper/Fast-algorithms-for-color-image-processing-by-Cheng-Hsia/05f63bdf9e60d0a299cfe5e8d7ba043904f1fea1",
            "/paper/A-new-FPCA-based-fast-segmentation-method-for-color-Abodpour-Kasaei/0c2ced886708cc3aea4705f8765d152cd3f69cd2",
            "/paper/Linear-color-segmentation-and-its-implementation-Nikolaev-Nikolayev/a2882b8b0c9635d39d15a28138e3f47907f3177b",
            "/paper/Multithresholding-of-color-and-gray-level-images-a-Papamarkos-Strouthopoulos/61579369e7b97dec9c699c058edaafcde2817d21",
            "/paper/Color-Segmentation-Based-on-Separate-Anisotropic-of-Lucchese-Mitra/4a6c5c9b1fb106f7d82508ae593d30e207c8ea45",
            "/paper/Color-information-for-region-segmentation-Ohta-Kanade/ab67b9d0da50e251a4f7e42370540547b891ceb1",
            "/paper/Transferring-color-to-greyscale-images-Welsh-Ashikhmin/d5c6edb53dc41f298f145041cd2c53e40e3acf2b",
            "/paper/GRAYSCALE-IMAGE-MATTING-AND-COLORIZATION-Chen-Wang/ed7e166f65bcecc522c6c4bbb29fcf8048010873"
        ]
    },
    {
        "id": "1fbb4201af091aef55360f113ba35814063923e4",
        "title": "Deep Learning for Visual Tracking: A Comprehensive Survey",
        "abstract": "This survey aims to systematically investigate the current DL-based visual tracking methods, benchmark datasets, and evaluation metrics, and extensively evaluates and analyzes the leading visualtracking methods. Visual target tracking is one of the most sought-after yet challenging research topics in computer vision. Given the ill-posed nature of the problem and its popularity in a broad range of real-world scenarios, a number of large-scale benchmark datasets have been established, on which considerable methods have been developed and demonstrated with significant progress in recent years \u2013 predominantly by recent deep learning (DL)-based methods. This survey aims to systematically investigate the current DL-based visual tracking methods, benchmark datasets, and evaluation metrics. It also extensively evaluates and analyzes the leading visual tracking methods. First, the fundamental characteristics, primary motivations, and contributions of DL-based methods are summarized from nine key aspects of: network architecture, network exploitation, network training for visual tracking, network objective, network output, exploitation of correlation filter advantages, aerial-view tracking, long-term tracking, and online tracking. Second, popular visual tracking benchmarks and their respective properties are compared, and their evaluation metrics are summarized. Third, the state-of-the-art DL-based methods are comprehensively examined on a set of well-established benchmarks of OTB2013, OTB2015, VOT2018, LaSOT, UAV123, UAVDT, and VisDrone2019. Finally, by conducting critical analyses of these state-of-the-art trackers quantitatively and qualitatively, their pros and cons under various common scenarios are investigated. It may serve as a gentle use guide for practitioners to weigh when and under what conditions to choose which method(s). It also facilitates a discussion on ongoing issues and sheds light on promising research directions.",
        "publication_year": "2019",
        "authors": [
            "Seyed Mojtaba Marvasti-Zadeh",
            "Li Cheng",
            "Hossein Ghanei-Yakhdan",
            "S. Kasaei"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "156",
        "reference_count": "281",
        "references": [
            "/paper/Adaptive-exploitation-of-pre-trained-deep-neural-Marvasti-Zadeh-Ghanei-Yakhdan/3aaf88163e9e502daf5be57917470c30c63da6a6",
            "/paper/Siamese-Attentional-Cascade-Keypoints-Network-for-Wang-Wang/45bde350082fe5ee366c8f1b761429d2277fbcca",
            "/paper/Improving-Object-Tracking-by-Added-Noise-and-Fiaz-Mahmood/93742493c26652d4a95627de699869795698a554",
            "/paper/Comparison-of-four-visual-tracking-algorithms-based-Jin-Sun/7231131741ece10fe59ba59361abcf70414055d9",
            "/paper/Multi-expert-visual-tracking-using-hierarchical-via-Moorthy-Joo/90e1220930ecf6dc052632310efd1301d6ab1587",
            "/paper/COMET%3A-Context-Aware-IoU-Guided-Network-for-Small-Marvasti-Zadeh-Khaghani/842451bbece5958301283c9398139130643dcb73",
            "/paper/CRACT%3A-Cascaded-Regression-Align-Classification-for-Fan-Ling/6e0442456b3a475e1d836d7e345fdce98ef5ad26",
            "/paper/Multiple-Pedestrians-and-Vehicles-Tracking-in-A-Azimi-Kraus/005da5074bada2c7fa81ba3ecd9be2721b1d69a2",
            "/paper/Towards-Accurate-Pixel-wise-Object-Tracking-by-Zhang-Li/171c292989d4d34163b59bd5e5cc1a383db9c0ab",
            "/paper/High-Performance-Long-Term-Tracking-With-Dai-Zhang/adacccd99a42c3145ec6392a1a6b08878376e38b",
            "/paper/Deep-visual-tracking%3A-Review-and-experimental-Li-Wang/26e2ca763087be09e3799ad294302aa91077942d",
            "/paper/Real-Time-Deep-Tracking-via-Corrective-Domain-Li-Wang/021d0c7013da519b508610064f264c76d768fdf1",
            "/paper/An-In-Depth-Analysis-of-Visual-Tracking-with-Neural-Pflugfelder/0a400fd7f0ee28694889baaa4faef150b6912dfa",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Good-Features-to-Correlate-for-Visual-Tracking-Gundogdu-Alatan/388d29f001411ff80650f80cf197afc440d98b51",
            "/paper/Deep-tracking-with-objectness-Wang-Li/f24015a365ea2454391c285cd30b8ae723dbb05e",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Densely-Connected-Discriminative-Correlation-for-Peng-Liu/1855818c492d5f42dbe14814e4dd9b5733d54790",
            "/paper/Visual-Tracking-via-Auto-Encoder-Pair-Correlation-Cheng-Zhang/e2e34b202363e4a46a14cd35fd4088d88b2e650e"
        ]
    },
    {
        "id": "f1d53e9c301d78e0b148e2f91adfc4fde2621ee5",
        "title": "The Ninth Visual Object Tracking VOT2021 Challenge Results",
        "abstract": "The Visual Object Tracking challenge VOT2021 is the ninth annual tracker benchmarking activity organized by the VOT initiative; results of 71 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in recent years. The Visual Object Tracking challenge VOT2021 is the ninth annual tracker benchmarking activity organized by the VOT initiative. Results of 71 trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in recent years. The VOT2021 challenge was composed of four sub-challenges focusing on different tracking domains: (i) VOT-ST2021 challenge focused on short-term tracking in RGB, (ii) VOT-RT2021 challenge focused on \"real-time\" short-term tracking in RGB, (iii) VOT-LT2021 focused on long-term tracking, namely coping with target disappearance and reappearance and (iv) VOT-RGBD2021 challenge focused on long-term tracking in RGB and depth imagery. The VOT-ST2021 dataset was refreshed, while VOT-RGBD2021 introduces a training dataset and sequestered dataset for winner identification. The source code for most of the trackers, the datasets, the evaluation kit and the results along with the source code for most trackers are publicly available at the challenge website1.",
        "publication_year": "2021",
        "authors": [
            "M. Kristan",
            "Jiri Matas",
            "A. Leonardis",
            "M. Felsberg",
            "R. Pflugfelder",
            "J. K\u00e4m\u00e4r\u00e4inen",
            "H. Chang",
            "Martin Danelljan",
            "L. \u010c. Zajc",
            "A. Luke\u017ei\u010d",
            "O. Drbohlav",
            "Jani K\u00e4pyl\u00e4",
            "Gustav H\u00e4ger",
            "Song Yan",
            "Jinyu Yang",
            "Zhongqun Zhang",
            "G. Fernandez",
            "Mohamed H. Abdelpakey",
            "Goutam Bhat",
            "L. Cerkezi",
            "Hakan \u00c7evikalp",
            "Shengyong Chen",
            "Xin Chen",
            "Miao Cheng",
            "Ziyi Cheng",
            "Yu-Chen Chiu",
            "Ozgun Cirakman",
            "Yutao Cui",
            "Kenan Dai",
            "Mohana Murali Dasari",
            "Qili Deng",
            "Xingping Dong",
            "Daniel K. Du",
            "Matteo Dunnhofer",
            "Zhenhua Feng",
            "Zhiyong Feng",
            "Z. Fu",
            "Shiming Ge",
            "Rama Krishna Sai Subrahmanyam Gorthi",
            "Yuzhang Gu",
            "B. Gunsel",
            "Qing Guo",
            "Filiz Gurkan",
            "Wencheng Han",
            "Yanyan Huang",
            "Felix J\u00e4remo Lawin",
            "Shang-Jhih Jhang",
            "Rongrong Ji",
            "Cheng Jiang",
            "Yingjie Jiang",
            "Felix Juefei-Xu",
            "Yin Jun",
            "Xiaolong Ke",
            "F. Khan",
            "B. Kim",
            "J. Kittler",
            "X. Lan",
            "Jun Ha Lee",
            "Bastian Leibe",
            "Hui Li",
            "Jianhua Li",
            "Xianxian Li",
            "Yuezhou Li",
            "Bo Liu",
            "Chang Liu",
            "Jingen Liu",
            "Li Liu",
            "Qingjie Liu",
            "Huchuan Lu",
            "Wei Lu",
            "Jonathon Luiten",
            "Jie Ma",
            "Ziang Ma",
            "N. Martinel",
            "Christoph Mayer",
            "Alireza Memarmoghadam",
            "C. Micheloni",
            "Yuzhen Niu",
            "D. Paudel",
            "Houwen Peng",
            "Shoumeng Qiu",
            "Aravindh Rajiv",
            "M. Rana",
            "Andreas Robinson",
            "Hasan Saribas",
            "Ling Shao",
            "M. Shehata",
            "Furao Shen",
            "Jianbing Shen",
            "Kristian Simonato",
            "Xiaoning Song",
            "Zhangyong Tang",
            "R. Timofte",
            "Philip H. S. Torr",
            "Chi-Yi Tsai",
            "Bedirhan Uzun",
            "L. Gool",
            "P. Voigtlaender",
            "Dong Wang",
            "Guangting Wang",
            "Liangliang Wang",
            "Lijun Wang",
            "Limin Wang",
            "Linyuan Wang",
            "Yong Wang",
            "Yunhong Wang",
            "Chenyang Wu",
            "Gangshan Wu",
            "Xiaojun Wu",
            "Fei Xie",
            "Tianyang Xu",
            "Xiang Xu",
            "Wanli Xue",
            "Bin Yan",
            "Wankou Yang",
            "Xiaoyun Yang",
            "Yu Ye",
            "J. Yin",
            "Chengwei Zhang",
            "Chunhui Zhang",
            "Haitao Zhang",
            "Kaihua Zhang",
            "Kangkai Zhang",
            "Xiaohan Zhang",
            "Xiaolin Zhang",
            "Xinyu Zhang",
            "Zhibing Zhang",
            "Shao-Chuan Zhao",
            "Mingmin Zhen",
            "Bineng Zhong",
            "Jiawen Zhu",
            "Xuefeng Zhu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "47",
        "reference_count": "70",
        "references": [
            "/paper/Long-term-Visual-Tracking%3A-Review-and-Experimental-Liu-Chen/2d4713ce1df60f771b65e900fd02352989df82ef",
            "/paper/Switch-and-Refine%3A-A-Long-Term-Tracking-and-Xu-Zhao/ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
            "/paper/CoCoLoT%3A-Combining-Complementary-Trackers-in-Visual-Dunnhofer-Micheloni/23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
            "/paper/Long-term-tracking-with-transformer-and-template-Zhang-Peng/4bb2df1e5dc09ec7f0e50b5ba304a6e51943c72f",
            "/paper/Depth-only-Object-Tracking-Yan-Yang/80ddaf09abab8894622204ed6ac637f5f0f4b8ce",
            "/paper/Visual-Object-Tracking-in-First-Person-Vision-Dunnhofer-Furnari/c89da5aa9697ab9d5366353ec29b3e9c1b610469",
            "/paper/RGBD-Object-Tracking%3A-An-In-depth-Review-Yang-Li/dc9a66e0f329de8054f4ab845331fb7183987418",
            "/paper/Trans2k%3A-Unlocking-the-Power-of-Deep-Models-for-Luke%C5%BEi%C4%8D-Trojer/b678c2a9ccf5ac68f462ef2abf1aaf72787f43cb",
            "/paper/Visual-Object-Tracking-on-Multi-modal-RGB-D-Videos%3A-Zhu-Xu/903509c17cf0e013df71f2534ac3527719faa555",
            "/paper/HOOT%3A-Heavy-Occlusions-in-Object-Tracking-Benchmark-Sahin-Itti/a5c22a5bab4428d9cf3be6a398d883e1f5ea9a00",
            "/paper/The-Eighth-Visual-Object-Tracking-VOT2020-Challenge-Kristan-Leonardis/12508951ba96b7d4c0906ed95542287d3ebdfd95",
            "/paper/The-Seventh-Visual-Object-Tracking-VOT2019-Results-Kristan-Matas/786577081e00d69eeac8e9612eaf2dad59765e73",
            "/paper/The-Visual-Object-Tracking-VOT2017-Challenge-Kristan-Leonardis/350d507f5d899e4d7293b1aa951aa0f81b9fd30a",
            "/paper/The-Visual-Object-Tracking-VOT2016-Challenge-Kristan-Leonardis/966aad492f75b17f698e981e008b73b51816c6aa",
            "/paper/The-Visual-Object-Tracking-VOT2015-Challenge-Kristan-Matas/15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d",
            "/paper/The-Visual-Object-Tracking-VOT2013-Challenge-Kristan-Matas/4b1a47709d0546e5bc614bf9a521c550e6881d04",
            "/paper/Performance-Evaluation-Methodology-for-Long-Term-Luke%C5%BEi%C4%8D-Zajc/c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "/paper/D3S-%E2%80%93-A-Discriminative-Single-Shot-Segmentation-Luke%C5%BEi%C4%8D-Matas/45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "/paper/CDTB%3A-A-Color-and-Depth-Visual-Object-Tracking-and-Luke%C5%BEi%C4%8D-Kart/f202feae9ca7b3766e072b6af657beed2236a93c",
            "/paper/Reptile-Meta-Tracking-Jhang-Tsai/0f50914e86b6010586f1772308858de9a418fb9f"
        ]
    },
    {
        "id": "2d4713ce1df60f771b65e900fd02352989df82ef",
        "title": "Long-term Visual Tracking: Review and Experimental Comparison",
        "abstract": "This paper provides a thorough review of long-term tracking, summarizing long- term tracking algorithms from two perspectives: framework architectures and utilization of intermediate tracking results, and discusses the future prospects from multiple perspectives. As a fundamental task in computer vision, visual object tracking has received much attention in recent years. Most studies focus on short-term visual tracking which addresses shorter videos and always-visible targets. However, long-term visual tracking is much closer to practical applications with more complicated challenges. There exists a longer duration such as minute-level or even hour-level in the long-term tracking task, and the task also needs to handle more frequent target disappearance and reappearance. In this paper, we provide a thorough review of long-term tracking, summarizing long-term tracking algorithms from two perspectives: framework architectures and utilization of intermediate tracking results. Then we provide a detailed description of existing benchmarks and corresponding evaluation protocols. Furthermore, we conduct extensive experiments and analyse the performance of trackers on six benchmarks: VOTLT2018, VOTLT2019 (2020/2021), OxUvA, LaSOT, TLP and the long-term subset of VTUAV-V. Finally, we discuss the future prospects from multiple perspectives, including algorithm design and benchmark construction. To our knowledge, this is the first comprehensive survey for long-term visual object tracking. The relevant content is available at https://github.com/wangdong-dut/Long-term-Visual-Tracking.",
        "publication_year": "2022",
        "authors": [
            "Chang Liu",
            "Xiao-Fan Chen",
            "Chunjuan Bo",
            "Dong Wang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3",
        "reference_count": "100",
        "references": [
            "/paper/AR-Long-Term-Tracking-Combining-Multi-Attention-and-Guo-Chen/55d093ac4d40359e64b0208bc7a910858511e1b3",
            "/paper/Visual-object-tracking%3A-Progress%2C-challenge%2C-and-Zhang-Fan/d99e537321a88f1bc3800ff15b4438f52ef94f7b",
            "/paper/Hierarchical-memory-guided-long-term-tracking-with-Wang-Nie/8146887f58b77f1e2b319fd5a2e7a0b9442b3a1f",
            "/paper/Performance-Evaluation-Methodology-for-Long-Term-Luke%C5%BEi%C4%8D-Zajc/23f8927f996d56f3b5076d8993a70bcfc70182a1",
            "/paper/Now-you-see-me%3A-evaluating-performance-in-long-term-Luke%C5%BEi%C4%8D-Zajc/3275944117b43cc44beebe7c82bffc13ec8cb0fa",
            "/paper/Long-Term-Visual-Object-Tracking-Benchmark-Moudgil-Gandhi/19d6b9725a59f4b624205829d5f03ac893ca1367",
            "/paper/The-Sixth-Visual-Object-Tracking-VOT2018-Challenge-Kristan-Leonardis/219e9a4527110baf1feb3df20db12064eeafdfb7",
            "/paper/The-Eighth-Visual-Object-Tracking-VOT2020-Challenge-Kristan-Leonardis/12508951ba96b7d4c0906ed95542287d3ebdfd95",
            "/paper/Long-term-Tracking-in-the-Wild%3A-A-Benchmark-Valmadre-Bertinetto/ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "/paper/Real-time-long-term-tracker-with-Liao-Qi/1ae15ff20d54d9ffd2a45a9c124c77ad2b419ae3",
            "/paper/The-Seventh-Visual-Object-Tracking-VOT2019-Results-Kristan-Matas/786577081e00d69eeac8e9612eaf2dad59765e73",
            "/paper/Real-Time-Long-Term-Tracking-With-Liang-Wu/894e4376750b83b63649cc518b121f345ca0df83",
            "/paper/Exploring-3-R%E2%80%99s-of-Long-term-Tracking%3A-Recovery-and-Karthik-Moudgil/913cebc279c363fb9476496f096519e27212b3d5"
        ]
    },
    {
        "id": "ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
        "title": "Switch and Refine: A Long-Term Tracking and Segmentation Framework",
        "abstract": "A new long-term VOT framework is proposed that combines the benefits of two mainstream short-term tracking pipelines, i.e., the discriminative online tracker and the one-shot Siamese tracker, with a global re-detector awakened when the target is lost. In long-term video object tracking (VOT) tasks, most long-term trackers are modified from short-term trackers, which contain more and more machine learning modules to improve their performance. However, we empirically find that more modules do not necessarily lead to better results. In this paper, we make the long-term tracking framework simple by carefully selecting the cutting-edge trackers. Specifically, we propose a new long-term VOT framework that combines the benefits of two mainstream short-term tracking pipelines, i.e., the discriminative online tracker and the one-shot Siamese tracker, with a global re-detector awakened when the target is lost. Such a framework fully exploits existing advanced works from three complementary perspectives. Experimental results show that by exploiting the capabilities of existing methods instead of designing new neural networks, we can still achieve remarkable results on seven long-term VOT datasets. By introducing a continuous adjustable speed control parameter, our tracker reaches 20+FPS with only a small performance loss. The refine module not only improves the bounding box estimations but also outputs segmentation masks, so that our framework can handle the video object segmentation (VOS) tasks by using only VOT trackers. We obtain a trade-off between time and accuracy on two representative VOS datasets by only using bounding boxes as the initial input.",
        "publication_year": "2023",
        "authors": [
            "Xiang Xu",
            "Jian Zhao",
            "Jianmin Wu",
            "F. Shen"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "76",
        "references": [
            "/paper/Hierarchical-memory-guided-long-term-tracking-with-Wang-Nie/8146887f58b77f1e2b319fd5a2e7a0b9442b3a1f",
            "/paper/High-Performance-Long-Term-Tracking-With-Dai-Zhang/adacccd99a42c3145ec6392a1a6b08878376e38b",
            "/paper/Long-Term-Visual-Object-Tracking-Benchmark-Moudgil-Gandhi/19d6b9725a59f4b624205829d5f03ac893ca1367",
            "/paper/Performance-Evaluation-Methodology-for-Long-Term-Luke%C5%BEi%C4%8D-Zajc/c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Robust-Long-Term-Object-Tracking-via-Improved-Model-Choi-Lee/f1c9f81ce054619f30b5c27fd97579f7216d7048",
            "/paper/D3S-%E2%80%93-A-Discriminative-Single-Shot-Segmentation-Luke%C5%BEi%C4%8D-Matas/45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "/paper/%E2%80%98Skimming-Perusal%E2%80%99-Tracking%3A-A-Framework-for-and-Yan-Zhao/09b734072ad4f610478847c9cdc59a4a0c309b37",
            "/paper/GlobalTrack%3A-A-Simple-and-Strong-Baseline-for-Huang-Zhao/5664e24cacf3f6374c26b5597765099ee9537413",
            "/paper/Contour-Aware-Long-Term-Tracking-With-Reliable-Tang-Ling/f6186788541d332af19a96183787e01ef9080fb0",
            "/paper/Siamese-networks-with-distractor-reduction-method-Xuan-Li/3985382474245388bbc73e2c849e783010901775"
        ]
    },
    {
        "id": "d884af3933148cef3b50fd38c810f5a7763d0fc9",
        "title": "Multi-modal Visual Tracking: Review and Experimental Comparison",
        "abstract": "The multi-modal tracking algorithms, especially visible-depth ( RGB-D) tracking and visible-thermal (RGB-T) tracking in a unified taxonomy from different aspects are summarized. Visual object tracking, as a fundamental task in computer vision, has drawn much attention in recent years. To extend trackers to a wider range of applications, researchers have introduced information from multiple modalities to handle specific scenes, which is a promising research prospect with emerging methods and benchmarks. To provide a thorough review of multi-modal track-ing, we summarize the multi-modal tracking algorithms, especially visible-depth (RGB-D) tracking and visible-thermal (RGB-T) tracking in a unified taxonomy from different aspects. Second, we provide a detailed description of the related benchmarks and challenges. Furthermore, we conduct extensive experiments to analyze the effectiveness of trackers on five datasets: PTB, VOT19-RGBD, GTOT, RGBT234, and VOT19-RGBT. Finally, we discuss various future directions from different perspectives, including model design and dataset construction for further research.",
        "publication_year": "2020",
        "authors": [
            "Pengyu Zhang",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "5",
        "reference_count": "126",
        "references": [
            "/paper/RGBD-Object-Tracking%3A-An-In-depth-Review-Yang-Li/dc9a66e0f329de8054f4ab845331fb7183987418",
            "/paper/Visible-Thermal-UAV-Tracking%3A-A-Large-Scale-and-New-Zhang-Zhao/e80a02ee86f78ed5e0adfcb7f78a13c28cbedf31",
            "/paper/MFGNet%3A-Dynamic-Modality-Aware-Filter-Generation-Wang-Shu/35d238b9a170d7456422f32796ff41cc26f72a57",
            "/paper/Unsupervised-Cross-Modal-Distillation-for-Thermal-Sun-Zhang/841bd808a62a9d7da7b5075a8da0266297502785",
            "/paper/RGBT-Tracking-via-Progressive-Fusion-Transformer-Zhu-Li/cb0dc168bdb8381421ca7324a80781e817e0dd54",
            "/paper/RGB-T-Object-Tracking%3A-Benchmark-and-Baseline-Li-Liang/1975bee228ac228df235d20777e32331bb21566d",
            "/paper/CDTB%3A-A-Color-and-Depth-Visual-Object-Tracking-and-Luke%C5%BEi%C4%8D-Kart/f202feae9ca7b3766e072b6af657beed2236a93c",
            "/paper/Learning-Multi-Domain-Convolutional-Network-for-Zhang-Zhang/d8d847b085e9af12eeafc0af8df95ff2a1a98fb5",
            "/paper/Learning-Target-Oriented-Dual-Attention-for-Robust-Yang-Zhu/bc40c09be4d74a0665f507447982d226fcd8f18d",
            "/paper/Tracking-Revisited-Using-RGBD-Camera%3A-Unified-and-Song-Xiao/487eb86379e979a72ebfef67db6eb8f048d1d258",
            "/paper/Multi-cue-based-tracking-Wang-Fang/5f2ff21932fec3882c4c85c474a1be4645bfd92b",
            "/paper/Context-Aware-Three-Dimensional-Mean-Shift-With-for-Liu-Jing/d10861d377be150b1e03cb942deb8763095de88f",
            "/paper/Object-fusion-tracking-based-on-visible-and-images%3A-Zhang-Ye/761a9b5d8750eb63a9717650c4aaca53ce36a364",
            "/paper/Jointly-Modeling-Motion-and-Appearance-Cues-for-Zhang-Zhao/d49ba5146ab759be3b257228d7095649b3d48b57",
            "/paper/Multiple-human-tracking-in-RGB-depth-data%3A-a-survey-Camplani-Paiement/3fbf32a428db505e0bb45177016e8851d9b31e97"
        ]
    },
    {
        "id": "23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
        "title": "CoCoLoT: Combining Complementary Trackers in Long-Term Visual Tracking",
        "abstract": "This paper provides a framework, named CoCoLoT, that combines the characteristics of complementary visual trackers to achieve enhanced long-term tracking performance and competes favourably with the state-of-the-art on the most popular long- term visual tracking benchmarks. How to combine the complementary capabilities of an ensemble of different algorithms has been of central interest in visual object tracking. A significant progress on such a problem has been achieved, but considering short-term tracking scenarios. Instead, long-term tracking settings have been substantially ignored by the solutions. In this paper, we explicitly consider long-term tracking scenarios and provide a framework, named CoCoLoT, that combines the characteristics of complementary visual trackers to achieve enhanced long-term tracking performance. CoCoLoT perceives whether the trackers are following the target object through an online learned deep verification model, and accordingly activates a decision policy which selects the best performing tracker as well as it corrects the performance of the failing one. The proposed methodology is evaluated extensively and the comparison with several other solutions reveals that it competes favourably with the state-of-the-art on the most popular long-term visual tracking benchmarks.",
        "publication_year": "2022",
        "authors": [
            "Matteo Dunnhofer",
            "C. Micheloni"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "47",
        "references": [
            "/paper/Visualizing-Skiers'-Trajectories-in-Monocular-Dunnhofer-Sordi/45e2b30cc94d2f6c7642d9c183a6d8a827fc99f4",
            "/paper/Hierarchical-memory-guided-long-term-tracking-with-Wang-Nie/8146887f58b77f1e2b319fd5a2e7a0b9442b3a1f",
            "/paper/Performance-Evaluation-Methodology-for-Long-Term-Luke%C5%BEi%C4%8D-Zajc/c6dc55afe9fbe46f4f4dd48ae620ad455bfa5508",
            "/paper/High-Performance-Long-Term-Tracking-With-Dai-Zhang/adacccd99a42c3145ec6392a1a6b08878376e38b",
            "/paper/Tracking-by-Trackers-with-a-Distilled-and-Model-Dunnhofer-Martinel/bd4f219ce6bc5c22f9da71959d5192cf0b0141fe",
            "/paper/Is-First-Person-Vision-Challenging-for-Object-Dunnhofer-Furnari/ca97f741f331b5b43d0577a46c05984f0785a8fa",
            "/paper/A-Method-of-Stable-Long-Term-Single-Object-Tracking-Yi-Tong/c734274f43575bc5f4bcf8719f0be55a5e89be5e",
            "/paper/%E2%80%98Skimming-Perusal%E2%80%99-Tracking%3A-A-Framework-for-and-Yan-Zhao/09b734072ad4f610478847c9cdc59a4a0c309b37",
            "/paper/Online-Decision-Based-Visual-Tracking-via-Learning-Song-Zhang/5b73cd259a3fa72f95e8bac9e520250b950acf3a",
            "/paper/GlobalTrack%3A-A-Simple-and-Strong-Baseline-for-Huang-Zhao/5664e24cacf3f6374c26b5597765099ee9537413",
            "/paper/Visual-Tracking-by-Means-of-Deep-Reinforcement-and-Dunnhofer-Martinel/eb00b8453b23d4f6f142378e2fb0f0a9e6f9c5e2",
            "/paper/Long-Term-Visual-Object-Tracking-Benchmark-Moudgil-Gandhi/19d6b9725a59f4b624205829d5f03ac893ca1367"
        ]
    },
    {
        "id": "ca97f741f331b5b43d0577a46c05984f0785a8fa",
        "title": "Is First Person Vision Challenging for Object Tracking?",
        "abstract": "The study extensively analyses the performance of recent visual trackers and baseline FPV trackers with respect to different aspects and considering a new performance measure, and shows that object tracking in FPV is challenging. Understanding human-object interactions is fundamental in First Person Vision (FPV). Tracking algorithms which follow the objects manipulated by the camera wearer can provide useful cues to effectively model such interactions. Visual tracking solutions available in the computer vision literature have significantly improved their performance in the last years for a large variety of target objects and tracking scenarios. However, despite a few previous attempts to exploit trackers in FPV applications, a methodical analysis of the performance of state-of-the-art trackers in this domain is still missing. In this paper, we fill the gap by presenting the first systematic study of object tracking in FPV. Our study extensively analyses the performance of recent visual trackers and baseline FPV trackers with respect to different aspects and considering a new performance measure. This is achieved through TREK-150, a novel benchmark dataset composed of 150 densely annotated video sequences. Our results show that object tracking in FPV is challenging, which suggests that more research efforts should be devoted to this problem so that tracking could benefit FPV tasks.",
        "publication_year": "2020",
        "authors": [
            "Matteo Dunnhofer",
            "Antonino Furnari",
            "G. Farinella",
            "C. Micheloni"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "12",
        "reference_count": "106",
        "references": [
            "/paper/Predictive-Visual-Tracking%3A-A-New-Benchmark-and-Li-Li/19fe26d0cfe16471f4d2a05053c9f51cf14f0fa8",
            "/paper/CoCoLoT%3A-Combining-Complementary-Trackers-in-Visual-Dunnhofer-Micheloni/23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
            "/paper/EgoTracks%3A-A-Long-term-Egocentric-Visual-Object-Tang-Liang/5fea6a7854c6debb8010ae217f0c83370bbca784",
            "/paper/EXOT%3A-Exit-aware-Object-Tracker-for-Safe-Robotic-of-Kim-Yoon/a7e7659c84e257bafe69f6934ff37b728f0a0775",
            "/paper/PVT%2B%2B%3A-A-Simple-End-to-End-Latency-Aware-Visual-Li-Huang/8fa3221af7f6d3008e00cc0c459ecbde6f1014da",
            "/paper/Missing-Person-Detection-Using-AI-DhanushM-T./58bc1f07784089ed3c49c93bd835e8b73af5ff70",
            "/paper/MECCANO%3A-A-Multimodal-Egocentric-Dataset-for-Humans-Ragusa-Furnari/0fd84fa8f5a24fa55c1e1fe35bf81e2126b23225",
            "/paper/Deep-Convolutional-Correlation-Iterative-Particle-Mozhdehi-Medeiros/e85e8c5851ab1cc44ea6dd2d3d4b5af9789db9af",
            "/paper/Egocentric-Prediction-of-Action-Target-in-3D-Li-Cao/ae0d3f3f13f10d72ba151405b751b273ed3e82d5",
            "/paper/Ego4D%3A-Around-the-World-in-3%2C000-Hours-of-Video-Grauman-Westbury/847a153286d7f6f496f1ff61089831c267d68e30",
            "/paper/Object-Tracking-Benchmark-Wu-Lim/b7d540cd0de72e984cdec44afa4a4d039cfd5eea",
            "/paper/NUS-PRO%3A-A-New-Visual-Tracking-Challenge-Li-Lin/91f2b2aeb7e65d0b673ed7e782488b3365027979",
            "/paper/Meta-Tracker%3A-Fast-and-Robust-Online-Adaptation-for-Park-Berg/50c60583dc0ef09484358deab329f82ee22c2b66",
            "/paper/D3S-%E2%80%93-A-Discriminative-Single-Shot-Segmentation-Luke%C5%BEi%C4%8D-Matas/45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "/paper/MOTChallenge%3A-A-Benchmark-for-Single-Camera-Target-Dendorfer-Osep/44990f618f46f02da321b1043a64e72d5f7c0486",
            "/paper/Need-for-Speed%3A-A-Benchmark-for-Higher-Frame-Rate-Galoogahi-Fagg/703505a00579c0aa67712836acc41d94fa6d6edc",
            "/paper/Know-Your-Surroundings%3A-Exploiting-Scene-for-Object-Bhat-Danelljan/d1e61fa7824709cae37fb59483dd0772e3101c08",
            "/paper/Online-Object-Tracking%3A-A-Benchmark-Wu-Lim/bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "/paper/Struck%3A-Structured-Output-Tracking-with-Kernels-Hare-Golodetz/61394599ed0aabe04b724c7ca3a778825c7e776f",
            "/paper/Egocentric-Object-Tracking%3A-An-Odometry-Based-Alletto-Serra/9559f0b77932a3c5f17aeb8564b400430d173ec7"
        ]
    },
    {
        "id": "0530cbeb847f5e5002d1183c482759dff5f8c439",
        "title": "Visual Object Tracking With Discriminative Filters and Siamese Networks: A Survey and Outlook",
        "abstract": "This survey presents a systematic and thorough review of more than 90 DCFs and Siamese trackers, based on results in nine tracking benchmarks, and distinguishes and comprehensively review the shared as well as specific open research challenges in both these tracking paradigms. Accurate and robust visual object tracking is one of the most challenging and fundamental computer vision problems. It entails estimating the trajectory of the target in an image sequence, given only its initial location, and segmentation, or its rough approximation in the form of a bounding box. Discriminative Correlation Filters (DCFs) and deep Siamese Networks (SNs) have emerged as dominating tracking paradigms, which have led to significant progress. Following the rapid evolution of visual object tracking in the last decade, this survey presents a systematic and thorough review of more than 90 DCFs and Siamese trackers, based on results in nine tracking benchmarks. First, we present the background theory of both the DCF and Siamese tracking core formulations. Then, we distinguish and comprehensively review the shared as well as specific open research challenges in both these tracking paradigms. Furthermore, we thoroughly analyze the performance of DCF and Siamese trackers on nine benchmarks, covering different experimental aspects of visual tracking: datasets, evaluation metrics, performance, and speed comparisons. We finish the survey by presenting recommendations and suggestions for distinguished open challenges based on our analysis.",
        "publication_year": "2021",
        "authors": [
            "S. Javed",
            "Martin Danelljan",
            "F. Khan",
            "Muhammad Haris Khan",
            "M. Felsberg",
            "Jiri Matas"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "27",
        "reference_count": "181",
        "references": [
            "/paper/SiamDA%3A-distribution-aware-Siamese-network-for-Ji-Shi/0bc6b7c8e3d2efdb6cf35a15192029c078bebbc6",
            "/paper/Siamese-Attention-Networks-with-Adaptive-Templates-Zhang-Liang/0a17d476e9543756e09b9c3240398af2f7f5af9f",
            "/paper/Robust-Tracking-for-Visual-Complex-Environments/c7d39e7be53dd2538d60db48cc47cae177a554a1",
            "/paper/Siamese-hierarchical-feature-fusion-transformer-for-Dai-Fu/bbd2e7552f586bce5adf015213b62d30792e55fd",
            "/paper/Towards-Sequence-Level-Training-for-Visual-Tracking-Kim-Lee/a7acd088493d06c117b839e7a62eec2de3fd8f9b",
            "/paper/Accelerated-Video-Annotation-driven-by-Deep-and-Price-Ahmad/992c156a95d301634df8b1f4de9c3b6194fcf4d6",
            "/paper/An-adaptive-spatiotemporal-correlation-filtering-Liu-Yan/f3b92cacb8fe1ca675b7b441071447e2287e7f56",
            "/paper/SiamOA%3A-siamese-offset-aware-object-tracking-Zhang-Xie/53201f6a34e5bedd793f0132e3b4c59a098c2274",
            "/paper/Transformers-in-Single-Object-Tracking%3A-An-Survey-Thangavel-Kokul/9fc56eca82c3a7b3f38d993ab7f47cf345f26946",
            "/paper/A-Novel-Algorithm-Based-on-a-Common-Subspace-Fusion-Javed-Mahmood/6ad7652a8891b7b54b3ef1edeeee0e114f7da85c",
            "/paper/Siamese-Box-Adaptive-Network-for-Visual-Tracking-Chen-Zhong/cce1fecc800d2782da638f3060d5b2e887739f74",
            "/paper/Handcrafted-and-Deep-Trackers%3A-Recent-Visual-Object-Fiaz-Mahmood/19476caeb9305540faf84adba9a0bb12bd2c29a8",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/SiamCAR%3A-Siamese-Fully-Convolutional-Classification-Guo-Wang/738165f33c50b059e87b14d8b4a129230e14eacd",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/SiamFC%2B%2B%3A-Towards-Robust-and-Accurate-Visual-with-Xu-Wang/be412c7c7128cf91455233b652d6c94a6001a7c8",
            "/paper/Recent-advances-and-trends-in-visual-tracking%3A-A-Yang-Shao/2bcf2bd59219d89f335cbc8d1dd4f431076b4c4c",
            "/paper/D3S-%E2%80%93-A-Discriminative-Single-Shot-Segmentation-Luke%C5%BEi%C4%8D-Matas/45512d44f1205bc92775f2e880858b3f23c9f5fd"
        ]
    },
    {
        "id": "dd950231a9c5fb671a23c97a5038ba061937f6f4",
        "title": "A Large-scale Sports Tracking Dataset and Progressive Re-detection Based Sports Tracking",
        "abstract": "The proposed framework method is robust to motion blur and object occlusion issues and achieves state-of-the-art tracking results on the authors' challenging dataset. Recent years have witnessed the great progress of Visual Object Tracking (VOT) which aims to predict the position of an object in each video frame given only its initial appearance. However, even the state-of-the-art methods are confronted with performance degradation, i.e., the tracker drift problem, in sports video scenes (e.g., soccer, basketball). There are two main causes that should be responsible for the tracker drift problem. First, the object of interest is often occluded by other objects that share a similar appearance. Such severe occlusion prevents the model from distinguishing the correct tracking object from other distractors in the future frames. Second, in sports videos, the objects often move fast from one place to another, which incurs severe blurry visual effects among consecutive frames. To address the issues of the tracker drift problem, we treat VOT as a tracking-by-re-detection task. Specifically, we detect candidate objects within a searching area (determined by object location in the previous frame) in the current frame and develop a progressive algorithm to filter out distractors in the area, which proves robust towards occlusion scenarios and tracker drift problems. Combining the advantages of our settings, the proposed framework method is robust to motion blur and object occlusion issues and achieves state-of-the-art tracking results on our challenging dataset.",
        "publication_year": "2022",
        "authors": [
            "Han Wang",
            "Xiaojun Zhou",
            "Qinyu Xu",
            "Huaqiang Ren",
            "Rong Xie",
            "Li Song"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "24",
        "references": [
            "/paper/A-Deep-Tracking-and-Segmentation-Approach-for-Peng-Song/b33bcf197d1ec7c432a5eda4ed07f17898fb5f4b",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/The-Visual-Object-Tracking-VOT2017-Challenge-Kristan-Leonardis/350d507f5d899e4d7293b1aa951aa0f81b9fd30a",
            "/paper/Object-Tracking-Benchmark-Wu-Lim/b7d540cd0de72e984cdec44afa4a4d039cfd5eea",
            "/paper/MOT20%3A-A-benchmark-for-multi-object-tracking-in-Dendorfer-Rezatofighi/d11c50e49998e2156da7c179a3caea86e9601abd",
            "/paper/MOTChallenge-2015%3A-Towards-a-Benchmark-for-Tracking-Leal-Taix%C3%A9-Milan/5bae9822d703c585a61575dced83fa2f4dea1c6d",
            "/paper/Know-Your-Surroundings%3A-Exploiting-Scene-for-Object-Bhat-Danelljan/d1e61fa7824709cae37fb59483dd0772e3101c08",
            "/paper/The-Eighth-Visual-Object-Tracking-VOT2020-Challenge-Kristan-Leonardis/12508951ba96b7d4c0906ed95542287d3ebdfd95",
            "/paper/Detecting-Events-and-Key-Actors-in-Multi-person-Ramanathan-Huang/195df1106f4d7aff0e9cb609358abbf80f54a716",
            "/paper/Graph-Attention-Tracking-Guo-Shao/37929e9283d214a1688bd6fa9759cd6e89b2312d"
        ]
    },
    {
        "id": "c89da5aa9697ab9d5366353ec29b3e9c1b610469",
        "title": "Visual Object Tracking in First Person Vision",
        "abstract": "It is proved that trackers bring benefits to FPV downstream tasks requiring short-term object tracking and expected that generic object tracking will gain popularity in FPV as new and FPV-specific methodologies are investigated. The understanding of human-object interactions is fundamental in First Person Vision (FPV). Visual tracking algorithms which follow the objects manipulated by the camera wearer can provide useful information to effectively model such interactions. In the last years, the computer vision community has significantly improved the performance of tracking algorithms for a large variety of target objects and scenarios. Despite a few previous attempts to exploit trackers in the FPV domain, a methodical analysis of the performance of state-of-the-art trackers is still missing. This research gap raises the question of whether current solutions can be used \u201coff-the-shelf\u201d or more domain-specific investigations should be carried out. This paper aims to provide answers to such questions. We present the first systematic investigation of single object tracking in FPV. Our study extensively analyses the performance of 42 algorithms including generic object trackers and baseline FPV-specific trackers. The analysis is carried out by focusing on different aspects of the FPV setting, introducing new performance measures, and in relation to FPV-specific tasks. The study is made possible through the introduction of TREK-150, a novel benchmark dataset composed of 150 densely annotated video sequences. Our results show that object tracking in FPV poses new challenges to current visual trackers. We highlight the factors causing such behavior and point out possible research directions. Despite their difficulties, we prove that trackers bring benefits to FPV downstream tasks requiring short-term object tracking. We expect that generic object tracking will gain popularity in FPV as new and FPV-specific methodologies are investigated.",
        "publication_year": "2022",
        "authors": [
            "Matteo Dunnhofer",
            "Antonino Furnari",
            "G. Farinella",
            "C. Micheloni"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "7",
        "reference_count": "122",
        "references": [
            "/paper/EgoTracks%3A-A-Long-term-Egocentric-Visual-Object-Tang-Liang/5fea6a7854c6debb8010ae217f0c83370bbca784",
            "/paper/Aria-Digital-Twin%3A-A-New-Benchmark-Dataset-for-3D-Pan-Charron/27942ff872056d886977e465df90d2bf93675cf4",
            "/paper/On-Deep-Recurrent-Reinforcement-Learning-for-Active-Zhou-Sun/bd6be9e58ed58aad21b2f334c3a8c18fd3072786",
            "/paper/Bayesian-Decision-Making-to-Localize-Visual-Queries-Asjad-Gupta/ea7abfe481f3bf3273ffcbe2aa7147aee1cd0f74",
            "/paper/Visualizing-Skiers'-Trajectories-in-Monocular-Dunnhofer-Sordi/45e2b30cc94d2f6c7642d9c183a6d8a827fc99f4",
            "/paper/Efficient-thermal-infrared-tracking-with-compress-Li-Zha/247396bb7639e9e4217801025c537df5245f764e",
            "/paper/Trackez%3A-An-IoT-Based-3D-Object-Tracking-From-2D-Faruqui-Kabir/3ff456c3804177b2fe0dc72b3642a6aacb87e277",
            "/paper/Transparent-Object-Tracking-Benchmark-Fan-Miththanthaya/ce1da08be62183845cac70b7236ee9de5f2dde43",
            "/paper/Object-Tracking-Benchmark-Wu-Lim/b7d540cd0de72e984cdec44afa4a4d039cfd5eea",
            "/paper/Visual-Tracking%3A-An-Experimental-Survey-Smeulders-Chu/eda3368a5198ca55768b07b6f5667aea28baf2cd",
            "/paper/Egocentric-Object-Tracking%3A-An-Odometry-Based-Alletto-Serra/9559f0b77932a3c5f17aeb8564b400430d173ec7",
            "/paper/NUS-PRO%3A-A-New-Visual-Tracking-Challenge-Li-Lin/91f2b2aeb7e65d0b673ed7e782488b3365027979",
            "/paper/Online-Object-Tracking%3A-A-Benchmark-Wu-Lim/bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "/paper/EYEWATCHME%E2%80%943D-Hand-and-object-tracking-for-inside-Sun-Klank/b90010d61509bcacff64003b7e31e817487ea018",
            "/paper/Meta-Tracker%3A-Fast-and-Robust-Online-Adaptation-for-Park-Berg/50c60583dc0ef09484358deab329f82ee22c2b66",
            "/paper/Tracking-Learning-Detection-Kalal/c63a34ac6a4e049118070e707ca7679fbb132d33",
            "/paper/Visual-object-tracking-using-adaptive-correlation-Bolme-Beveridge/70c3c9b9a40ca55264e454586dca2a6cf416f6e0"
        ]
    },
    {
        "id": "ef19859f204048cc83bed9d3eeaa74f75e2fbabc",
        "title": "Global Tracking via Ensemble of Local Trackers",
        "abstract": "This work combines the advantages of both strategies: tracking the target in a global view while exploiting the temporal context; and performs global tracking via ensemble of local trackers spreading the full image. The crux of long-term tracking lies in the difficulty of tracking the target with discontinuous moving caused by out-of-view or occlusion. Existing long-term tracking methods follow two typical strategies. The first strategy employs a local tracker to perform smooth tracking and uses another re-detector to detect the target when the target is lost. While it can exploit the temporal context like historical appearances and locations of the target, a potential limitation of such strategy is that the local tracker tends to misidentify a nearby distractor as the target instead of activating the re-detector when the real target is out of view. The other long-term tracking strategy tracks the target in the entire image globally instead of local tracking based on the previous tracking results. Unfortunately, such global tracking strategy cannot leverage the temporal context effectively. In this work, we combine the advantages of both strategies: tracking the target in a global view while exploiting the temporal context. Specifically, we perform global tracking via ensemble of local trackers spreading the full image. The smooth moving of the target can be handled steadily by one local tracker. When the local tracker accidentally loses the target due to suddenly discontinuous moving, another local tracker close to the target is then activated and can readily take over the tracking to locate the target. While the activated local tracker performs tracking locally by leveraging the temporal context, the ensemble of local trackers renders our model the global view for tracking. Extensive experiments on six datasets demonstrate that our method performs favorably against state-of-the-art algorithms.",
        "publication_year": "2022",
        "authors": [
            "Zikun Zhou",
            "Jianqiu Chen",
            "Wenjie Pei",
            "Kaige Mao",
            "Hongpeng Wang",
            "Zhenyu He"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "7",
        "reference_count": "48",
        "references": [
            "/paper/SRRT%3A-Search-Region-Regulation-Tracking-Zhu-Chen/06d334fdddec3b0003924e3acccf05c295094605",
            "/paper/A-robust-spatial-temporal-correlation-filter-for-Chen-Liu/1e318864509718478dbf363fee229594a7fda5d3",
            "/paper/Beyond-SOT%3A-It's-Time-to-Track-Multiple-Generic-at-Mayer-Danelljan/da40d11c6ac32469ade0878e89e5586602411705",
            "/paper/Frame-Event-Alignment-and-Fusion-Network-for-High-Zhang-Wang/6317b905d6ce1d5a64c922d742cff9e06a12a1ad",
            "/paper/Joint-Visual-Grounding-and-Tracking-with-Natural-Zhou-Zhou/b6b01ece4d628f2654657205fc5e9a0fcff217e2",
            "/paper/Adversarial-Blur-Deblur-Network-for-Robust-UAV-Zuo-Fu/f06a22165f0bcc92eac865e3eeddec3c7e2cf44c",
            "/paper/Mutual-Information-Learned-Classifiers%3A-an-of-Deep-Yi-Zhang/1b412b76087530a1969517da271393519f30fad1",
            "/paper/GlobalTrack%3A-A-Simple-and-Strong-Baseline-for-Huang-Zhao/5664e24cacf3f6374c26b5597765099ee9537413",
            "/paper/Tracking-Learning-Detection-Kalal/c63a34ac6a4e049118070e707ca7679fbb132d33",
            "/paper/%E2%80%98Skimming-Perusal%E2%80%99-Tracking%3A-A-Framework-for-and-Yan-Zhao/09b734072ad4f610478847c9cdc59a4a0c309b37",
            "/paper/Saliency-Associated-Object-Tracking-Zhou-Pei/48c6ca17f17038ff9933dca86a23f8516168a3ea",
            "/paper/Learning-regression-and-verification-networks-for-Zhang-Wang/3d372b63020c4d2c9510624f370b50d9f292bcde",
            "/paper/Bridging-the-Gap-Between-Detection-and-Tracking%3A-A-Huang-Zhao/ae066f27f2edc1c51847ce4cb21b6e1a3db44fa2",
            "/paper/Long-term-Tracking-in-the-Wild%3A-A-Benchmark-Valmadre-Bertinetto/ed0bab800e5e8fcf1b4e05024b2bcd1c2b1632f7",
            "/paper/High-Performance-Long-Term-Tracking-With-Dai-Zhang/adacccd99a42c3145ec6392a1a6b08878376e38b",
            "/paper/STMTrack%3A-Template-free-Visual-Tracking-with-Memory-Fu-Liu/811ffb185bc90ac5d02d6dbfbcdb6173756b52ef",
            "/paper/Distractor-Aware-Fast-Tracking-via-Dynamic-and-MOT-Zhang-Zhong/16cf8225d1b86c54a18b917a9475bfbd68b46306"
        ]
    },
    {
        "id": "2415fc06de82ab41ad8b9615162247afb02974af",
        "title": "Classification of benign and malignant breast tumors based on hybrid level set segmentation",
        "abstract": "Semantic Scholar extracted view of \"Classification of benign and malignant breast tumors based on hybrid level set segmentation\" by R. Rouhi et al.",
        "publication_year": "2016",
        "authors": [
            "R. Rouhi",
            "M. Jafari"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "56",
        "reference_count": "58",
        "references": [
            "/paper/STUDY-ON-LEVEL-SET-SEGMENTATION-BASED-USING-Nemade-Sharma/06b05fa46ea957aa1e16aa809a23828e601d4f49",
            "/paper/Fuzzy-C-means-and-region-growing-based-of-tumor-Sadad-Munir/301fee6989cc75f84343836cab5a25691d58dc5c",
            "/paper/Automatic-Breast-Tumor-Classification-Using-a-Level-Pashoutan-Shokouhi/64aa7896b11ab90ccb4828c2faa6f86cc5f647cd",
            "/paper/A-hybrid-computer-aided-diagnosis-system-for-in-Kumar-Mohanty/6d5cee9e834907d557ad6eaf1a3edfd610fa77c8",
            "/paper/Automatic-computer-aided-diagnosis-system-for-mass-Lbachir-Daoudi/f647a77af6ee361456d7c9c876d78780413b2695",
            "/paper/Segmentation-of-mammogram-abnormalities-using-ant-Subramanian-Thevar/add610bbec6306dbf911cd38ecb5b5a1e36ae978",
            "/paper/An-Efficient-Method-for-Breast-Mass-Segmentation-in-Hmida-Hamrouni/14f52a48954bdc3fd7ab872a05c49d60da1fdb8d",
            "/paper/Mammographic-mass-segmentation-using-fuzzy-contours-Hmida-Hamrouni/5f0f6207844c6ef3df53dcfa21591d6359fbffe0",
            "/paper/DELINEATION-AND-CLASSIFICATION-OF-LIVER-CANCER-SET-Das-Panda/20e8be521ad4aa3dac54b5cc6c3acceba4f298f2",
            "/paper/Computer-aided-detection-and-diagnosis-of-masses-of-Chakraborty-Midya/3692b880928fe13be40b281e7286b6917fdd90a3",
            "/paper/Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52",
            "/paper/An-adaptive-region-growing-algorithm-for-breast-in-Cao-Hao/3c948ca247c6f55ef994400e713412b5f845dd40",
            "/paper/Classification-of-benign-and-malignant-masses-based-Tahmasbi-Saki/46c409dd878e643271ef63f1817ded8b57abc01e",
            "/paper/A-fully-automated-scheme-for-mammographic-and-based-Tzikopoulos-Mavroforakis/5992c015bf844e958bc78f09b46b39e80c903ac0",
            "/paper/Performance-evaluation-of-a-region-growing-for-Rabottino-Mencattini/76389ebb7c1496239e66fd663b0e7e43d391bca9",
            "/paper/Automatic-detection-of-abnormal-mammograms-in-Jen-Yu/6d8c60d47e2be4de763bb2f8044e09981016396e",
            "/paper/Directional-features-for-automatic-tumor-of-images-Buciu-Gacs%C3%A1di/9b5d0a48b0feb156a1270da54d90d0963a3f0404",
            "/paper/An-effective-breast-mass-diagnosis-system-using-Tahmasbi-Saki/63a83600a73ffbfd6a58e315a247aa4f3da90a9b",
            "/paper/Mass-diagnosis-in-mammography-images-using-novel-Tahmasbi-Saki/474ae46626676d01c7b38328c107b1531b181b46",
            "/paper/Fast-opposite-weight-learning-rules-with-in-breast-Saki-Tahmasbi/22d4b480e03e7d068356860d8c4485b41a0acfeb"
        ]
    },
    {
        "id": "8133e66c9c03095ef605090e6a72b752dc774d92",
        "title": "Benign and malignant breast cancer segmentation using optimized region growing technique",
        "abstract": "Semantic Scholar extracted view of \"Benign and malignant breast cancer segmentation using optimized region growing technique\" by S. Punitha et al.",
        "publication_year": "2018",
        "authors": [
            "S. Punitha",
            "A. Amuthan",
            "K. S. Joseph"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "68",
        "reference_count": "30",
        "references": [
            "/paper/Breast-Cancer-Detection-and-Classification-from-Gurudas-Shaila/a7f3cd1885a0f1548596b288eb6768d818b8463d",
            "/paper/Automated-Deep-Learning-Empowered-Breast-Cancer-Escorcia-Gutierrez-Mansour/ddf4b612c1c944dbf47999b9c319c8186df709e0",
            "/paper/Computer-aided-mass-segmentation-in-mammogram-using-Ashok-Vijayan/ea703124880e7bc03792602b500a8ed37b56843a",
            "/paper/Breast-tumor-localization-and-segmentation-using-of-Ranjbarzadeh-Dorosti/f038517036bf5974ed662cdab74089d2db5e02b4",
            "/paper/A-Robust-Feature-Extraction-Technique-for-Breast-on-Kumari-Jagadesh/19e8f7afcba501eab8bff7c216ee532f0c73ed4f",
            "/paper/Mammogram-Classification-using-Supervising-Vector-Khanbari-Haider/75212204e96763e1e8d505740c0097afa45ce77a",
            "/paper/Mammogram-Classification-with-Forest-Optimization-KanyaKumari-J./840e3ac4f017b602068b1c04a43d88761e53fb94",
            "/paper/An-enhancement-of-mammogram-images-for-breast-using-Patel-Hadia/f3eb7b753cf2a9569a1d9fdd11618bce28b1ef46",
            "/paper/Early-detection-of-breast-cancer-using-hybrid-of-Jahangeer-Rajkumar/169f59ac3987301bfdecd77301eeb16ecc0e9358",
            "/paper/Early-detection-of-breast-cancer-using-hybrid-of-Jahangeer-Rajkumar/6c9bd710e71b5da6ff471a84354695d4f79cbfc5",
            "/paper/Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52",
            "/paper/A-new-automatic-mass-detection-method-for-breast-Liu-Zeng/19a85922bcc7c932934a79a85c59e15655d88b2e",
            "/paper/Breast-tumor-detection-in-digital-mammography-based-Wang-Yu/d83750e6f32d0172cbb6e10d640aa6cdaf03e2b0",
            "/paper/A-Swarm-Optimized-Neural-Network-System-for-of-in-Dheeba-Selvi/4f325d8be07594c94f0d7568c80c91c55bdb3774",
            "/paper/Mammography-Feature-Analysis-and-Mass-Detection-in-Patel-Sinha/7ff70f493df1a7d5dfc0c437e7fce6a5172be933",
            "/paper/Approaches-for-automated-detection-and-of-masses-in-Cheng-Shi/34c44883a6152c5298f2c452670c1127072400e6",
            "/paper/Breast-mass-classification-in-digital-mammography-Xie-Li/2c5db1c44dddf0d8e6ea667f3f7afb13032ea386",
            "/paper/Segmentation-and-detection-of-breast-cancer-in-and-Pereira-Ramos/8fd20170a036d6f139a3b031058865b6c02ad65a",
            "/paper/Computerized-detection-of-breast-masses-in-Varela-Tahoces/54ceeab87535d4048eb262dcc8cdbc46ae735ea4",
            "/paper/Breast-cancer-detection-in-digital-mammograms-Kashyap-Bajpai/37257b305a7a5962306505b60f3384fc0d03f184"
        ]
    },
    {
        "id": "f3eb7b753cf2a9569a1d9fdd11618bce28b1ef46",
        "title": "An enhancement of mammogram images for breast cancer classification using artificial neural networks",
        "abstract": "In this proposed method a novel hybrid optimum feature selection (HOFS) method is used to find out the significant features to reach maximum accuracy for this classification of mammogram images. Breast cancer is the most driving reason for death in women in both developed and developing nations. For the plan of effective classification of a system, the selection of features method must be used to decrease irregularity part in mammogram images. The proposed approach is used to crop the region of interests (ROIs) manually. Based on that number of features are extracted. In this proposed method a novel hybrid optimum feature selection (HOFS) method is used to find out the significant features to reach maximum accuracy for this classification. A number of selected features is applied to train the neural network. In this proposed method accessible informational index from the mini\u2013mammographic image analysis society (MIAS) database was used. The classification of this mammogram database involved a neural networks classifier which attained an accuracy of 99.7% with a sensitivity of 99.5%, and specificity of 100% as the area under the curve (AUC) is 0.9975 and matthew\u2019s correlation coefficient (MCC) represents a binary class value which reached the value of 0.9931. It can be useful in a computer-aided diagnosis system (CAD) framework to help the radiologist in analyzing breast cancer. Results achieved with the proposed method are better compared to recent work.",
        "publication_year": "2021",
        "authors": [
            "J. J. Patel",
            "S. Hadia"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "7",
        "reference_count": "43",
        "references": [
            "/paper/A-Novel-CNN-Inception-V4-Based-Hybrid-Approach-for-Nazir-Khan/bb1beb57009f356e300ed2c83bb2fc4f8198daf3",
            "/paper/International-Journal-of-Electrical-and-Computer-Pour-Esmaeili/c3e59d2027d1f190eb56678655ef901287108566",
            "/paper/Brain-stroke-computed-tomography-images-analysis-A-Ali-Abdullah/95b1fce2fbe3b8a6fca7ea83825a0bc7945773e7",
            "/paper/Expert-System-of-Dengue-Disease-Using-Artificial-Hamdani-Arifin/732ab2c6404f8322d0067f39503bdfd7f3577825",
            "/paper/Indonesian-Journal-of-Electrical-Engineering-and-Bhuiya-Hamdan/d225a689a37a20ef5e9cc6cfa405886ca3b5c622",
            "/paper/IAES-International-Journal-of-Artificial-(IJ-AI)-Tan-Hijazi/18d95baa8b21d0449b6f26ef82e577c3782db397",
            "/paper/Breast-cancer-diagnosis-from-mammographic-images-Shivhare-Saxena/b7cd3daaf49d0d68579015680d123044683d70ee",
            "/paper/Breast-Cancer-Detection-using-Artificial-Neural-Tariq/b06a69cf5663829d4f4f8168d820b2f7baf88918",
            "/paper/Feature-Selection-Mammogram-based-on-Breast-Cancer-Uyun-Choridah/dd7282d139c163df0243f7cb508c7be979aa9fda",
            "/paper/Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52",
            "/paper/Investigation-on-ROI-size-and-location-to-classify-Kamra-Sood/4a0c851fbffcfdd4d191cff4b38ac42e1d4d6fc9",
            "/paper/Computer-aided-detection-of-breast-cancer-on-A-Dheeba-Singh/95bb0ee471480da79e41ae196bb4da02abe52a27",
            "/paper/Classification-of-Breast-Tissues-in-Mammogram-Using-Martins-Junior/d592e7ed372adffd2d2d8f5c72565d71e911237e",
            "/paper/Identification-of-masses-in-digital-mammogram-using-Khuzi-Besar/1ca488cbbc84138e12e405661a2db7fcc9f2e4f1",
            "/paper/An-Enhancement-in-Cancer-Classification-Accuracy-a-Rahman-Muniyandi/ca30be76395ba443b24d20d7df5d3b5372df55ff",
            "/paper/Shape-analysis-for-classification-of-breast-nodules-Nugroho-Yusufiyah/0fc4a247cce443bc2a3d64818d8981ab8586bb32"
        ]
    },
    {
        "id": "301fee6989cc75f84343836cab5a25691d58dc5c",
        "title": "Fuzzy C-means and region growing based classification of tumor from mammograms using hybrid texture feature",
        "abstract": "Semantic Scholar extracted view of \"Fuzzy C-means and region growing based classification of tumor from mammograms using hybrid texture feature\" by Tariq Sadad et al.",
        "publication_year": "2018",
        "authors": [
            "Tariq Sadad",
            "A. Munir",
            "T. Saba",
            "Ayyaz Hussain"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "71",
        "reference_count": "53",
        "references": [
            "/paper/Segmentation-of-malignant-tumours-in-mammogram-A-Roy-Singh/32320a5c9a49868b851ef6727cc3d0bd80f9f011",
            "/paper/Automated-diagnosis-of-breast-cancer-using-kernel-Mohanty-Rup/5ad977a94976b8b5d5572639e17b5d0fb73cb589",
            "/paper/Breast-cancer%3A-Three%E2%80%90class-masses-classification-in-Soulami-Kaabouch/f24b31da5bb6d71869dff1d210eaf761265ba9dd",
            "/paper/Identification-of-Breast-Malignancy-by-Watershed-Sadad-Hussain/a0bb41d54673d2e9c514620bbc8967f8d0d867be",
            "/paper/Detection-of-breast-cancer-using-the-infinite-with-Ittannavar-Havaldar/6931ca52d7504ae72ea3ddc0707e8231529665e9",
            "/paper/TTCNN%3A-A-Breast-Cancer-Detection-and-Classification-Maqsood-Dama%C5%A1evi%C4%8Dius/4ed705bc9b8fdaf6a2cd4e0d6e35e9ef15569de5",
            "/paper/Automatic-Detection-of-Malignant-Masses-in-Digital-Rodr%C3%AD%C2%ADguez-Esparza-Zanella-Calzada/1c91ca81e779df5a5180e722f0fc0acd184a1ffc",
            "/paper/Systematic-Review-of-Computing-Approaches-for-Based-Zebari-Ibrahim/9ee9eeed8fc56f600eb3ff07f6e38dff81957ff8",
            "/paper/Efficient-Breast-Cancer-Diagnosis-from-Complex-Deep-Rahman-Bukht/06e3c3353bf177212f3d968b36ca3ac2e13b2cd9",
            "/paper/Mammograms-Classification%3A-A-Review-Elbatel/f198748f80ce1fd6fca6d2a7f2f0cdfcf118f5fa",
            "/paper/Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52",
            "/paper/Effective-mammogram-classification-based-on-center-Singh-Srivastava/d5abfbdef48c20ed124833024e61e4773e944966",
            "/paper/Breast-cancer-detection-and-classification-in-based-Pak-Kanan/82917b65ec0e7fa1bbdc0ba08642fd8c336196a1",
            "/paper/Usefulness-of-Texture-Analysis-for-Computerized-of-Pereira-Marques/d10fa706f2383510f8ecbf69a6c404fc4a5837f8",
            "/paper/Classification-of-benign-and-malignant-breast-based-Rouhi-Jafari/2415fc06de82ab41ad8b9615162247afb02974af",
            "/paper/Application-of-texture-analysis-method-for-density-Nithya-Santhi/2537fbb22837de4c9f2b167d7905b2810d84e28f",
            "/paper/MRT-letter%3A-Segmentation-and-texture%E2%80%90based-of-Naveed-Jaffar/00aefc484dd5684ea999ba310dc152ecbfc7963e",
            "/paper/Effect-of-Pixel-Resolution-on-Texture-Features-of-Rangayyan-Nguyen/9c204bc24341157f1dbd72fde781a4906d8f17cd",
            "/paper/Texture-analysis-of-masses-malignant-in-mammograms-Rocha-Junior/c82fff512c57f12059632bfc02912b494144d984",
            "/paper/Medical-Image-Feature-%2C-Extraction-%2C-Selection-And-Vasantha-Bharathi/4fc76c0c9c547cae53da984918cf4893dbca38a4"
        ]
    },
    {
        "id": "8e75f635dd7578926aa7ae19ff29a8fc5c3911ae",
        "title": "Segmentation and classification of breast cancer using novel deep learning architecture",
        "abstract": "A novel deep-learning architecture for tumor segmentation is proposed in this study, and machine learning algorithms are used to categorize benign or malignant tumors. Breast cancer is one of the most frequent cancers in women, and it has a higher mortality rate than other cancers. As a result, early detection is critical. In computer-assisted disease diagnosis, accurate segmentation of the region of interest is a vital concept. The segmentation techniques have been widely used by doctors and physicians to locate the pathology, identify the abnormality, compute the tissue volume, analyze the anatomical structures, and provide treatment. Cancer diagnostic efficiency is based on two aspects: The precision value associated with the segmentation and calculation of the tumor area and the accuracy of the features extracted from the images to categorize the benign or malignant tumors. A novel deep-learning architecture for tumor segmentation is therefore proposed in this study, and machine learning algorithms are used to categorize benign or malignant tumors. The segmentation results improve the decision-making capability of the physicians to identify whether a tumor is malignant or not and normally, the machine learning techniques need expert annotation and pathology reports to identify this. This challenge is overcome in this work with the help of the GoogLeNet architecture used for segmentation. The segmentation results are then offered to the Support Vector Mchine, Decision Tree, Random Forest, and Na\u00efve Bayes classifier to improve their efficiency. Our work has provided better results in terms of accuracy, Jaccard and dice coefficient, sensitivity, and specificity compared to conventional architectures. The proposed model offers an accuracy score of 99.12% which is relatively higher than the other techniques. A 3.78% accuracy improvement is noticed by the proposed model against the AlexNet classifier and the actual increase is 4.61% on average when compared to the existing techniques.",
        "publication_year": "2022",
        "authors": [
            "S. Ramesh",
            "S. Sasikala",
            "S. Gomathi",
            "V. Geetha",
            "V. Anbumani"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "10",
        "reference_count": "68",
        "references": [
            "/paper/Improving-Breast-Cancer-Detection-and-Diagnosis-the-Alam-Shia/d29cfbc42f0e77f546ae3a437432c9234aa48aab",
            "/paper/Recent-Advances-in-Selection-Techniques-for-Image-Chinnasamy-Ramachandran/7d04abd3fcfd4adbaf2738e996d4352826145575",
            "/paper/Exploring-the-recent-trends-in-Big-Data-Analysis-Suresh-Ramachandran/42097772228e070b44f0af7c520a88e8656b9c53",
            "/paper/Artificial-Intelligence-of-Things-for-Smarter-A-of-Baker-Xiang/8fcc6d8576911d34d4b2454a41685eea5408a93d",
            "/paper/Sustainable-Transportation-Systems-Analysis-using-Venkateswaran-Ramachandran/0ec81d2dd816c3ed4fa4b001de4a0bbbad380b03",
            "/paper/Microcontroller-Based-Sensor-Interface-and-Its-Ramu-Ramachandran/dfe3b46cf91ecb7e92c2f2da0b07f87c367d472f",
            "/paper/Mechanical-and-Thermal-Properties-of-Poly-Butylene/77ab913d02d1f6094d6c70e21abae0837ca0b620",
            "/paper/Evaluation-of-Unreliable-Retrial-G-queue-Using-ARAS-Suresh-Ramachandran/87fea488cb66d2b610d4f10986640b8874febdb6",
            "/paper/Prediction-of-Hybrid-Wind-Farms-using-Fuzzy-TOPSIS-Sivaji-Ramachandran/b65c471e01f0687557efa52e5398a5384ec9eed8",
            "/paper/Exploring-Various-Digital-Communication-and-its-Saravanan-Ramachandran/c1bc7d04620c32e8aca79c9c1b6f3b2258adcc1d",
            "/paper/Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52",
            "/paper/BREAST-CANCER-DETECTION-USING-MARK-RCNN-AND-WITH-PrasathAliasSurendhar-Vasuki/95cdb70035d13910f2bb7d76ad29d67f619e01d2",
            "/paper/Automated-detection-of-breast-cancer-lesions-using-Sahar-Nugroho/18aed16d53bfd0570d40d0be0f3b35338d1c9ead",
            "/paper/Deep-learning-for-magnification-independent-breast-Bayramoglu-Kannala/f7cbb1b2a56d4c74ce4920e353619103098a886e",
            "/paper/Classification-of-mammogram-for-early-detection-of-Vijayarajeswari-Parthasarathy/7c66be06a11210b10afe5dddde51f7c355b98b14",
            "/paper/Transfer-learning-based-deep-CNN-for-segmentation-Wahab-Khan/160befefc99d88390d40910b8cd77a5b4d4e310a",
            "/paper/Breast-cancer-classification-using-machine-learning-Amrane-Oukid/ea98b9481ec6943b799a89f0647becafa303066f",
            "/paper/Classification-of-breast-abnormality-using-decision-Kamalakannan-Babu/b845188382aa1eaaecea4879f41e78f6e0ef03df",
            "/paper/Breast-cancer-diagnosis-using-GA-feature-selection-Alickovic-Subasi/542f7bd9c6853d69561e090a8ff82829bf1691f7",
            "/paper/MRI-Brain-Tumor-Classification-Using-SVM-and-Based/3d119f56d43d3d14f4e790da3936b17bac96bab0"
        ]
    },
    {
        "id": "156733fbf757d4e8a333537518c8961163c4fbf7",
        "title": "Classification of Mammograms Using Convolutional Neural Network Based Feature Extraction",
        "abstract": "The convolutional Neural Networks (CNN) based feature extraction method is proposed and the features dimensionality was reduced using Principal Component Analysis (PCA) and reduced features are given to the K-Nearest Neighbors (KNN) to classify mammograms as normal or abnormal using 10-fold cross-validation. Breast cancer is the most common cause of death among women in the entire world and the second cause of death after lung cancer. The use of automatic breast cancer detection and classification might possibly enhance the survival rate of the patients through starting early treatment. In this paper, the convolutional Neural Networks (CNN) based feature extraction method is proposed. The features dimensionality was reduced using Principal Component Analysis (PCA). The reduced features are given to the K-Nearest Neighbors (KNN) to classify mammograms as normal or abnormal using 10-fold cross-validation. The experimental result of the proposed approach performed on Mammography Image Analysis Society (MIAS) and Digital Database for Screening Mammography (DDSM) datasets were found to be promising compared to previous studies in the area of image processing, artificial intelligence and CNN with an accuracy of 98.75\\(\\%\\) and 98.90\\(\\%\\) on MIAS and DDSM dataset respectively.",
        "publication_year": "2017",
        "authors": [
            "Taye Girma Debelee",
            "Mohammadreza Amirian",
            "A. Ibenthal",
            "G. Palm",
            "F. Schwenker"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "26",
        "reference_count": "29",
        "references": [
            "/paper/Classifier-Based-Breast-Cancer-Segmentation-Kebede-Debelee/2af1310dd611857d8bd1b1374695fc9c8913a4c9",
            "/paper/Classification-of-Mammograms-Using-Texture-and-CNN-Debelee-Gebreselasie/4cf3911b8613a47c82bd58d2597822d5de70606d",
            "/paper/Computer-Aided-Breast-Cancer-Detection-Using-of-and-Roy-Das/577bfaf8c1797a1ce8c55f1bdb620eaa3b15f9db",
            "/paper/Feature-fused-breast-cancer-detection-K.P.-S./8d77a44b3dc85186a08a1b132ca73bc08aad7583",
            "/paper/A-Survey-of-Convolutional-Neural-Network-in-Breast-Zhu-Wang/0295f85e0db918b26ff7e8d1d547f8d98eea7feb",
            "/paper/Deep-Learning-in-Selected-Cancers%E2%80%99-Image-Analysis%E2%80%94A-Debelee-Kebede/abfb457caba5314466652825260c12da01a41e0f",
            "/paper/Survey-of-deep-learning-in-breast-cancer-image-Debelee-Schwenker/ea5b27748ee154c978534505a177f280d87d52d4",
            "/paper/Survey-of-deep-learning-in-breast-cancer-image-Debelee-Schwenker/58dcb43c4909ca0178bbd5d4a4199731ad16c404",
            "/paper/Deep-Learning-Based-Computer-Aided-Systems-for-%3A-A-Jim'enez-Gaona-Rodr'iguez-'Alvarez/44eb9aafb0d32bf9c8dfd2a4f0f4a418be779a08",
            "/paper/Classification-of-medical-images-based-on-deep-Ali-Ejbali/7fdbbe7c8f16dc47865c37acf834581b129a54a9",
            "/paper/An-Efficient-Approach-for-Automated-Mass-and-in-Dong-Lu/8271f755d7cea799600e25662dd3f2ac8d23aeb1",
            "/paper/Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52",
            "/paper/Computer-aided-detection-of-breast-cancer-on-A-Dheeba-Singh/95bb0ee471480da79e41ae196bb4da02abe52a27",
            "/paper/Local-Binary-Patterns-Applied-to-Breast-Cancer-in-Pereira-Eleut%C3%A9rio/d38eeb88a928c1a1c6e1fdd18d504669b112ed8b",
            "/paper/Breast-mass-classification-in-digital-mammography-Xie-Li/2c5db1c44dddf0d8e6ea667f3f7afb13032ea386",
            "/paper/Breast-Masses-Identification-through-Pixel-Based-Torrents-Barrena-Puig/4592b296b02a97ce7c49e7ef53bfa5e809bb548e",
            "/paper/A-deep-feature-based-framework-for-breast-masses-Jiao-Gao/eb8f6f8c48e6b61ea35e7294f26f7cfbbc3dd833",
            "/paper/Representation-learning-for-mammography-mass-lesion-Arevalo-Gonz%C3%A1lez/a4360f362168a6107e1014b7fc61bed32038fc70",
            "/paper/Mammograms-Classification-Using-Gray-level-Matrix-Pratiwi-Alexander/0003a45ae653fcccb568b90dbb339cf5811a18bf",
            "/paper/Classification-of-benign-and-malignant-breast-based-Rouhi-Jafari/2415fc06de82ab41ad8b9615162247afb02974af"
        ]
    },
    {
        "id": "6e69f7ea9f63b651ebd676d51d6cca1a483cb2ec",
        "title": "Machine learning based computer aided diagnosis system for classification of breast masses in mammograms",
        "abstract": "The experimental results reveal that the proposed improved seeded region growing approach has been proven helpful in improving the classification performance of the proposed CAD system. Breast cancer continues to be the most common cancer in the fastest developing and the developed nations. Early detection by using mammography has been proven as the best prognosis. Computer Aided Diagnosis (CAD) systems are being used as second reader for the analysis and interpretation of mammogram images. In the last two decades, although breast cancer incidence has increased by many folds but unfortunately the progress in this field has almost stagnated. Therefore, the CAD systems need to be improved to be considered useful. In this study, a machine learning based CAD system for segmentation and classification of breast masses have been proposed. The IRMA Version of DDSM dataset has been used for experimentation and evaluation of the proposed system. Exact breast masses were segmented from manually extracted ROIs of 700*700 pixels by employing an improved seeded region growing algorithm. Various geometry and texture features were computed from the segmented mass lesions and corresponding ROIs respectively. The classification performances of nine state-of-the-art classifiers namely K-Nearest Neighbour (KNN), Support Vector Machine (SVM), Gaussian Mixture Model (GMM), Multi-class Support Vector Machine (mSVM), Decision Tree (DT), Discriminate Analysis (DA), Naive Bayes (NB), Random Forest (RF), Ensemble Tree (ET) have been investigated in this study. On evaluating the experimental results for all the classifiers, highest classification accuracy is obtained with SVM classifier. The experimental results reveal that the proposed improved seeded region growing approach has been proven helpful in improving the classification performance of the proposed system.",
        "publication_year": "2022",
        "authors": [
            "Harmandeep Singh",
            "V. Sharma",
            "Damanpreet Singh"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "26",
        "references": [
            "/paper/Preparation-and-characterization-of-oxide-ceramics-Sharma-Singh/426573514d4d64451ca34aa5e794d56fb1d98d69",
            "/paper/Classification-of-breast-masses-in-mammograms-using-Nandi-Nandi/c0cdec8e8149ab2e7d185893e3f18eb0089a0f2f",
            "/paper/Performance-evaluation-of-a-region-growing-for-Rabottino-Mencattini/76389ebb7c1496239e66fd663b0e7e43d391bca9",
            "/paper/Computer-Aided-Diagnosis-of-Malignant-Mammograms-Sharma-Khanna/662d927da3c17bfde61da5dbc24c037dce30ce25",
            "/paper/Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52",
            "/paper/A-review-of-computer-aided-diagnosis-of-breast-the-Rangayyan-Ayres/9bf32a68edfea8b8c072bcd3ee0d696687bab403",
            "/paper/Optimization-of-breast-mass-classification-using-a-Tan-Pu/da4237818523ea3e3d44dbe18c261afc6d6d404f",
            "/paper/Mammogram-Classification-using-Law's-Texture-Energy-Setiawan-Elysia/f0a4d96b5d7f6668d607512f8744e07dbe1c9b27",
            "/paper/MammoSys%3A-A-content-based-image-retrieval-system-Oliveira-Machado/db480b3f22244151cfeaa4beedc406a5137d1a4e",
            "/paper/Texture-features-for-classification-of-ultrasonic-Wu-Chen/037587b28682886bb02d87a150028ed931f967a4",
            "/paper/Measures-of-acutance-and-shape-for-classification-Rangayyan-El-Faramawy/0ffefa7d286b24f7c20bedf779cd7bc7d1f64ceb"
        ]
    },
    {
        "id": "662df45afea0d80922a86a4af431eb80ede87da2",
        "title": "Bayesian System And Copula For Event Detection And Summarization Of Soccer Videos",
        "abstract": "An efficient structure for analyzing and summarization of soccer videos utilizing object-based features using the t-cherry junction tree to create a compact representation and great approximation intractable model for client\u2019s relationships in an interpersonal organization is proposed. Event detection is a standout amongst the most key parts for distinctive sorts of area applications of video data framework. Recently, it has picked up an extensive interest of experts and in scholastics from different zones. While detecting video event has been the subject of broad study efforts recently, impressively less existing methodology has considered multimodel data and issues related efficiency. Start of soccer matches different doubtful circumstances rise that can't be effectively judged by the referee committee. A framework that checks objectively image arrangements would prevent not right interpretations because of some errors, or high velocity of the events. Bayesian networks give a structure for dealing with this vulnerability using an essential graphical structure likewise the probability analytics. We propose an efficient structure for analyzing and summarization of soccer videos utilizing object-based features. The proposed work utilizes the t-cherry junction tree, an exceptionally recent advancement in probabilistic graphical models, to create a compact representation and great approximation intractable model for client\u2019s relationships in an interpersonal organization. There are various advantages in this approach firstly; the t-cherry gives best approximation by means of junction tree class. Secondly, to construct a t-cherry junction tree can be to a great extent parallelized; and at last inference can be performed utilizing distributed computation. Examination results demonstrates the effectiveness, adequacy, and the strength of the proposed work which is shown over a far reaching information set, comprising more soccer feature, caught at better places. Keywords\u2014Summarization; Detection;Bayesian network; t-cherry tree.",
        "publication_year": "2015",
        "authors": [
            "D. S. Patil"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3",
        "reference_count": "38",
        "references": [
            "/paper/Water-atom-search-algorithm-based-deep-recurrent-on-Dabbu-Karuppusamy/b6ae09893bd6938e5ce87e78a4b61e3cb44946f7",
            "/paper/Glaucoma-detection-using-hybrid-architecture-based-Gampala-Maram/b40d92224140ef3e448c033848ba12ac47c0b50d",
            "/paper/Rider-chicken-optimization-dependent-recurrent-for-Aher-Jena/7e777b16e45b4c1f7581c77b6e3863e524298e84",
            "/paper/Event-Detection-and-Summarization-in-Soccer-Videos-Tavassolipour-Karimian/ac2e54cec3aa2d1e67288d00c7fce7b7b17f9a73",
            "/paper/Bayesian-network-based-soccer-video-event-detection-Sun-Jin/b5a23310cdc5b492175325ba90af69ecda3dc377",
            "/paper/Structure-analysis-of-soccer-video-with-hidden-Xie-Chang/5055097cd7279bc3cd16474e1629cf0c24ed7249",
            "/paper/Algorithms-and-system-for-segmentation-and-analysis-Xu-Xie/d6151de801659937574c3efe13c2d207e9e2f2cd",
            "/paper/Structure-analysis-of-sports-video-using-domain-Zhong-Chang/5a7571db7df03cca52c48f89595c4abefeb51e5c",
            "/paper/Semantic-annotation-of-soccer-videos%3A-automatic-Assfalg-Bertini/cc85119fdac7f6e9b0afa5e5a87983f6bca2f1c9",
            "/paper/Multi-modal-extraction-of-highlights-from-TV-1-Petkovic-Mihajlovi%C4%87/d804cdf4f348a3f4ef782f8818064f0dc17c62d7",
            "/paper/Detection-of-the-highlights-in-baseball-video-Shih-Huang/c7967ff0c51732110e0e1470975fe0a974fa8a2e",
            "/paper/Detecting-semantic-events-in-soccer-games%3A-towards-Tovinkere-Qian/63ddcd8ffd48628df02c7cbe5ede83d35af2f0c6",
            "/paper/Events-recognition-by-semantic-inference-for-sports-Wu-Ma/c51d7cbfb95ee370d1eddb4e0ff03290b8bb479a"
        ]
    },
    {
        "id": "24dccf31b0347d138cdca66d71d9cb264e7498b1",
        "title": "Color transfer based on multiscale gradient-aware decomposition and color distribution mapping",
        "abstract": "A novel color transfer method, which is based on the gradient-aware decomposition and the color distribution mapping, is proposed, which can achieve a visual satisfied result without post-processing gradient correction. Automatic global color transfer is a challenging problem in image editing. In this paper, we propose a novel color transfer method, which is based on the gradient-aware decomposition and the color distribution mapping. Firstly, a gradient-aware decomposition model is established to separate the target image into the base and detail layers. Then, the colors of each separated base layer are enforced to match those of a given reference image by Pitie's multi-dimensional probability density function transfer method. After that, all mapped base layers are combined with corresponding boosted detail layers to produce the final output. The experiments demonstrate that our method can achieve a visual satisfied result without post-processing gradient correction.",
        "publication_year": "2012",
        "authors": [
            "Zhuohan Su",
            "Daiguo Deng",
            "Xue Yang",
            "Xiaonan Luo"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "7",
        "reference_count": "17",
        "references": [
            "/paper/Color-Consistency-Correction-Based-on-Remapping-for-Xia-Yao/591dd6573403679ba82c0d4b12d058ae74af942c",
            "/paper/A-closed-form-solution-for-multi-view-color-with-Xia-Yao/84a259fcb9268c82403a76e6db897d67e84c572d",
            "/paper/Scribble-based-gradient-mesh-recoloring-Wan-Xiao/c9577eba0e0342fbb0c4be15bf22705759ddde00",
            "/paper/A-comprehensive-survey-on-non-photorealistic-and-Kumar-Poornima/6915ee29be51fb6e2c15b0a96a406c6b338d335e",
            "/paper/A-comprehensive-survey-on-non-photorealistic-and-Kumar-Poornima/0b4057afafe849d02eabf8336e8a42a046ddfd36",
            "/paper/Optimization%E2%80%90Based-Gradient-Mesh-Colour-Transfer-Xiao-Wan/54ed56f49c30148b32d77e6c42069bfaafb5ea54",
            "/paper/Data-driven-photographic-style-using-local-transfer-Shih/9947687ffe0bd2d6cd4fe717e534cfcb59302a4e",
            "/paper/Fast-local-color-transfer-via-dominant-colors-Dong-Bao/93b26a24b0b6cc4a92232874f04c75bc8f44c84b",
            "/paper/Gradient%E2%80%90Preserving-Color-Transfer-Xiao-Ma/7fd89d13508e4bba173eaf08f71ba2fa86e3e5ca",
            "/paper/An-efficient-PCA-based-color-transfer-method-Abadpour-Kasaei/53fc0415e0d00f9691994a49b8232a1cc2dfad5f",
            "/paper/Soft-Color-Segmentation-and-Its-Applications-Tai-Jia/7c2517f714a9015cf1673f9f7d2e024cb2be7230",
            "/paper/Distribution-aware-image-color-transfer-Wu-Dong/85683b702e59eddacef2d3fd82c9deab0b26ba8f",
            "/paper/Example-based-image-color-and-tone-style-Wang-Yu/6fe1789ca63598aec095df3c57c9607ac3b46dc7",
            "/paper/Edge-preserving-decompositions-for-multi-scale-tone-Farbman-Fattal/9a200182547f0b761e29258fa2459f63b5b64e2e",
            "/paper/Data-driven-image-color-theme-enhancement-Wang-Yu/fe2dcdb4c39fd519c7a44bb29a7655237475f91f",
            "/paper/Automated-colour-grading-using-colour-distribution-Piti%C3%A9-Kokaram/787874fedd7384be5b04530bf9334cb58e0c1bd1",
            "/paper/Image-smoothing-via-L0-gradient-minimization-Xu-Lu/000b9a90bbea62e4222704c616c0c2ee65609aa7"
        ]
    },
    {
        "id": "3aaf88163e9e502daf5be57917470c30c63da6a6",
        "title": "Adaptive exploitation of pre-trained deep convolutional neural networks for robust visual tracking",
        "abstract": "Due to the automatic feature extraction procedure via multi-layer nonlinear transformations, the deep learning-based visual trackers have recently achieved a great success in challenging scenarios for visual tracking purposes. Although many of those trackers utilize the feature maps from pre-trained convolutional neural networks (CNNs), the effects of selecting different models and exploiting various combinations of their feature maps are still not compared completely. To the best of our knowledge, all those methods use a fixed number of convolutional feature maps without considering the scene attributes (e.g., occlusion, deformation, and fast motion) that might occur during tracking. As a pre-requisition, this paper proposes adaptive discriminative correlation filters (DCF) based on the methods that can exploit CNN models with different topologies. First, the paper provides a comprehensive analysis of four commonly used CNN models to determine the best feature maps of each model. Second, with the aid of analysis results as attribute dictionaries, an adaptive exploitation of deep features is proposed to improve the accuracy and robustness of visual trackers regarding video characteristics. Third, the generalization of proposed method is validated on various tracking datasets as well as CNN models with similar architectures. Finally, extensive experimental results demonstrate the effectiveness of proposed adaptive method compared with the state-of-the-art visual tracking methods.",
        "publication_year": "2021",
        "authors": [
            "Seyed Mojtaba Marvasti-Zadeh",
            "Hossein Ghanei-Yakhdan",
            "S. Kasaei"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3",
        "reference_count": "70",
        "references": [
            "/paper/Multi-layer-Rotation-Memory-Model-based-correlation-Zhao-Song/d3d4c2ac5db37e54800615454cfa098847059c13",
            "/paper/Deep-Learning-for-Visual-Tracking%3A-A-Comprehensive-Marvasti-Zadeh-Cheng/1fbb4201af091aef55360f113ba35814063923e4",
            "/paper/A-Novel-Algorithm-Based-on-a-Common-Subspace-Fusion-Javed-Mahmood/6ad7652a8891b7b54b3ef1edeeee0e114f7da85c",
            "/paper/Visual-Tracking-with-Fully-Convolutional-Networks-Wang-Ouyang/bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0",
            "/paper/UCT%3A-Learning-Unified-Convolutional-Networks-for-Zhu-Huang/1131c53b9baaa740a4deef4c1282821b23d18687",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Deep-feature-tracking-based-on-interactive-multiple-Tang-Lu/511b6263795b8921e9f980b0ac7be5f6282337f6",
            "/paper/Occlusion-robust-object-tracking-based-on-the-of-Liu-Jin/4b39e8494cf031b2b87c6bd5c65c2a2dfb02c531",
            "/paper/Hierarchical-Convolutional-Features-for-Visual-Ma-Huang/5c8a6874011640981e4103d120957802fa28f004",
            "/paper/Target-Aware-Deep-Tracking-Li-Ma/503bafe063e410050c174fcc741e39b3b1e0eb22",
            "/paper/Robust-Visual-Tracking-via-Hierarchical-Features-Ma-Huang/f233c16a87d518bfe9f923ea7af48ed3eb6bb7d5",
            "/paper/Deep-visual-tracking%3A-Review-and-experimental-Li-Wang/26e2ca763087be09e3799ad294302aa91077942d",
            "/paper/WAEF%3A-Weighted-Aggregation-with-Enhancement-Filter-Rout-Mishra/ed84a17bd753d1ba9404131cff5186db4da6edd8"
        ]
    },
    {
        "id": "4bb2df1e5dc09ec7f0e50b5ba304a6e51943c72f",
        "title": "Long-term tracking with transformer and template update",
        "abstract": "A feature extraction network based on the transformer and adopt a knowledge distillation strategy to improve the effectiveness of the network for global feature extraction and demonstrates the superiority of the method. Aiming at the tracking failure due to the disappearance of the target in the long-term target tracking process, this paper proposes a long-term target tracking network based on the visual transformer and template update. First of all, we construct a feature extraction network based on the transformer and adopt a knowledge distillation strategy to improve the effectiveness of the network for global feature extraction. Secondly, in the modeling transformer, the target features are fully fused with the search area features by using encoder, and the position information in the target query is learned by the decoder. Then, target predictions are performed on the information from the encoder\u2013decoder to obtain tracking results. Meanwhile, we design a score head model to judge the validity of the dynamic template of the current frame before tracking in the next frame. We select the appropriate dynamic template for the tracking of the next frame according to the score result. In this paper, we performed extensive experiments on LaSOT, VOT2021-LT, TrackingNet, TLP, and UAV123 datasets, and the experimental results prove the effectiveness of our method. In particular, it exceeds STARK by 0.8 $$\\%$$ % (F score) on VOT2021-LT, 1.0 $$\\%$$ % (S score) on LaSOT, and TrackingNet exceed STARK by 1.1 $$\\%$$ % (NP score), which also demonstrates the superiority of the method in this paper.",
        "publication_year": "2022",
        "authors": [
            "Hongying Zhang",
            "Xiao-Xiao Peng",
            "Xuyong Wang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "47",
        "references": [
            "/paper/A-stable-long-term-object-tracking-method-with-Li-Zhao/43cae64a7f0b0942b0409fd4ef4009b0a07a8e5f",
            "/paper/Learning-regression-and-verification-networks-for-Zhang-Wang/3d372b63020c4d2c9510624f370b50d9f292bcde",
            "/paper/VTT%3A-Long-term-Visual-Tracking-with-Transformers-Bian-Hua/82a9596957fafd92893195dc9ad4bb2aca86f72c",
            "/paper/TrackFormer%3A-Multi-Object-Tracking-with-Meinhardt-Kirillov/0357156aef567fb5b709222894ddea1ce5d4e721",
            "/paper/Learning-Spatio-Temporal-Transformer-for-Visual-Yan-Peng/72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
            "/paper/Reliable-Re-Detection-for-Long-Term-Tracking-Wang-Zhou/46fc2e550dd695eaa899a07a01e306a48b73b656",
            "/paper/STMTrack%3A-Template-free-Visual-Tracking-with-Memory-Fu-Liu/811ffb185bc90ac5d02d6dbfbcdb6173756b52ef",
            "/paper/Learning-the-Model-Update-for-Siamese-Trackers-Zhang-Gonzalez-Garcia/157ff6e216985911cc2f9775155d2a424ba2984b",
            "/paper/MixFormer%3A-End-to-End-Tracking-with-Iterative-Mixed-Cui-Cheng/b6eaec7917439d79ce840fa97bc371552e9b6685",
            "/paper/High-Performance-Long-Term-Tracking-With-Dai-Zhang/adacccd99a42c3145ec6392a1a6b08878376e38b"
        ]
    },
    {
        "id": "55d093ac4d40359e64b0208bc7a910858511e1b3",
        "title": "AR Long-Term Tracking Combining Multi-Attention and Template Updating",
        "abstract": "An attention-based feature fusion network effectively fuses the template and search area features through a combination of dual self-attention and cross attention, and the anchor frameless mechanism is adopted in the classification and regression network, resulting in a significant reduction in the number of parameters. Aiming at the problem that the augmented reality system is susceptible to complex scenes and easily leads to the failure of tracking registration, a long-term augmented reality tracking algorithm combining multi-attention and template updating is proposed. Firstly, we improved the ResNet-50 network to extract richer semantic features instead of AlexNet. Secondly, the attention-based feature fusion network effectively fuses the template and search area features through a combination of dual self-attention and cross attention. Dual self-attention effectively enhances the information in the context, whereas cross attention adaptively enhanced the features of both self-attention branches. Thirdly, the ORB feature-matching algorithm is utilized to match the template and search image features, with the template updated if more than 150 matching feature points are found. Lastly, the anchor frameless mechanism is adopted in the classification and regression network, resulting in a significant reduction in the number of parameters. The results of experiments conducted on various public datasets demonstrate the algorithm\u2019s high success rate and accuracy, as well as its robustness in complex environments.",
        "publication_year": "2023",
        "authors": [
            "Mengru Guo",
            "Qiang Chen"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "13",
        "references": [
            "/paper/Learning-Regression-and-Verification-Networks-for-Zhang-Wang/fb2ea0a5ef40caedfb5a10d929a331662bde78e4",
            "/paper/A-survey-of-appearance-models-in-visual-object-Li-Hu/2822a883d149956934a20614d6934c6ddaac6857",
            "/paper/Object-Tracking-Benchmark-Wu-Lim/b7d540cd0de72e984cdec44afa4a4d039cfd5eea",
            "/paper/Long-term-Visual-Tracking%3A-Review-and-Experimental-Liu-Chen/2d4713ce1df60f771b65e900fd02352989df82ef",
            "/paper/LaSOT%3A-A-High-quality-Large-scale-Single-Object-Fan-Bai/e284bc13c2b76d0d0c7ad61d976f8a9d3eef8461",
            "/paper/Tracking-Learning-Detection-Kalal/c63a34ac6a4e049118070e707ca7679fbb132d33",
            "/paper/GlobalTrack%3A-A-Simple-and-Strong-Baseline-for-Huang-Zhao/5664e24cacf3f6374c26b5597765099ee9537413",
            "/paper/Siamese-Visual-Object-Tracking%3A-A-Survey-Ondra%C5%A1ovi%C4%8D-Tar%C3%A1bek/177d12634a4df3a6f67a4aecd03714ff39845d0e",
            "/paper/Distinctive-Image-Features-from-Scale-Invariant-Lowe/8c04f169203f9e55056a6f7f956695babe622a38",
            "/paper/High-Speed-Tracking-with-Kernelized-Correlation-Henriques-Caseiro/65c9b4b1d49f46b3f8f64a5f617acfc14f85d031"
        ]
    },
    {
        "id": "8146887f58b77f1e2b319fd5a2e7a0b9442b3a1f",
        "title": "Hierarchical memory-guided long-term tracking with meta transformer inquiry network",
        "abstract": "Semantic Scholar extracted view of \"Hierarchical memory-guided long-term tracking with meta transformer inquiry network\" by Xingmei Wang et al.",
        "publication_year": "2023",
        "authors": [
            "Xingmei Wang",
            "Guohao Nie",
            "Boquan Li",
            "Yilin Zhao",
            "Minyang Kang",
            "Bo Liu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "38",
        "references": [
            "/paper/Switch-and-Refine%3A-A-Long-Term-Tracking-and-Xu-Zhao/ef61778d85357bdab8c71cf79cf5e0024f5b39c5",
            "/paper/Long-term-Visual-Tracking%3A-Review-and-Experimental-Liu-Chen/2d4713ce1df60f771b65e900fd02352989df82ef",
            "/paper/A-motion-model-based-on-recurrent-neural-networks-Shahbazi-Bayat/dbe8a9d31c045aa8429dbbbe31542b44577dec8f",
            "/paper/CoCoLoT%3A-Combining-Complementary-Trackers-in-Visual-Dunnhofer-Micheloni/23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
            "/paper/Combining-complementary-trackers-for-enhanced-Dunnhofer-Simonato/48fe9bbdc61ab6a7c9922273f527514bcecc0e40",
            "/paper/Residual-memory-inference-network-for-regression-Zhang-Zhang/0f29ee600eea84e3826d090322b93b16674450bb",
            "/paper/Effective-Local-and-Global-Search-for-Fast-Tracking-Zhao-Yan/b47943161a0cefb8963ad0a7830e51c396bff3b1",
            "/paper/Visual-Object-Tracking-With-Discriminative-Filters-Javed-Danelljan/0530cbeb847f5e5002d1183c482759dff5f8c439",
            "/paper/Learning-Regression-and-Verification-Networks-for-Zhang-Wang/fb2ea0a5ef40caedfb5a10d929a331662bde78e4",
            "/paper/Tracking-by-Joint-Local-and-Global-Search%3A-A-Wang-Tang/27849a90109b93ec80d190d570041722fb2b0576"
        ]
    },
    {
        "id": "dc9a66e0f329de8054f4ab845331fb7183987418",
        "title": "RGBD Object Tracking: An In-depth Review",
        "abstract": "This paper is the first to provide depth quality evaluation and analysis of tracking results in depth-friendly scenarios in RGBD tracking, and proposes robustness evaluation against input perturbations. \u2014RGBD object tracking is gaining momentum in computer vision research thanks to the development of depth sensors. Although numerous RGBD trackers have been pro- posed with promising performance, an in-depth review for comprehensive understanding of this area is lacking. In this paper, we \ufb01rstly review RGBD object trackers from different perspectives, including RGBD fusion, depth usage, and tracking framework. Then, we summarize the existing datasets and the evaluation metrics. We benchmark a representative set of RGBD trackers, and give detailed analyses based on their performances. Particularly, we are the \ufb01rst to provide depth quality evaluation and analysis of tracking results in depth-friendly scenarios in RGBD tracking. For long-term settings in most RGBD tracking videos, we give an analysis of trackers\u2019 performance on handling target disappearance. To enable better understanding of RGBD trackers, we propose robustness evaluation against input perturbations. Finally, we summarize the challenges and provide open directions for this community. All resources are publicly available at https://github.com/memoryunreal/RGBD-tracking-review.",
        "publication_year": "2022",
        "authors": [
            "Jinyu Yang",
            "Zhe Li",
            "Song Yan",
            "Feng Zheng",
            "Alevs Leonardis",
            "J. Kamarainen",
            "Ling Shao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "75",
        "references": [
            "/paper/Learning-Dual-Fused-Modality-Aware-Representations-Gao-Yang/2147cfb8ef36bb938fbae4c9b7c9536ecadac424",
            "/paper/Shakes-on-a-Plane%3A-Unsupervised-Depth-Estimation-Chugunov-Zhang/ef09e6d12f3d7d8e04e444c11477017a5bcc5fa3",
            "/paper/DepthTrack%3A-Unveiling-the-Power-of-RGBD-Tracking-Yan-Yang/625aec94369715717158843c3ee288869cbe098f",
            "/paper/Target-Aware-Correlation-Filter-Tracking-in-RGBD-Kuai-Wen/f33b4ba5efdef921383bde48ed1ed4edff86edb9",
            "/paper/Tracking-Revisited-Using-RGBD-Camera%3A-Unified-and-Song-Xiao/487eb86379e979a72ebfef67db6eb8f048d1d258",
            "/paper/Multi-modal-Visual-Tracking%3A-Review-and-Comparison-Zhang-Wang/d884af3933148cef3b50fd38c810f5a7763d0fc9",
            "/paper/Robust-fusion-for-RGB-D-tracking-using-CNN-features-Wang-Wei/6290d7a7e353fbfe77e21e4d1086143f5e66312b",
            "/paper/CDTB%3A-A-Color-and-Depth-Visual-Object-Tracking-and-Luke%C5%BEi%C4%8D-Kart/f202feae9ca7b3766e072b6af657beed2236a93c",
            "/paper/How-to-Make-an-RGBD-Tracker-Kart-K%C3%A4m%C3%A4r%C3%A4inen/c06ecdf5b149c322db0381adb6b3fd5ccb31a720",
            "/paper/Robust-RGBD-Tracking-via-Weighted-Convolution-Liu-Tang/7681f4c80774c6661980c5a76ffab357cca5f5cf",
            "/paper/Online-RGB-D-tracking-via-An-Zhao/3f02406b9b59d6f966c735953930fede1d751d0d",
            "/paper/Object-fusion-tracking-based-on-visible-and-images%3A-Zhang-Ye/761a9b5d8750eb63a9717650c4aaca53ce36a364"
        ]
    },
    {
        "id": "45e2b30cc94d2f6c7642d9c183a6d8a827fc99f4",
        "title": "Visualizing Skiers' Trajectories in Monocular Videos",
        "abstract": "SkiTraVis is proposed, an algorithm to visualize the sequence of points traversed by a skier during its performance and demonstrates the potential of the solution for broadcasting media enhancement and coach assistance. Trajectories are fundamental to winning in alpine skiing. Tools enabling the analysis of such curves can enhance the training activity and enrich broadcasting content. In this paper, we propose SkiTraVis, an algorithm to visualize the sequence of points traversed by a skier during its performance. SkiTraVis works on monocular videos and constitutes a pipeline of a visual tracker to model the skier's motion and of a frame correspondence module to estimate the camera's motion. The separation of the two motions enables the visualization of the trajectory according to the moving camera's perspective. We performed experiments on videos of real-world professional competitions to quantify the visualization error, the computational efficiency, as well as the applicability. Overall, the results achieved demonstrate the potential of our solution for broadcasting media enhancement and coach assistance.",
        "publication_year": "2023",
        "authors": [
            "Matteo Dunnhofer",
            "L. Sordi",
            "C. Micheloni"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "64",
        "references": [
            "/paper/3D-ball-trajectory-reconstruction-from-sports-video-Chen-Chou/4832a96f8c59d85c98175b84594ae24b7b9ed0ca",
            "/paper/Spatiotemporal-Motion-Synchronization-for-Snowboard-Matsumura-Mikami/4ea19217a73ca7fb5998198e752583947ba447d8",
            "/paper/Pass-Receiver-Prediction-in-Soccer-using-Video-and-Honda-Kawakami/19caa2d6ad3e335954441dca9ff27fb66b50afc2",
            "/paper/What-Players-do-with-the-Ball%3A-A-Physically-Maksai-Wang/32b07bcca0f86dc1f9319666fd574f9b85a68250",
            "/paper/Robust-Camera-Calibration-and-Player-Tracking-in-Hu-Chang/f13da4ae645a0b5c90ca20272d3513f96e7a136a",
            "/paper/Video-Based-Ski-Jump-Style-Scoring-from-Pose-%C5%A0tepec-Sko%C4%8Daj/c86d81f5ed30c93780e9b0f5dcfffdfa2e1f1cda",
            "/paper/Ski-Fall-Detection-from-Digital-Images-Using-Deep-Zhu-Yan/ac3844634ca870eff5e3ffd8cdd6bf4f07626dac",
            "/paper/Learning-to-Track-and-Identify-Players-from-Sports-Lu-Ting/38ee79ef860def18e026e0e72188cd93e14b640d",
            "/paper/Table-Tennis-ball-kinematic-parameters-estimation-Calandre-P%C3%A9teri/a77bf0816b596461bd02bb3a94a14c79698dea40",
            "/paper/Motion-Capture-from-Pan-Tilt-Cameras-with-Unknown-Bachmann-Sp%C3%B6rri/8342aa4993988f6e4fe030f2b09ff1c9f6196e1a"
        ]
    },
    {
        "id": "19fe26d0cfe16471f4d2a05053c9f51cf14f0fa8",
        "title": "Predictive Visual Tracking: A New Benchmark and Baseline Approach",
        "abstract": "A new predictive visual tracking baseline is developed to compensate for the latency stemming from the onboard computation and can provide a more realistic evaluation of the trackers for the robotic applications. \u2014As a crucial robotic perception capability, visual tracking has been intensively studied recently. In the real-world scenarios, the onboard processing time of the image streams inevitably leads to a discrepancy between the tracking results and the real-world states. However, existing visual tracking benchmarks commonly run the trackers of\ufb02ine and ignore such latency in the evaluation. In this work, we aim to deal with a more realistic problem of latency-aware tracking. The state-of-the-art trackers are evaluated in the aerial scenarios with new metrics jointly assessing the tracking accuracy and ef\ufb01ciency. Moreover, a new predictive visual tracking baseline is developed to compensate for the latency stemming from the onboard computation. Our latency-aware benchmark can provide a more realistic evaluation of the trackers for the robotic applications. Besides, exhaustive experiments have proven the effectiveness of the proposed predictive visual tracking baseline approach. Our code is on https://github.com/vision4robotics/ LAE-PVT-master .",
        "publication_year": "2021",
        "authors": [
            "Bowen Li",
            "Yiming Li",
            "Junjie Ye",
            "Changhong Fu",
            "Hang Zhao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "38",
        "references": [
            "/paper/PVT%2B%2B%3A-A-Simple-End-to-End-Latency-Aware-Visual-Li-Huang/8fa3221af7f6d3008e00cc0c459ecbde6f1014da",
            "/paper/Siamese-Object-Tracking-for-Unmanned-Aerial-A-and-Fu-Lu/1171234cb2f3e1589592e3d04eb10c132fc6a5c8",
            "/paper/Energy-Efficient-Object-Tracking-Using-Adaptive-ROI-Katoch-Iqbal/0b09bb62c39def8affd240b0ebbdeba398521a2b",
            "/paper/Adaptive-Subsampling-for-ROI-Based-Visual-Tracking%3A-Iqbal-Muro/e0737922b961c6f6a1ba8e73f52bcd8c4689c712",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "/paper/Visual-Object-Tracking-for-Unmanned-Aerial-A-and-Li-Yeung/6ebc40a061433c24a3ea1f305bb6533b8f3dd5f4",
            "/paper/Learning-Discriminative-Model-Prediction-for-Bhat-Danelljan/2c8315ae713b3e27c6e9f291a158134d9c516166",
            "/paper/Fast-Online-Object-Tracking-and-Segmentation%3A-A-Wang-Zhang/d58e13f7e5e06440c9470a9101ccbb1bfd91b5a1",
            "/paper/A-Benchmark-and-Simulator-for-UAV-Tracking-Mueller-Smith/27850781e39df9f750e05409b8072261124068e8",
            "/paper/All-Day-Object-Tracking-for-Unmanned-Aerial-Vehicle-Li-Fu/f3d7eb617179db9f9621fa2c978dfb9f2c39341f",
            "/paper/Is-First-Person-Vision-Challenging-for-Object-Dunnhofer-Furnari/ca97f741f331b5b43d0577a46c05984f0785a8fa",
            "/paper/DR2Track%3A-Towards-Real-Time-Visual-Tracking-for-UAV-Fu-Ding/86aac093dcef187bdfb296888ba2a62bccb15c81",
            "/paper/Towards-Robust-Visual-Tracking-for-Unmanned-Aerial-He-Fu/c29199b0cd3c9b60288a0b726939fa829d6c2a34",
            "/paper/Towards-Streaming-Perception-Li-Wang/2c62be4b55661e8037117d697a2c0b296453ed11"
        ]
    },
    {
        "id": "0bc6b7c8e3d2efdb6cf35a15192029c078bebbc6",
        "title": "SiamDA: distribution-aware Siamese network for visual tracking",
        "abstract": "An efficient Siamese distribution-aware (DA) anchor-free (SiamDA) network for object tracking that is modeled by a flexible general distribution, avoiding the ambiguity of traditional rectangle representation in some complex scenes, such as occlusion and background clutter. Abstract. Siamese-based trackers have achieved remarkable performance and widespread applications in visual tracking. However, most of the existing Siamese trackers are usually restricted by the difficulty of handling the misalignment between target localization and state estimation. To address the misalignment issue, we propose an efficient Siamese distribution-aware (DA) anchor-free (SiamDA) network for object tracking. First, the bounding box of regression targets is modeled by a flexible general distribution, avoiding the ambiguity of traditional rectangle representation in some complex scenes, such as occlusion and background clutter. On this basis, to eliminate the misalignment issue, a DA module is designed to connect the target localization and state estimation. In addition, the Pseudo-Intersection over Union sample assignment rule provides our anchor-free tracker with high-quality samples effectively during the training process, making it possible to further improve the tracking performance. Extensive experiments on official visual tracking benchmarks demonstrate the effectiveness and efficiency of SiamDA.",
        "publication_year": "2022",
        "authors": [
            "Qiuhan Ji",
            "H. Shi",
            "Shuai Tan",
            "Bing Song",
            "Yang Tao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "49",
        "references": [
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Siamese-Box-Adaptive-Network-for-Visual-Tracking-Chen-Zhong/cce1fecc800d2782da638f3060d5b2e887739f74",
            "/paper/SiamCorners%3A-Siamese-Corner-Networks-for-Visual-Yang-He/4b6a190bb897a626639849c09426d30d5a1462c8",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/SiamCAR%3A-Siamese-Fully-Convolutional-Classification-Guo-Wang/738165f33c50b059e87b14d8b4a129230e14eacd",
            "/paper/Deformable-Siamese-Attention-Networks-for-Visual-Yu-Xiong/5eca15a355b2a9a1e80879e850afe49d3c398c53",
            "/paper/Visual-Object-Tracking-With-Discriminative-Filters-Javed-Danelljan/0530cbeb847f5e5002d1183c482759dff5f8c439",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "/paper/SiamFC%2B%2B%3A-Towards-Robust-and-Accurate-Visual-with-Xu-Wang/be412c7c7128cf91455233b652d6c94a6001a7c8",
            "/paper/Robust-visual-tracking-algorithm-with-coattention-Dai-Jiang/d81dc62436e93135bc5d4a13b2856a2db4f815b3"
        ]
    },
    {
        "id": "b33bcf197d1ec7c432a5eda4ed07f17898fb5f4b",
        "title": "A Deep Tracking and Segmentation Approach for Soccer Videos Visual Effects",
        "abstract": "A novel deformable cross-similarity correlation (DF_CORR) is adopted to estimate the deformation of players and the model outperforms the state-of-the-art trackers whose accuracy is decreased significantly compared with the general tracking tasks. The applications of deep learning algorithm in sports contain enormous potential. Specifically, in soccer, tracking algorithm could record the tracks of players, which could play as an assistant to assess team performance and evaluate strategies. Moreover, through segmentation model, we could extract semantic attributes of players. This auxiliary information may contribute to the special visual effects processing in broadcasting or entertainment area. Unlike general tracking tasks, soccer videos contain much more cases of deformation, blur, and occlusion. In this paper, we propose a novel model which could combine tracking and segmentation together. A novel deformable cross-similarity correlation (DF_CORR) is adopted to estimate the deformation of players. A new soccer tracking dataset is established to evaluate the performance of top-ranked trackers in soccer videos. In soccer tracking dataset, our model outperforms the state-of-the-art trackers whose accuracy is decreased significantly compared with the general tracking tasks. Moreover, our extensive experiments show comparable segmentation performance against SiamMask, while running in a real-time speed of 36.2FPS.",
        "publication_year": "2020",
        "authors": [
            "Shenhui Peng",
            "Li Song",
            "Jun Ling",
            "Rong Xie",
            "Song Xu",
            "Lin Li"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "9",
        "references": [
            "/paper/A-Large-scale-Sports-Tracking-Dataset-and-Based-Wang-Zhou/dd950231a9c5fb671a23c97a5038ba061937f6f4",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/Video-Object-Segmentation-by-Learning-Embeddings-Ci-Wang/4960ab1cef23e5ccd60173725ea280f462164a0e",
            "/paper/Video-Object-Segmentation-without-Temporal-Maninis-Caelles/a2fcb9088e6dca3ea850de871c08b6fa084a190e",
            "/paper/The-Sixth-Visual-Object-Tracking-VOT2018-Challenge-Kristan-Leonardis/219e9a4527110baf1feb3df20db12064eeafdfb7",
            "/paper/Object-Tracking-Benchmark-Wu-Lim/b7d540cd0de72e984cdec44afa4a4d039cfd5eea",
            "/paper/DeepLab%3A-Semantic-Image-Segmentation-with-Deep-and-Chen-Papandreou/cab372bc3824780cce20d9dd1c22d4df39ed081a",
            "/paper/High-Speed-Tracking-with-Kernelized-Correlation-Henriques-Caseiro/65c9b4b1d49f46b3f8f64a5f617acfc14f85d031",
            "/paper/Microsoft-COCO%3A-Common-Objects-in-Context-Lin-Maire/71b7178df5d2b112d07e45038cb5637208659ff7"
        ]
    },
    {
        "id": "5fea6a7854c6debb8010ae217f0c83370bbca784",
        "title": "EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset",
        "abstract": "EgoTracks is a new dataset for long-term egocentric visual object tracking, sourced from the Ego4D dataset, and presents a significant challenge to recent state-of-the-art single-object tracking models, which are found to score poorly on traditional tracking metrics for this new dataset, compared to popular benchmarks. Visual object tracking is a key component to many egocentric vision problems. However, the full spectrum of challenges of egocentric tracking faced by an embodied AI is underrepresented in many existing datasets; these tend to focus on relatively short, third-person videos. Egocentric video has several distinguishing characteristics from those commonly found in past datasets: frequent large camera motions and hand interactions with objects commonly lead to occlusions or objects exiting the frame, and object appearance can change rapidly due to widely different points of view, scale, or object states. Embodied tracking is also naturally long-term, and being able to consistently (re-)associate objects to their appearances and disappearances over as long as a lifetime is critical. Previous datasets under-emphasize this re-detection problem, and their\"framed\"nature has led to adoption of various spatiotemporal priors that we find do not necessarily generalize to egocentric video. We thus introduce EgoTracks, a new dataset for long-term egocentric visual object tracking. Sourced from the Ego4D dataset, this new dataset presents a significant challenge to recent state-of-the-art single-object tracking models, which we find score poorly on traditional tracking metrics for our new dataset, compared to popular benchmarks. We further show improvements that can be made to a STARK tracker to significantly increase its performance on egocentric data, resulting in a baseline model we call EgoSTARK. We publicly release our annotations and benchmark, hoping our dataset leads to further advancements in tracking.",
        "publication_year": "2023",
        "authors": [
            "Hao Tang",
            "Kevin J Liang",
            "K. Grauman",
            "Matt Feiszli",
            "Weiyao Wang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "80",
        "references": [
            "/paper/Aria-Digital-Twin%3A-A-New-Benchmark-Dataset-for-3D-Pan-Charron/27942ff872056d886977e465df90d2bf93675cf4",
            "/paper/Transforming-Model-Prediction-for-Tracking-Mayer-Danelljan/dc47b17250b639d3a89a716c7216ef69b33f9e33",
            "/paper/Learning-Spatio-Temporal-Transformer-for-Visual-Yan-Peng/72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
            "/paper/High-Performance-Long-Term-Tracking-With-Dai-Zhang/adacccd99a42c3145ec6392a1a6b08878376e38b",
            "/paper/Know-Your-Surroundings%3A-Exploiting-Scene-for-Object-Bhat-Danelljan/d1e61fa7824709cae37fb59483dd0772e3101c08",
            "/paper/EPIC-KITCHENS-VISOR-Benchmark%3A-VIdeo-Segmentations-Darkhalil-Shan/8c0469d102e02e942a74fd319f0ac20fa9702111",
            "/paper/Open-World-Instance-Segmentation%3A-Exploiting-Pseudo-Wang-Feiszli/162dba3ef611480e959ada4ec54b0714f5564808",
            "/paper/Ego4D%3A-Around-the-World-in-3%2C000-Hours-of-Video-Grauman-Westbury/847a153286d7f6f496f1ff61089831c267d68e30",
            "/paper/Microsoft-COCO%3A-Common-Objects-in-Context-Lin-Maire/71b7178df5d2b112d07e45038cb5637208659ff7",
            "/paper/1st-Place-Solution-for-YouTubeVOS-Challenge-2022%3A-Hu-Chen/cc4e3700906008d47a73ae46a22b69d665157793",
            "/paper/Visual-Object-Tracking-in-First-Person-Vision-Dunnhofer-Furnari/c89da5aa9697ab9d5366353ec29b3e9c1b610469"
        ]
    },
    {
        "id": "06d334fdddec3b0003924e3acccf05c295094605",
        "title": "SRRT: Search Region Regulation Tracking",
        "abstract": "A novel tracking paradigm is proposed, called Search Region Reg- ulation Tracking (SRRT), which applies a proposed search region regulator to estimate an optimal search region dynam- ically for every frame to adapt the object\u2019s appearance variation during tracking. Dominant trackers generate a \ufb01xed-size rectangular region based on the previous prediction or initial bounding box as the model input, i . e ., search region. While this manner ob- tains improved tracking ef\ufb01ciency, a \ufb01xed-size search region lacks \ufb02exibility and is likely to fail in cases, e . g ., fast motion and distractor interference. Trackers tend to lose the target object due to the limited search region or be interfered by dis- tractors due to excessive search region. In this work, we propose a novel tracking paradigm, called Search Region Reg- ulation Tracking (SRRT), which applies a proposed search region regulator to estimate an optimal search region dynam- ically for every frame. To adapt the object\u2019s appearance variation during tracking, we further propose a locking-state de- termined updating strategy for reference frame updating. Our SRRT framework is very concise without fancy design, yet achieves evident improvements on the baselines and competitive results with other state-of-the-art trackers on seven chal- lenging benchmarks. On the large-scale LaSOT benchmark, our SRRT improves SiamRPN++ and TransT with the abso- lute gains of 4.6% and 3.1% in terms of AUC.",
        "publication_year": "2022",
        "authors": [
            "Jiawen Zhu",
            "Xin Chen",
            "D. Wang",
            "Wenda Zhao",
            "Huchuan Lu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "45",
        "references": [
            "/paper/Learning-Target-Candidate-Association-to-Keep-Track-Mayer-Danelljan/fb058786bbcb2cead98a3ef55b33d2b73b2119fc",
            "/paper/LaSOT%3A-A-High-Quality-Benchmark-for-Large-Scale-Fan-Lin/900ab48d25b44c076e31224b7befa503d9550c53",
            "/paper/TrackingNet%3A-A-Large-Scale-Dataset-and-Benchmark-in-M%C3%BCller-Bibi/8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b",
            "/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "/paper/Transformer-Tracking-Chen-Yan/7c3ce1b3ad598a282546e03e2dc8b52c338caed6",
            "/paper/Siam-R-CNN%3A-Visual-Tracking-by-Re-Detection-Voigtlaender-Luiten/069ccdbab6ea6ca2d9c3b75c76360ca1e4e9a5e9",
            "/paper/Learning-Discriminative-Model-Prediction-for-Bhat-Danelljan/2c8315ae713b3e27c6e9f291a158134d9c516166",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "/paper/GOT-10k%3A-A-Large-High-Diversity-Benchmark-for-in-Huang-Zhao/c75180ab22b80b7ac3c8853a934ac515313b9aad",
            "/paper/A-Benchmark-and-Simulator-for-UAV-Tracking-Mueller-Smith/27850781e39df9f750e05409b8072261124068e8"
        ]
    },
    {
        "id": "06b05fa46ea957aa1e16aa809a23828e601d4f49",
        "title": "STUDY ON LEVEL SET SEGMENTATION BASED CLASSIFICATION USING MAMMOGRAMS",
        "abstract": "An algorithm based on region and contour based and some clustering segmentation techniques to recognize tumors in breast images and classification algorithms are used to make difference between tumours affected image or normal image. Application of image processing algorithms on medical images is always challenging. This always help in early detection stages and later for diagnosis of different cancer issues. As breast cancer being second major cause of death in women, so our interest is on this topic to improve the accuracy to segment the tumors in mammograms. For this purpose in our paper, we present algorithm based on region and contour based and some clustering segmentation techniques to recognize tumors in breast images. Our first step in this algorithm require the level set. We have used Spatial Fuzzy Clustering (SFC) to improve region growing. In Second step, Artificial Neural Network is used to regulate all level set parameters. This approach is used with Genetic Algorithm. Third step is of feature extraction. This step is used to extract features from segmented images to train a classified to determine whether a tumour in label as normal or tumour area. As last step, classification algorithms are used to make difference between tumours affected image or normal image. We will test with ANN algorithms.",
        "publication_year": "2017",
        "authors": [
            "Payal Nemade",
            "Aarti Sharma",
            "C. L. Chowdhary"
        ],
        "related_topics": [
            "Computer Science",
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "26",
        "references": [
            "/paper/Classification-of-benign-and-malignant-breast-based-Rouhi-Jafari/2415fc06de82ab41ad8b9615162247afb02974af",
            "/paper/A-Hybrid-Scheme-for-Breast-Cancer-Detection-using-Chowdhary-Acharjya/c55550ff224ff080d95d93200aea8838267fc21c",
            "/paper/Automatic-detection-of-abnormal-mammograms-in-Jen-Yu/6d8c60d47e2be4de763bb2f8044e09981016396e",
            "/paper/Multi-class-abnormal-breast-tissue-segmentation-Jenefer-Cyrilraj/e065a189e661b7fc0391bfea819355bf476b8d74",
            "/paper/Classification-of-benign-and-malignant-masses-based-Tahmasbi-Saki/46c409dd878e643271ef63f1817ded8b57abc01e",
            "/paper/Computer-Aided-Breast-Cancer-Detection-Using-A-Ganesan-Acharya/9a105c4792cda90bbd81efe3912da80302bcbe6d",
            "/paper/Breast-Cancer-Detection-using-Intuitionistic-Fuzzy-Chowdhary-Acharjya/8ece55a94960504cd809b4058382afa746085911",
            "/paper/Classification-of-Breast-Masses-Using-Selected-and-Mu-Nandi/a23937d6345c6e707cf17509ec27c76fef4ce7cc",
            "/paper/Segmentation-of-interest-region-in-medical-volume-Lee-Cho/d0fabc41e0b1d39a8e21968fa8d54a6ff12fa4cd",
            "/paper/Integrating-FCM-and-Level-Sets-for-Liver-Tumor-Li-Chui/8a15dcdc34a229897e5a756f8963f8101941e76b"
        ]
    },
    {
        "id": "a7f3cd1885a0f1548596b288eb6768d818b8463d",
        "title": "Breast Cancer Detection and Classification from Mammogram Images Using Multi-model Shape Features",
        "abstract": "This paper proposes a framework that automatically classifies the benign and malignant tumors in mammogram images using the support vector machine (SVM) and artificial neural network (ANN) classifiers, which have achieved good results and are promising. Nowadays, breast cancer has become one of the common diseases and is leading in causes of deaths in women. Early detection of breast cancer is very much needed and critical, and mammography is considered as one of the best-suited procedures. The masses are classified as benign or malignant tumors. The size and shape of the masses are characterized by its shapes as per BI-RADS (Breast Imaging-Reporting and Data System), which can discriminate benign and malignant effectively. In this paper, we propose a framework that automatically classifies the benign and malignant tumors in mammogram images. We have considered INBreast and CBIS-DDSM dataset experiments. The histogram-processing multi-level Otsu thresholding on the extracted Region of Interest (ROI) is applied as pre-processing steps for segmenting it. Eighteen features are extracted from the ROI and characterized structure, shape, size, and boundaries of mass present in images belong to both the datasets. The features extracted from the datasets are cross-validated for training and testing using stratified cross-validation techniques. The support vector machine (SVM) and artificial neural network (ANN) classifiers are trained and validated for benign and malignant tumor classification. The experimental results have achieved good results and are promising.",
        "publication_year": "2022",
        "authors": [
            "V. R. Gurudas",
            "S. Shaila",
            "A. Vadivel"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "42",
        "references": [
            "/paper/Automatic-Breast-Tumor-Classification-Using-a-Level-Pashoutan-Shokouhi/64aa7896b11ab90ccb4828c2faa6f86cc5f647cd",
            "/paper/Detection-and-classification-of-breast-cancer-from-Ghongade-Wakde/04f3e376672318a7c8bfe12059069d033c688951",
            "/paper/Automatic-classification-of-masses-from-digital-Mohamed-Salem/82b09f56d68eefdb3a3442cfe29433cd72480d64",
            "/paper/Mammography-classification-using-modified-hybrid-Sonar-Bhosle/870d52e025216445bbc51434527f450c7be630fd",
            "/paper/Benign-and-malignant-breast-cancer-segmentation-Punitha-Amuthan/8133e66c9c03095ef605090e6a72b752dc774d92",
            "/paper/Classification-of-benign-and-malignant-breast-based-Jiang-Xu/c4f73e387ad859a54de1d2f9938b8c7dbd1c2a97",
            "/paper/Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52",
            "/paper/Breast-Mass-Detection-And-Classification-Algorithm-Loizidou-Skouroumouni/34b5e888f948e384f434ba3fd1b74e26ba104d5a",
            "/paper/Mammogram-Segmentation-by-Contour-Searching-and-Cascio-Fauci/f757d4cf2a1a885810bd0c97f9735612750f329c",
            "/paper/Mammography-Feature-Analysis-and-Mass-Detection-in-Patel-Sinha/7ff70f493df1a7d5dfc0c437e7fce6a5172be933"
        ]
    },
    {
        "id": "bb1beb57009f356e300ed2c83bb2fc4f8198daf3",
        "title": "A Novel CNN-Inception-V4-Based Hybrid Approach for Classification of Breast Cancer in Mammogram Images",
        "abstract": "This study proposes a novel-based hybrid approach, CNN-Inception-V4, based on the fusing of these two networks to develop a unique hybrid technique to identify breast cancer mass pictures as benign or malignant abnormalities. Breast cancer is the most frequent disease in women, with one in every 19 women at risk. Breast cancer is the fifth leading cause of cancer death in women around the world. The most effective and efficient technique of controlling cancer development is early identification. Mammography helps in the early detection of cancer, which saves lives. Many studies conducted various tests to categorize the tumor and obtained positive findings. However, there are certain limits. Mass categorization in mammography is still a problem, although it is critical in aiding radiologists in establishing correct diagnoses. The purpose of this study is to develop a unique hybrid technique to identify breast cancer mass pictures as benign or malignant. The combination of two networks helps accelerate the categorization process. This study proposes a novel-based hybrid approach, CNN-Inception-V4, based on the fusing of these two networks. Mass images are used in this research from the CBIS-DDSM dataset. 450 images are taken for benign, and 450 images are used for malignant. The images are first cleaned by removing pectoral muscles, labels, and white borders. Then, CLAHE is used to these images to improve their quality in order to produce promising classification results. Following preprocessing, our model classifies cancer in mammography pictures as benign or malignant abnormalities. Our proposed model\u2019s accuracy is 99.2%, with sensitivity of 99.8%, specificity of 96.3%, and F1-score of 97%. We also compared our proposed model to CNN, Inception-V4, and ResNet-50. Our proposed model outperforms existing classification models, according to the results.",
        "publication_year": "2022",
        "authors": [
            "Muhammad Saquib Nazir",
            "Usman Ghani Khan",
            "Aqsa Mohiyuddin",
            "Mana Saleh Al Reshan",
            "A. Shaikh",
            "Muhammad Rizwan",
            "Monika D\u00e1videkov\u00e1"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "55",
        "references": [
            "/paper/A-Survey-of-Convolutional-Neural-Network-in-Breast-Zhu-Wang/0295f85e0db918b26ff7e8d1d547f8d98eea7feb",
            "/paper/Computational-Framework-of-Inverted-Fuzzy-C-Means-Kodipalli-Fernandes/11f281ed14558307fc81d0dabceebd83116d517f",
            "/paper/Breast-Tumor-Detection-and-Classification-in-Images-Mohiyuddin-Basharat/0bac967d54ecbca93cadeca63edf273e9ee9b6e6",
            "/paper/Mammography-Image-Based-Diagnosis-of-Breast-Cancer-Alshammari-Almuhanna/ce93c9b3ce2b3e2c51260a3a0d98b5f0848e8c27",
            "/paper/Deep-learning-algorithm-for-breast-masses-in-Gnanasekaran-Joypaul/b4efe64814bb38a0f7bcc5ebcec2b978e71b8472",
            "/paper/Breast-Cancer-Detection-Using-Convolutional-Neural-Hadush-Girmay/1a88ddff5eec7c40ec7d1205277871664fa7c68c",
            "/paper/Malignant-and-nonmalignant-classification-of-breast-Houby-Yassin/0677d83fbc116d6a9eb71ab3e73d45350720033d",
            "/paper/Bio-Imaging-Based-Machine-Learning-Algorithm-for-Safdar-Rizwan/f174e9f2ac85407353bf1ea9cde1ee7a769b37fe",
            "/paper/Breast-Cancer-Detection-Based-on-Deep-Learning-Ismail-Sovuthy/ebc0a2c2433ff2c89794882c6c2afd005e24ef3d",
            "/paper/An-enhancement-of-mammogram-images-for-breast-using-Patel-Hadia/f3eb7b753cf2a9569a1d9fdd11618bce28b1ef46",
            "/paper/Breast-Cancer-Detection-in-Mammograms-Using-Deep-Pillai-Nizam/75b2813ee47c9c13dd48a634363dedc91640c093",
            "/paper/CLASSIFICATION-MODEL-FOR-BREAST-CANCER-MAMMOGRAMS-Samuri-Nova/d6edaac8f8fc254e2c66470bb5e6d4c14f4dfad2"
        ]
    },
    {
        "id": "32320a5c9a49868b851ef6727cc3d0bd80f9f011",
        "title": "Segmentation of malignant tumours in mammogram images: A hybrid approach using convolutional neural networks and connected component analysis",
        "abstract": "A novel hybrid approach is proposed by combining a convolution neural network (CNN) with connected component analysis (CCA) to segment malignant breast lesions without any pre\u2010processing to avoid any distortion in image sharpness at the initial stages. The segmentation of breast lesions is an important step in the computer\u2010aided analysis of the mammogram. The presence of noise in mammograms makes lesion detection challenging particularly for complex malignant lesions. Pre\u2010processing techniques can deal with the noise issue but distorts the important shape features. This motivates us to propose a novel hybrid approach by combining a convolution neural network (CNN) with connected component analysis (CCA) to segment malignant breast lesions without any pre\u2010processing to avoid any distortion in image sharpness at the initial stages. Two well\u2010known segmentation techniques namely, K\u2010means (KM) and Fuzzy c\u2010means (FCM) are also used to compare the results. From a pool of 1045 mammographic cancer images acquired from the Digital Database for Screening Mammography (DDSM), 1016 are used for training and validation, and 29 are used for testing. All three results (Hybrid, KM and FCM) are compared against the results by the expert Radiologist. The results indicate that, among various segmentation techniques, the proposed hybrid approach achieves the highest accuracy (90%), Matthew's correlation coefficient (0.79), Jaccard index (0.73) and the Dice similarity coefficient (0.84). Other performance evaluation techniques such as; precision, sensitivity, specificity, false\u2010positive rate, false discovery rate, negative predictive value and false\u2010negative rate also show the superior performance of the proposed hybrid approach. Statistical analysis (Mann\u2013Whitney U test, T\u2010test, Chi\u2010square test, Kolmogorov\u2013Smirnov test and Wilcoxon test), graphical analysis (Regression and Bland\u2013Altman plots) and receiver operating characteristic curve further demonstrate the stability and consistency of the results.",
        "publication_year": "2021",
        "authors": [
            "Abhijit Roy",
            "B. Singh",
            "Sumit K. Banchhor",
            "K. Verma"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3",
        "reference_count": "71",
        "references": [
            "/paper/A-Hybrid-Workflow-of-Residual-Convolutional-Encoder-Al-Tam-Al-Hejri/e8cf7af81715aef98f2aba32efdaf69d9c64cac1",
            "/paper/ETECADx%3A-Ensemble-Self-Attention-Transformer-for-Al-Hejri-Al-Tam/dc9e589f4c576a963d5cd2cc57c7378a8ec91178",
            "/paper/Dengesiz-Veri-K%C3%BCmelerinin-S%C4%B1n%C4%B1fland%C4%B1r%C4%B1lmas%C4%B1nda-Alan-Aydemir/5df81cffdc97554c0bfc326f601fd4b0ab694600",
            "/paper/Detection-and-classification-of-normal-and-abnormal-Suresh-Rao/0e0de80f4ff66cd3aef5ff05fd30406fb85da0ea",
            "/paper/Representation-learning-for-mammography-mass-lesion-Arevalo-Gonz%C3%A1lez/a4360f362168a6107e1014b7fc61bed32038fc70",
            "/paper/Fuzzy-C-means-and-region-growing-based-of-tumor-Sadad-Munir/301fee6989cc75f84343836cab5a25691d58dc5c",
            "/paper/Breast-tumor-segmentation-and-shape-classification-Singh-Rashwan/1220fbc61ec47d5978d42c8b5a48b7a33b3c8374",
            "/paper/Advanced-image-segmentation-techniques-for-accurate-Nandhitha/7b3a3e1bd771b7f1ecf0ad0be49f0aed1a358049",
            "/paper/Breast-image-pre-processing-for-mammographic-tissue-He-Hogg/0650a2e55a41e154bb43f75ac2cf68d99fefcdda",
            "/paper/A-novel-region-growing-segmentation-algorithm-for-Senthilkumar-Umamaheswari/21b694ae64f12b5a372e9a48ce0896772037574e",
            "/paper/Automatically-density-based-breast-segmentation-for-Elmoufidi-Fahssi/fc2ee21b2c625b2f480dcc6bc54bbc60c9a48d54",
            "/paper/Designing-an-Algorithm-for-Cancerous-Tissue-Using-Rezaee-Haddadnia/f08551744f15848eecd645717c0071cd3839db13",
            "/paper/Breast-mass-lesion-classification-in-mammograms-by-Jiang-Liu/46ad856e44cd3b5314ebb785789a9d1b898c137c"
        ]
    },
    {
        "id": "d29cfbc42f0e77f546ae3a437432c9234aa48aab",
        "title": "Improving Breast Cancer Detection and Diagnosis through Semantic Segmentation Using the Unet3+ Deep Learning Framework",
        "abstract": "An advanced semantic segmentation method and a deep convolutional neural network are used to identify the Breast Imaging Reporting and Data System (BI-RADS) lexicon for breast ultrasound images to provide a more accurate and objective diagnosis of breast cancer, leading to improved patient outcomes. We present an analysis and evaluation of breast cancer detection and diagnosis using segmentation models. We used an advanced semantic segmentation method and a deep convolutional neural network to identify the Breast Imaging Reporting and Data System (BI-RADS) lexicon for breast ultrasound images. To improve the segmentation results, we used six models to analyse 309 patients, including 151 benign and 158 malignant tumour images. We compared the Unet3+ architecture with several other models, such as FCN, Unet, SegNet, DeeplabV3+ and pspNet. The Unet3+ model is a state-of-the-art, semantic segmentation architecture that showed optimal performance with an average accuracy of 82.53% and an average intersection over union (IU) of 52.57%. The weighted IU was found to be 89.14% with a global accuracy of 90.99%. The application of these types of segmentation models to the detection and diagnosis of breast cancer provides remarkable results. Our proposed method has the potential to provide a more accurate and objective diagnosis of breast cancer, leading to improved patient outcomes.",
        "publication_year": "2023",
        "authors": [
            "Taukir Alam",
            "W. Shia",
            "F. Hsu",
            "Taimoor Hassan"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "29",
        "references": [
            "/paper/Incorporating-the-Breast-Imaging-Reporting-and-Data-Hsieh-Hsu/e941511c42e749153fdec4c738483915c0967906",
            "/paper/Breast-cancer%3A-One-stage-automated-detection%2C-and-Soulami-Kaabouch/ba4a787088f7e1fc278e45bc1663d402c5922d4e",
            "/paper/Segmentation-and-classification-of-breast-cancer-Ramesh-Sasikala/8e75f635dd7578926aa7ae19ff29a8fc5c3911ae",
            "/paper/Fully-automatic-tumor-segmentation-of-breast-images-Zhang-Liao/6a12d6f8f7b7f29ffd8abbe442ec86fc7a96f4c8",
            "/paper/Dilated-Semantic-Segmentation-for-Breast-Ultrasonic-Irfan-Almazroi/d9ce59102f97cca7826bd5ea4dc066ae9da57116",
            "/paper/Breast-Cancer-Segmentation-Methods%3A-Current-Status-Michael-Ma/084ad84604d1afb914a92986bbaf8db326433112",
            "/paper/Fully-Convolutional-DenseNet-with-Multiscale-for-Hai-Qiao/00a4e3f469ff6e1e90d87e4c6aca7210266f6f84",
            "/paper/Benign-and-malignant-breast-cancer-segmentation-Punitha-Amuthan/8133e66c9c03095ef605090e6a72b752dc774d92",
            "/paper/Classification-of-malignant-tumours-in-breast-using-Shia-Lin/0c3b4750353a73c2d72dd636c20aa01d1ee9d05e",
            "/paper/Segmentation-and-recognition-of-breast-ultrasound-Guo-Duan/1cbba75f6c77c1650ae175c690cec86506e7cd9d"
        ]
    },
    {
        "id": "2af1310dd611857d8bd1b1374695fc9c8913a4c9",
        "title": "Classifier Based Breast Cancer Segmentation",
        "abstract": "The ROIs obtained using the proposed classifier-based segmentation algorithm was compared with the ground truth annotated by the radiologists and the performance of the proposed algorithm was evaluated. Breast cancer occurs as a result of erratic growth and proliferation cells that originate in the breast. In this paper, the classifiers were used to identify the abnormalities on mammograms to get the region of interest (ROI). Before classifier based segmentation, noise, pectoral muscles, and tags were removed for a successful segmentation process. Then the proposed approach extracted the brightest regions using modified k-means. From the extracted brightest regions, shape and texture features were extracted and given to classifiers (KNN and SVM) and marked as ROI only those non-overlapping abnormal regions. The ROIs obtained using the proposed classifier-based segmentation algorithm was compared with the ground truth annotated by the radiologists. The datasets used to evaluate the performance of the proposed algorithm was public (MIAS) and local datasets (BGH and DADC).",
        "publication_year": "2020",
        "authors": [
            "Samuel Rahimeto Kebede",
            "Taye Girma Debelee",
            "F. Schwenker",
            "Dereje Yohannes"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "10",
        "reference_count": "34",
        "references": [
            "/paper/A-Survey-of-Brain-Tumor-Segmentation-and-Algorithms-Biratu-Schwenker/3ade51abf05fd744765837e7ab6cade384b2e6c4",
            "/paper/Enhanced-Region-Growing-for-Brain-Tumor-MR-Image-Biratu-Schwenker/7ba7121abdfee8442c3486e003a964e57562634c",
            "/paper/Deep-Learning-in-Selected-Cancers%E2%80%99-Image-Analysis%E2%80%94A-Debelee-Kebede/abfb457caba5314466652825260c12da01a41e0f",
            "/paper/An-Enhanced-Approach-on-Brain-Tumor-Segmentation-by-Raj-Malik/fbff2d0638f7edfd183ba300fc7285ab842a4437",
            "/paper/TAGU-Net%3A-Transformer-Convolution-Hybrid-Based-With-Huang-Liu/806059567925c1e0601c01aba4f0b393f4cd76be",
            "/paper/Coffee-disease-detection-using-a-robust-HSV-and-for-Waldamichael-Debelee/c00b8160417803259abd1f9e0c66bfdf6db15056",
            "/paper/A-Hybrid-Machine-Learning-Model-Based-on-Global-and-Rufo-Debelee/13b7ad7ea8b47da9443475fc3c8f62e6484eeb1d",
            "/paper/WBM-DLNets%3A-Wrapper-Based-Metaheuristic-Deep-for-Ali-Hussain/5a10f0358294b3d4c378ccb2c123c93bd2fa0dc1",
            "/paper/SSRNet%3A-In-Field-Counting-Wheat-Ears-Using-Neural-Wang-Zhang/6e4b4c79f2eb1027aaf565c9dac3b1b54da157ca",
            "/paper/Diagnosis-of-Diabetes-Mellitus-Using-Gradient-Rufo-Debelee/2834aaef5293959eba7108e222d91708c2759ced",
            "/paper/Classification-of-Mammograms-Using-Convolutional-Debelee-Amirian/156733fbf757d4e8a333537518c8961163c4fbf7",
            "/paper/Classification-of-Mammograms-Using-Texture-and-CNN-Debelee-Gebreselasie/4cf3911b8613a47c82bd58d2597822d5de70606d",
            "/paper/Fully-automated-computer-aided-diagnosis-system-for-Mabrouk-Afify/fa967eac78f47b283de80399a28f5d7c3ca09f20",
            "/paper/Detection-of-Masses-in-Digital-Mammograms-using-and-Martins-Junior/0837f79a0efa2180c7bd35a6792c0e3a3e8cd258",
            "/paper/A-Hybrid-Scheme-for-Breast-Cancer-Detection-using-Chowdhary-Acharjya/c55550ff224ff080d95d93200aea8838267fc21c",
            "/paper/Detecting-and-classifying-lesions-in-mammograms-Ribli-Horv%C3%A1th/3978804d0824c3bf1e17a1d0cd2e9734060fa058",
            "/paper/A-Region-Growing-Segmentation-for-Detection-of-in-Deshpande-Narote/6325f450237d437e2072ef4939da023b08a6f2f1",
            "/paper/Development-of-Features-and-Feature-Reduction-for-Beura/b046bdd97715dfcd7eeaadb709fe081d4d72d8a3",
            "/paper/Decrease-in-False-Assumption-for-Detection-Using-Chowdhary-Sai/63b6557f0e49ae79574e6a62715d64264e2f8013",
            "/paper/Survey-of-deep-learning-in-breast-cancer-image-Debelee-Schwenker/ea5b27748ee154c978534505a177f280d87d52d4"
        ]
    },
    {
        "id": "426573514d4d64451ca34aa5e794d56fb1d98d69",
        "title": "Preparation and characterization of hafnium-zirconium oxide ceramics as a CMOS compatible material for non-volatile memories",
        "abstract": "HfxZr1\u2212xO2 (HZO) ceramics with x\u2009=\u20090.25, 0.50 and 0.75 were prepared by conventional solid-state reaction technique. Structural studies of the synthesized stoichiometric compounds were done by X-ray diffraction. The microstructure/morphology and composition of ceramics were obtained with field emission-scanning electron microscopy and energy dispersive X-ray, respectively. Impedance spectroscopic studies were done to study the temperature-dependent and frequency-dependent dielectric properties. The dielectric constants at 10 kHz of HfxZr1\u2212xO2 ceramics were increased from 17 to 40 with Hf varying from 0.25 to 0.75 at ambient conditions. Ferroelectric studies were also done on these compositions with the help of a polarization-electric field plot and are discussed.",
        "publication_year": "2023",
        "authors": [
            "Urvashi Sharma",
            "C. Singh",
            "Vishnu M Varma",
            "G. Santhosh Kumar",
            "S. Mishra",
            "Ajay Kumar",
            "Reji Thomas"
        ],
        "related_topics": [
            "Materials Science"
        ],
        "citation_count": 0,
        "reference_count": "32",
        "references": [
            "/paper/Ferroelectric-Zr0.5Hf0.5O2-thin-films-for-memory-M%C3%BCller-B%C3%B6scke/ba2f7afcf11015cccd094771ea5015a3405788c0",
            "/paper/Tunable-dielectric-properties-of-sol%E2%80%93gel-derived-Bhasker-Sharma/c8f1a55222c3bb6d8a1038c5e837ef7bb07e8239",
            "/paper/Ferroelectric-ceramics-%3A-History-and-technology-Haertling/2d9ab64500030e78c249f329855d3418b100459c",
            "/paper/Study-on-the-degradation-mechanism-of-the-of-thin-Park-Kim/126d6c47da0b3bc4c07b5715b786e84441deaa19",
            "/paper/Effect-of-film-thickness-on-the-ferroelectric-and-Kim-Mohan/febc0ff35caa44a3674f17825d21bc8b40f84abe",
            "/paper/Ferroelectric-Hf0.5Zr0.5O2-Thin-Films%3A-A-Review-of-Kim-Mohan/c4b81dbbe80da09b7779d5fce6f267b1be2b8376",
            "/paper/The-effects-of-crystallographic-orientation-and-of-Park-Kim/9fb3cb299e1349bf9b18d4dd6d07cf58ad647338",
            "/paper/Evolution-of-phases-and-ferroelectric-properties-of-Park-Kim/bacdc5c73886ea571276fdfed9fdbd6e08a783a2",
            "/paper/Preparation-and-Characterization-of-Sol%E2%80%94Gel%E2%80%90Derived-Fan-Park/8206eeea1b71e6d8afb633082d953aed55d27e36",
            "/paper/Improved-Ferroelectric-Switching-Endurance-of-Thin-Chernikova-Kozodaev/6be7188841668e38a1a000e2db92bea3f9eacd9d"
        ]
    },
    {
        "id": "b6ae09893bd6938e5ce87e78a4b61e3cb44946f7",
        "title": "Water atom search algorithm-based deep recurrent neural network for the big data classification based on spark architecture",
        "abstract": "The proposed WASO-based DeepRNN method effectively classifies big data using reduced dimension features to produce satisfactory results, and is compared with Neural Network (NN), Support Vector Machine (SVM), Edited Nearest Neighbor for Big Data (ENN-BD), Speed-up Dendritic Cell Algorithm (Sp-DCA), Compact Fuzzy Models in Big Data The innovation of big data has an intense impact on data context analytics. The big data processing platforms have gained immense popularity in evaluating big data as they offer low latency needed for Big Data applications. This paper introduces a novel method for big data classification using spark architecture that follows master\u2013slave nodes. The input data is initially partitioned by data griding in the master node using Black Hole Entropy Fuzzy Clustering (BHEFC). Then, the feature selection is executed for each slave node based on Renyi entropy for choosing better features for further processing. Finally, the features selected from each slave node are concatenated together to form the feature vector. Consequently, obtained features from the slave node are passed to the classification module in the master node for performing the classification of big data. In this case, classification is carried out using the Deep Recurrent Neural network (DeepRNN), tuned by the novel Water Atom Search Optimization (WASO). The WASO is newly designed by integrating the Water Wave Optimization (WWO) algorithm and Atom Search Optimization (ASO) characteristics. Thus, the proposed WASO-based DeepRNN method effectively classifies big data using reduced dimension features to produce satisfactory results. Therefore, the output of developed WASO enabled DeepRNN is employed for big data classification. The proposed method is compared with Neural Network (NN), Support Vector Machine (SVM), Edited Nearest Neighbor for Big Data (ENN-BD), Speed-up Dendritic Cell Algorithm (Sp-DCA), Compact Fuzzy Models in Big Data (CFM-BD), and DeepRNN. The proposed method obtained improved results with a high specificity of 0.979, accuracy of 0.951, sensitivity of 0.963, and precision of 0.988 based on the skin disease dataset.",
        "publication_year": "2022",
        "authors": [
            "Murali Dabbu",
            "Loheswaran Karuppusamy",
            "Dileep Pulugu",
            "Subba Ramaiah Vootla",
            "Venkateswar Reddy Reddyvari"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3",
        "reference_count": "41",
        "references": [
            "/paper/Performance-analysis-of-a-cloud-based-network-with-Fowdur-Babooram/e79e769d03826b9f5ae4917924b00d53e27873e5",
            "/paper/Clustering-Large-Scale-Data-using-an-Incremental-Fasanghari-Bahrami/d3e1fcf8d586c59b34001bb1e31c3907ba6457f8",
            "/paper/Building-hierarchical-class-structures-for-extreme-Huang-Wang/d32609be94fa74a5467e21fa74387789782fddce",
            "/paper/Big-Data%3A-A-Parallel-Particle-Swarm-Neural-Network-Cao-Cui/e88b677cbb566652f7544289b7ff094ac386a950",
            "/paper/Social-big-data%3A-Recent-achievements-and-new-Orgaz-Jung/32bebd75012f43840ada24c6ab3c917d6dff99bf",
            "/paper/A-Distributed-Fuzzy-Associative-Classifier-for-Big-Segatori-Bechini/b2c7beb2594772a391a5997c211334d8cf933af7",
            "/paper/Enabling-Smart-Data%3A-Noise-filtering-in-Big-Data-Garc%C3%ADa-Gil-Luengo/761e95d70e02f3598ed70d757844367c911abff9",
            "/paper/Big-Data%3A-The-New-Challenges-in-Data-Mining-Jadhav/fce2a7b643cf197ec552d128e41acdc4b0e4b9c6",
            "/paper/A-scalable-and-distributed-dendritic-cell-algorithm-Dagdia/2ef68d326deb5d549366065e9b71783ff468c931",
            "/paper/A-dynamic-data-classification-techniques-and-tools-Rani-Priyanka/d727cea02ef38b93e6c2d1c63dfb2fbc6e2aba06",
            "/paper/A-mapreduce-fuzzy-techniques-of-big-data-Hegazy-Safwat/87f9c68c232cfcd2f9c07ec3004e2a5327ce0359",
            "/paper/Big-data-classification%3A-problems-and-challenges-in-Suthaharan/1be4e7a87c509d8647144f9e8fa66756fd289111",
            "/paper/CFM-BD%3A-A-Distributed-Rule-Induction-Algorithm-for-Elkano-Sanz/b48d2177e2636d86be8828620ce3e3fc57d2d80d"
        ]
    },
    {
        "id": "591dd6573403679ba82c0d4b12d058ae74af942c",
        "title": "Color Consistency Correction Based on Remapping Optimization for Image Stitching",
        "abstract": "This paper proposes an effective color correction method which is feasible to optimize the color consistency across images and guarantee the imaging quality of individual image meanwhile, formulated as a convex quadratic programming problem which provides the global optimal solution efficiently. Color consistency correction is a challenging problem in image stitching, because it matters several factors, including tone, contrast and fidelity, to present a natural appearance. In this paper, we propose an effective color correction method which is feasible to optimize the color consistency across images and guarantee the imaging quality of individual image meanwhile. Our method first apply well-directed alteration detection algorithms to find coherent-content regions in inter-image overlaps where reliable color correspondences are extracted. Then, we parameterize the color remapping curve as transform model, and express the constraints of color consistency, contrast and gradient in an uniform energy function. It can be formulated as a convex quadratic programming problem which provides the global optimal solution efficiently. Our method has a good performance in color consistency and suffers no pixel saturation or tonal dimming. Experimental results of representative datasets demonstrate the superiority of our method over state-of-the-art algorithms.",
        "publication_year": "2017",
        "authors": [
            "Menghan Xia",
            "Jian Yao",
            "Renping Xie",
            "Mi Zhang",
            "Jinsheng Xiao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "19",
        "reference_count": "32",
        "references": [
            "/paper/A-closed-form-solution-for-multi-view-color-with-Xia-Yao/84a259fcb9268c82403a76e6db897d67e84c572d",
            "/paper/Jointly-optimizing-global-and-local-color-for-image-Li-Menghan/cc752b73595183142f4d6b839fb2af06febbd6ee",
            "/paper/Generalized-Content-Preserving-Warps-for-Image-Chen-Tu/e10cb56a560dfe0bbe3ae95ec62b4437a21f3999",
            "/paper/Efficient-Global-Color%2C-Luminance%2C-and-Contrast-for-Hong-Xu/0a9eefbcfbd20a326c0ee539a871a7205a3aefba",
            "/paper/Color-Matching-Images-With-Unknown-Non-Linear-Rodr%C3%ADguez-Vazquez-Corral/6b6760b21c03cff3c7617598dd07e216419984c0",
            "/paper/Robust-global-and-local-color-matching-in-content-Dudek-Croci/c546c68c496f3cc7ae8f4338623bad376989a1bf",
            "/paper/High-Quality-Light-Field-Extraction-and-for-Raw-Matysiak-Grogan/4845f9739f42a141a2187f46de022c69512319ca",
            "/paper/Patch-Based-Colour-Transfer-with-Optimal-Transport-Alghamdi-Grogan/5220c58dd3ec7b204db07c3f92803429fa2b6c2d",
            "/paper/Patch-based-Colour-Transfer-using-SIFT-Flow-Alghamdi-Dahyot/0096fc5e48ad6dc5dcd485688119c4851252990d",
            "/paper/Stable-stitching-method-for-stereoscopic-panoramic-Tan-Zhang/14e4040853f7aa28a27caab4279f561e9c961910",
            "/paper/Manifold-alignment-based-color-transfer-for-image-Qian-Liao/fffe1da7d986cd782537118f697f2c3018b6f793",
            "/paper/Performance-evaluation-of-color-correction-for-and-Xu-Mulligan/2d27bc174c30444c5d0939defbcda734af824e05",
            "/paper/Corruptive-Artifacts-Suppression-for-Example-Based-Su-Zeng/702a6d1168a6601086817c72e6fb0f2bf2f2bbba",
            "/paper/A-Unified-Framework-for-Street-View-Panorama-Li-Yao/739417eeff27bef92f514a9309289f489ee6db92",
            "/paper/Efficient-and-Robust-Color-Consistency-for-Photo-Park-Tai/8aef04d7118798dc83b66ee75d790ae8429f81c9",
            "/paper/Local-color-transfer-via-probabilistic-segmentation-Tai-Jia/6f1d7d584b39b423151488832b98880c6c7391cd",
            "/paper/Non-rigid-dense-correspondence-with-applications-HaCohen-Shechtman/fec3714f5a2d7d4cd8c8e2e5ba83a162fbefcb38",
            "/paper/Color-transfer-based-on-multiscale-gradient-aware-Su-Deng/24dccf31b0347d138cdca66d71d9cb264e7498b1",
            "/paper/Color-Transfer-Using-Probabilistic-Moving-Least-Hwang-Lee/54e8f504c3fc6b8e8e27c9a9cd7285698272c81d",
            "/paper/Automatic-Panoramic-Image-Stitching-using-Invariant-Brown-Lowe/bfd25e5360414e39eedd88c27da22f3d7701bb0b"
        ]
    },
    {
        "id": "d3d4c2ac5db37e54800615454cfa098847059c13",
        "title": "Multi-layer Rotation Memory Model-based correlation filter for visual tracking",
        "abstract": "A Multi-layer Rotation Memory Model-based Correlation Filter (MRMCF) for visual tracking that realizes the dynamic updating of classifier parameters in the correlation filter when the object is occluded or there is similar interference. Object tracking technology is of great significance in laser image processing. However, occlusion or similar interference during visual object tracking may reduce the tracking precision or even cause tracking failure. Aiming at this issue, we propose a Multi-layer Rotation Memory Model-based Correlation Filter (MRMCF) for visual trackingin this paper. First, we establish a Multi-layer Rotation Memory (MRM) model, in which a set of three rotating concentric rings is used to simulate the three memory spacesand their updating processsimulate the memory spaces. Then we introduce the MRM model into the correlation filter tracking framework, which realizes realizing the dynamic updating of classifier parametersin the correlation filter. When the object is occluded or there is similar interference, the proposed tracker can use the Pre-occ classifier parameters stored in the memory spaces in the MRM model MRM memory spaces to retarget the object, thereby reducing the impact of these factors. The experimental results on the OTB50 dataset show that compared with trackers such as CNN-SVM, MEEM, Struck, etc., the proposed tracker achieves higher accuracy and success rate.",
        "publication_year": "2022",
        "authors": [
            "Yufei Zhao",
            "Yong-Liang Song",
            "Guoqi Li",
            "Lei Deng",
            "Yashuo Bai",
            "Xiyan Wu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "46",
        "references": [
            "/paper/Learning-Dynamic-Memory-Networks-for-Object-Yang-Chan/26c50d272883fc8f72656526f915bb283f772b27",
            "/paper/Adaptive-Correlation-Filters-with-Long-Term-and-for-Ma-Huang/7a078987c929b1aef72f4ca283dddef187c48132",
            "/paper/Visual-object-tracking-using-adaptive-correlation-Bolme-Beveridge/70c3c9b9a40ca55264e454586dca2a6cf416f6e0",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Visual-tracking-decomposition-Kwon-Lee/29e1e20323f7cb6c15c6acf5cc6573a2f84e6478",
            "/paper/Residual-LSTM-Attention-Network-for-Object-Tracking-Kim-Park/b195dbf4e507c4616ef549b6061fca45ec663106",
            "/paper/Long-term-correlation-tracking-Ma-Yang/754504cf01ef3846259783e748b1d3ea52fa2c81",
            "/paper/Recurrent-Filter-Learning-for-Visual-Tracking-Yang-Chan/db39754bde43c5555d7086261d1a6fd55af7de06",
            "/paper/Fast-Visual-Tracking-via-Dense-Spatio-temporal-Zhang-Zhang/7069a994c150b0228c4e471ca48ed55d7646bc62",
            "/paper/Memory-based-Particle-Filter-for-face-pose-tracking-Mikami-Otsuka/092a9a915a9ee19ef66e0110a2b82f6caef8cf23"
        ]
    },
    {
        "id": "43cae64a7f0b0942b0409fd4ef4009b0a07a8e5f",
        "title": "A stable long-term object tracking method with re-detection strategy",
        "abstract": "Semantic Scholar extracted view of \"A stable long-term object tracking method with re-detection strategy\" by Tao Li et al.",
        "publication_year": "2019",
        "authors": [
            "Tao Li",
            "Sanyuan Zhao",
            "Qinghao Meng",
            "Yufeng Chen",
            "Jianbing Shen"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "9",
        "reference_count": "37",
        "references": [
            "/paper/Long-term-target-tracking-combined-with-Wang-Yang/4be712d3c72940f35d43a3e9e6bdaa323b915e59",
            "/paper/An-Optimal-Long-Term-Aerial-Infrared-Object-With-Wang-Zhang/4415ab6adf726075615c34864309c1f1efc5dcd4",
            "/paper/Long-term-tracking-with-transformer-and-template-Zhang-Peng/4bb2df1e5dc09ec7f0e50b5ba304a6e51943c72f",
            "/paper/Aerial-Infrared-Object-Tracking-via-an-improved-and-Wang-Zhang/518a93b941b4e0687ea14179f568e106a205a5f6",
            "/paper/Remote-Sensing-Object-Tracking-With-Deep-Learning-Cui-Hou/6d13bb2947b9225d755ca92bde563a9d830e8abd",
            "/paper/FPSN-FNCC%3A-an-accurate-and-fast-motion-tracking-in-He-Shen/f3e1655b0d03736b534ad3370b5a81749ed0fba7",
            "/paper/CGAN-TM%3A-A-Novel-Domain-to-Domain-Transferring-for-Tang-Xi/450df4347f77f4b5bb74fe108c0f8dcec937791d",
            "/paper/Data-driven-two-layer-visual-dictionary-structure-Yu-Yu/0c938eae24fd963bedb58a9665909934aaa91ab1",
            "/paper/Error-tolerant-approximate-graph-matching-utilizing-Dwivedi-Singh/bf904b3c20261641ac122867998ef1556d51848a",
            "/paper/Occlusion-Aware-Real-Time-Object-Tracking-Dong-Shen/72577214469716cb3d41bc00f23c300bcee7a7de",
            "/paper/Tracking-Learning-Detection-Kalal/c63a34ac6a4e049118070e707ca7679fbb132d33",
            "/paper/Reliable-Patch-Trackers%3A-Robust-visual-tracking-by-Li-Zhu/a9448de3265ed9ecdc11c79273aee92daa31f79a",
            "/paper/Long-term-correlation-tracking-Ma-Yang/754504cf01ef3846259783e748b1d3ea52fa2c81",
            "/paper/Beyond-Local-Search%3A-Tracking-Objects-Everywhere-Zhu-Porikli/d20d7d3490fd970992b3631048c75a8c5fe2e4e3",
            "/paper/Fast-Online-Tracking-With-Detection-Refinement-Shen-Yu/1922082de84afb69c966a7628e86012342b475fd",
            "/paper/In-defense-of-color-based-model-free-tracking-Possegger-Mauthner/48fe90ffcfa90170de68c5ef504a5bc2b70ccfad",
            "/paper/Large-Margin-Object-Tracking-with-Circulant-Feature-Wang-Liu/ece7625a346edbc5f6fab541c0c246ec06939121",
            "/paper/A-Scale-Adaptive-Kernel-Correlation-Filter-Tracker-Li-Zhu/0cae491292feccbc9ad1d864cf8b7144923ce6de",
            "/paper/Discriminative-Correlation-Filter-Tracker-with-and-Luke%C5%BEi%C4%8D-Voj%C3%ADr/8cc3651488e02d51fa5ae8d3563b346e9e370f5a"
        ]
    },
    {
        "id": "fb2ea0a5ef40caedfb5a10d929a331662bde78e4",
        "title": "Learning Regression and Verification Networks for Robust Long-term Tracking",
        "abstract": "A new visual tracking algorithm is proposed, which leverages the merits of both template matching approaches and classification models for long-term object detection and tracking and achieves state-of-the-art performance on the OxUvA long- term tracking dataset. This paper proposes a new visual tracking algorithm, which leverages the merits of both template matching approaches and classification models for long-term object detection and tracking. To this end, a regression network is learned offline to detect a set of target candidates through target template matching. To cope with target appearance variations in long-term scenarios, a target-aware feature fusion mechanism is also developed, giving rise to more effective template matching. Meanwhile, a verification network is trained online to better capture target appearance and identify the target from potential candidates. During online update, contaminated training samples can be filtered out through a monitoring module, alleviating model degeneration caused by error accumulation. The regression and verification networks operate in a cascaded manner, which allows tracking to be performed in a coarse-to-fine manner and enforces the discriminative power. To further address the target reappearance issues in long-term tracking, a learning-based switching scheme is proposed, which learns to switch the tracking mode between local and global search based on the tracking results. Extensive evaluations on long-term tracking in the wild have been conducted. We achieve state-of-the-art performance on the OxUvA long-term tracking dataset. Our submission based on the proposed method has also won the 1st place of the long-term tracking challenge in VOT-2018 competition.",
        "publication_year": "2021",
        "authors": [
            "Yunhua Zhang",
            "Lijun Wang",
            "Dong Wang",
            "Jinqing Qi",
            "Huchuan Lu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "8",
        "reference_count": "38",
        "references": [
            "/paper/AR-Long-Term-Tracking-Combining-Multi-Attention-and-Guo-Chen/55d093ac4d40359e64b0208bc7a910858511e1b3",
            "/paper/Long-term-Visual-Tracking%3A-Review-and-Experimental-Liu-Chen/2d4713ce1df60f771b65e900fd02352989df82ef",
            "/paper/A-joint-local-global-search-mechanism-for-long-term-Gao-Zhuang/dc03a79ce543fef5cbbedf85c928dde1e92ad711",
            "/paper/Hierarchical-memory-guided-long-term-tracking-with-Wang-Nie/8146887f58b77f1e2b319fd5a2e7a0b9442b3a1f",
            "/paper/Visual-tracking-in-complex-scenes%3A-A-location-based-Liu-Huang/81f12f67502bf98265fc1205649c1fee31e70345",
            "/paper/Attention-meets-involution-in-visual-tracking-Hou-Luo/6114861593499f4be129b28f04a9276d49556b7b",
            "/paper/Visual-object-tracking%3A-A-survey-Chen-Wang/6f21ad54a1c253481e8d3065133d12e2d34daa6c",
            "/paper/Exploring-reliable-visual-tracking-via-target-He-Chen/d33ff3d7a6d8603a2978f1e7646f1221fd8313a7",
            "/paper/Siamese-Instance-Search-for-Tracking-Tao-Gavves/c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Tracking-Learning-Detection-Kalal/c63a34ac6a4e049118070e707ca7679fbb132d33",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Long-term-correlation-tracking-Ma-Yang/754504cf01ef3846259783e748b1d3ea52fa2c81",
            "/paper/Learning-Multi-domain-Convolutional-Neural-Networks-Nam-Han/2ce63d77eecc35faef85a3b752a314c93a077ac9",
            "/paper/Visual-Tracking-with-Fully-Convolutional-Networks-Wang-Ouyang/bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0",
            "/paper/MUlti-Store-Tracker-(MUSTer)%3A-A-cognitive-inspired-Hong-Chen/e73590fdfd6dab391111bb734053ae24207e2c71",
            "/paper/Correlation-Tracking-via-Joint-Discrimination-and-Sun-Wang/3f0da079ac950a4dfb699c41a90c087000e6ac38",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1"
        ]
    },
    {
        "id": "dbe8a9d31c045aa8429dbbbe31542b44577dec8f",
        "title": "A motion model based on recurrent neural networks for visual object tracking",
        "abstract": "Semantic Scholar extracted view of \"A motion model based on recurrent neural networks for visual object tracking\" by M. Shahbazi et al.",
        "publication_year": "2022",
        "authors": [
            "M. Shahbazi",
            "M. Bayat",
            "Bahram Tarvirdizadeh"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "6",
        "reference_count": "30",
        "references": [
            "/paper/Robust-visual-tracking-based-on-modified-mayfly-Xiao-Wu/0ccb7a661cf884cf8a4845c9b331174afbd756e3",
            "/paper/Visual-inertial-object-tracking%3A-Incorporating-pose-Shahbazi-Mirtajadini/21eec9e779c08460cd8c089d84e6dbf960c71cd5",
            "/paper/Pedestrian-Detection-and-Tracking-System-Based-on-Razzok-Badri/4fbf9a29892b196f8cb461a050417ab9d0113200",
            "/paper/Hierarchical-memory-guided-long-term-tracking-with-Wang-Nie/8146887f58b77f1e2b319fd5a2e7a0b9442b3a1f",
            "/paper/An-end-to-end-convolutional-network-for-estimating-Yang-Zhang/19f43153f3868518a82cc926f1f11d13ddeb2309",
            "/paper/Multi-Camera-Multi-Object-Tracking%3A-A-Review-of-and-Amosa-Sebastian/0db225783cc3bc4d421254040af183a2e517f380",
            "/paper/Two-motion-models-for-improving-video-object-Qiu-Wang/ffbe03ce17bb0ba7be40a7d4139c4d53a8709461",
            "/paper/Know-Your-Surroundings%3A-Exploiting-Scene-for-Object-Bhat-Danelljan/d1e61fa7824709cae37fb59483dd0772e3101c08",
            "/paper/A-Recurrent-Neural-Network-for-Particle-Tracking-in-Spilger-Imle/4f9a936278e2efa5f721528aa66026edb82d804b",
            "/paper/Spatial-Temporal-Relation-Networks-for-Multi-Object-Xu-Cao/c8de9f5482d9ee9c8f5422872d02723df39aad66",
            "/paper/Learning-Discriminative-Model-Prediction-for-Bhat-Danelljan/2c8315ae713b3e27c6e9f291a158134d9c516166",
            "/paper/ATOM%3A-Accurate-Tracking-by-Overlap-Maximization-Danelljan-Bhat/d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "/paper/Occlusion-Handling-in-Tracking-Multiple-People-RNN-Babaee-Li/0bf90fd8239bbaed46df6c525160e999c7e6c5e7",
            "/paper/Visual-Object-Tracking-based-on-Adaptive-Siamese-Kashiani-Shokouhi/4e1145d52830bbf046525edb26bf841e87feeb44",
            "/paper/The-Sixth-Visual-Object-Tracking-VOT2018-Challenge-Kristan-Leonardis/219e9a4527110baf1feb3df20db12064eeafdfb7",
            "/paper/Deep-visual-tracking%3A-Review-and-experimental-Li-Wang/26e2ca763087be09e3799ad294302aa91077942d"
        ]
    },
    {
        "id": "2147cfb8ef36bb938fbae4c9b7c9536ecadac424",
        "title": "Learning Dual-Fused Modality-Aware Representations for RGBD Tracking",
        "abstract": "A novel Dual-fused Modality-aware Tracker (termed DMTracker) which aims to learn informative and discriminative representations of the target objects for robust RGBD tracking and achieves very promising results on challenging RGBD benchmarks. With the development of depth sensors in recent years, RGBD object tracking has received significant attention. Compared with the traditional RGB object tracking, the addition of the depth modality can effectively solve the target and background interference. However, some existing RGBD trackers use the two modalities separately and thus some particularly useful shared information between them is ignored. On the other hand, some methods attempt to fuse the two modalities by treating them equally, resulting in the missing of modality-specific features. To tackle these limitations, we propose a novel Dual-fused Modality-aware Tracker (termed DMTracker) which aims to learn informative and discriminative representations of the target objects for robust RGBD tracking. The first fusion module focuses on extracting the shared information between modalities based on cross-modal attention. The second aims at integrating the RGB-specific and depth-specific information to enhance the fused features. By fusing both the modality-shared and modality-specific information in a modality-aware scheme, our DMTracker can learn discriminative representations in complex tracking scenes. Experiments show that our proposed tracker achieves very promising results on challenging RGBD benchmarks.",
        "publication_year": "2022",
        "authors": [
            "Shang Gao",
            "Jinyu Yang",
            "Zhe Li",
            "Fengcai Zheng",
            "Alevs Leonardis",
            "Jingkuan Song"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "36",
        "references": [
            "/paper/Specificity-preserving-RGB-D-saliency-detection-Zhou-Fu/eea134f36dec9c7d1e7ef7f1f961cc193272a1fc",
            "/paper/Online-RGB-D-tracking-via-An-Zhao/3f02406b9b59d6f966c735953930fede1d751d0d",
            "/paper/Robust-Fusion-of-Color-and-Depth-Data-for-RGB-D-and-Xiao-Stolkin/965b01ffc25e643acd16e91dd74ed0d1879f99ec",
            "/paper/RGBD-Object-Tracking%3A-An-In-depth-Review-Yang-Li/dc9a66e0f329de8054f4ab845331fb7183987418",
            "/paper/DepthTrack%3A-Unveiling-the-Power-of-RGBD-Tracking-Yan-Yang/625aec94369715717158843c3ee288869cbe098f",
            "/paper/End-to-end-Multi-modal-Video-Temporal-Grounding-Chen-Tsai/3fe41ee4ec74a6aa6b4fcf21dbeff38a7a1fc59c",
            "/paper/Context-Aware-Three-Dimensional-Mean-Shift-With-for-Liu-Jing/d10861d377be150b1e03cb942deb8763095de88f",
            "/paper/DAL%3A-A-Deep-Depth-Aware-Long-term-Tracker-Qian-Luke%C5%BEi%C4%8D/62cf427d9c89099139b5e9a79d43b7495367740d",
            "/paper/How-to-Make-an-RGBD-Tracker-Kart-K%C3%A4m%C3%A4r%C3%A4inen/c06ecdf5b149c322db0381adb6b3fd5ccb31a720",
            "/paper/CDTB%3A-A-Color-and-Depth-Visual-Object-Tracking-and-Luke%C5%BEi%C4%8D-Kart/f202feae9ca7b3766e072b6af657beed2236a93c"
        ]
    },
    {
        "id": "4832a96f8c59d85c98175b84594ae24b7b9ed0ca",
        "title": "3D ball trajectory reconstruction from single-camera sports video for free viewpoint virtual replay",
        "abstract": "The proposed scheme incorporates the domain knowledge of court specification and the physical characteristics of ball motion to accomplish the 2D-to-3D inference and enables the free viewpoint virtual replay and enriched visual presentation. Free viewpoint video presentation is a new challenge in multimedia analysis. This paper presents an innovative physics-based scheme to reconstruct the 3D ball trajectory from single-camera volleyball video sequences for free viewpoint virtual replay. The problem of 2D-to-3D inference is arduous due to the loss of 3D information in projection to 2D images. The proposed scheme incorporates the domain knowledge of court specification and the physical characteristics of ball motion to accomplish the 2D-to-3D inference. Motion equations with the parameters are set up to define the 3D trajectories based on physical characteristics. Utilizing the geometric transformation of camera calibration, the 2D ball coordinates extracted over frames are used to approximate the parameters of the 3D motion equations, and finally the 3D ball trajectory can be reconstructed from single-camera sequences. The experiments show promising results. The reconstructed 3D trajectory enables the free viewpoint virtual replay and enriched visual presentation, making game watching a whole new experience.",
        "publication_year": "2011",
        "authors": [
            "Hua-Tsung Chen",
            "Chien-Li Chou",
            "Wen-Jiin Tsai",
            "Suh-Yin Lee"
        ],
        "related_topics": [
            "Education"
        ],
        "citation_count": "8",
        "reference_count": "15",
        "references": [
            "/paper/3D-Trajectory-Reconstruction-of-the-Soccer-Ball-for-Metzler-Pagel/8a8fb6c1956813c128e99f7dd8ae1d57b81a0e46",
            "/paper/Extraction-and-analysis-of-3D-kinematic-parameters-Calandre-P%C3%A9teri/58bbc7a13c8d83c659f4f426b30bdd444ba6bf20",
            "/paper/Video-Based-Reconstruction-of-the-Trajectories-by-Dunnhofer-Zurini/f5825766eb9626cf9d1e1eabf321d01ab607d0a7",
            "/paper/Table-Tennis-ball-kinematic-parameters-estimation-Calandre-P%C3%A9teri/a77bf0816b596461bd02bb3a94a14c79698dea40",
            "/paper/Visualizing-Skiers'-Trajectories-in-Monocular-Dunnhofer-Sordi/45e2b30cc94d2f6c7642d9c183a6d8a827fc99f4",
            "/paper/Ball-Tracking-and-Action-Recognition-of-Soccer-in-Durus/f017ab84412ad64e07818d37be289aa638bd5e69",
            "/paper/3D-Trajectory-Reconstruction-From-Monocular-Vision-Liu-Zhang/f23e6d860b72fa38fe4c697dc5b059b320ec5c85",
            "/paper/Individual-and-group-dynamic-behaviour-patterns-in-G%C4%85siorowski/742446db90ac006e2450f2aa6240da8fce814e0e",
            "/paper/Free-viewpoint-video-of-human-actors-Carranza-Theobalt/89a7dd58e31b4ea69f59351fef7dfd4fa1cc6273",
            "/paper/3D-Video-and-Free-Viewpoint-Video-Technologies%2C-and-Smolic-M%C3%BCller/f5b5474f4dbcac2fb03ae9f7f3942e7dfbc25e6a",
            "/paper/Shot-Classification-of-Basketball-Videos-and-its-in-Tien-Chen/2f5bedd7f5a9381c4a59ec73132ca405bf3bb4b9",
            "/paper/Physics-Based-Ball-Tracking-in-Volleyball-Videos-to-Chen-Chen/e5b671ec95adae8827b567aab8e120ee25b7e814",
            "/paper/Semantic-annotation-of-soccer-videos%3A-automatic-Assfalg-Bertini/cc85119fdac7f6e9b0afa5e5a87983f6bca2f1c9",
            "/paper/Fusion-of-audio-and-motion-information-on-HMM-based-Cheng-Hsu/995b6c1d26c30663c6215f08e5ebbdac66efb549",
            "/paper/A-unified-framework-for-semantic-shot-in-sports-Duan-Xu/961e623fe75ad3a00d51856fd6d67c4e12b81595",
            "/paper/Tracking-Pitches-for-Broadcast-Television-Gu%C3%A9ziec/ac2fc8678968519fe7c153c24f4e878800465bad",
            "/paper/A-baseball-exploration-system-using-spatial-pattern-Chen-Hsiao/289fd68d43ccb066219f452236da0cad7a5ebf94",
            "/paper/Pitch-by-Pitch-Extraction-from-Single-View-Baseball-Chen-Chen/d5f7ebb1807e1487764482249c2585083d2b260f"
        ]
    },
    {
        "id": "8fa3221af7f6d3008e00cc0c459ecbde6f1014da",
        "title": "PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework",
        "abstract": "This work presents an extended latency-aware evaluation benchmark for assessing an any-speed tracker in the online setting and proposes a relative motion factor, empowering PVT++ to generalize to the challenging and complex UAV tracking scenes. Visual object tracking is essential to intelligent robots. Most existing approaches have ignored the online latency that can cause severe performance degradation during real-world processing. Especially for unmanned aerial vehicles (UAVs), where robust tracking is more challenging and onboard computation is limited, the latency issue can be fatal. In this work, we present a simple framework for end-to-end latency-aware tracking, i.e., end-to-end predictive visual tracking (PVT++). Unlike existing solutions that naively append Kalman Filters after trackers, PVT++ can be jointly optimized, so that it takes not only motion information but can also leverage the rich visual knowledge in most pre-trained tracker models for robust prediction. Besides, to bridge the training-evaluation domain gap, we propose a relative motion factor, empowering PVT++ to generalize to the challenging and complex UAV tracking scenes. These careful designs have made the small-capacity lightweight PVT++ a widely effective solution. Additionally, this work presents an extended latency-aware evaluation benchmark for assessing an any-speed tracker in the online setting. Empirical results on a robotic platform from the aerial perspective show that PVT++ can achieve significant performance gain on various trackers and exhibit higher accuracy than prior solutions, largely mitigating the degradation brought by latency. Our code will be made public.",
        "publication_year": "2022",
        "authors": [
            "Bowen Li",
            "Ziyuan Huang",
            "Junjie Ye",
            "Yiming Li",
            "S. Scherer",
            "Hang Zhao",
            "Changhong Fu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "64",
        "references": [
            "/paper/Continuity-Aware-Latent-Interframe-Information-for-Fu-Cai/fca66237e0047d28a588fb6e25c0308c58fa253c",
            "/paper/Predictive-Visual-Tracking%3A-A-New-Benchmark-and-Li-Li/19fe26d0cfe16471f4d2a05053c9f51cf14f0fa8",
            "/paper/PnPNet%3A-End-to-End-Perception-and-Prediction-With-Liang-Yang/4ef52feb1997b1a71f1ca4d49f72a5ce4d43a8b0",
            "/paper/Real-Time-Long-Term-Tracking-With-Liang-Wu/894e4376750b83b63649cc518b121f345ca0df83",
            "/paper/ADTrack%3A-Target-Aware-Dual-Filter-Learning-for-UAV-Li-Fu/689b230b228c7ff5e2bb5d500c5349f54bcc6d3c",
            "/paper/Learning-Discriminative-Model-Prediction-for-Bhat-Danelljan/2c8315ae713b3e27c6e9f291a158134d9c516166",
            "/paper/AutoTrack%3A-Towards-High-Performance-Visual-Tracking-Li-Fu/0619650ae0f698bcc38244a6858cc270df9dfaad",
            "/paper/SiamFC%2B%2B%3A-Towards-Robust-and-Accurate-Visual-with-Xu-Wang/be412c7c7128cf91455233b652d6c94a6001a7c8",
            "/paper/Real-time-Object-Detection-for-Streaming-Perception-Yang-Liu/921901edc30c30554dc78cab724d06ea9097389f",
            "/paper/Keyfilter-Aware-Real-Time-UAV-Object-Tracking-Li-Fu/9269b52994d8af23dc17dfbc225cd25c0902686c",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602"
        ]
    },
    {
        "id": "776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
        "title": "Distractor-aware Siamese Networks for Visual Object Tracking",
        "abstract": "This paper focuses on learning distractor-aware Siamese networks for accurate and long-term tracking, and extends the proposed approach for long- term tracking by introducing a simple yet effective local-to-global search region strategy. Recently, Siamese networks have drawn great attention in visual tracking community because of their balanced accuracy and speed. However, features used in most Siamese tracking approaches can only discriminate foreground from the non-semantic backgrounds. The semantic backgrounds are always considered as distractors, which hinders the robustness of Siamese trackers. In this paper, we focus on learning distractor-aware Siamese networks for accurate and long-term tracking. To this end, features used in traditional Siamese trackers are analyzed at first. We observe that the imbalanced distribution of training data makes the learned features less discriminative. During the off-line training phase, an effective sampling strategy is introduced to control this distribution and make the model focus on the semantic distractors. During inference, a novel distractor-aware module is designed to perform incremental learning, which can effectively transfer the general embedding to the current video domain. In addition, we extend the proposed approach for long-term tracking by introducing a simple yet effective local-to-global search region strategy. Extensive experiments on benchmarks show that our approach significantly outperforms the state-of-the-arts, yielding 9.6% relative gain in VOT2016 dataset and 35.9% relative gain in UAV20L dataset. The proposed tracker can perform at 160 FPS on short-term benchmarks and 110 FPS on long-term benchmarks.",
        "publication_year": "2018",
        "authors": [
            "Zheng Zhu",
            "Qiang Wang",
            "Bo Li",
            "Wei Wu",
            "Junjie Yan",
            "Weiming Hu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "820",
        "reference_count": "40",
        "references": [
            "/paper/Distractor-Aware-Visual-Tracking-by-Online-Siamese-Zha-Wu/03883930b96cf7386c8708bff6c1e6157ac16689",
            "/paper/CSASN%3A-Learning-Complementary-Spatial-Aware-Siamese-She-Yi/71b143b73f2093ebd431e010bda9dd1b953da580",
            "/paper/Distractor-Aware-Deep-Regression-for-Visual-Du-Ding/82038ccb2069b5cf18e88ee1230a79d920e3a344",
            "/paper/Residual-Attention-SiameseRPN-for-Visual-Tracking-Cheng-Li/d5f3e58f3b0c532320e64f398aaee215a6530bea",
            "/paper/SiamDA%3A-distribution-aware-Siamese-network-for-Ji-Shi/0bc6b7c8e3d2efdb6cf35a15192029c078bebbc6",
            "/paper/Deformable-Siamese-Attention-Networks-for-Visual-Yu-Xiong/5eca15a355b2a9a1e80879e850afe49d3c398c53",
            "/paper/SiamET%3A-a-Siamese-based-visual-tracking-network-Zhou-Zhang/cf1559f1dc37fb77705efee31f01b2dda49e5ced",
            "/paper/Visual-Tracking-with-Attentional-Convolutional-Tan-Wei/9189d19304764c819b6e07dfb78f969f1db35628",
            "/paper/SiamMan%3A-Siamese-Motion-aware-Network-for-Visual-Zhou-Wen/2f8853e3b3aa47018fe8c9a04c5e6c3892653612",
            "/paper/SiamAtt%3A-Siamese-attention-network-for-visual-Yang-He/238764091b1aff3ede9ec16c5ca1db168bac7ad3",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/Learning-Attentions%3A-Residual-Attentional-Siamese-Wang-Teng/6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "/paper/Siamese-Instance-Search-for-Tracking-Tao-Gavves/c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "/paper/In-defense-of-color-based-model-free-tracking-Possegger-Mauthner/48fe90ffcfa90170de68c5ef504a5bc2b70ccfad",
            "/paper/DeepTrack%3A-Learning-Discriminative-Feature-Online-Li-Li/084bd219dd239dc4c9a02621a5333d3bc1446566",
            "/paper/Learning-a-Deep-Compact-Image-Representation-for-Wang-Yeung/b2180fc4f5cb46b5b5394487842399c501381d67",
            "/paper/Do-not-Lose-the-Details%3A-Reinforced-Representation-Wang-Zhang/d7bcd9c0d741c5c8b6e73a5905a8af8796300620"
        ]
    },
    {
        "id": "29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
        "title": "Fully-Convolutional Siamese Networks for Object Tracking",
        "abstract": "A basic tracking algorithm is equipped with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video and achieves state-of-the-art performance in multiple benchmarks. The problem of arbitrary object tracking has traditionally been tackled by learning a model of the object\u2019s appearance exclusively online, using as sole training data the video itself. Despite the success of these methods, their online-only approach inherently limits the richness of the model they can learn. Recently, several attempts have been made to exploit the expressive power of deep convolutional networks. However, when the object to track is not known beforehand, it is necessary to perform Stochastic Gradient Descent online to adapt the weights of the network, severely compromising the speed of the system. In this paper we equip a basic tracking algorithm with a novel fully-convolutional Siamese network trained end-to-end on the ILSVRC15 dataset for object detection in video. Our tracker operates at frame-rates beyond real-time and, despite its extreme simplicity, achieves state-of-the-art performance in multiple benchmarks.",
        "publication_year": "2016",
        "authors": [
            "Luca Bertinetto",
            "Jack Valmadre",
            "Jo\u00e3o F. Henriques",
            "A. Vedaldi",
            "Philip H. S. Torr"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3,021",
        "reference_count": "52",
        "references": [
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/Triplet-Loss-in-Siamese-Network-for-Object-Tracking-Dong-Shen/fdb98f5a7015de0956ef8d4e468257dc3079b5e5",
            "/paper/A-Deep-Hyper-Siamese-Network-for-Real-Time-Object-Zhao-Yu/0257c05637f5cd5d215c77a6ad3c87f8e41d15dd",
            "/paper/Self-supervised-Object-Tracking-with-Siamese-Yuan-Wang/3e7cd4547db5558158b19b9fd9e38a0bbeaa3b38",
            "/paper/Learning-Robust-Features-for-Planar-Object-Tracking-Chen-Chen/fa86945501dc387807e4379e32be55604a1e7049",
            "/paper/Towards-real-time-object-tracking-with-deep-Siamese-Yu-Zhao/61f2bdb431f484b26b0ecba43f3803a05714d31e",
            "/paper/SiamMT%3A-Real-Time-Arbitrary-Multi-Object-Tracking-Vaquero-Mucientes/8d596e8b015e959b018326ab4f71010c27dad666",
            "/paper/Dice-Loss-in-Siamese-Network-for-Visual-Object-Wei-Zhang/af2b251e97e6539387b9536942920369d81c8b51",
            "/paper/Unsupervised-Deep-Representation-Learning-for-Wang-Zhou/b29eef7a4b8597ab4fc90579e1326980d6d16b51",
            "/paper/Unsupervised-Deep-Representation-Learning-for-Wang-Zhou/913d4c3fa75dd142f1f9c2c24651f4bf003320ed",
            "/paper/Learning-to-Track-at-100-FPS-with-Deep-Regression-Held-Thrun/5f0850ec47a17f22ba2611a5cb67a30cb02cf306",
            "/paper/Once-for-All%3A-A-Two-Flow-Convolutional-Neural-for-Chen-Tao/3dc60732c1c08165c9d4e7b334ce66e511474bb2",
            "/paper/Visual-Tracking-with-Fully-Convolutional-Networks-Wang-Ouyang/bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0",
            "/paper/Siamese-Instance-Search-for-Tracking-Tao-Gavves/c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "/paper/Hierarchical-Convolutional-Features-for-Visual-Ma-Huang/5c8a6874011640981e4103d120957802fa28f004",
            "/paper/Transferring-Rich-Feature-Hierarchies-for-Robust-Wang-Li/e3c433ab9608d7329f944552ba1721e277a42d74",
            "/paper/First-Step-toward-Model-Free%2C-Anonymous-Object-with-Gan-Guo/9ad8c207d66553d0fa7a7cb57c5e1be12896d1d9",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Visual-tracking-with-online-Multiple-Instance-Babenko-Yang/421bf4eeba623f722bf98340d71e3d229881e92d",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763"
        ]
    },
    {
        "id": "27942ff872056d886977e465df90d2bf93675cf4",
        "title": "Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception",
        "abstract": "The Aria Digital Twin is introduced - an egocentric dataset captured using Aria glasses with extensive object, environment, and human level ground truth and several existing state-of-the-art methods for object detection, segmentation and image translation tasks that demonstrate the usefulness of ADT as a benchmarking dataset are evaluated. We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured using Aria glasses with extensive object, environment, and human level ground truth. This ADT release contains 200 sequences of real-world activities conducted by Aria wearers in two real indoor scenes with 398 object instances (324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two monochrome camera streams, one RGB camera stream, two IMU streams; b) complete sensor calibration; c) ground truth data including continuous 6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye gaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d) photo-realistic synthetic renderings. To the best of our knowledge, there is no existing egocentric dataset with a level of accuracy, photo-realism and comprehensiveness comparable to ADT. By contributing ADT to the research community, our mission is to set a new standard for evaluation in the egocentric machine perception domain, which includes very challenging research problems such as 3D object detection and tracking, scene reconstruction and understanding, sim-to-real learning, human pose prediction - while also inspiring new machine perception tasks for augmented reality (AR) applications. To kick start exploration of the ADT research use cases, we evaluated several existing state-of-the-art methods for object detection, segmentation and image translation tasks that demonstrate the usefulness of ADT as a benchmarking dataset.",
        "publication_year": "2023",
        "authors": [
            "Xiaqing Pan",
            "Nicholas Charron",
            "Yongqian Yang",
            "Scott Peters",
            "Thomas Whelan",
            "Chen Kong",
            "O. Parkhi",
            "Richard A. Newcombe",
            "C. Ren"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "44",
        "references": [
            "/paper/HOI4D%3A-A-4D-Egocentric-Dataset-for-Category-Level-Liu-Liu/2a0b8be3594e8163f9ea4988658223d7c46cfcb3",
            "/paper/Mo2Cap2%3A-Real-time-Mobile-3D-Motion-Capture-with-a-Xu-Chatterjee/c3b4f711f415d5559e0626543c5413e4f6ec71c6",
            "/paper/UnrealEgo%3A-A-New-Dataset-for-Robust-Egocentric-3D-Akada-Wang/5b4fa334de29dfe18b649171d2de57fb399822ce",
            "/paper/EgoGlass%3A-Egocentric-View-Human-Pose-Estimation-an-Zhao-Wei/730d7da60deeb810d463c41c61ee926933830a8d",
            "/paper/SUN-RGB-D%3A-A-RGB-D-scene-understanding-benchmark-Song-Lichtenberg/b73e2d40da901ad3da3813670a51c52803d7af7e",
            "/paper/The-Replica-Dataset%3A-A-Digital-Replica-of-Indoor-Straub-Whelan/60f511eef446120fe59563ee976f948d5e3f9064",
            "/paper/Scaling-Egocentric-Vision%3A-The-EPIC-KITCHENS-Damen-Doughty/fc50c9392fd23b6c88915177c6ae904a498aacea",
            "/paper/BOP%3A-Benchmark-for-6D-Object-Pose-Estimation-Hodan-Michel/57e9ef61cf09274f09a4fe83dbdb38e0da4609d2",
            "/paper/Hypersim%3A-A-Photorealistic-Synthetic-Dataset-for-Roberts-Paczan/84c40cf28afdf84ec6941d92cacd49fed3c7ef9a",
            "/paper/Learning-6D-Object-Pose-Estimation-Using-3D-Object-Brachmann-Krull/25c740bb2a94ad3942e3a6ee329724cb3905e619"
        ]
    },
    {
        "id": "fb058786bbcb2cead98a3ef55b33d2b73b2119fc",
        "title": "Learning Target Candidate Association to Keep Track of What Not to Track",
        "abstract": "This work proposes a training strategy that combines partial annotations with self-supervision to keep track of distractor objects in order to continue tracking the target, and introduces a learned association network to propagate the identities of all target candidates from frame-to-frame. The presence of objects that are confusingly similar to the tracked target, poses a fundamental challenge in appearance-based visual tracking. Such distractor objects are easily misclassified as the target itself, leading to eventual tracking failure. While most methods strive to suppress distractors through more powerful appearance models, we take an alternative approach.We propose to keep track of distractor objects in order to continue tracking the target. To this end, we introduce a learned association network, allowing us to propagate the identities of all target candidates from frame-to-frame. To tackle the problem of lacking ground-truth correspondences between distractor objects in visual tracking, we propose a training strategy that combines partial annotations with self-supervision. We conduct comprehensive experimental validation and analysis of our approach on several challenging datasets. Our tracker sets a new state-of-the-art on six benchmarks, achieving an AUC score of 67.1% on LaSOT [21] and a +5.8% absolute gain on the OxUvA long-term dataset [41]. The code and trained models are available at https://github.com/visionml/pytracking",
        "publication_year": "2021",
        "authors": [
            "Christoph Mayer",
            "Martin Danelljan",
            "D. Paudel",
            "L. Gool"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "74",
        "reference_count": "72",
        "references": [
            "/paper/Target-Aware-Tracking-with-Long-term-Context-He-Zhang/62fc770746f6a283fbc5cfc32275e23bfef82eb7",
            "/paper/Exploring-Motion-Information-for-Distractor-in-Liu-Gao/49aee126088915627a0a83e336c9b5807086e4fe",
            "/paper/Capsule-Based-Regression-Tracking-via-Background-Ma-Wu/0de0e788e7ed72d480c276e7ef89ced656ba90b1",
            "/paper/Visual-Object-Tracking-With-Discriminative-Filters-Javed-Danelljan/0530cbeb847f5e5002d1183c482759dff5f8c439",
            "/paper/Transforming-Model-Prediction-for-Tracking-Mayer-Danelljan/dc47b17250b639d3a89a716c7216ef69b33f9e33",
            "/paper/CoCoLoT%3A-Combining-Complementary-Trackers-in-Visual-Dunnhofer-Micheloni/23409262ddcfc2f66fe999711a1fd9f7c700a1e2",
            "/paper/Part-Aware-Framework-for-Robust-Object-Tracking-Li-Zhao/1cd30c9900f73195f9159183406019bb1561e5f3",
            "/paper/Beyond-SOT%3A-Tracking-Multiple-Generic-Objects-at-Mayer-Danelljan/d8a72d56e4d8ec40f07189e9892ea0f888d7c321",
            "/paper/NeighborTrack%3A-Improving-Single-Object-Tracking-by-Chen-Wang/9cb5f7aae159bd6d4323f2d6d63701194e06206b",
            "/paper/A-General-Data-Augmentation-Strategy-for-Siamese-Pan-Chu/14ae6262520cc3e2d0618a2b88a9f8201eea237d",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Probabilistic-Regression-for-Visual-Tracking-Danelljan-Gool/6b6d31b022b7984a25fa9ee7fef64086ce7c464d",
            "/paper/Learning-regression-and-verification-networks-for-Zhang-Wang/3d372b63020c4d2c9510624f370b50d9f292bcde",
            "/paper/Know-Your-Surroundings%3A-Exploiting-Scene-for-Object-Bhat-Danelljan/d1e61fa7824709cae37fb59483dd0772e3101c08",
            "/paper/Learning-Discriminative-Model-Prediction-for-Bhat-Danelljan/2c8315ae713b3e27c6e9f291a158134d9c516166",
            "/paper/ATOM%3A-Accurate-Tracking-by-Overlap-Maximization-Danelljan-Bhat/d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "/paper/Distractor-Supported-Single-Target-Tracking-in-Xiao-Qiao/ba9975c8cc84a0d27ecaf23de81c76d37d50420a",
            "/paper/STMTrack%3A-Template-free-Visual-Tracking-with-Memory-Fu-Liu/811ffb185bc90ac5d02d6dbfbcdb6173756b52ef",
            "/paper/TLPG-Tracker%3A-Joint-Learning-of-Target-Localization-Li-Zhang/58fc267bf29e8982107a5212e02f35ba4f461310",
            "/paper/SiamFC%2B%2B%3A-Towards-Robust-and-Accurate-Visual-with-Xu-Wang/be412c7c7128cf91455233b652d6c94a6001a7c8"
        ]
    },
    {
        "id": "c55550ff224ff080d95d93200aea8838267fc21c",
        "title": "A Hybrid Scheme for Breast Cancer Detection using Intuitionistic Fuzzy Rough Set Technique",
        "abstract": "This paper hybridizes intuitionistic fuzzy set and rough set in combination with statistical feature extraction techniques and shows the overall accuracy of 98.3% is higher than the accuracy achieved by hybridizing fuzzy rough set model. Diagnosis of cancer is of prime concern in recent years. Medical imaging is used to analyze these diseases. But, these images contain uncertainties due to various factors and thus intelligent techniques are essential to process these uncertainties. This paper hybridizes intuitionistic fuzzy set and rough set in combination with statistical feature extraction techniques. The hybrid scheme starts with image segmentation using intuitionistic fuzzy set to extract the zone of interest and then to enhance the edges surrounding it. Further feature extraction using gray-level co-occurrence matrix is presented. Additionally, rough set is used to engender all minimal reducts and rules. These rules then fed into a classifier to identify different zones of interest and to check whether these points contain decision class value as either cancer or not. The experimental analysis shows the overall accuracy of 98.3% and it is higher than the accuracy achieved by hybridizing fuzzy rough set model.",
        "publication_year": "2016",
        "authors": [
            "C. L. Chowdhary",
            "D. Acharjya"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "57",
        "reference_count": "22",
        "references": [
            "/paper/Breast-Cancer-Detection-Using-Hybrid-Computational-Acharjya-Chowdhary/af35f26ab94eb338eaef852a1191e09f52a85796",
            "/paper/STUDY-ON-LEVEL-SET-SEGMENTATION-BASED-USING-Nemade-Sharma/06b05fa46ea957aa1e16aa809a23828e601d4f49",
            "/paper/An-Efficient-Segmentation-and-Classification-System-Chowdhary-Mittal/3c7cf14cb095c3f8578d5848b7766c03368b6725",
            "/paper/Breast-Cancer-Detection-using-Intuitionistic-Fuzzy-Chowdhary-Acharjya/8ece55a94960504cd809b4058382afa746085911",
            "/paper/Classifier-Based-Breast-Cancer-Segmentation-Kebede-Debelee/2af1310dd611857d8bd1b1374695fc9c8913a4c9",
            "/paper/Clustering-Algorithm-in-Possibilistic-Exponential-Chowdhary-Acharjya/fea0580b5a6dff77fd62436cca0552b22a95486e",
            "/paper/Mammogram-pectoral-muscle-removal-and-using-based-GirijaO-S.Elayidom/d3b3ec90c51a7933b7c849d0477b0d46de507789",
            "/paper/Automated-detection-of-brain-tumor-disease-using-Patil-Hamde/d23fc861c6cc72fb212ff2274d0809288174f778",
            "/paper/Confidence-Level-Aggregation-Operators-Based-on-in-Mahmood-Ahmmad/1accb0651a6df28ce5e7a6ff2e2870f0509f35f1",
            "/paper/Automated-brain-tumor-detection-and-classification-Anantharajan-Gunasekaran/db7c2c922c303ce79d43b4168ca7f4e7ec2afe38",
            "/paper/Fuzzy-rough-sets-hybrid-scheme-for-breast-cancer-Hassanien/5b935d2aa2aa36f67f084b4c1ad285bbc642c064",
            "/paper/A-fuzzy-genetic-approach-to-breast-cancer-diagnosis-Pe%C3%B1a-Reyes-Sipper/e663fa59a758b945ed0e8e620a52022d617a9b03",
            "/paper/Detection-of-Spiculated-Masses-in-Mammograms-Based-Hassanien-Ali/12a2e79340aa653bee50270dac7352ffa79f2c8d",
            "/paper/Improvement-of-Breast-Cancer-Detection-Using-and-in-Pak-Kanan/48249c493f8b1bdf2ebcfbe8967d1e8ee375b1eb",
            "/paper/Breast-Cancer-Detection-with-Reduced-Feature-Set-Mert-Kili%C3%A7/296e4a151a752b434d36aa7e4e9ee532c46536fc",
            "/paper/Advances-in-Optimal-Detection-of-Cancer-by-Image-Mohammadzadeh-Safdari/b7ade135adb326467e9c636cd3d0b8d16218aad5",
            "/paper/Detection-of-breast-masses-in-mammograms-by-density-Mudigonda-Rangayyan/ac0ca4295b0622e370c19716071614b6d2d723f9",
            "/paper/A-Novel-Approach-for-Breast-Cancer-Detection-and-in-Singh-Gupta/2eb766d776ccd9872a0353fbf095e77314b244eb",
            "/paper/Automatic-detection-of-anomalies-in-screening-Kendall-Barnett/82a305ff589fe79d78f98c6a77e9104c9804d288",
            "/paper/Rough-set-approach-for-attribute-reduction-and-rule-Hassanien/871d9863771d0c165011ee2a7469f6294ae98a41"
        ]
    },
    {
        "id": "64aa7896b11ab90ccb4828c2faa6f86cc5f647cd",
        "title": "Automatic Breast Tumor Classification Using a Level Set Method and Feature Extraction in Mammography",
        "abstract": "The results focus on the appropriate efficiency of proposed segmentation and features extraction methods and features related to GLCM are among the best results with the accuracy of 93.37 and sensitivity of 94.18. Breast cancer is one of the leading factors of cancer-related deaths among women, therefore designing Computer Aided Diagnosis (CADx) systems to detect malignant and benign tumors of breast masses is extensively essential. Using a segmentation method and subsequently a proper feature extraction is crucial to obtain an appropriate performance in CADx system. In this paper, the Mammography Imaging Analysis Society (MIAS) data is used in order to detect whether breast masses are malignant or benign. A method is based on level set and with the purpose of segmenting the region of the tumor in mammography for the first time and in following, four additional methods were introduced. Including wavelet transform, Gabor wavelet transform Zernike moments and Gray-Level Co-occurrence Matrix (GLCM) to extract features and each one leads to the extraction of a group of the segmented tumor features. Proper features are selected using P value. Consequently, in order to investigate the efficiency of selected features, each group of features are used within one Multilayer Perceptron (MLP). In this paper, the results focus on the appropriate efficiency of proposed segmentation and features extraction methods. Among these considered features, the features related to GLCM are among the best results with the accuracy of 93.37 and sensitivity of 94.18.",
        "publication_year": "2017",
        "authors": [
            "Soheil Pashoutan",
            "Shahriar Baradaran Shokouhi",
            "Meisam Pashoutan"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "27",
        "references": [
            "/paper/Breast-Cancer-Detection-and-Classification-from-Gurudas-Shaila/a7f3cd1885a0f1548596b288eb6768d818b8463d",
            "/paper/Tumor-Classification-in-Breast-Magnetic-Resonance-Pashoutan-Ayatollahi/f45f232c5d1e096be06ac4217de739b16aefccfd",
            "/paper/Systematic-Review-of-Computing-Approaches-for-Based-Zebari-Ibrahim/9ee9eeed8fc56f600eb3ff07f6e38dff81957ff8",
            "/paper/Comparative-Analysis-of-Segment-Anything-Model-and-Ahmadi-Nia/092e217f44783a0a9d14f9752e679c0aa563fd3d",
            "/paper/Classification-of-benign-and-malignant-breast-based-Rouhi-Jafari/2415fc06de82ab41ad8b9615162247afb02974af",
            "/paper/Performance-evaluation-of-a-region-growing-for-Rabottino-Mencattini/76389ebb7c1496239e66fd663b0e7e43d391bca9",
            "/paper/Classification-of-benign-and-malignant-masses-based-Tahmasbi-Saki/46c409dd878e643271ef63f1817ded8b57abc01e",
            "/paper/Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52",
            "/paper/Feature-Extraction-from-Contours-Shape-for-Tumor-in-Boujelben-Chaabani/40bbb7667b260bad22b0e2cb34562f57735d67f8",
            "/paper/Mass-diagnosis-in-mammography-images-using-novel-Tahmasbi-Saki/474ae46626676d01c7b38328c107b1531b181b46",
            "/paper/Approaches-For-Automated-Detection-And-Of-Masses-In-Rekha-Gayathri/001ec101a06cf8ce254db94a9c6130b5fe43aabe",
            "/paper/Classification-of-benign-and-malignant-patterns-in-Verma-McLeod/2dfb1fd3adfa58a3448251e03b1a5a78239958b3",
            "/paper/A-review-of-computer-aided-diagnosis-of-breast-the-Rangayyan-Ayres/9bf32a68edfea8b8c072bcd3ee0d696687bab403",
            "/paper/Computer-Aided-Analysis-of-Dynamic-Contrast-MRI-of-Gal/33350f776b506e0e8cdee488480d0ce7531282ee"
        ]
    },
    {
        "id": "0295f85e0db918b26ff7e8d1d547f8d98eea7feb",
        "title": "A Survey of Convolutional Neural Network in Breast Cancer",
        "abstract": "A comprehensive review of the diagnosis of breast cancer based on the convolutional neural network (CNN) after reviewing a sea of recent papers and dividing the diagnosis into three different tasks. Problems For people all over the world, cancer is one of the most feared diseases. Cancer is one of the major obstacles to improving life expectancy in countries around the world and one of the biggest causes of death before the age of 70 in 112 countries. Among all kinds of cancers, breast cancer is the most common cancer for women. The data showed that female breast cancer had become one of the most common cancers. Aims A large number of clinical trials have proved that if breast cancer is diagnosed at an early stage, it could give patients more treatment options and improve the treatment effect and survival ability. Based on this situation, there are many diagnostic methods for breast cancer, such as computer-aided diagnosis (CAD). Methods We complete a comprehensive review of the diagnosis of breast cancer based on the convolutional neural network (CNN) after reviewing a sea of recent papers. Firstly, we introduce several different imaging modalities. The structure of CNN is given in the second part. After that, we introduce some public breast cancer data sets. Then, we divide the diagnosis of breast cancer into three different tasks: 1. classification; 2. detection; 3. segmentation. Conclusion Although this diagnosis with CNN has achieved great success, there are still some limitations. (i) There are too few good data sets. A good public breast cancer dataset needs to involve many aspects, such as professional medical knowledge, privacy issues, financial issues, dataset size, and so on. (ii) When the data set is too large, the CNN-based model needs a sea of computation and time to complete the diagnosis. (iii) It is easy to cause overfitting when using small data sets.",
        "publication_year": "2023",
        "authors": [
            "Ziquan Zhu",
            "Shuihua Wang",
            "Yudong Zhang"
        ],
        "related_topics": [
            "Computer Science",
            "Medicine"
        ],
        "citation_count": "2",
        "reference_count": "197",
        "references": [
            "/paper/Neuroevolution-of-Convolutional-Neural-Networks-for-Llaguno-Roque-Barrientos-Mart%C3%ADnez/1732a276e62c63801b46d8f0ee2e62ae7148de22",
            "/paper/Active-contour-and-texture-features-hybrid-model-Rautela-Kumar/57f336a9f0f511459b37b322632ccac41ff6c59b",
            "/paper/Improved-the-detection-and-classification-of-breast-Kousalya-Saranya/00b2c1394bb6195c00fd0fc895aeac26b65e9d5f",
            "/paper/Breast-cancer-detection-in-mammograms-using-neural-Charan-Khan/1500a27efd9832beb0b92c34c5bb4e12009d9197",
            "/paper/Boosting-Breast-Cancer-Detection-Using-Neural-Alanazi-Kamruzzaman/a812ebb6761ff453a02d866e97295932153537e2",
            "/paper/Multi-Class-Breast-Cancer-Classification-using-Deep-Nawaz-Sewissy/1a19f55bc7f5d3d90bd5cb239c0202b3428b6aa2",
            "/paper/Transfer-learning-based-histopathologic-image-for-Deniz-%C5%9Eeng%C3%BCr/f4008deb11f6a2ec3d52dde72be48cd53c898a45",
            "/paper/A-Technical-Review-of-Convolutional-Neural-Breast-Zou-Yu/7d4b1e7c47c02cc62705dce132e187cc53207ff3",
            "/paper/A-Novel-CNN-Inception-V4-Based-Hybrid-Approach-for-Nazir-Khan/bb1beb57009f356e300ed2c83bb2fc4f8198daf3",
            "/paper/Classification-of-Mammogram-Images-Using-Multiscale-Agnes-Anitha/09a81f37797ee12713b5ddec259bf5be1a1cc226",
            "/paper/Using-generative-adversarial-networks-and-transfer-Guan-Loew/7b60621086e42a19354f88fd6bc3af85135a07fd",
            "/paper/A-New-Enhanced-Recurrent-Extreme-Learning-Machine-Agarwal-Sharma/d2638b1f9b86fb7edabc7a5e033bff366e35bb87"
        ]
    },
    {
        "id": "e8cf7af81715aef98f2aba32efdaf69d9c64cac1",
        "title": "A Hybrid Workflow of Residual Convolutional Transformer Encoder for Breast Cancer Classification Using Digital X-ray Mammograms",
        "abstract": "The experimental results reveal that the proposed hybrid AI model could identify benign and malignant breast tissues significantly, which is important for radiologists to recommend further investigation of abnormal mammograms and provide the optimal treatment plan. Breast cancer, which attacks the glandular epithelium of the breast, is the second most common kind of cancer in women after lung cancer, and it affects a significant number of people worldwide. Based on the advantages of Residual Convolutional Network and the Transformer Encoder with Multiple Layer Perceptron (MLP), this study proposes a novel hybrid deep learning Computer-Aided Diagnosis (CAD) system for breast lesions. While the backbone residual deep learning network is employed to create the deep features, the transformer is utilized to classify breast cancer according to the self-attention mechanism. The proposed CAD system has the capability to recognize breast cancer in two scenarios: Scenario A (Binary classification) and Scenario B (Multi-classification). Data collection and preprocessing, patch image creation and splitting, and artificial intelligence-based breast lesion identification are all components of the execution framework that are applied consistently across both cases. The effectiveness of the proposed AI model is compared against three separate deep learning models: a custom CNN, the VGG16, and the ResNet50. Two datasets, CBIS-DDSM and DDSM, are utilized to construct and test the proposed CAD system. Five-fold cross validation of the test data is used to evaluate the accuracy of the performance results. The suggested hybrid CAD system achieves encouraging evaluation results, with overall accuracies of 100% and 95.80% for binary and multiclass prediction challenges, respectively. The experimental results reveal that the proposed hybrid AI model could identify benign and malignant breast tissues significantly, which is important for radiologists to recommend further investigation of abnormal mammograms and provide the optimal treatment plan.",
        "publication_year": "2022",
        "authors": [
            "Riyadh M. Al-Tam",
            "Aymen M. Al-Hejri",
            "S. M. Narangale",
            "N. A. Samee",
            "Noha F. Mahmoud",
            "M. A. Al-masni",
            "M. A. Al-antari"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "57",
        "references": [
            "/paper/ETECADx%3A-Ensemble-Self-Attention-Transformer-for-Al-Hejri-Al-Tam/dc9e589f4c576a963d5cd2cc57c7378a8ec91178",
            "/paper/Analysis-of-Histopathological-Images-for-Early-of-Ahmed-Senan/697b853386ddb894ccf3e45613bced6cb0c63155",
            "/paper/Automatic-Classification-of-Colour-Fundus-Images-on-Shamsan-Senan/4d2e1639f6e9772721ce6fc722f66db2de059ef4",
            "/paper/Blood-Slide-Image-Analysis-to-Classify-WBC-Types-on-Olayah-Senan/a60cd82174d37e4a861f6ac510dbebbf67e0722e",
            "/paper/A-Hybrid-Deep-Transfer-Learning-of-CNN-Based-LR-PCA-Samee-Alhussan/53dbfdb4d3bf8d498586b309a98c048b6da3e422",
            "/paper/Semi-supervised-vision-transformer-with-adaptive-Wang-Jiang/95a8200eaeb9065c7ef7350d61750fb839890692",
            "/paper/A-VGG-attention-vision-transformer-network-for-and-Qu-Lu/e2e93ae3cf54cc677908e473cf3f5ab9390a28be",
            "/paper/Breast-lesions-classifications-of-mammographic-a-Mahmood-Li/36b7f12195b0cf2061925b5456c3672e5f186100",
            "/paper/DenseNet-for-Breast-Tumor-Classification-in-Images-Jim%C3%A9nez-Gaona-Rodr%C3%ADguez-%C3%81lvarez/bb00995b50a6915e01a5db10d9d2645f2e0ed056",
            "/paper/ResNet-SCDA-50-for-Breast-Abnormality-Yu-Kang/5d7cf0d50d5fd373bafef6e3ab554936a54e3cda",
            "/paper/Deep-Learning-Cascaded-Feature-Selection-Framework-Samee-Atteia/db968440e2aa3fec0c4144752651b596faf6d8b9",
            "/paper/Abnormality-Detection-in-Mammography-using-Deep-Xi-Shu/606f48d66487316279f1eaa1cd8341f4bda11409",
            "/paper/Simultaneous-Detection-and-Classification-of-Breast-Al-masni-Al-antari/3cbe19d5e217d1bc27d45dd867a6e185db9970d3",
            "/paper/Evaluation-of-deep-learning-detection-and-towards-Al-antari-Han/f36b3ee8d83ec14808a268183e28f05f4b7b3bc3"
        ]
    },
    {
        "id": "e941511c42e749153fdec4c738483915c0967906",
        "title": "Incorporating the Breast Imaging Reporting and Data System Lexicon with a Fully Convolutional Network for Malignancy Detection on Breast Ultrasound",
        "abstract": "The results suggest that the utilization of a deep learning network in combination with the BI-RADS lexicon can be an important supplemental tool when using ultrasound to diagnose breast malignancy. In this study, we applied semantic segmentation using a fully convolutional deep learning network to identify characteristics of the Breast Imaging Reporting and Data System (BI-RADS) lexicon from breast ultrasound images to facilitate clinical malignancy tumor classification. Among 378 images (204 benign and 174 malignant images) from 189 patients (102 benign breast tumor patients and 87 malignant patients), we identified seven malignant characteristics related to the BI-RADS lexicon in breast ultrasound. The mean accuracy and mean IU of the semantic segmentation were 32.82% and 28.88, respectively. The weighted intersection over union was 85.35%, and the area under the curve was 89.47%, showing better performance than similar semantic segmentation networks, SegNet and U-Net, in the same dataset. Our results suggest that the utilization of a deep learning network in combination with the BI-RADS lexicon can be an important supplemental tool when using ultrasound to diagnose breast malignancy.",
        "publication_year": "2021",
        "authors": [
            "Yung-Hsien Hsieh",
            "F. Hsu",
            "Seng-Tong Dai",
            "Hsin-Ya Huang",
            "Dar-Ren Chen",
            "W. Shia"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "3",
        "reference_count": "24",
        "references": [
            "/paper/Semantic-Segmentation-of-the-Malignant-Breast-and-Shia-Hsu/a887b64738307c68672a10363dd7b95b53a6ebf7",
            "/paper/Improving-Breast-Cancer-Detection-and-Diagnosis-the-Alam-Shia/d29cfbc42f0e77f546ae3a437432c9234aa48aab",
            "/paper/Application-of-Convolution-Neural-Network-Algorithm-Zhang-Tao/06b705f7840fa10f49817dad5a26fee1c87e1b8b",
            "/paper/Classification-of-malignant-tumors-in-breast-using-Shia-Chen/fa9839b38cfd62ba7437463bb2ac631cbd8532fb",
            "/paper/Automatic-classification-of-ultrasound-breast-using-Ciritsis-Rossi/7f4dc6d557e492faf3b919eec73463e70ee939b7",
            "/paper/Classification-of-breast-cancer-in-ultrasound-using-Becker-Mueller/0ef50960ebe7d6760d4ace0d369bc348f8314969",
            "/paper/Effect-of-a-Deep-Learning-Framework-Based-Diagnosis-Choi-Han/2143939013a0fef5269bb4fcb7fe56e01d80f608",
            "/paper/Classification-of-malignant-tumours-in-breast-using-Shia-Lin/0c3b4750353a73c2d72dd636c20aa01d1ee9d05e",
            "/paper/Fully-Convolutional-Network-for-Liver-Segmentation-Ben-Cohen-Diamant/d152ffe125a3f61161a15311f03ef1c5a5ff4a58",
            "/paper/BI-RADS-lexicon-for-US-and-mammography%3A-variability-Lazarus-Mainiero/719c5796fecc646b15cf7c0bf3bebf0435617f64",
            "/paper/U-Net%3A-Convolutional-Networks-for-Biomedical-Image-Ronneberger-Fischer/6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "/paper/Combined-screening-with-ultrasound-and-mammography-Berg-Blume/e0196af80881aadb6e227e432e3dc5aadb91a393",
            "/paper/Breast-imaging-reporting-and-data-system-(BI-RADS).-Liberman-Menell/3dcba561e2ab953d110631dd56d83c9208c9c099"
        ]
    },
    {
        "id": "3ade51abf05fd744765837e7ab6cade384b2e6c4",
        "title": "A Survey of Brain Tumor Segmentation and Classification Algorithms",
        "abstract": "A comprehensive survey of three, recently proposed, major brain tumor segmentation and classification model techniques, namely, region growing, shallow machine learning and deep learning. A brain Magnetic resonance imaging (MRI) scan of a single individual consists of several slices across the 3D anatomical view. Therefore, manual segmentation of brain tumors from magnetic resonance (MR) images is a challenging and time-consuming task. In addition, an automated brain tumor classification from an MRI scan is non-invasive so that it avoids biopsy and make the diagnosis process safer. Since the beginning of this millennia and late nineties, the effort of the research community to come-up with automatic brain tumor segmentation and classification method has been tremendous. As a result, there are ample literature on the area focusing on segmentation using region growing, traditional machine learning and deep learning methods. Similarly, a number of tasks have been performed in the area of brain tumor classification into their respective histological type, and an impressive performance results have been obtained. Considering state of-the-art methods and their performance, the purpose of this paper is to provide a comprehensive survey of three, recently proposed, major brain tumor segmentation and classification model techniques, namely, region growing, shallow machine learning and deep learning. The established works included in this survey also covers technical aspects such as the strengths and weaknesses of different approaches, pre- and post-processing techniques, feature extraction, datasets, and models\u2019 performance evaluation metrics.",
        "publication_year": "2021",
        "authors": [
            "Erena Siyoum Biratu",
            "F. Schwenker",
            "Yehualashet Megersa",
            "Taye Girma Debelee"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "30",
        "reference_count": "173",
        "references": [
            "/paper/Deep-Learning-Based-Segmentation-and-Classification-Ghadi-Salman/12b4de408a2d994b1b924ae7f47ad44a0bc6569a",
            "/paper/Automatic-segmentation-of-brain-tumor-in-magnetic-Cavieres-Tejos/0e49436b29559461fd92c3b0f95f559a7d830478",
            "/paper/Deep-learning-based-brain-tumor-segmentation%3A-a-Liu-Chen/5430c70a14d1272199ddc3b6c21559ba2f3bd479",
            "/paper/Brain-Tumor-Detection-and-Classification-Using-An-Solanki-Singh/e20c6cdd8bec735c0d5793651ce3edd745ccda1b",
            "/paper/The-Automatic-Brain-Tumor-Segmentation-Based-on-MRI-Pongsatitpat-Prathepha/ef7be22cdc45945fd362b45b67bba3267cd86e63",
            "/paper/A-new-brain-tumor-diagnostic-model%3A-Selection-of-Ba%C5%9Faran/94835f519efab7b9f3901f8bab3c72313b1118c8",
            "/paper/Mass-Sphering-Approach-for-Three-Dimensional-of-MRI-Preetha-Vanaja/bdc0bff6d07653535d83990291adda62c5136526",
            "/paper/Geometric-Analysis-Of-Brain-Pathology-Using-MRI-For-Akmakchi-Cevik/7dde2555b37d718034534f2c8e833394ddbfa49c",
            "/paper/Brain-Tumor-Diagnosis-Using-Machine-Learning%2C-and-A-Akinyelu-Zaccagna/25f63514a413a623ed1e3fd1ed720ca5747fc228",
            "/paper/A-Comprehensive-Review-of-Brain-Tumour-Detection-Ramtekkar-Pandey/d0742120f1052e7744cc10f9411b077fdf592ac0",
            "/paper/A-comprehensive-review-on-brain-tumor-segmentation-Rao-Karunakara/0a39a9ec80c0f7245a8f7cbe4217bd22c65543ae",
            "/paper/Brain-Tumor-Segmentation-Using-Convolutional-Neural-Pereira-Pinto/420f3f1078a6d8e0696572c032877079286051c6",
            "/paper/Brain-tumor-segmentation-and-classification-from-of-Tiwari-Srivastava/1a6b7243455dff0db201f11d1cb64d3b1db4ebb4",
            "/paper/Magnetic-Resonance-Image-Segmentation-Based-on-Hao-Li/91e3434211738d4315860feac5355310b577ce75",
            "/paper/Automatic-Segmentation-of-Brain-Tumor-Parts-from-a-Csaholczi-Kov%C3%A1cs/12b8b18ec247218bace1a8180a078fad54fc09a4",
            "/paper/A-Deep-Learning-Approach-for-Brain-Tumor-and-Using-D%C3%ADaz-Pernas-Mart%C3%ADnez-Zarzuela/50988431d1010952877e81d3c0e5c222859a33b6",
            "/paper/Brain-tumor-detection-and-segmentation-using-hybrid-Megersa-Alemu/dc8970b9d74f2720a4bdc64c341b3b13a1665621",
            "/paper/Brain-Tumor-Segmentation-on-MR-Image-Using-K-Means-Bal-Banerjee/919144d43d528fab7693b5d1047cee1f23924e51",
            "/paper/Tumor-segmentation-from-single-contrast-MR-images-Tang-Lu/afe4b23087e572698fd0a213ffa3fd0d18b1e604",
            "/paper/Multi-Classification-of-Brain-Tumor-Images-Using-Sultan-Salem/abb9b66bc548dbbf25ae242e0996f8a61027cf23"
        ]
    },
    {
        "id": "ba2f7afcf11015cccd094771ea5015a3405788c0",
        "title": "Ferroelectric Zr0.5Hf0.5O2 thin films for nonvolatile memory applications",
        "abstract": "We report the observation of ferroelectricity in capacitors based on hafnium-zirconium-oxide. Hf0.5Zr0.5O2 thin films of 7.5 to 9.5 nm thickness were found to exhibit ferroelectric polarization-voltage hysteresis loops when integrated into TiN-based metal-insulator-metal capacitors. A remnant polarization of 16 \u03bcC/cm2 and a high coercive field of 1 MV/cm were observed. Further proof for the ferroelectric nature was collected by quasi-static polarization-voltage hysteresis, small signal capacitance-voltage, and piezoelectric measurements. Data retention characteristics were evaluated by a Positive Up Negative Down pulse technique. No significant decay of the initial polarization state was observed within a measurement range of up to two days.",
        "publication_year": "2011",
        "authors": [
            "Johannes M\u00fcller",
            "T. B\u00f6scke",
            "D. Br\u00e4uhaus",
            "U. Schr\u00f6der",
            "U. B\u00f6ttger",
            "J. Sundqvist",
            "P. Ku\u0308cher",
            "T. Mikolajick",
            "L. Frey"
        ],
        "related_topics": [
            "Physics"
        ],
        "citation_count": "352",
        "reference_count": "14",
        "references": [
            "/paper/Ferroelectric-Hf0.5Zr0.5O2-Thin-Films%3A-A-Review-of-Kim-Mohan/c4b81dbbe80da09b7779d5fce6f267b1be2b8376",
            "/paper/Ferroelectricity-in-yttrium-doped-hafnium-oxide-M%C3%BCller-Schr%C3%B6der/6f89045f9c713e09d1bd3fd10e041a71588fc4dc",
            "/paper/Stable-ferroelectric-properties-of-Hf0.5Zr0.5O2-a-Wang-Wang/f4cc538380a16f75a21b37ff47d1cf08c106dcb2",
            "/paper/Mechanical-Polarization-Switching-in-Hf0.5Zr0.5O2-Guan-Li/71a161c1450931fda797833655f99969799a18c2",
            "/paper/Ferroelectricity-in-Rare-Earth-Modified-Hafnia-Thin-Sharma-Barrionuevo/67e259c7748d0593a077b3cf9be11b4a605b5d28",
            "/paper/Evolution-of-ferroelectric-HfO2-in-ultrathin-region-Tian-Shibayama/639df39ff46db1e763727bfd17e8032c86e0778e",
            "/paper/Impact-of-different-dopants-on-the-switching-of-Schroeder-Yurchuk/cbc7e369629eddc193beb5a99d83a87c1eb37303",
            "/paper/Microstructure-research-for-ferroelectric-origin-in-Bi-Sun/d0987a6153191115045468c4b303f8c889020a28",
            "/paper/Impact-of-Iridium-Oxide-Electrodes-on-the-Phase-of-Mittmann-Szyjka/614306ca7a74f364acfde3f95b8a5e4f52ff8bc7",
            "/paper/Accelerated-ferroelectric-phase-transformation-in-Migita-Ota/313a4911b3efbcd623033b244b568224ada8fee0",
            "/paper/Ferroelectricity-in-hafnium-oxide-thin-films-B%C3%B6scke-M%C3%BCller/0e409a8bc268281b47051d35dcf86231cee4bad7",
            "/paper/Improved-manufacturability-of-ZrO2-MIM-capacitors-M%C3%BCller-B%C3%B6scke/f8e4b37c1df06f97d3bc212cee73759331cfb650",
            "/paper/Reversible-and-irreversible-domain-wall-to-the-in-Bolten-Lohse/f7f18361adaef2024bfbd3e9439e1b290db7b4c9",
            "/paper/Hafnium-zirconate-gate-dielectric-for-advanced-gate-Hegde-Triyoso/4f65a2fa734ca02ad1d244917d49a7c399ff1a3c",
            "/paper/Increasing-permittivity-in-HfZrO-thin-films-by-B%C3%B6scke-Hung/8e99d734f954bc9c45a3d8ec5037b56663490011",
            "/paper/Why-is-nonvolatile-ferroelectric-memory-transistor-Ma-Han/62c348a14686741e07633eb3b183a581a12a00a2",
            "/paper/Transient-modeling-of-ferroelectric-capacitors-for-Sheikholeslami-Gulak/b64353de782fade5833aaafab87f3f1b02a6c33d",
            "/paper/Ferroelectric-like-hysteresis-loop-in-systems-Pintilie-Alexe/62ee812b27fd6536b0264e31b03f5427f2210e0c",
            "/paper/Electrical-measurements-on-capacitor-sizes-in-the-Schmitz-Prume/585d670fc7d181fe77ed4165b0cc61edb8e185ed",
            "/paper/Integration-of-lead-zirconium-titanate-thin-films-Kim-Lee/4b97024abb481f3685cbedb21d8d52d245a60f6c"
        ]
    },
    {
        "id": "e79e769d03826b9f5ae4917924b00d53e27873e5",
        "title": "Performance analysis of a cloud-based network analytics system with multiple-source data aggregation",
        "abstract": "It was found that KNN performs significantly better than MLP and SVM with a comparative precision gain of approximately 7%, when classifying both Wi-Fi and long term evolution (LTE) traffic. \nPurpose\nThe purpose of this paper is geared towards the capture and analysis of network traffic using an array ofmachine learning (ML) and deep learning (DL) techniques to classify network traffic into different classes and predict network traffic parameters.\n\n\nDesign/methodology/approach\nThe classifier models include k-nearest neighbour (KNN), multilayer perceptron (MLP) and support vector machine (SVM), while the regression models studied are multiple linear regression (MLR) as well as MLP. The analytics were performed on both a local server and a servlet hosted on the international business machines cloud. Moreover, the local server could aggregate data from multiple devices on the network and perform collaborative ML to predict network parameters. With optimised hyperparameters, analytical models were incorporated in the cloud hosted Java servlets that operate on a client\u2013server basis where the back-end communicates with Cloudant databases.\n\n\nFindings\nRegarding classification, it was found that KNN performs significantly better than MLP and SVM with a comparative precision gain of approximately 7%, when classifying both Wi-Fi and long term evolution (LTE) traffic.\n\n\nOriginality/value\nCollaborative regression models using traffic collected from two devices were experimented and resulted in an increased average accuracy of 0.50% for all variables, with a multivariate MLP model.\n",
        "publication_year": "2022",
        "authors": [
            "T. P. Fowdur",
            "Lavesh Babooram"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "29",
        "references": [
            "/paper/Performance-Analysis-of-Network-Traffic-Predictors-Dalmazo-Vilela/44ff7a652d53d1849bde9606a4a8d0eb0d3d8e57",
            "/paper/Performance-analysis-of-network-traffic-capture-and-Fowdur-Baulum/7a3069c854f34a5d67c3e96bb106540c5979a76a",
            "/paper/Network-Traffic-Analysis-Measurement-and-Using-Joshi-Bhandari/729622fad0dd2296bf8ef2146f7f9ed3c958e5dc",
            "/paper/Online-classification-of-user-activities-using-on-Labayen-Maga%C3%B1a/fce6074c018feb4846e97a29eeb7c9259a5c0fde",
            "/paper/Traffic-Classification-with-Machine-Learning-in-a-Bakker-Ng/b6968f831df97b2e0cb4ae733077563ce20a109d",
            "/paper/A-Multilayer-Perceptron-Classifier-for-Monitoring-Guezzaz-Asimi/7b27d1eed4751c6649fca364d4bb858038a6246b",
            "/paper/Data-mining-approach-for-predicting-the-daily-data-Adekitan-Abolade/f302abccf185a4410d3f4289aa46cf8e05d94707",
            "/paper/Internet-Traffic-Forecasting-using-Neural-Networks-Cortez-Rio/52374c4e2a7c4677159d2a1a2bc226fea4aa4bab",
            "/paper/A-Time-Series-Modeling-and-Prediction-of-Wireless-Gowrishankar-Satyanarayana/f40cf94e090da9ade53077f9230f96cca7a329ae",
            "/paper/Traffic-identification-and-traffic-analysis-based-Zhu-Zheng/1ddd4de0836c373d910a70e2b97e9fba5b640fba"
        ]
    },
    {
        "id": "84a259fcb9268c82403a76e6db897d67e84c572d",
        "title": "A closed-form solution for multi-view color correction with gradient preservation",
        "abstract": "Semantic Scholar extracted view of \"A closed-form solution for multi-view color correction with gradient preservation\" by Menghan Xia et al.",
        "publication_year": "2019",
        "authors": [
            "Menghan Xia",
            "Jian Yao",
            "Zhigiang Gao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "11",
        "reference_count": "41",
        "references": [
            "/paper/Efficient-global-color-correction-for-large-scale-Yang-Liu/9868b1a123ec1fd2724f48380c35eb7e12c3ca4b",
            "/paper/Jointly-optimizing-global-and-local-color-for-image-Li-Menghan/cc752b73595183142f4d6b839fb2af06febbd6ee",
            "/paper/A-Sequential-Color-Correction-Approach-for-Texture-Dal%E2%80%99Col-Coelho/49c2f3de221ed0a688731b6812540cc11d5a0300",
            "/paper/Efficient-Global-Color%2C-Luminance%2C-and-Contrast-for-Hong-Xu/0a9eefbcfbd20a326c0ee539a871a7205a3aefba",
            "/paper/Robust-Radiometric-Normalization-of-Multitemporal-Liu-Ke/14dab115bf8247053a33b1dacbcd6a3a8f333886",
            "/paper/Block-Adjustment-Based-Radiometric-Normalization-by-Zhang-Feng/dbef11296e22491548c2cbccf979af404cc539c3",
            "/paper/Study-on-Local-to-Global-Radiometric-Balance-for-Liu-Zhou/e39f01fbdd4e28059daec196bcfc820c43d9d1e0",
            "/paper/Optimizing-Local-Alignment-along-the-Seamline-for-Yin-Li/c54b974580b29857d9b2591237deac6a5c3449ea",
            "/paper/Research-on-the-quantification-of-recognizability-Li-Zhu/aa1a657adafb984a69ad9ef64a9846cb29e5f014",
            "/paper/A-unified-probabilistic-framework-of-robust-and-for-Li-Yin/be6a7d3f90c6a9d2575bf42745a3e27fe17884ce",
            "/paper/Color-Consistency-Correction-Based-on-Remapping-for-Xia-Yao/591dd6573403679ba82c0d4b12d058ae74af942c",
            "/paper/Global-Multiple-View-Color-Consistency-Moulon-Duisit/2fb66a0c2975363d0cb070e3315572298f1b4382",
            "/paper/Color-Correction-for-Image-Based-Modeling-in-the-Shen-Wang/488dc439af342a702bfcc5ab50e5e35272e4a4b1",
            "/paper/Tone-Correction-with-Dynamic-Objects-for-Seamless-Shin-Park/01ede88a6a06c81224fc8db5486e81167bffd5ef",
            "/paper/Non-rigid-dense-correspondence-with-applications-HaCohen-Shechtman/fec3714f5a2d7d4cd8c8e2e5ba83a162fbefcb38",
            "/paper/Performance-evaluation-of-color-correction-for-and-Xu-Mulligan/2d27bc174c30444c5d0939defbcda734af824e05",
            "/paper/Efficient-and-Robust-Color-Consistency-for-Photo-Park-Tai/8aef04d7118798dc83b66ee75d790ae8429f81c9",
            "/paper/Color-Transfer-Using-Probabilistic-Moving-Least-Hwang-Lee/54e8f504c3fc6b8e8e27c9a9cd7285698272c81d",
            "/paper/Optimal-Transportation-for-Example-Guided-Color-Frigo-Sabater/2611e2502864ace363f6decbaec591a25c8e4544",
            "/paper/Local-color-transfer-via-probabilistic-segmentation-Tai-Jia/6f1d7d584b39b423151488832b98880c6c7391cd"
        ]
    },
    {
        "id": "26c50d272883fc8f72656526f915bb283f772b27",
        "title": "Learning Dynamic Memory Networks for Object Tracking",
        "abstract": "A dynamic memory network to adapt the template to the target's appearance variations during tracking, which can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information. Template-matching methods for visual tracking have gained popularity recently due to their comparable performance and fast speed. However, they lack effective ways to adapt to changes in the target object's appearance, making their tracking accuracy still far from state-of-the-art. In this paper, we propose a dynamic memory network to adapt the template to the target's appearance variations during tracking. An LSTM is used as a memory controller, where the input is the search feature map and the outputs are the control signals for the reading and writing process of the memory block. As the location of the target is at first unknown in the search feature map, an attention mechanism is applied to concentrate the LSTM input on the potential target. To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template. Unlike tracking-by-detection methods where the object's information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the target's appearance changes by updating the external memory. Moreover, unlike other tracking methods where the model capacity is fixed after offline training --- the capacity of our tracker can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information. Extensive experiments on OTB and VOT demonstrates that our tracker MemTrack performs favorably against state-of-the-art tracking methods while retaining real-time speed of 50 fps.",
        "publication_year": "2018",
        "authors": [
            "Tianyu Yang",
            "Antoni B. Chan"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "208",
        "reference_count": "38",
        "references": [
            "/paper/Visual-Tracking-via-Dynamic-Memory-Networks-Yang-Chan/aaac08ee1b4cae016fdd531b2487794007811d8b",
            "/paper/STMTrack%3A-Template-free-Visual-Tracking-with-Memory-Fu-Liu/811ffb185bc90ac5d02d6dbfbcdb6173756b52ef",
            "/paper/Learning-Dynamic-Compact-Memory-Embedding-for-Zhu-Yu/6e7376847caeb0c5b9620a24adb865f9ce8dfe1e",
            "/paper/DMV%3A-Visual-Object-Tracking-via-Part-level-Dense-Nam-Oh/924999cf37dbc578adac800557f547fb03172dfe",
            "/paper/Dynamic-Template-Selection-Through-Change-Detection-Kiran-Nguyen-Meidine/2f02729d35704bbec4ccd42fc59e80953f1c3f6b",
            "/paper/Learning-to-Remember-Past-to-Predict-Future-for-Baik-Kwon/008264abeb7d5b3d4d327e241f622da8f06c3123",
            "/paper/Long-short-term-memory-improved-Siamese-network-for-Li-Yang/bc7451d5049c13a8a3e42635231c54b592824451",
            "/paper/Object-Tracking-Using-Siamese-Network-Based-Park-Hwang/28d7cfd81ef0c194fb4a442bc8f0a80605ee7eb2",
            "/paper/GradNet%3A-Gradient-Guided-Network-for-Visual-Object-Li-Chen/47a58f8bec1d34004a7d7cf837e27a26de64f0f7",
            "/paper/Siamese-Based-Attention-Learning-Networks-for-Rahman-Jung/d47b65b7e39bce84c58ee5fc450935cd4891d4e1",
            "/paper/Recurrent-Filter-Learning-for-Visual-Tracking-Yang-Chan/db39754bde43c5555d7086261d1a6fd55af7de06",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/Learning-Policies-for-Adaptive-Tracking-with-Deep-Huang-Lucey/3281c2fa244834400c970f33a20aa1fb0ca2f53d",
            "/paper/Attentional-Correlation-Filter-Network-for-Adaptive-Choi-Chang/f2c050fa106b2e6f27d32e21e75ecbdb0cc75f67",
            "/paper/Visual-Tracking-with-Fully-Convolutional-Networks-Wang-Ouyang/bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0",
            "/paper/Siamese-Instance-Search-for-Tracking-Tao-Gavves/c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "/paper/Modeling-and-Propagating-CNNs-in-a-Tree-Structure-Nam-Baek/0dd92d7a55bbab7c5d12ae3858a6baeb99b65e67",
            "/paper/Large-Margin-Object-Tracking-with-Circulant-Feature-Wang-Liu/ece7625a346edbc5f6fab541c0c246ec06939121",
            "/paper/Learning-to-Track-at-100-FPS-with-Deep-Regression-Held-Thrun/5f0850ec47a17f22ba2611a5cb67a30cb02cf306",
            "/paper/Hierarchical-Convolutional-Features-for-Visual-Ma-Huang/5c8a6874011640981e4103d120957802fa28f004"
        ]
    },
    {
        "id": "4be712d3c72940f35d43a3e9e6bdaa323b915e59",
        "title": "Long-term target tracking combined with re-detection",
        "abstract": "This work presents a reliable yet simple long-term tracking method, which extends the state-of-the-art learning adaptive discriminative correlation filters (LADCF) tracking algorithm with a re-detection component based on the support vector machine (SVM) model. Long-term visual tracking undergoes more challenges and is closer to realistic applications than short-term tracking. However, the performances of most existing methods have been limited in the long-term tracking tasks. In this work, we present a reliable yet simple long-term tracking method, which extends the state-of-the-art learning adaptive discriminative correlation filters (LADCF) tracking algorithm with a re-detection component based on the support vector machine (SVM) model. The LADCF tracking algorithm localizes the target in each frame, and the re-detector is able to efficiently re-detect the target in the whole image when the tracking fails. We further introduce a robust confidence degree evaluation criterion that combines the maximum response criterion and the average peak-to-correlation energy (APCE) to judge the confidence level of the predicted target. When the confidence degree is generally high, the SVM is updated accordingly. If the confidence drops sharply, the SVM re-detects the target. We perform extensive experiments on the OTB-2015 and UAV123 datasets. The experimental results demonstrate the effectiveness of our algorithm in long-term tracking.",
        "publication_year": "2020",
        "authors": [
            "Juanjuan Wang",
            "Haoran Yang",
            "Ning Xu",
            "Chengqin Wu",
            "Zengshun Zhao",
            "Jixiang Zhang",
            "Dapeng Oliver Wu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "36",
        "references": [
            "/paper/SiamRPN%2B%2BD%3A-improved-SiamRPN%2B%2B-using-cascaded-Nageli-Gorthi/e0928193387ddb00a6e836988a41e5d2a31cd718",
            "/paper/Adaptive-response-maps-fusion-of-correlation-with-Zhang-Liu/127a7e6e827aab53dc4590838ae17f783b7875d3",
            "/paper/Multi-Templates-Based-Robust-Tracking-for-Robot-Cao-Wang/12fcc72fc70249f10d58c70173e1e3056367c831",
            "/paper/Pedestrian-Target-Tracking-Algorithm-on-Fusion-Jiang-Li/9a772961435ced24666ec156db597e089f07c9fd",
            "/paper/A-stable-long-term-object-tracking-method-with-Li-Zhao/43cae64a7f0b0942b0409fd4ef4009b0a07a8e5f",
            "/paper/Tracking-Learning-Detection-Kalal/c63a34ac6a4e049118070e707ca7679fbb132d33",
            "/paper/Long-term-correlation-tracking-Ma-Yang/754504cf01ef3846259783e748b1d3ea52fa2c81",
            "/paper/Large-Margin-Object-Tracking-with-Circulant-Feature-Wang-Liu/ece7625a346edbc5f6fab541c0c246ec06939121",
            "/paper/Combining-Spatio-Temporal-Context-and-Kalman-for-Yang-Wang/17751ee4fe6af30d82cb66e7fd912ba0b931f2b2",
            "/paper/Good-Features-to-Correlate-for-Visual-Tracking-Gundogdu-Alatan/388d29f001411ff80650f80cf197afc440d98b51",
            "/paper/Discriminative-Correlation-Filter-with-Channel-and-Luke%C5%BEi%C4%8D-Voj%C3%ADr/b16a583ee173f222c690242aaff7925838893fe8",
            "/paper/Visual-object-tracking-using-adaptive-correlation-Bolme-Beveridge/70c3c9b9a40ca55264e454586dca2a6cf416f6e0",
            "/paper/A-Scale-Adaptive-Kernel-Correlation-Filter-Tracker-Li-Zhu/0cae491292feccbc9ad1d864cf8b7144923ce6de",
            "/paper/%E2%80%98Skimming-Perusal%E2%80%99-Tracking%3A-A-Framework-for-and-Yan-Zhao/09b734072ad4f610478847c9cdc59a4a0c309b37"
        ]
    },
    {
        "id": "dc03a79ce543fef5cbbedf85c928dde1e92ad711",
        "title": "A joint local-global search mechanism for long-term tracking with dynamic memory network",
        "abstract": "Semantic Scholar extracted view of \"A joint local-global search mechanism for long-term tracking with dynamic memory network\" by Zeng Gao et al.",
        "publication_year": "2023",
        "authors": [
            "Zeng Gao",
            "Yi-feng Zhuang",
            "Jingjing Gu",
            "Bo Yang",
            "Zhicheng Nie"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "43",
        "references": [
            "/paper/Enhancing-discriminative-appearance-model-for-He-Chen/8822c8ccff68073523b1f9bd60439e5a7c9cc0bb",
            "/paper/Scale-Invariant-low-frame-rate-tracking-Braga-Acchetta/a243cee08b47cbc8deaaca2d304ecd05d0fe1246",
            "/paper/SiamOA%3A-siamese-offset-aware-object-tracking-Zhang-Xie/53201f6a34e5bedd793f0132e3b4c59a098c2274",
            "/paper/Learning-Adaptive-Spatial-Temporal-Context-Aware-Yuan-Chang/874dd24dce8aa99e9095d8f6218a41c639e0d4cc",
            "/paper/An-object-tracking-framework-with-recapture-based-Zhang-Sun/672384fb4658078293c0d1ea7d22bdccd70ea86e",
            "/paper/SCSTCF%3A-Spatial-Channel-Selection-and-Temporal-for-Zhang-Feng/c8f936a9483706d7ae13ea044326cb705da3bafd",
            "/paper/Deep-Learning-in-Visual-Tracking%3A-A-Review.-Jiao-Wang/203b2f02790ead8b23faff0c5b4d4e351eb8d2ee",
            "/paper/Siamese-anchor-free-object-tracking-with-multiscale-Zhang-Huang/5bc41394405396910998554d83ec22cf3ebf0fab",
            "/paper/Learning-Regression-and-Verification-Networks-for-Zhang-Wang/fb2ea0a5ef40caedfb5a10d929a331662bde78e4",
            "/paper/Tracking-by-Joint-Local-and-Global-Search%3A-A-Wang-Tang/27849a90109b93ec80d190d570041722fb2b0576"
        ]
    },
    {
        "id": "0ccb7a661cf884cf8a4845c9b331174afbd756e3",
        "title": "Robust visual tracking based on modified mayfly optimization algorithm",
        "abstract": "Semantic Scholar extracted view of \"Robust visual tracking based on modified mayfly optimization algorithm\" by Yuqi Xiao et al.",
        "publication_year": "2023",
        "authors": [
            "Yuqi Xiao",
            "Yongjun Wu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "17",
        "references": [
            "/paper/NullSpaceRDAR%3A-Regularized-discriminative-adaptive-Abdelpakey-Shehata/bd38bb4fb00081ee76cedce8e51aa4e0aae94e7e",
            "/paper/Rotation-aware-dynamic-temporal-consistency-with-Yu-Wang/e603061ad1b5205736972b43421c2641eded2012",
            "/paper/Online-multi-object-tracking-with-%CE%B4-GLMB-filter-on-Abbaspour-Masnadi-Shirazi/71f66a7e63f3136223339bfbd0537970c4d64af8",
            "/paper/A-Hybrid-Mayfly-Aquila-Optimization-Algorithm-Based-Natesan-Konda/b12674b3cccec5f4bf9dc8196fc3fdf73aee46c8",
            "/paper/A-motion-model-based-on-recurrent-neural-networks-Shahbazi-Bayat/dbe8a9d31c045aa8429dbbbe31542b44577dec8f",
            "/paper/SiamPCF%3A-siamese-point-regression-with-coarse-fine-Zeng-Zeng/2f80275f6e7c146c2eee4ea9e13544e07b85e305",
            "/paper/Target-Tracking-Using-a-Mean-Shift-Occlusion-Aware-Bhat-Subudhi/3c8be02059f6a7e05753763f2427771c62bf5a3d",
            "/paper/Swarm-Intelligence-Algorithms-in-Gene-Selection-on-Jahwar-Ahmed/52e11497a43b4c9beb75aa74020982f5c13e9e2d",
            "/paper/Spatial-temporal-constrained-particle-filter-for-Xu-Wang/75e57e4c7b992f7b88d4a96ba8eff436f59e134e",
            "/paper/Forecasting-Monthly-Tourism-Demand-Using-Enhanced-Wang-Wu/987f2b7e018156948fdeec6604cc630be0b4be23"
        ]
    },
    {
        "id": "eea134f36dec9c7d1e7ef7f1f961cc193272a1fc",
        "title": "Specificity-preserving RGB-D saliency detection",
        "abstract": "A novel framework, the specificity-preserving network (SPNet), which improves SOD performance by exploring both the shared information and modality-specific properties, and which outperforms cutting-edge approaches on six popular RGB-D SOD and three camouflaged object detection benchmarks. Salient object detection (SOD) in RGB and depth images has attracted increasing research interest. Existing RGB-D SOD models usually adopt fusion strategies to learn a shared representation from RGB and depth modalities, while few methods explicitly consider how to preserve modality-specific characteristics. In this study, we propose a novel framework, the specificity-preserving network (SPNet), which improves SOD performance by exploring both the shared information and modality-specific properties. Specifically, we use two modality-specific networks and a shared learning network to generate individual and shared saliency prediction maps. To effectively fuse cross-modal features in the shared learning network, we propose a cross-enhanced integration module (CIM) and propagate the fused feature to the next layer to integrate cross-level information. Moreover, to capture rich complementary multi-modal information to boost SOD performance, we use a multi-modal feature aggregation (MFA) module to integrate the modality-specific features from each individual decoder into the shared decoder. By using skip connections between encoder and decoder layers, hierarchical features can be fully combined. Extensive experiments demonstrate that our SPNet outperforms cutting-edge approaches on six popular RGB-D SOD and three camouflaged object detection benchmarks. The project is publicly available at https://github.com/taozh2017/SPNet .",
        "publication_year": "2021",
        "authors": [
            "Tao Zhou",
            "H. Fu",
            "Geng Chen",
            "Yi Zhou",
            "Deng-Ping Fan",
            "Ling Shao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "57",
        "reference_count": "109",
        "references": [
            "/paper/HiDAnet%3A-RGB-D-Salient-Object-Detection-via-Depth-Wu-Allibert/f874d500f82a7d3b49f373fa68abb4453c1a9207",
            "/paper/HDNet%3A-Multi-Modality-Hierarchy-Aware-Decision-for-Xia-Duan/41fbcf961a4180538210b8d432cc640b5c4fed8b",
            "/paper/Pyramidal-Attention-for-Saliency-Detection-Hussain-Anwar/ff8dcbbebab1612a564b27cd0e70de8f4785251c",
            "/paper/Robust-RGB-D-Fusion-for-Saliency-Detection-Wu-Gobichettipalayam/1bc08388f83c7a2fccf7b38c97fab8c1d751cc22",
            "/paper/Modality-Induced-Transfer-Fusion-Network-for-RGB-D-Chen-Shao/19494214b353de2ffc1b101e62baa4928f9381dd",
            "/paper/Three-Stage-Bidirectional-Interaction-Network-for-Wang-Zhang/16953743c42e0804a9e76ab04ba4e5234ccdcf32",
            "/paper/Learning-Dual-Fused-Modality-Aware-Representations-Gao-Yang/2147cfb8ef36bb938fbae4c9b7c9536ecadac424",
            "/paper/Cross-Stage-Multi-Scale-Interaction-Network-for-Yi-Zhu/821b9fa227d7881708bac6e408839bf8ef54fcde",
            "/paper/Multi-enhanced-Adaptive-Attention-Network-for-RGB-T-Hao-Cheng/17e594ccd04708980768e1c79167841c4549ca1b",
            "/paper/TANet%3A-Transformer-based-Asymmetric-Network-for-Liu-Yang/3f8bb06b291bfda5ca5cd7655ae19b166b57cec0",
            "/paper/RGB-D-Salient-Object-Detection-with-Cross-Modality-Li-Cong/fa59970f27a2d169114cac28be5139fbeac71d02",
            "/paper/Learning-Selective-Self-Mutual-Attention-for-RGB-D-Liu-Zhang/f237473bba70ec13737dbd4ef8f0f67c3cb5c3ff",
            "/paper/Adaptive-Fusion-for-RGB-D-Salient-Object-Detection-Wang-Gong/51f20e6c35cbaaca6ef11011da57dd35bb5e68f8",
            "/paper/Attention-Aware-Cross-Modal-Cross-Level-Fusion-for-Chen-Li/17e0d4019d9509f15b438d2e5c387cd01285b68a",
            "/paper/Hierarchical-Dynamic-Filtering-Network-for-RGB-D-Pang-Zhang/20e503cefe30a8ac9f996f6eebbfa5df3d87112a",
            "/paper/RGB-D-Saliency-Detection-via-Cascaded-Mutual-Zhang-Fan/b17eb490ed7fed64d286c3497be6ddeae74be975",
            "/paper/CNNs-Based-RGB-D-Saliency-Detection-via-Cross-View-Han-Chen/3c1e02da40ddbbbd18e8f498356a635c633f42dc",
            "/paper/CNN-Based-RGB-D-Salient-Object-Detection%3A-Learn%2C-Chen-Li/9564dcb05da9691e8dc0182e9e2dcdd7e240b20e",
            "/paper/Multi-level-Cross-modal-Interaction-Network-for-Huang-Chen/19bea537a268e7b7387ccc0a9b5c201ffe76aaab",
            "/paper/Siamese-Network-for-RGB-D-Salient-Object-Detection-Fu-Fan/0b4e5ac26563873539ebc398ee7df934858175c1"
        ]
    },
    {
        "id": "8a8fb6c1956813c128e99f7dd8ae1d57b81a0e46",
        "title": "3D Trajectory Reconstruction of the Soccer Ball for Single Static Camera Systems",
        "abstract": "A novel approach for 3D reconstruction of the ball trajectory from monocular lowresolution soccer image sequences is presented, using robust ball tracklets extracted from the motion history images and used to reconstruct the ball 3D trajectory based on physical characteristics and calibration information. The interest on acquiring player and ball data during soccer games is increasing in several domains such as media or training. Consequently, tracking systems are becoming widely used for live data gathering. However, due to costs, stadium infrastructure, media rights etc. there is a trend for stand-alone mobile low-cost soccer tracking systems. The drawback of such systems is that generally only low-resolution images of the players are available. Thus, the problem of detection and tracking the soccer ball is also strongly exacerbated. This paper presents a novel approach for 3D reconstruction of the ball trajectory from monocular lowresolution soccer image sequences. The ball detection is done on accumulated motion-segmented binary images by extracting shape-validated ball candidates from these so-called motion history images. Then, robust ball tracklets are extracted from the motion history images and used to reconstruct the ball 3D trajectory based on physical characteristics and calibration information. The approach is tested on a Bundesliga data set: the tracklet extraction and the ball trajectory reconstruction are evaluated.",
        "publication_year": "2013",
        "authors": [
            "J. Metzler",
            "F. Pagel"
        ],
        "related_topics": [
            "Education"
        ],
        "citation_count": 0,
        "reference_count": "9",
        "references": [
            "/paper/Robuste-Detektion%2C-Verfolgung-und-Wiedererkennung-Metzler/a831a3ad3183fdf1f115dc3639be7bf414eeb7c6",
            "/paper/3D-ball-trajectory-reconstruction-from-sports-video-Chen-Chou/4832a96f8c59d85c98175b84594ae24b7b9ed0ca",
            "/paper/A-general-framework-for-3D-soccer-ball-estimation-Ren-Orwell/056d8e0087a62dffc276028fc24b7d5a7dcb5431",
            "/paper/Probabilistic-Tracking-of-the-Soccer-Ball-Choi-Seo/0adbfeb07040027cb8dafd8b68e06b26ee7b3f45",
            "/paper/Physics-based-3D-position-analysis-of-a-soccer-ball-Kim-Seo/3e3f50e0d1f382bb34870fcf69acbb2151080fb9",
            "/paper/Automatic-Golf-Ball-Trajectory-Reconstruction-and-Zupan%C4%8Di%C4%8D-Jakli%C4%8D/ecf7f6518efefb6ed7665bed1358822ea3dab2f7",
            "/paper/Feature-based-Localization-Refinement-of-Players-in-Herrmann-Manger/1cee901483321a7b1eb31276d2582b887f50ef3b",
            "/paper/A-Flexible-New-Technique-for-Camera-Calibration-Zhang/6bdacaf992b0394cc73ff94fcbf6b31483406286",
            "/paper/A-review-of-vision-based-systems-for-soccer-video-D%E2%80%99orazio-Leo/f7233af70a77b1c89c31a9d03ed6f3f3b7d6bce7",
            "/paper/Real-time-foreground-background-segmentation-using-Kim-Chalidabhongse/278d7642d835fcd0c246d7ef536e369880d1bae0"
        ]
    },
    {
        "id": "fca66237e0047d28a588fb6e25c0308c58fa253c",
        "title": "Continuity-Aware Latent Interframe Information Mining for Reliable UAV Tracking",
        "abstract": "A new efficient continuity-aware latent interframe information mining network (ClimNet) is proposed for UAV tracking, which can generate highly-effective latent frame between two adjacent frames and a novel location-continuity Transformer (LCT) is designed to fully explore continuity- Aware spatial-temporal information, thereby markedly enhancing UAVtracking. Unmanned aerial vehicle (UAV) tracking is crucial for autonomous navigation and has broad applications in robotic automation fields. However, reliable UAV tracking remains a challenging task due to various difficulties like frequent occlusion and aspect ratio change. Additionally, most of the existing work mainly focuses on explicit information to improve tracking performance, ignoring potential interframe connections. To address the above issues, this work proposes a novel framework with continuity-aware latent interframe information mining for reliable UAV tracking, i.e., ClimRT. Specifically, a new efficient continuity-aware latent interframe information mining network (ClimNet) is proposed for UAV tracking, which can generate highly-effective latent frame between two adjacent frames. Besides, a novel location-continuity Transformer (LCT) is designed to fully explore continuity-aware spatial-temporal information, thereby markedly enhancing UAV tracking. Extensive qualitative and quantitative experiments on three authoritative aerial benchmarks strongly validate the robustness and reliability of ClimRT in UAV tracking performance. Furthermore, real-world tests on the aerial platform validate its practicability and effectiveness. The code and demo materials are released at https://github.com/vision4robotics/ClimRT.",
        "publication_year": "2023",
        "authors": [
            "Changhong Fu",
            "Mutian Cai",
            "Sihang Li",
            "Kunhan Lu",
            "Haobo Zuo",
            "Chongjun Liu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "40",
        "references": [
            "/paper/Siamese-Object-Tracking-for-Unmanned-Aerial-A-and-Fu-Lu/1171234cb2f3e1589592e3d04eb10c132fc6a5c8",
            "/paper/PVT%2B%2B%3A-A-Simple-End-to-End-Latency-Aware-Visual-Li-Huang/8fa3221af7f6d3008e00cc0c459ecbde6f1014da",
            "/paper/AutoTrack%3A-Towards-High-Performance-Visual-Tracking-Li-Fu/0619650ae0f698bcc38244a6858cc270df9dfaad",
            "/paper/HiFT%3A-Hierarchical-Feature-Transformer-for-Aerial-Cao-Fu/9916ed982600be133ed2d185b70fe721809a3096",
            "/paper/Onboard-Real-Time-Aerial-Tracking-With-Efficient-Fu-Cao/069cece5dc7c52914f6a9dfcb14dd10834bc98a3",
            "/paper/TCTrack%3A-Temporal-Contexts-for-Aerial-Tracking-Cao-Huang/3b4b9d403be348d87d21cf69a8a789f118131e29",
            "/paper/A-Benchmark-and-Simulator-for-UAV-Tracking-Mueller-Smith/27850781e39df9f750e05409b8072261124068e8",
            "/paper/Transformer-Tracking-Chen-Yan/7c3ce1b3ad598a282546e03e2dc8b52c338caed6",
            "/paper/Ocean%3A-Object-aware-Anchor-free-Tracking-Zhang-Peng/27d52bf3265bea0f9929980f6ffb4c2009eecfee",
            "/paper/Motion-Prediction-in-Visual-Object-Tracking-Wang-He/985b3593bb5981d933c6002e4e316a1d8f448c5c"
        ]
    },
    {
        "id": "03883930b96cf7386c8708bff6c1e6157ac16689",
        "title": "Distractor-Aware Visual Tracking by Online Siamese Network",
        "abstract": "This work proposes to update the features extracted by a Siamese network online with an integrated updating strategy to enhance the discrimination ability of the learned features to identify the current target from the background and distractors. The idea of most trackers based on Siamese network is off-line training and online tracking. In fact, online tracking is conducted in terms of deep features, which are extracted from the predefined network trained on a large amount of data off-line. However, these features are the general representation for similar objects, and therefore, their discrimination ability is not enough to identify the current tracking target, particularly distractors, from the background. To tackle this problem, we propose to update the features extracted by a Siamese network online. These features can fit the target variations when tracking is on-the-fly. Especially, we extract the common features from the shallow convolutional layers trained off-line, and then, they are employed as inputs of the deep convolutional layers to learn the special features of the current target online. Besides, an integrated updating strategy is proposed to accelerate network convergence. It is beneficial to enhance the discrimination ability of the learned features to identify the current target from the background and distractors. We conducted abundant experiments on the OTB2015 and VOT2016 databases. And the results demonstrate that our tracker effectively improves the baseline algorithm and performs favorably against most of the state-of-the-art trackers in the comparison of accuracy and robustness.",
        "publication_year": "2019",
        "authors": [
            "Yufei Zha",
            "Min Wu",
            "Zhuling Qiu",
            "Shuangyu Dong",
            "Fei Yang",
            "Peng Zhang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "15",
        "reference_count": "46",
        "references": [
            "/paper/Foreground-Information-Guidance-for-Siamese-Visual-Li-Yu/c19fd532794ee61998404e303cf1f08bc81793db",
            "/paper/DCFNet%2B%2B%3A-More-Advanced-Correlation-Filters-Network-Tian-Huang/582516c4c9de688e2b80de0bde3bacd2bd904e52",
            "/paper/SiamMFC%3A-Visual-Object-Tracking-Based-on-Mainfold-Chen-Wang/6dea735e7b9bcfccbaa3099a525b202f5d6e9954",
            "/paper/Cooperative-Use-of-Recurrent-Neural-Network-and-for-Zhao-Liu/2090306991d0f5027bac1b237e1ef31455ff83d4",
            "/paper/Online-Semantic-Subspace-Learning-with-Siamese-for-Zha-Wu/0ee509d91e98def11b13ec84eda64e730f8a1769",
            "/paper/A-Novel-Relational-Deep-Network-for-Single-Object-Cheewaprakobkit-Shih/ccb201a434e198cd9ecf22181f0431d930cfa4df",
            "/paper/Siamese-Visual-Object-Tracking%3A-A-Survey-Ondra%C5%A1ovi%C4%8D-Tar%C3%A1bek/177d12634a4df3a6f67a4aecd03714ff39845d0e",
            "/paper/D-TransT%3A-Deformable-Transformer-Tracking-Zhou-Yao/0e219961b31abad9a94b30829483b6b30c974f60",
            "/paper/An-Overview-of-Correlation-Filter-and-Deep-Learning-Mi-Liu/da39cff34e0525c1581d17e09aeb991f056d0581",
            "/paper/Structural-pixel-wise-target-attention-for-robust-Zhang-Cheng/6ed7d69e1357fe6d8affcdaebd0e2c2847ab45ba",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Learning-Attentions%3A-Residual-Attentional-Siamese-Wang-Teng/6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Initial-Matting-Guided-Visual-Tracking-With-Siamese-Qin-Fan/63b4214451f8a7ba0725298a726928cf04349a29",
            "/paper/DCFNet%3A-Discriminant-Correlation-Filters-Network-Wang-Gao/5404718135548b01516a668e0c022c5cb22b422e",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/CREST%3A-Convolutional-Residual-Learning-for-Visual-Song-Ma/c2046fc4744a9d358ea7a8e9c21c92fd58df7a64",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Siamese-Instance-Search-for-Tracking-Tao-Gavves/c316d5ec14e5768d7eda3d8916bddc1de142a1c2"
        ]
    },
    {
        "id": "7574b7e5a75fdd338c27af5aeb77ab79460c4437",
        "title": "Learning Dynamic Siamese Network for Visual Object Tracking",
        "abstract": "This paper proposes dynamic Siamese network, via a fast transformation learning model that enables effective online learning of target appearance variation and background suppression from previous frames, and presents elementwise multi-layer fusion to adaptively integrate the network outputs using multi-level deep features. How to effectively learn temporal variation of target appearance, to exclude the interference of cluttered background, while maintaining real-time response, is an essential problem of visual object tracking. Recently, Siamese networks have shown great potentials of matching based trackers in achieving balanced accuracy and beyond realtime speed. However, they still have a big gap to classification & updating based trackers in tolerating the temporal changes of objects and imaging conditions. In this paper, we propose dynamic Siamese network, via a fast transformation learning model that enables effective online learning of target appearance variation and background suppression from previous frames. We then present elementwise multi-layer fusion to adaptively integrate the network outputs using multi-level deep features. Unlike state-of-theart trackers, our approach allows the usage of any feasible generally- or particularly-trained features, such as SiamFC and VGG. More importantly, the proposed dynamic Siamese network can be jointly trained as a whole directly on the labeled video sequences, thus can take full advantage of the rich spatial temporal information of moving objects. As a result, our approach achieves state-of-the-art performance on OTB-2013 and VOT-2015 benchmarks, while exhibits superiorly balanced accuracy and real-time response over state-of-the-art competitors.",
        "publication_year": "2017",
        "authors": [
            "Qing Guo",
            "Wei Feng",
            "Ce Zhou",
            "Rui Huang",
            "Liang Wan",
            "Song Wang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "610",
        "reference_count": "36",
        "references": [
            "/paper/SiamET%3A-a-Siamese-based-visual-tracking-network-Zhou-Zhang/cf1559f1dc37fb77705efee31f01b2dda49e5ced",
            "/paper/Visual-tracking-via-dynamic-weighting-with-based-Cao-Ji/4f33bb4d9af9234232aeb18a7d71831ace015943",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Discriminative-and-Robust-Online-Learning-for-Zhou-Wang/365d7308a10da357009621d22b1548ab464277c3",
            "/paper/Learning-adaptive-updating-siamese-network-for-Zhou-Li/c8282e6ac5cb1c65e1482be848a6a51f3e5c48ff",
            "/paper/Learning-Deep-Lucas-Kanade-Siamese-Network-for-Yao-Han/965c6699b5a4b046090ed5e69a2506acf4a229ad",
            "/paper/A-Deep-Hyper-Siamese-Network-for-Real-Time-Object-Zhao-Yu/0257c05637f5cd5d215c77a6ad3c87f8e41d15dd",
            "/paper/Siamese-Network-Using-Adaptive-Background-for-Zhu-Chen/6a9857c8b268d295a8716d57cc2f7429fccfc309",
            "/paper/GradNet%3A-Gradient-Guided-Network-for-Visual-Object-Li-Chen/47a58f8bec1d34004a7d7cf837e27a26de64f0f7",
            "/paper/Cross-Layer-Convolutional-Siamese-Network-for-Chen-Chen/0c106e33230ecf22696cba057e7c25cbc8bfd14a",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/Learning-to-Track-at-100-FPS-with-Deep-Regression-Held-Thrun/5f0850ec47a17f22ba2611a5cb67a30cb02cf306",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Siamese-Instance-Search-for-Tracking-Tao-Gavves/c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "/paper/Hierarchical-Convolutional-Features-for-Visual-Ma-Huang/5c8a6874011640981e4103d120957802fa28f004",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Staple%3A-Complementary-Learners-for-Real-Time-Bertinetto-Valmadre/0f12a3aaf3851078d93a9bba4e3ebece6d4bcfe5",
            "/paper/Once-for-All%3A-A-Two-Flow-Convolutional-Neural-for-Chen-Tao/3dc60732c1c08165c9d4e7b334ce66e511474bb2",
            "/paper/Structure-Regularized-Compressive-Tracking-With-Guo-Feng/3d5fe9ef560c08f0c56249360247c7d4b40ce023",
            "/paper/Online-Tracking-by-Learning-Discriminative-Saliency-Hong-You/c46b08850b9c458704a3ca69172e6a0d40a6cb7f"
        ]
    },
    {
        "id": "2a0b8be3594e8163f9ea4988658223d7c46cfcb3",
        "title": "HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction",
        "abstract": "Three benchmarking tasks to pro-mote category-level HOI from 4D visual signals are established including semantic segmentation of 4D dynamic point cloud se-quences, category- level object pose tracking, and egocen-tric action segmentation with diverse interaction targets. We present HOI4D, a large-scale 4D egocentric dataset with rich annotations, to catalyze the research of category-level human-object interaction. HOI4D consists of 2.4M RGB-D egocentric video frames over 4000 sequences col-lected by 9 participants interacting with 800 different ob-ject instances from 16 categories over 610 different indoor rooms. Frame-wise annotations for panoptic segmentation, motion segmentation, 3D hand pose, category-level object pose and hand action have also been provided, together with reconstructed object meshes and scene point clouds. With HOI4D, we establish three benchmarking tasks to pro-mote category-level HOI from 4D visual signals including semantic segmentation of 4D dynamic point cloud se-quences, category-level object pose tracking, and egocen-tric action segmentation with diverse interaction targets. In-depth analysis shows HOI4D poses great challenges to existing methods and produces huge research opportunities.",
        "publication_year": "2022",
        "authors": [
            "Yunze Liu",
            "Yun Liu",
            "Chen Jiang",
            "Zhoujie Fu",
            "Kangbo Lyu",
            "Weikang Wan",
            "Hao Shen",
            "Bo-Hua Liang",
            "He Wang",
            "Li Yi"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "22",
        "reference_count": "52",
        "references": [
            "/paper/Aria-Digital-Twin%3A-A-New-Benchmark-Dataset-for-3D-Pan-Charron/27942ff872056d886977e465df90d2bf93675cf4",
            "/paper/Compositional-3D-Human-Object-Neural-Animation-Hou-Yu/0c256b5ae76156b8cda006a0abbf7c84efdfcd1b",
            "/paper/ContactArt%3A-Learning-3D-Interaction-Priors-for-and-Zhu-Wang/a8bad63846fcc0c0fd6c66f16d54d5080efa86a6",
            "/paper/Complete-to-Partial-4D-Distillation-for-Point-Cloud-Dong-Zhang/c9c9fd6ea27a86b374c3429756abd6230959282f",
            "/paper/ARCTIC%3A-A-Dataset-for-Dexterous-Bimanual-Fan-Taheri/35ddd30024c4390d8e8f363d1a1e8c0e74654643",
            "/paper/Self-Supervised-Category-Level-Articulated-Object-Liu-Zhang/82d5216eab8dbcce6f00dc5d1e96baa767720909",
            "/paper/Object-pop-up%3A-Can-we-infer-3D-objects-and-their-Petrov-Marin/c512353f5cf723da20018b0dfc73d22c5af06d23",
            "/paper/Enhancing-Transformer-Backbone-for-Egocentric-Video-Reza-Sundareshan/02e4abcdb5e99860c1dac1ba8239c830e2c5c65d",
            "/paper/DQS3D%3A-Densely-matched-Quantization-aware-3D-Gao-Tian/dcde2e099b2ad2ce40c8a3edace11dd8d0275c9a",
            "/paper/From-Semi-supervised-to-Omni-supervised-Room-Layout-Gao-Tian/868c6beeaa80f6e046a2f28ead64f277f5f2e84d",
            "/paper/H2O%3A-Two-Hands-Manipulating-Objects-for-First-Kwon-Tekin/3cb0a29bc4414d5979a9d3a94ec30dcab3ae43f7",
            "/paper/6-PACK%3A-Category-level-6D-Pose-Tracker-with-Wang-Mart'in-Mart'in/a0e7763eb743ec159222b04dde6e9cd95008e8b0",
            "/paper/PartNet%3A-A-Large-Scale-Benchmark-for-Fine-Grained-Mo-Zhu/3ecbd9b6153e9ff2f490950a87853e68c808db4d",
            "/paper/InstancePose%3A-Fast-6DoF-Pose-Estimation-for-Objects-Aing-Lie/726e472d9ea82baa5cb92e41afb0de15f24e3a8b",
            "/paper/Scaling-Egocentric-Vision%3A-The-EPIC-KITCHENS-Damen-Doughty/fc50c9392fd23b6c88915177c6ae904a498aacea",
            "/paper/BundleTrack%3A-6D-Pose-Tracking-for-Novel-Objects-or-Wen-Bekris/36a774834c0a19e3467338b73ab0fc1c78fb45b0",
            "/paper/Normalized-Object-Coordinate-Space-for-6D-Object-Wang-Sridhar/c8844833b24cc60a0fd5622b1eac7c234da58a75",
            "/paper/Category-Level-Articulated-Object-Pose-Estimation-Li-Wang/1cfa3fdd34948441d3242b9f80278cf8cf82cadf",
            "/paper/HOnnotate%3A-A-Method-for-3D-Annotation-of-Hand-and-Hampali-Rad/8483db9d730bea79f99584a1e8629e34ce7c3068",
            "/paper/CAPTRA%3A-CAtegory-level-Pose-Tracking-for-Rigid-and-Weng-Wang/0b5d26624f5487909d67404e6041bdd64bc56e96"
        ]
    },
    {
        "id": "62fc770746f6a283fbc5cfc32275e23bfef82eb7",
        "title": "Target-Aware Tracking with Long-term Context Attention",
        "abstract": "By embedding the LCA module in Transformer, this work builds a powerful online tracker with a target-aware backbone, termed as TATrack, and proposes a dynamic online update algorithm based on the classification confidence of historical information without additional calculation burden. Most deep trackers still follow the guidance of the siamese paradigms and use a template that contains only the target without any contextual information, which makes it difficult for the tracker to cope with large appearance changes, rapid target movement, and attraction from similar objects. To alleviate the above problem, we propose a long-term context attention (LCA) module that can perform extensive information fusion on the target and its context from long-term frames, and calculate the target correlation while enhancing target features. The complete contextual information contains the location of the target as well as the state around the target. LCA uses the target state from the previous frame to exclude the interference of similar objects and complex backgrounds, thus accurately locating the target and enabling the tracker to obtain higher robustness and regression accuracy. By embedding the LCA module in Transformer, we build a powerful online tracker with a target-aware backbone, termed as TATrack. In addition, we propose a dynamic online update algorithm based on the classification confidence of historical information without additional calculation burden. Our tracker achieves state-of-the-art performance on multiple benchmarks, with 71.1\\% AUC, 89.3\\% NP, and 73.0\\% AO on LaSOT, TrackingNet, and GOT-10k. The code and trained models are available on https://github.com/hekaijie123/TATrack.",
        "publication_year": "2023",
        "authors": [
            "Kaijie He",
            "Canlong Zhang",
            "Sheng Xie",
            "Zhixin Li",
            "Zhiwen Wang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "22",
        "references": [
            "/paper/STMTrack%3A-Template-free-Visual-Tracking-with-Memory-Fu-Liu/811ffb185bc90ac5d02d6dbfbcdb6173756b52ef",
            "/paper/Ocean%3A-Object-aware-Anchor-free-Tracking-Zhang-Peng/27d52bf3265bea0f9929980f6ffb4c2009eecfee",
            "/paper/Transformer-Tracking-Chen-Yan/7c3ce1b3ad598a282546e03e2dc8b52c338caed6",
            "/paper/Learning-Target-Candidate-Association-to-Keep-Track-Mayer-Danelljan/fb058786bbcb2cead98a3ef55b33d2b73b2119fc",
            "/paper/MixFormer%3A-End-to-End-Tracking-with-Iterative-Mixed-Cui-Cheng/b6eaec7917439d79ce840fa97bc371552e9b6685",
            "/paper/ATOM%3A-Accurate-Tracking-by-Overlap-Maximization-Danelljan-Bhat/d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "/paper/Learning-Spatio-Temporal-Transformer-for-Visual-Yan-Peng/72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
            "/paper/Learning-the-Model-Update-for-Siamese-Trackers-Zhang-Gonzalez-Garcia/157ff6e216985911cc2f9775155d2a424ba2984b",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/TrackingNet%3A-A-Large-Scale-Dataset-and-Benchmark-in-M%C3%BCller-Bibi/8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b"
        ]
    },
    {
        "id": "af35f26ab94eb338eaef852a1191e09f52a85796",
        "title": "Breast Cancer Detection Using Hybrid Computational Intelligence Techniques",
        "abstract": "This chapter highlights two hybridizations pertaining to breast cancer, in which intuitionistic fuzzy histogram hyperbolization is hybridized with possibilistic fuzzy c-mean clustering algorithm. Diagnosis of cancer is of prime concern in recent years. Medical imaging is used to analyze these diseases. But, these images contain uncertainties due to various factors and thus intelligent techniques are essential to process these uncertainties. This chapter highlights two hybridizations pertaining to breast cancer. In one hybridization technique, it hybridizes intuitionistic fuzzy set and rough set in combination with statistical feature extraction methods. In the second case, intuitionistic fuzzy histogram hyperbolization is hybridized with possibilistic fuzzy c-mean clustering algorithm. Both hybridizations are studied to extract the region of interest and then to enhance the edges surrounding it. Experimental analysis is carried out for both models and an exhaustive study on these models is presented in this chapter.",
        "publication_year": "2018",
        "authors": [
            "D. Acharjya",
            "C. L. Chowdhary"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "35",
        "references": [
            "/paper/A-New-Model-for-Brain-Tumor-Detection-Using-and-Amin-Anjum/d1f1b49e876dae4bb4165d91a59654f4b23bffae",
            "/paper/Fully-Automatic-Detection-and-Segmentation-Approach-Mekali-Girijamma/a2953fffe318c76d661fe4c5e1e46f6b69c757cb",
            "/paper/Simple-Linear-Iterative-Clustering-(SLIC)-and-Graph-Chowdhary/d2d3f54cbf92adbd29ffc65c97c6ab655c348006",
            "/paper/A-Medical-Image-Segmentation-Method-With-Anti-Noise-Xu-Ye/bede3dc0bcf6dc73a17dbfa28d4b558ecfbd237d",
            "/paper/A-Hybrid-Scheme-for-Breast-Cancer-Detection-using-Chowdhary-Acharjya/c55550ff224ff080d95d93200aea8838267fc21c",
            "/paper/Fuzzy-rough-sets-hybrid-scheme-for-breast-cancer-Hassanien/5b935d2aa2aa36f67f084b4c1ad285bbc642c064",
            "/paper/Breast-Cancer-Detection-using-Intuitionistic-Fuzzy-Chowdhary-Acharjya/8ece55a94960504cd809b4058382afa746085911",
            "/paper/A-fuzzy-genetic-approach-to-breast-cancer-diagnosis-Pe%C3%B1a-Reyes-Sipper/e663fa59a758b945ed0e8e620a52022d617a9b03",
            "/paper/Advances-in-Optimal-Detection-of-Cancer-by-Image-Mohammadzadeh-Safdari/b7ade135adb326467e9c636cd3d0b8d16218aad5",
            "/paper/A-Novel-Approach-for-Breast-Cancer-Detection-and-in-Singh-Gupta/2eb766d776ccd9872a0353fbf095e77314b244eb",
            "/paper/Improvement-of-Breast-Cancer-Detection-Using-and-in-Pak-Kanan/48249c493f8b1bdf2ebcfbe8967d1e8ee375b1eb",
            "/paper/Detection-of-Spiculated-Masses-in-Mammograms-Based-Hassanien-Ali/12a2e79340aa653bee50270dac7352ffa79f2c8d",
            "/paper/Automatic-detection-of-anomalies-in-screening-Kendall-Barnett/82a305ff589fe79d78f98c6a77e9104c9804d288",
            "/paper/Breast-Cancer-Detection-with-Reduced-Feature-Set-Mert-Kili%C3%A7/296e4a151a752b434d36aa7e4e9ee532c46536fc"
        ]
    },
    {
        "id": "f45f232c5d1e096be06ac4217de739b16aefccfd",
        "title": "Tumor Classification in Breast Magnetic Resonance Images (MRI) Using the Level Set\u2013Based Segmentation Method and Gabor-Haralik Feature",
        "abstract": "A computer-aided autodiagnosis system is developed for diagnosis and classification of axial magnetic resonance images of the breast in two classes of benign and malignant breast lesions in magnetic resonance imaging. Introduction: Breast cancer can be considered as the most common cancer among women in the world. Hence, finding appropriate diagnosis methods is a critical and sensitive challenge in the health of the human community. Various methods have been proposed for breast screening in women, and one of the safest methods is magnetic resonance imaging. Tumors do not have morphological features of their own. Therefore, differentiating between benign and malignant lesions is normally very time-consuming and difficult. In this study, a computer-aided autodiagnosis system is developed for diagnosis and classification of axial magnetic resonance images of the breast in two classes of benign and malignant. Methods: Initially, suspected parts of the lesion were separated as a rectangular box around the lesion by an experienced radiologist. Then, we used, for the first time, a level set\u2013based algorithm to precisely separate the lesion considering the unevenness of the images and to remove false positive regions using morphological operations and removing veins. In the next stage, four groups of features expressing particular states of the lesion structure are extracted from the separated parts of the lesions. These four groups are textural, kinetic, frequency, and morphological features. Here a new group of features called the Gabor-Haralik features, which present a particular efficiency, was extracted for each lesion. Finally, MLP classification was used to classify the lesions. Results: The proposed method was tested on 46 lesions. By utilizing Gabor-Haralik features, we achieved mean sensitivity, specificity, accuracy, and F-measure of 95.41, 90.70, 92.76, and 92.19%, respectively. Conclusion: The performance measures indicate the efficiency of the proposed diagnosis system for classification of benign and malignant breast lesions in magnetic resonance imaging.",
        "publication_year": "2019",
        "authors": [
            "Soheil Pashoutan",
            "Fazael Ayatollahi",
            "S. B. Shokouhi"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "80",
        "references": [
            "/paper/Automatic-Breast-Tumor-Classification-Using-a-Level-Pashoutan-Shokouhi/64aa7896b11ab90ccb4828c2faa6f86cc5f647cd",
            "/paper/Computer-aided-diagnosis-of-mass-like-lesion-in-of-Huang-Chang/a597f9e5586884eaafc3aeb4fbf9164536d3e045",
            "/paper/Computer-Aided-Diagnosis-Scheme-for-Distinguishing-Honda-Nakayama/b6e527dbb086db948064ba00c71c62646664ec36",
            "/paper/Prediction-of-malignant-breast-lesions-from-MRI-a-McLaren-Chen/57735b4b386dbc28f458404894aad4d8045d70be",
            "/paper/Computerized-breast-lesions-detection-using-kinetic-Chang-Huang/524477567a8d63d1ad8895d52c34c17eb94a2eaf",
            "/paper/Localized-atlas-based-segmentation-of-breast-MRI-in-Fooladivanda-Shokouhi/fd47af5b4143e570266693a5f85e757762b155e0",
            "/paper/Image-features-extraction-for-masses-classification-Chaieb-Bacha/d6e88aba083767a5d6bb0c25193c7be0f3bc7777",
            "/paper/Selection-of-diagnostic-features-on-breast-MRI-to-Newell-Nie/c5b56669b6218b7e65011e3be54561c25c6f984e",
            "/paper/Breast-region-segmentation-in-MRI-using-chest-atlas-Fooladivanda-Shokouhi/cf246a192a755a2d0b363bd8c40504e574bd1cbf",
            "/paper/Analysis-of-tissue-abnormality-and-breast-density-a-Abdel-Nasser-Rashwan/2a937dc9c5e24728bfee5e60045ca6d710930aa4"
        ]
    },
    {
        "id": "1732a276e62c63801b46d8f0ee2e62ae7148de22",
        "title": "Neuroevolution of Convolutional Neural Networks for Breast Cancer Diagnosis Using Western Blot Strips",
        "abstract": "This work uses deep learning techniques to discriminate between healthy and breast cancer patients, based on the banding patterns obtained from the Western Blot strip images of the autoantibody response to antigens of the T47D tumor line. Breast cancer has become a global health problem, ranking first in incidences and fifth in mortality in women around the world. In Mexico, the first cause of death in women is breast cancer. This work uses deep learning techniques to discriminate between healthy and breast cancer patients, based on the banding patterns obtained from the Western Blot strip images of the autoantibody response to antigens of the T47D tumor line. The reaction of antibodies to tumor antigens occurs early in the process of tumorigenesis, years before clinical symptoms. One of the main challenges in deep learning is the design of the architecture of the convolutional neural network. Neuroevolution has been used to support this and has produced highly competitive results. It is proposed that neuroevolve convolutional neural networks (CNN) find an optimal architecture to achieve competitive ranking, taking Western Blot images as input. The CNN obtained reached 90.67% accuracy, 90.71% recall, 95.34% specificity, and 90.69% precision in classifying three different classes (healthy, benign breast pathology, and breast cancer).",
        "publication_year": "2023",
        "authors": [
            "Jos\u00e9-Luis Llaguno-Roque",
            "R. Barrientos-Mart\u00ednez",
            "H. Acosta-Mesa",
            "T. Romo-Gonz\u00e1lez",
            "E. Mezura-Montes"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "31",
        "references": [
            "/paper/Boosting-Breast-Cancer-Detection-Using-Neural-Alanazi-Kamruzzaman/a812ebb6761ff453a02d866e97295932153537e2",
            "/paper/A-Survey-of-Convolutional-Neural-Network-in-Breast-Zhu-Wang/0295f85e0db918b26ff7e8d1d547f8d98eea7feb",
            "/paper/Deep-Learning-Based-Methods-for-Breast-Cancer-A-and-Nasser-Yusof/a1fc5d747b2c0ef536ae5731e40916f10ee2a42f",
            "/paper/Machine-Learning-with-Applications-in-Breast-Cancer-Yue-Wang/9c36232db29f23f156df3a1b5db8f035c917b25c",
            "/paper/Predicting-HER2-Status-in-Breast-Cancer-on-Images-Xu-Yang/5c13dad7c049805dfb505cf2ae63714394c50882",
            "/paper/BC2NetRF%3A-Breast-Cancer-Classification-from-Images-Jabeen-Khan/b0925e7744cb305c4a6b1fcf012b66eec249fc69",
            "/paper/Lymph-Node-Metastasis-Prediction-from-Primary-US-Zhou-Wu/4b71be562b55fa01f022ade3b266be9a684d8b82",
            "/paper/Autoantibodies-in-Early-Detection-of-Breast-Cancer-Rauf-Anderson/1596f118437dc632b2b033c13b00cfbdc768142f",
            "/paper/Autoantibodies-as-biomarkers-for-breast-cancer-and-Yang-Han/058d81986770220897524f16a22c0727e1288616",
            "/paper/Breast-cancer-diagnosis-using-thermography-and-Ekici-Jawzal/c3c1101ade4dc976659bf90c615367200ba760ca"
        ]
    },
    {
        "id": "dc9e589f4c576a963d5cd2cc57c7378a8ec91178",
        "title": "ETECADx: Ensemble Self-Attention Transformer Encoder for Breast Cancer Diagnosis Using Full-Field Digital X-ray Breast Images",
        "abstract": "A new AI-based computer-aided diagnosis (CAD) framework called ETECADx is proposed by fusing the benefits of both ensemble transfer learning of the convolutional neural networks as well as the self-attention mechanism of vision transformer encoder (ViT). Early detection of breast cancer is an essential procedure to reduce the mortality rate among women. In this paper, a new AI-based computer-aided diagnosis (CAD) framework called ETECADx is proposed by fusing the benefits of both ensemble transfer learning of the convolutional neural networks as well as the self-attention mechanism of vision transformer encoder (ViT). The accurate and precious high-level deep features are generated via the backbone ensemble network, while the transformer encoder is used to diagnose the breast cancer probabilities in two approaches: Approach A (i.e., binary classification) and Approach B (i.e., multi-classification). To build the proposed CAD system, the benchmark public multi-class INbreast dataset is used. Meanwhile, private real breast cancer images are collected and annotated by expert radiologists to validate the prediction performance of the proposed ETECADx framework. The promising evaluation results are achieved using the INbreast mammograms with overall accuracies of 98.58% and 97.87% for the binary and multi-class approaches, respectively. Compared with the individual backbone networks, the proposed ensemble learning model improves the breast cancer prediction performance by 6.6% for binary and 4.6% for multi-class approaches. The proposed hybrid ETECADx shows further prediction improvement when the ViT-based ensemble backbone network is used by 8.1% and 6.2% for binary and multi-class diagnosis, respectively. For validation purposes using the real breast images, the proposed CAD system provides encouraging prediction accuracies of 97.16% for binary and 89.40% for multi-class approaches. The ETECADx has a capability to predict the breast lesions for a single mammogram in an average of 0.048 s. Such promising performance could be useful and helpful to assist the practical CAD framework applications providing a second supporting opinion of distinguishing various breast cancer malignancies.",
        "publication_year": "2022",
        "authors": [
            "Aymen M. Al-Hejri",
            "Riyadh M. Al-Tam",
            "Muneer Fazea",
            "A. Sable",
            "Soojeong Lee",
            "M. A. Al-antari"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "68",
        "references": [
            "/paper/Analysis-of-Histopathological-Images-for-Early-of-Ahmed-Senan/697b853386ddb894ccf3e45613bced6cb0c63155",
            "/paper/Hybrid-Techniques-for-the-Diagnosis-of-Acute-Based-Ahmed-Senan/9a1cc376fd99f55ec4d7034c359649265270c720",
            "/paper/Automatic-Classification-of-Colour-Fundus-Images-on-Shamsan-Senan/4d2e1639f6e9772721ce6fc722f66db2de059ef4",
            "/paper/Blood-Slide-Image-Analysis-to-Classify-WBC-Types-on-Olayah-Senan/a60cd82174d37e4a861f6ac510dbebbf67e0722e",
            "/paper/A-Hybrid-Workflow-of-Residual-Convolutional-Encoder-Al-Tam-Al-Hejri/e8cf7af81715aef98f2aba32efdaf69d9c64cac1",
            "/paper/A-Hybrid-Deep-Transfer-Learning-of-CNN-Based-LR-PCA-Samee-Alhussan/53dbfdb4d3bf8d498586b309a98c048b6da3e422",
            "/paper/A-VGG-attention-vision-transformer-network-for-and-Qu-Lu/e2e93ae3cf54cc677908e473cf3f5ab9390a28be",
            "/paper/Semi-supervised-vision-transformer-with-adaptive-Wang-Jiang/95a8200eaeb9065c7ef7350d61750fb839890692",
            "/paper/Breast-Cancer-Detection-in-Mammography-Images-Using-Altameem-Mahanty/acc95a9ccc8f69b69c63540567ece261f4f770ec",
            "/paper/Deep-Learning-Cascaded-Feature-Selection-Framework-Samee-Atteia/db968440e2aa3fec0c4144752651b596faf6d8b9",
            "/paper/Breast-lesions-classifications-of-mammographic-a-Mahmood-Li/36b7f12195b0cf2061925b5456c3672e5f186100",
            "/paper/Vision-Transformers-for-Classification-of-Breast-Gheflati-Rivaz/373ae25ca1cfefc6ba0ff02c23fdc1ce45fee942",
            "/paper/Computer-aided-diagnosis-of-breast-ultrasound-using-Moon-Lee/e65e3aaa1d49b4f2cb1aac49230e6339a77290cc",
            "/paper/DenseNet-for-Breast-Tumor-Classification-in-Images-Jim%C3%A9nez-Gaona-Rodr%C3%ADguez-%C3%81lvarez/bb00995b50a6915e01a5db10d9d2645f2e0ed056"
        ]
    },
    {
        "id": "a887b64738307c68672a10363dd7b95b53a6ebf7",
        "title": "Semantic Segmentation of the Malignant Breast Imaging Reporting and Data System Lexicon on Breast Ultrasound Images by Using DeepLab v3+",
        "abstract": "DeepLab v3+ with the ResNet-50 decoder was suitable for solving the identification of the malignant BI-RADS lexicon on breast ultrasound images, offering a better balance of performance and computational resource usage than a fully connected network and other decoders. In this study, an advanced semantic segmentation method and deep convolutional neural network was applied to identify the Breast Imaging Reporting and Data System (BI-RADS) lexicon for breast ultrasound images, thereby facilitating image interpretation and diagnosis by providing radiologists an objective second opinion. A total of 684 images (380 benign and 308 malignant tumours) from 343 patients (190 benign and 153 malignant breast tumour patients) were analysed in this study. Six malignancy-related standardised BI-RADS features were selected after analysis. The DeepLab v3+ architecture and four decode networks were used, and their semantic segmentation performance was evaluated and compared. Subsequently, DeepLab v3+ with the ResNet-50 decoder showed the best performance in semantic segmentation, with a mean accuracy and mean intersection over union (IU) of 44.04% and 34.92%, respectively. The weighted IU was 84.36%. For the diagnostic performance, the area under the curve was 83.32%. This study aimed to automate identification of the malignant BI-RADS lexicon on breast ultrasound images to facilitate diagnosis and improve its quality. The evaluation showed that DeepLab v3+ with the ResNet-50 decoder was suitable for solving this problem, offering a better balance of performance and computational resource usage than a fully connected network and other decoders.",
        "publication_year": "2022",
        "authors": [
            "W. Shia",
            "F. Hsu",
            "Seng-Tong Dai",
            "Shih-Lin Guo",
            "Dar-Ren Chen"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "31",
        "references": [
            "/paper/Performance-and-Robustness-of-Regional-Image-Driven-Kub%C3%ADcek-Varysova/d8b67c16876d8557ce91485bd68e71cf41792ac1",
            "/paper/Incorporating-the-Breast-Imaging-Reporting-and-Data-Hsieh-Hsu/e941511c42e749153fdec4c738483915c0967906",
            "/paper/Automatic-semantic-segmentation-of-breast-tumors-in-Badawy-Mohamed/d0e511d31beaac99d58b563295c4b07a2b21e71b",
            "/paper/Classification-of-malignant-tumors-in-breast-using-Shia-Chen/fa9839b38cfd62ba7437463bb2ac631cbd8532fb",
            "/paper/Fuzzy-Semantic-Segmentation-of-Breast-Ultrasound-Huang-Zhang/244b105296a231f57e5c5e6d12ee9b2246889a39",
            "/paper/Dilated-Semantic-Segmentation-for-Breast-Ultrasonic-Irfan-Almazroi/d9ce59102f97cca7826bd5ea4dc066ae9da57116",
            "/paper/Semantic-Segmentation-with-DenseNets-for-Breast-Yifeng-Yufeng/77d2c6297f29de36900883712ded814af2e047e3",
            "/paper/Classification-of-malignant-tumours-in-breast-using-Shia-Lin/0c3b4750353a73c2d72dd636c20aa01d1ee9d05e",
            "/paper/Segmentation-and-recognition-of-breast-ultrasound-Guo-Duan/1cbba75f6c77c1650ae175c690cec86506e7cd9d",
            "/paper/Breast-ultrasound-computer-aided-diagnosis-using-Shen-Chang/e74323476d3aed494b362b16491c373aca3232f6",
            "/paper/Foundation-and-methodologies-in-computer-aided-for-Jalalian-Mashohor/831a1e8106571d82f22cd8dac7725964852b0154"
        ]
    },
    {
        "id": "12b4de408a2d994b1b924ae7f47ad44a0bc6569a",
        "title": "Deep Learning-Based Segmentation and Classification Techniques for Brain Tumor MRI: A Review",
        "abstract": "This study looks at the most up-to-date methodologies for medical image analytics that use convolutional neural networks on MRI images that help save lives faster and rectify some medical errors. Early detection of brain tumors is critical for enhancing treatment options and extending patient survival. Magnetic resonance imaging (MRI) scanning gives more detailed information, such as greater contrast and clarity than any other scanning method. Manually dividing brain tumors from many MRI images collected in clinical practice for cancer diagnosis is a tough and time-consuming task. Tumors and MRI scans of the brain can be discovered using algorithms and machine learning technologies, making the process easier for doctors because MRI images can appear healthy when the person may have a tumor or be malignant. Recently, deep learning techniques based on deep convolutional neural networks have been used to analyze medical images with favorable results. It can help save lives faster and rectify some medical errors. In this study, we look at the most up-to-date methodologies for medical image analytics that use convolutional neural networks on MRI images. There are several approaches to diagnosing and classifying brain cancers. Inside the brain, irregular cells grow so that a brain tumor appears. The size of the tumor and the part of the brain affected impact the symptoms.",
        "publication_year": "2022",
        "authors": [
            "Noor Mohammed Ghadi",
            "N. Salman"
        ],
        "related_topics": [
            "Medicine",
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "42",
        "references": [
            "/paper/An-Efficient-Wildfire-Detection-System-for-Using-James-Ansaf/08e088b3e1187398a0f1edd9b7bffffbe422d5fd",
            "/paper/Detection-and-diagnosis-of-brain-tumors-using-deep-Gurunathan-Krishnan/18bf5644218b92b829d28ca08a1b4a0ea64842f6",
            "/paper/Brain-Tumor-Classification-Using-Convolutional-Seetha-Raja/79461362af5899a45b420993af9ad9025dc6aedf",
            "/paper/Transfer-Learning-for-Automatic-Brain-Tumor-Using-Arbane-Benlamri/9056f40fe1c04ee5730e04c78938c05c691fdd53",
            "/paper/Brain-Tumor-Detection-and-Segmentation-in-MR-Images-Sajid-Hussain/66794ec56f19b226cb1b0ec7dca77c90ec8694ef",
            "/paper/Automated-Detection-of-Brain-Tumor-through-Magnetic-Gull-Akbar/d69e26473b5614d87567b48492023dde0dc27f8a",
            "/paper/Automated-Categorization-of-Brain-Tumor-from-MRI-Deepak-Ameer/1932d88f7449e4c0cd648d728049e2f0da6ecfbb",
            "/paper/Brain-Tumor-Detection-and-Classification-Using-Deep-Kokila-Devadharshini/1d3907b78886079426591fff94307b408b1c801a",
            "/paper/Detection-and-classification-of-brain-tumours-from-Av%C5%9Far-Sal%C3%A7in/b292f03ad6d262624731acd8073b1ab0c972811f",
            "/paper/Segmentation-of-Brain-Tumor-in-MRI-Images-Using-CNN-Archa-Kumar/46dcb77d4b2635978934ac6de7b0cc33601a3a02",
            "/paper/Multi-Classification-of-Brain-Tumor-Images-Using-Sultan-Salem/abb9b66bc548dbbf25ae242e0996f8a61027cf23"
        ]
    },
    {
        "id": "c4b81dbbe80da09b7779d5fce6f267b1be2b8376",
        "title": "Ferroelectric Hf0.5Zr0.5O2 Thin Films: A Review of Recent Advances",
        "abstract": "Ferroelectricity in HfO2-based materials, especially Hf0.5Zr0.5O2 (HZO), is today one of the most attractive topics because of its wide range of applications in ferroelectric random-access memory, ferroelectric field-effect transistors, ferroelectric tunneling junctions, steep-slope devices, and synaptic devices. The main reason for this increasing interest is that, when compared with conventional ferroelectric materials, HZO is compatible with complementary metal\u2013oxide\u2013semiconductor flow [even back-end of the line thermal budget] and can exhibit robust ferroelectricity even at extremely thin (<\u200910\u00a0nm) thicknesses. In this report, recent advances in the ferroelectric properties of HZO thin films since the first report in 2011, including doping effects, mechanical stress effects, interface effects, and ferroelectric film thickness effects, are comprehensively reviewed.",
        "publication_year": "2018",
        "authors": [
            "S. Kim",
            "Jaidah Mohan",
            "S. Summerfelt",
            "Jiyoung Kim"
        ],
        "related_topics": [
            "Chemistry"
        ],
        "citation_count": "168",
        "reference_count": "84",
        "references": [
            "/paper/Ferroelectric-Hf0.5Zr0.5O2-Thin-Films%3A-A-Review-of-Kim-Mohan/0eeea4143ffac54d3af9868fe677d164de4f12b7",
            "/paper/Mechanical-Polarization-Switching-in-Hf0.5Zr0.5O2-Guan-Li/71a161c1450931fda797833655f99969799a18c2",
            "/paper/Ferroelectricity-of-hafnium-oxide-based-materials%3A-Yang-Yu/4c80b8b3737a52a69d5d3bb38d8cd4aedc336c6e",
            "/paper/Enhancement-of-ferroelectricity-and-homogeneity-of-Zou-Tian/3f8a4d3a49a342aa701c0350ff7d6307c1addd70",
            "/paper/Graphene-bandgap-induced-by-ferroelectric-HfO2-with-Dragoman-Dinescu/6f26f7a615523b4e9f07cf7082be885876b85a0b",
            "/paper/As-deposited-ferroelectric-HZO-on-a-III%E2%80%93V-Andersen-Persson/b969c7c106cadf06eee40440fd95c29b2aa03a34",
            "/paper/Epitaxial-Ferroelectric-HfO2-Films%3A-Growth%2C-and-Fina-S%C3%A1nchez/e217c27b14618f6c965e1ad99ec3e285b107bdcc",
            "/paper/Ferroelectric-polarization-retention-with-scaling-Mohan-Hern%C3%A1ndez-Arriaga/d99363f3695e439bbe5ed4fb587029bd241ea070",
            "/paper/Defects-in-ferroelectric-HfO2.-Chouprik-Negrov/e758572b61b000e68b34ef9302186577b4ed83d6",
            "/paper/Solution-processed-high-performance-ferroelectric-Hasan-Ahn/506d5194afed936f2a9e5658032454c9164a9986",
            "/paper/Ferroelectric-Zr0.5Hf0.5O2-thin-films-for-memory-M%C3%BCller-B%C3%B6scke/ba2f7afcf11015cccd094771ea5015a3405788c0",
            "/paper/Ferroelectricity-and-ferroelectric-resistive-in-Fan-Xiao/4bbc22a1c6dcf6fa9f5ba269421116cffc38a880",
            "/paper/Stabilization-of-ferroelectric-phase-in-tungsten-Karbasian-Reis/7a1c3653ddc94f4e14b7c6a068eedd1c11aa2280",
            "/paper/Microstructure-research-for-ferroelectric-origin-in-Bi-Sun/d0987a6153191115045468c4b303f8c889020a28",
            "/paper/Realizing-ferroelectric-Hf0.5Zr0.5O2-with-elemental-Lin-Mcguire/f388057eae6ef7d1ad57c0ed7017f57ec78e2664",
            "/paper/Electron-transport-across-ultrathin-ferroelectric-Chouprik-Chernikova/f1001ff5df551fea43fca933ff7f238223567fda",
            "/paper/Ferroelectricity-and-Antiferroelectricity-of-Doped-Park-Lee/b9b7030b91903625a54325ef168389987c03143e",
            "/paper/Ferroelectric-properties-and-switching-endurance-of-Park-Kim/5e058e669a75cef66a447dea3f430248d069d37b",
            "/paper/Improved-Ferroelectric-Switching-Endurance-of-Thin-Chernikova-Kozodaev/6be7188841668e38a1a000e2db92bea3f9eacd9d",
            "/paper/Charge-transport-mechanism-in-thin-films-of-and-Islamov-Chernikova/af9854a3c11e6c8755723eff92a1076ef8b2621b"
        ]
    },
    {
        "id": "44ff7a652d53d1849bde9606a4a8d0eb0d3d8e57",
        "title": "Performance Analysis of Network Traffic Predictors in the Cloud",
        "abstract": "A taxonomy for network traffic prediction models, including the review of state of the art, is presented here and an analysis mechanism, focused on providing a standardized approach for evaluating the best candidate predictor models for these environments, is proposed. Predicting the inherent traffic behaviour of a network is an essential task, which can be used for various purposes, such as monitoring and managing the network\u2019s infrastructure. However, the recent surge of dynamic environments, such as Internet of Things and Cloud Computing have hampered this task. This means that the traffic on these networks is even more complex, displaying a nonlinear behaviour with specific aperiodic characteristics during daily operation. Traditional network traffic predictors are usually based on large historical data bases which are used to train algorithms. This may not be suitable for these highly volatile environments, where the strength of the force exerted in the interaction between past and current values may change quickly with time. In light of this, a taxonomy for network traffic prediction models, including the review of state of the art, is presented here. In addition, an analysis mechanism, focused on providing a standardized approach for evaluating the best candidate predictor models for these environments, is proposed. These contributions favour the analysis of the efficacy and efficiency of network traffic prediction among several prediction models in terms of accuracy, historical dependency, running time and computational overhead. An evaluation of several prediction mechanisms is performed by assessing the Normalized Mean Square Error and Mean Absolute Percent Error of the values predicted by using traces taken from two real case studies in cloud computing.",
        "publication_year": "2017",
        "authors": [
            "B. Dalmazo",
            "J. Vilela",
            "M. Curado"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "21",
        "reference_count": "78",
        "references": [
            "/paper/Performance-evaluation-of-elephant-flow-predictors-Bezerra-Pinheiro/c9b1a0dd1b93b607be36b12d62fe9c48ee55d0e1",
            "/paper/Performance-analysis-of-a-cloud-based-network-with-Fowdur-Babooram/e79e769d03826b9f5ae4917924b00d53e27873e5",
            "/paper/A-Prediction-based-Approach-for-Anomaly-Detection-Dalmazo-Vilela/df81ef736f77eb7ba96f4a7832ca1820886b9852",
            "/paper/Applying-Catastrophe-Theory-for-Network-Anomaly-in-Khatibzadeh-Bornaee/4a114d69beb26d793daf0a802c1d4cdc104fde5f",
            "/paper/Sample-Selection-Search-to-Predict-Elephant-Flows-Silva-Carvalho/290d90a3ec011108fa6f3bb827dd0890eb57d994",
            "/paper/Generation-of-Synthetic-Network-Traffic-Series-a-Cardoso-Vieira/882089e97b72f4e3130c7aa3ee90f8144e8eae62",
            "/paper/Modularized-and-Contract-Based-Prediction-Models-in-Neves-Riker/e67ea2d34464475895ff0db11e2ebd2ea857c228",
            "/paper/Triple-Similarity-Mechanism-for-alarm-management-in-Dalmazo-Vilela/590b0e3a0de3cbd25f7dee4abfd2275dc7156938",
            "/paper/A-short-term-traffic-flow-prediction-method-based-Yu-Sun/52226c62961bba21a9e6295bdf89daca35303c52",
            "/paper/On-demand-resource-provision-based-on-load-and-in-Guo-Li/ed3ef3b152706ab10e4652d7e9f3b680b4dab168",
            "/paper/Predicting-Traffic-in-the-Cloud%3A-A-Statistical-Dalmazo-Vilela/8648599d28b5bf8f88bac203bebc85a4a12b12e7",
            "/paper/Online-traffic-prediction-in-the-cloud-Dalmazo-Vilela/d16b6598963bab9d58132d46e4b8841f819e55a5",
            "/paper/Online-Traffic-Prediction-in-the-Cloud%3A-A-Dynamic-Dalmazo-Vilela/332df6ad6e60838c3baac8f7e8bc9d9757d35364",
            "/paper/Analysis-and-Prediction-of-Real-Network-Traffic-Zhani-Elbiaze/8fc451bf272c93ff58dd5c7cb864ab2da27847d3",
            "/paper/A-predictability-analysis-of-network-traffic-Sang-Li/87d137644e9846f85422ebe02f265f6cbe3f282a",
            "/paper/Internet-Traffic-Forecasting-using-Neural-Networks-Cortez-Rio/52374c4e2a7c4677159d2a1a2bc226fea4aa4bab",
            "/paper/Network-traffic-characteristics-of-data-centers-in-Benson-Akella/f7c3b5a070e3cb73708964d4d2199c1c146a5527",
            "/paper/Detecting-Hidden-Anomalies-Using-Sketch-for-Network-Aiping-Han/5b031bebc736bd44936c94b7c0d8fe7ffe2ce798",
            "/paper/Evaluation-of-short-term-traffic-forecasting-in-Papadopouli-Raftopoulos/fd0c389f882b9f8d5ac98ec63b52323d10ac0e3f",
            "/paper/A-Machine-Learning-Approach-for-Efficient-Traffic-Li-Moore/da238a390fd5ae063239c08589e9c7814d16caa0"
        ]
    },
    {
        "id": "9868b1a123ec1fd2724f48380c35eb7e12c3ca4b",
        "title": "Efficient global color correction for large-scale multiple-view images in three-dimensional reconstruction",
        "abstract": "Semantic Scholar extracted view of \"Efficient global color correction for large-scale multiple-view images in three-dimensional reconstruction\" by Junxing Yang et al.",
        "publication_year": "2021",
        "authors": [
            "Junxing Yang",
            "Lulu Liu",
            "Jiabin Xu",
            "Yang Wang",
            "Fei Deng"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "6",
        "reference_count": "40",
        "references": [
            "/paper/A-Sequential-Color-Correction-Approach-for-Texture-Dal%E2%80%99Col-Coelho/49c2f3de221ed0a688731b6812540cc11d5a0300",
            "/paper/Texture-Mapping-Error-Removal-Based-on-the-BRIEF-in-Yang-Lu/b00052f831979ac275459a0340fa8d2cf6265490",
            "/paper/Histogram-Adjustment-of-Images-for-Improving-Labedz-Skabek/76654f5e962bd4830a190808f1b9ba326673cb3e",
            "/paper/Efficient-Global-Color%2C-Luminance%2C-and-Contrast-for-Hong-Xu/0a9eefbcfbd20a326c0ee539a871a7205a3aefba",
            "/paper/Multi-View-Image-Reconstruction-Algorithm-Based-on-Liao-Wu/8ac44d2452d313dc614ef748eac3288d8e252388",
            "/paper/A-unified-probabilistic-framework-of-robust-and-for-Li-Yin/be6a7d3f90c6a9d2575bf42745a3e27fe17884ce",
            "/paper/A-closed-form-solution-for-multi-view-color-with-Xia-Yao/84a259fcb9268c82403a76e6db897d67e84c572d",
            "/paper/Global-Multiple-View-Color-Consistency-Moulon-Duisit/2fb66a0c2975363d0cb070e3315572298f1b4382",
            "/paper/Color-Correction-for-Image-Based-Modeling-in-the-Shen-Wang/488dc439af342a702bfcc5ab50e5e35272e4a4b1",
            "/paper/Color-adjustment-in-image-based-texture-maps-Pan-Taubin/15f476e7d23f36252fb7a5b19fea1f83914e4c4c",
            "/paper/Seamless-Mosaicing-of-Image-Based-Texture-Maps-Lempitsky-Ivanov/8f7ff9e9474a5eb1bac32c0a649d6ccd152a3674",
            "/paper/Let-There-Be-Color!-Large-Scale-Texturing-of-3D-Waechter-Moehrle/b8f1ea118487d8500d45e5fbf95ab80eedd7fa92",
            "/paper/Performance-evaluation-of-color-correction-for-and-Xu-Mulligan/2d27bc174c30444c5d0939defbcda734af824e05",
            "/paper/Partial-iterates-for-symmetrizing-non-parametric-Vallet-Lel%C3%A9gard/9b6868dc7cc6dff01c5e9899f9b7d0a063f94797",
            "/paper/Efficient-and-Robust-Color-Consistency-for-Photo-Park-Tai/8aef04d7118798dc83b66ee75d790ae8429f81c9",
            "/paper/Joint-Texture-and-Geometry-Optimization-for-RGB-D-Fu-Yan/1be4516222673337e2b4975ac1a3dcaf597b28d4"
        ]
    },
    {
        "id": "aaac08ee1b4cae016fdd531b2487794007811d8b",
        "title": "Visual Tracking via Dynamic Memory Networks",
        "abstract": "A dynamic memory network to adapt the template to the target's appearance variations during tracking and design a \u201cnegative\u201d memory unit that stores templates for distractors, which are used to cancel out wrong responses from the object template. Template-matching methods for visual tracking have gained popularity recently due to their good performance and fast speed. However, they lack effective ways to adapt to changes in the target object's appearance, making their tracking accuracy still far from state-of-the-art. In this paper, we propose a dynamic memory network to adapt the template to the target's appearance variations during tracking. The reading and writing process of the external memory is controlled by an LSTM network with the search feature map as input. A spatial attention mechanism is applied to concentrate the LSTM input on the potential target as the location of the target is at first unknown. To prevent aggressive model adaptivity, we apply gated residual template learning to control the amount of retrieved memory that is used to combine with the initial template. In order to alleviate the drift problem, we also design a \u201cnegative\u201d memory unit that stores templates for distractors, which are used to cancel out wrong responses from the object template. To further boost the tracking performance, an auxiliary classification loss is added after the feature extractor part. Unlike tracking-by-detection methods where the object's information is maintained by the weight parameters of neural networks, which requires expensive online fine-tuning to be adaptable, our tracker runs completely feed-forward and adapts to the target's appearance changes by updating the external memory. Moreover, the capacity of our model is not determined by the network size as with other trackers \u2013 the capacity can be easily enlarged as the memory requirements of a task increase, which is favorable for memorizing long-term object information. Extensive experiments on the OTB and VOT datasets demonstrate that our trackers perform favorably against state-of-the-art tracking methods while retaining real-time speed.",
        "publication_year": "2019",
        "authors": [
            "Tianyu Yang",
            "Antoni B. Chan"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "37",
        "reference_count": "75",
        "references": [
            "/paper/DMV%3A-Visual-Object-Tracking-via-Part-level-Dense-Nam-Oh/924999cf37dbc578adac800557f547fb03172dfe",
            "/paper/Siamese-Based-Attention-Learning-Networks-for-Rahman-Jung/d47b65b7e39bce84c58ee5fc450935cd4891d4e1",
            "/paper/Siamese-High-Level-Feature-Refine-Network-for-Rahman-Ahmed/057f0dbb8f94d7ec3b5ce3c42a76a492d5ad727e",
            "/paper/Efficient-Visual-Tracking-With-Stacked-Attention-Rahman-Fiaz/e085fb462789a8bca7933b5e1c7e9aa0ded8a711",
            "/paper/Reliable-correlation-tracking-via-dual-memory-model-Li-Peng/86291d26c621a40f3aff75dcf75a7d780b30bc4d",
            "/paper/ROAM%3A-Recurrently-Optimizing-Tracking-Model-Yang-Xu/e1cfc317da9268897cd8e9a594178ef63f6d5ddf",
            "/paper/Structural-pixel-wise-target-attention-for-robust-Zhang-Cheng/6ed7d69e1357fe6d8affcdaebd0e2c2847ab45ba",
            "/paper/Siamese-Template-Diffusion-Networks-for-Robust-Liang-Zhao/e6374b0ee87c8dbd2080cd38d14d2e912be73a9e",
            "/paper/Recursive-Least-Squares-Estimator-Aided-Online-for-Gao-Hu/b03302d00dedd2ac8364ea41cd57a80df88d8b03",
            "/paper/Learning-complementary-Siamese-networks-for-visual-Tan-Xu/9e73da2cb3bf592f4775fa8866ab58cfe555dd33",
            "/paper/Learning-Dynamic-Memory-Networks-for-Object-Yang-Chan/26c50d272883fc8f72656526f915bb283f772b27",
            "/paper/Recurrent-Filter-Learning-for-Visual-Tracking-Yang-Chan/db39754bde43c5555d7086261d1a6fd55af7de06",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/MAVOT%3A-Memory-Augmented-Video-Object-Tracking-Liu-Wang/f02b2c0fd48f5ce6e26b075b6c06f027fbd5841d",
            "/paper/Learning-Attentions%3A-Residual-Attentional-Siamese-Wang-Teng/6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "/paper/Attentional-Correlation-Filter-Network-for-Adaptive-Choi-Chang/f2c050fa106b2e6f27d32e21e75ecbdb0cc75f67",
            "/paper/Good-Features-to-Correlate-for-Visual-Tracking-Gundogdu-Alatan/388d29f001411ff80650f80cf197afc440d98b51",
            "/paper/Dual-Deep-Network-for-Visual-Tracking-Chi-Li/1812c98ad2a9aed7ef3e662449297d9b8807a3d9",
            "/paper/Learning-Policies-for-Adaptive-Tracking-with-Deep-Huang-Lucey/3281c2fa244834400c970f33a20aa1fb0ca2f53d",
            "/paper/Visual-Tracking-with-Fully-Convolutional-Networks-Wang-Ouyang/bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0"
        ]
    },
    {
        "id": "e0928193387ddb00a6e836988a41e5d2a31cd718",
        "title": "SiamRPN++D: improved SiamRPN++ using cascaded detector sensing",
        "abstract": "A novel and robust long-term tracking algorithm to address continuous target tracking problems and proposes integration of a lightweight and efficient Cascaded Classifier based detection mechanism with the Siamese trackers for re-initialization of the target. This paper presents a novel and robust long-term tracking algorithm to address continuous target tracking problems. The continuous target tracking demands handling of correct re-initialization of the lost target when it reappears. The main limitation of the currently popular Siamese class of deep trackers is their inability to re-initialize a target when it is lost for sufficiently long duration or when it re-appears at a location away from the lost location. Most of the Siamese class of deep trackers search for the lost targets in a limited region, close to where it disappears. Hence, they fail in automated re-initialization, tracking resumption and maintaining track after long-term occlusion or tracker-loss. This puts a serious impediment on the current state-of-the-art deep tracker frameworks for many real applications. Here, we propose integration of a lightweight and efficient Cascaded Classifier based detection mechanism with the Siamese trackers for re-initialization of the target. While the proposed approach is generic and applicable to all Siamese class of deep trackers, we have taken SiamRPN++ as a base tracker to illustrate the effectiveness of our tracking framework. The proposition enables Cascaded Classifier based detector to adaptively direct the search region for the base tracker. Extensive experimental results on the well known tracking benchmark datasets such as UAV123, VOT2019, VOT2016, VOT2018 and VOT2018-LT show that the proposed integration significantly improves the performance of the base tracker under occlusion and tracker-loss scenarios. Further, the proposed tracker improves the precision by 1.71% and the recall by 13.98% over the base tracker for the long-term tracking on the VOT2018-LT dataset.",
        "publication_year": "2021",
        "authors": [
            "V. Nageli",
            "R. S. S. Gorthi",
            "Arshad Jamal"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "28",
        "references": [
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/Siam-R-CNN%3A-Visual-Tracking-by-Re-Detection-Voigtlaender-Luiten/069ccdbab6ea6ca2d9c3b75c76360ca1e4e9a5e9",
            "/paper/High-Performance-Long-Term-Tracking-With-Dai-Zhang/adacccd99a42c3145ec6392a1a6b08878376e38b",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/Long-term-target-tracking-combined-with-Wang-Yang/4be712d3c72940f35d43a3e9e6bdaa323b915e59",
            "/paper/%E2%80%98Skimming-Perusal%E2%80%99-Tracking%3A-A-Framework-for-and-Yan-Zhao/09b734072ad4f610478847c9cdc59a4a0c309b37",
            "/paper/Beyond-Local-Search%3A-Tracking-Objects-Everywhere-Zhu-Porikli/d20d7d3490fd970992b3631048c75a8c5fe2e4e3"
        ]
    },
    {
        "id": "8822c8ccff68073523b1f9bd60439e5a7c9cc0bb",
        "title": "Enhancing discriminative appearance model for visual tracking",
        "abstract": "Semantic Scholar extracted view of \"Enhancing discriminative appearance model for visual tracking\" by Xuedong He et al.",
        "publication_year": "2023",
        "authors": [
            "Xuedong He",
            "Calvin Yu\u2010Chian Chen"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "12",
        "references": [
            "/paper/A-joint-local-global-search-mechanism-for-long-term-Gao-Zhuang/dc03a79ce543fef5cbbedf85c928dde1e92ad711",
            "/paper/Optimal-visual-tracking-using-Wasserstein-transport-Hong-Kwon/ac003f0a7cb8f7feaf1e42a73359bbedfd31940d",
            "/paper/Exploring-reliable-visual-tracking-via-target-He-Chen/d33ff3d7a6d8603a2978f1e7646f1221fd8313a7",
            "/paper/Learning-object-uncertainty-policy-for-visual-He-Chen/ca3330ead176d8a5cf30f8c74ab66a4ffb8c9491",
            "/paper/Variable-scale-learning-for-visual-object-tracking-He-Zhao/ba961479e4fee1452c00a3ccf3457933b777ebdf",
            "/paper/Recent-trends-in-multicue-based-visual-tracking%3A-A-Kumar-Walia/4f83865262b73a3aa9ecc2972eb06b73a2fe073e",
            "/paper/Efficient-scale-estimation-methods-using-deep-for-Marvasti-Zadeh-Ghanei-Yakhdan/c2f098ad98aba65514f796d600d9bc920c623243",
            "/paper/Deep-Learning-for-Visual-Tracking%3A-A-Comprehensive-Marvasti-Zadeh-Cheng/1fbb4201af091aef55360f113ba35814063923e4",
            "/paper/SiamFC%2B%2B%3A-Towards-Robust-and-Accurate-Visual-with-Xu-Wang/be412c7c7128cf91455233b652d6c94a6001a7c8",
            "/paper/GOT-10k%3A-A-Large-High-Diversity-Benchmark-for-in-Huang-Zhao/c75180ab22b80b7ac3c8853a934ac515313b9aad",
            "/paper/Discriminative-Scale-Space-Tracking-Danelljan-H%C3%A4ger/ce8c76bfedc5d86faabf0d49dc42a4924f75876d"
        ]
    },
    {
        "id": "bd38bb4fb00081ee76cedce8e51aa4e0aae94e7e",
        "title": "NullSpaceRDAR: Regularized discriminative adaptive nullspace for object tracking",
        "abstract": "Semantic Scholar extracted view of \"NullSpaceRDAR: Regularized discriminative adaptive nullspace for object tracking\" by Mohamed H. Abdelpakey et al.",
        "publication_year": "2022",
        "authors": [
            "Mohamed H. Abdelpakey",
            "M. Shehata"
        ],
        "related_topics": [
            "Mathematics"
        ],
        "citation_count": 0,
        "reference_count": "13",
        "references": [
            "/paper/Robust-visual-tracking-based-on-modified-mayfly-Xiao-Wu/0ccb7a661cf884cf8a4845c9b331174afbd756e3",
            "/paper/The-Eighth-Visual-Object-Tracking-VOT2020-Challenge-Kristan-Leonardis/12508951ba96b7d4c0906ed95542287d3ebdfd95",
            "/paper/AFOD%3A-Adaptive-Focused-Discriminative-Segmentation-Chen-Xu/4a2d0374dac0b4b43de606a610dde145054f01ba",
            "/paper/DomainSiam%3A-Domain-Aware-Siamese-Network-for-Visual-Abdelpakey-Shehata/8b8d4bfae23d6c6573dcfe08716fe695d95b33b1",
            "/paper/DensSiam%3A-End-to-End-Densely-Siamese-Network-with-Abdelpakey-Shehata/4bebfa7000d7da4c5e8cb48d662b6b95dcaed163",
            "/paper/End-to-End-Representation-Learning-for-Correlation-Valmadre-Bertinetto/000178cd12c8a6e5da8215b6365fae03c20fd18d",
            "/paper/Meta-Networks-Munkhdalai-Yu/470d11b8ca4586c930adbbfc3f60bff08f2a0161",
            "/paper/A-Benchmark-and-Simulator-for-UAV-Tracking-Mueller-Smith/27850781e39df9f750e05409b8072261124068e8",
            "/paper/Learning-to-Track-at-100-FPS-with-Deep-Regression-Held-Thrun/5f0850ec47a17f22ba2611a5cb67a30cb02cf306",
            "/paper/Deep-Linear-Discriminant-Analysis-Dorfer-Kelz/6ad76e94065936057b9bbd3c77b8cefef6d37846",
            "/paper/Two-deterministic-half-quadratic-regularization-for-Charbonnier-Blanc-F%C3%A9raud/870fdfe89c440f35ce83b915519001ea0da518ec"
        ]
    },
    {
        "id": "f874d500f82a7d3b49f373fa68abb4453c1a9207",
        "title": "HiDAnet: RGB-D Salient Object Detection via Hierarchical Depth Awareness",
        "abstract": "This paper proposes a novel Hierarchical Depth Awareness network (HiDAnet) for RGB-D saliency detection, and uses a granularity-based attention scheme to strengthen the discriminatory power of RGB and depth features separately and introduces a unified cross dual-attention module for multi-modal and multi-level fusion in a coarse-to-fine manner. RGB-D saliency detection aims to fuse multi-modal cues to accurately localize salient regions. Existing works often adopt attention modules for feature modeling, with few methods explicitly leveraging fine-grained details to merge with semantic cues. Thus, despite the auxiliary depth information, it is still challenging for existing models to distinguish objects with similar appearances but at distinct camera distances. In this paper, from a new perspective, we propose a novel Hierarchical Depth Awareness network (HiDAnet) for RGB-D saliency detection. Our motivation comes from the observation that the multi-granularity properties of geometric priors correlate well with the neural network hierarchies. To realize multi-modal and multi-level fusion, we first use a granularity-based attention scheme to strengthen the discriminatory power of RGB and depth features separately. Then we introduce a unified cross dual-attention module for multi-modal and multi-level fusion in a coarse-to-fine manner. The encoded multi-modal features are gradually aggregated into a shared decoder. Further, we exploit a multi-scale loss to take full advantage of the hierarchical information. Extensive experiments on challenging benchmark datasets demonstrate that our HiDAnet performs favorably over the state-of-the-art methods by large margins. The source code can be found in https://github.com/Zongwei97/HIDANet/.",
        "publication_year": "2023",
        "authors": [
            "Zongwei Wu",
            "Guillaume Allibert",
            "F. M\u00e9riaudeau",
            "Chao Ma",
            "C. Demonceaux"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "79",
        "references": [
            "/paper/Object-Segmentation-by-Mining-Cross-Modal-Semantics-Wu-Wang/ccc1b63f60acad94f3aef6ea3fcf25cdb867800d",
            "/paper/You-Only-Need-One-Detector%3A-Unified-Object-Detector-Shen-Li/d14b5505fec39c88b747c21c1d504140bd0aea99",
            "/paper/Dynamic-Selective-Network-for-RGB-D-Salient-Object-Wen-Yan/7b0035ee0b60c7e15f378d67cadada7028b01e8b",
            "/paper/Robust-RGB-D-Fusion-for-Saliency-Detection-Wu-Gobichettipalayam/1bc08388f83c7a2fccf7b38c97fab8c1d751cc22",
            "/paper/MVSalNet%3A-Multi-view-Augmentation-for-RGB-D-Salient-Zhou-Wang/13c9d9fbcf32d0fdbe8fde29a4fe241875c36a81",
            "/paper/Specificity-preserving-RGB-D-saliency-detection-Zhou-Fu/eea134f36dec9c7d1e7ef7f1f961cc193272a1fc",
            "/paper/Deep-RGB-D-Saliency-Detection-with-Depth-Sensitive-Sun-Zhang/a83a62da1564721cc41b8ae037ba17563c233955",
            "/paper/RGB-D-Saliency-Detection-via-Cascaded-Mutual-Zhang-Fan/b17eb490ed7fed64d286c3497be6ddeae74be975",
            "/paper/Accurate-RGB-D-Salient-Object-Detection-via-Ji-Li/6c6b7598380b0075db53377ccd6577f8e40f47d3",
            "/paper/RGBD-Salient-Object-Detection%3A-A-Benchmark-and-Peng-Li/fd455bef8dea04acc37f1a4b4be47068bbf58303",
            "/paper/Modality-Guided-Subnetwork-for-Salient-Object-Wu-Allibert/3b93d52d9c3c50f18610c52019b0d48bf41dbb1c",
            "/paper/Asymmetric-Two-Stream-Architecture-for-Accurate-Zhang-Fei/2545ce0cae295d30c9a831f0f9c2779338b29c72"
        ]
    },
    {
        "id": "a831a3ad3183fdf1f115dc3639be7bf414eeb7c6",
        "title": "Robuste Detektion, Verfolgung und Wiedererkennung von Personen in Videodaten mit niedriger Aufl\u00f6sung",
        "abstract": "Diese Arbeit umfasst Methoden and Verbesserungen auf Basis neuartiger Personenreprasentationen fur die Detektion, Verfolgung and erscheinungsbasierte Wiedererkennung von Personen. Mit der zunehmenden Menge an Bilddaten im Videouberwachungssektor wachst die Chance, Straftaten besser aufklaren zu konnen. Allerdings ist dafur ein immenser Aufwand fur die Auswertung der Bilder erforderlich, die oft nicht mehr vollstandig ohne Computerunterstutzung durch Personen gesichtet werden konnen. Diese Arbeit umfasst Methoden und Verbesserungen auf Basis neuartiger Personenreprasentationen fur die Detektion, Verfolgung und erscheinungsbasierte Wiedererkennung von Personen.",
        "publication_year": "2019",
        "authors": [
            "J. Metzler"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "286",
        "references": [
            "/paper/Image-Based-3D-Reconstruction-of-Dynamic-Objects-Bullinger/654f7f942706f17657e9608a42d8907dd0a7b136",
            "/paper/Wissensbasierte-probabilistische-Modellierung-f%C3%BCr-Fischer/1081a484fda628cdff9723eb0f9087a9f771baf7",
            "/paper/Permanente-Optimierung-dynamischer-Probleme-der-von-Dimitrov/44ae231ffbbc06861987739b19219e9d551b3415",
            "/paper/Multimodale-Interaktion-in-Multi-Display-Umgebungen-Bader/ac33986d8c3efac0842e5a162e030e2a89f6e086",
            "/paper/Planung-kooperativer-Fahrman%C3%B6ver-f%C3%BCr-kognitive-Frese/9c480b6a1acd77340fe985d0d233950b62ae509d",
            "/paper/Multisensorielle-diskret-kontinuierliche-und-Milighetti/d7931cb56453e304baa217bcf278ff5683cc74f3",
            "/paper/Privatheit-und-Datenschutz-in-der-intelligenten-Ein-Vagts/76e7753433be54fb77c05b340c0149447b7a88d3",
            "/paper/Ber%C3%BChrungslose-Bestimmung-der-Herz-und-Kroschel-Metzler/876fcb0a938be198433a4813f8dfdeaa303ebc2a",
            "/paper/Ein-generisches-System-zur-automatischen-Detektion%2C-J%C3%BCngling/1bea531e8271202462c7907f60a8458fa5aec00d",
            "/paper/Recurrent-Convolutional-Network-for-Video-Based-McLaughlin-Rinc%C3%B3n/57052d3357cfedf389e3c7b03d9e5f2772d01038",
            "/paper/Fast-human-detection-from-videos-using-covariance-Yao-Odobez/53f78b01fd5ad19abda19b56182b49367832432b"
        ]
    },
    {
        "id": "1171234cb2f3e1589592e3d04eb10c132fc6a5c8",
        "title": "Siamese Object Tracking for Unmanned Aerial Vehicle: A Review and Comprehensive Analysis",
        "abstract": "A comprehensive review of leading-edge Siamese trackers, along with an exhaustive UAV-speci\ufb01c analysis based on the evaluation using a typical UAV onboard processor, are presented. \u2014Unmanned aerial vehicle (UAV)-based visual object tracking has enabled a wide range of applications and attracted increasing attention in the \ufb01eld of intelligent transportation systems because of its versatility and effectiveness. As an emerging force in the revolutionary trend of deep learning, Siamese networks shine in UAV-based object tracking with their promising balance of accuracy, robustness, and speed. Thanks to the development of embedded processors and the gradual optimization of deep neural networks, Siamese trackers receive extensive research and realize preliminary combinations with UAVs. However, due to the UAV\u2019s limited onboard computational resources and the complex real-world circumstances, aerial tracking with Siamese networks still faces severe obstacles in many aspects. To further explore the deployment of Siamese networks in UAV-based tracking, this work presents a comprehensive review of leading-edge Siamese trackers, along with an exhaustive UAV-speci\ufb01c analysis based on the evaluation using a typical UAV onboard processor. Then, the onboard tests are conducted to validate the feasibility and ef\ufb01cacy of representative Siamese trackers in real-world UAV deployment. Furthermore, to better promote the development of the tracking community, this work analyzes the limitations of existing Siamese trackers and conducts additional experiments represented by low-illumination evaluations. In the end, prospects for the development of Siamese tracking for UAV-based intelligent transportation systems are deeply discussed. The uni\ufb01ed framework of leading-edge Siamese trackers, i.e. , code library, and the results of their experimental evaluations are available at https://github.com/vision4robotics/SiameseTracking4UAV.",
        "publication_year": "2022",
        "authors": [
            "Changhong Fu",
            "Kunhan Lu",
            "Guang-Zheng Zheng",
            "Junjie Ye",
            "Ziang Cao",
            "Bowen Li"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "8",
        "reference_count": "160",
        "references": [
            "/paper/Slight-Aware-Enhancement-Transformer-and-Multiple-Deng-Han/27054255eb56655da88ffa8f015e7398357d7f48",
            "/paper/SGDViT%3A-Saliency-Guided-Dynamic-Vision-Transformer-Yao-Fu/c846a8b1ffea73b8b1eee20d269e10c0a38aa1e8",
            "/paper/Boosting-UAV-Tracking-With-Voxel-Based-Pre-Training-Li-Fu/bed092b8140ea6392f7f69ebe56d616d589b3b0e",
            "/paper/A-Comprehensive-Review-for-Typical-Applications-Han-Liu/a57b49b8afca44c2a08adf53934874645518df88",
            "/paper/Cascaded-Denoising-Transformer-for-UAV-Nighttime-Lu-Fu/6a86342bc85a21f7169e03b7a5425c3ad95cb166",
            "/paper/Continuity-Aware-Latent-Interframe-Information-for-Fu-Cai/fca66237e0047d28a588fb6e25c0308c58fa253c",
            "/paper/Adversarial-Blur-Deblur-Network-for-Robust-UAV-Zuo-Fu/f06a22165f0bcc92eac865e3eeddec3c7e2cf44c",
            "/paper/AMST2%3A-aggregated-multi-level-spatial-and-temporal-Park-Lee/5c0b964fabb701523680c394303b5caa2bf1d1f7",
            "/paper/The-Unmanned-Aerial-Vehicle-Benchmark%3A-Object-and-Du-Qi/a213bbf9740854a276fdf71dad8f30cfbe3ea4d4",
            "/paper/Visual-Object-Tracking-for-Unmanned-Aerial-A-and-Li-Yeung/6ebc40a061433c24a3ea1f305bb6533b8f3dd5f4",
            "/paper/Correlation-Filters-for-Unmanned-Aerial-Aerial-A-Fu-Li/13232c0d4b863fecc3410286f6ddfe6535078d5c",
            "/paper/All-Day-Object-Tracking-for-Unmanned-Aerial-Vehicle-Li-Fu/f3d7eb617179db9f9621fa2c978dfb9f2c39341f",
            "/paper/Siamese-Anchor-Proposal-Network-for-High-Speed-Fu-Cao/827ee2cdc56ea65ef644bd8ca085f4274b106f03",
            "/paper/Efficient-Road-Detection-and-Tracking-for-Unmanned-Zhou-Kong/9ea475eef626bcbbad356d9b81d7ede1755e8e14",
            "/paper/ReCF%3A-Exploiting-Response-Reasoning-for-Correlation-Lin-Fu/df5ce51cfb089ce3c903bc613e867a7f7e226b4c",
            "/paper/Ad2Attack%3A-Adaptive-Adversarial-Attack-on-Real-Time-Fu-Li/532a31b2f74e43bb5607d3eca421e9953dc7ccd6",
            "/paper/Context-aware-MDNet-for-target-tracking-in-UAV-Bi-Lei/e8eaab2d8764f998c7dc321b825c9a1d0f0f4a71",
            "/paper/A-Benchmark-and-Simulator-for-UAV-Tracking-Mueller-Smith/27850781e39df9f750e05409b8072261124068e8"
        ]
    },
    {
        "id": "c19fd532794ee61998404e303cf1f08bc81793db",
        "title": "Foreground Information Guidance for Siamese Visual Tracking",
        "abstract": "This paper focuses on modifying the Siamese tracker by enriching the positive pairs and taking further advantage of the foreground information, and proposes an improved feature information fusion to update the template so that the tracker can adapt to the drastic appearance changes. Existing Siamese network based trackers are easily disturbed by large deformation, occlusion and distractor objects in the background. By comparing these trackers, we observe that the monotonous positive pairs usually have limited challenging factors (Occlusion, Deformation, etc.), which may make the learned features less robust. In addition, the foreground information of the substantial training data is utilized directly without deeper exploration. Thus, the trackers cannot effectively discriminate the foreground from the semantic backgrounds. In this paper, we focus on modifying the Siamese tracker by enriching the positive pairs and taking further advantage of the foreground information. During the offline training phase, a simple sampling strategy is adopted to enrich the challenging factors in positive pairs, which can effectively enhance the robustness of the tracker. At the same time, we highlight the foreground information by padding the background, and the information is utilized to generate a novel padding loss, which guides the tracker to pay less attention to the distractors in the background. Moreover, an improved feature information fusion is adopted to update the template, so that the tracker can adapt to the drastic appearance changes. Comprehensive experiments on the OTB and the VOT benchmarks demonstrate that our proposed tracker can achieve outstanding performance in both accuracy and robustness.",
        "publication_year": "2020",
        "authors": [
            "Daqun Li",
            "Yi Yu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "49",
        "references": [
            "/paper/Siamese-Visual-Object-Tracking%3A-A-Survey-Ondra%C5%A1ovi%C4%8D-Tar%C3%A1bek/177d12634a4df3a6f67a4aecd03714ff39845d0e",
            "/paper/Tracking-by-dynamic-template%3A-Dual-update-mechanism-Liu-Wang/0faaec6adf10015d9aac15d8689eb71d9a55d12a",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Towards-a-Better-Match-in-Siamese-Network-Based-He-Luo/01f46bd91e053ce0c92af126bb87d7381a9fbe29",
            "/paper/Siamese-Visual-Tracking-With-Deep-Features-and-Li-Wang/5766d1b3a5a176dc1fef891c54545f74620be0c9",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/Distractor-Aware-Visual-Tracking-by-Online-Siamese-Zha-Wu/03883930b96cf7386c8708bff6c1e6157ac16689",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/SINT%2B%2B%3A-Robust-Visual-Tracking-via-Adversarial-Wang-Li/8a075b0ed920f650315020d1420231172fbe5ed2",
            "/paper/Robust-Visual-Tracking-via-an-Improved-Background-Sheng-Liu/7eee4087e4087930726eee6b4395fa5e788b8f6a",
            "/paper/Siamese-Instance-Search-for-Tracking-Tao-Gavves/c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8"
        ]
    },
    {
        "id": "cf1559f1dc37fb77705efee31f01b2dda49e5ced",
        "title": "SiamET: a Siamese based visual tracking network with enhanced templates",
        "abstract": "SiamET, a Siamese-based network using Resnet-50 as its backbone with enhanced template module is proposed, which achieves superior performances than the state-of-the-art methods on 4 challenging benchmarks, including OTB100, VOT2018, Vot2019 and LaSOT. Discriminative correlation filter (DCF) played a dominant role in visual tracking tasks in early years. However, with the recent development of deep learning, the Siamese based networks begin to prevail. Unlike DCF, most Siamese network based tracking methods take the first frame as the reference, while ignoring the information from the subsequent frames. As a result, these methods may fail under unforeseeable situations (e.g. target scale/size changes, variant illuminations, occlusions etc.). Meanwhile, other deep learning based tracking methods learn discriminative filters online, where the training samples are extracted from a few fixed frames with predictable labels. However, these methods have the same limitations as Siamese-based trackers. The training samples are prone to have cumulative errors, which ultimately lead to tracking loss. In this situation, we propose SiamET, a Siamese-based network using Resnet-50 as its backbone with enhanced template module. Different from existing methods, our templates are acquired based on all historical frames. Extensive experiments have been carried out on popular datasets to verify the effectiveness of our method. It turns out that our tracker achieves superior performances than the state-of-the-art methods on 4 challenging benchmarks, including OTB100, VOT2018, VOT2019 and LaSOT. Specifically, we achieve an EAO score of 0.480 on VOT2018 with 31 FPS. Code is available at https://github.com/yu-1238/SiamET",
        "publication_year": "2022",
        "authors": [
            "Yuxin Zhou",
            "Yi Zhang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "44",
        "references": [
            "/paper/RHL-track%3A-visual-object-tracking-based-on-Meng-Gong/be2c3cb1d4412c71a3d7acff15e886855b211961",
            "/paper/Siamese-block-attention-network-for-online-update-Xiao-Tan/1c9f5cb6e46c4ad37cef7890256e9c0fa217c8ba",
            "/paper/Position-Attention-Network-for-Object-Tracking-Du-Wei/ca36c5d7578742352ff68e601cba168d496abc7d",
            "/paper/An-Adaptive-Dynamic-Multi-Template-Correlation-for-Hung-Lin/fa29830817ed76d05e07e4d18898e23ee313e320",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "/paper/GradNet%3A-Gradient-Guided-Network-for-Visual-Object-Li-Chen/47a58f8bec1d34004a7d7cf837e27a26de64f0f7",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Siamese-Cascaded-Region-Proposal-Networks-for-Fan-Ling/f98be9a91dbf00b52a494720bd36be9c73a1210e",
            "/paper/Siamese-Box-Adaptive-Network-for-Visual-Tracking-Chen-Zhong/cce1fecc800d2782da638f3060d5b2e887739f74",
            "/paper/Hierarchical-correlation-siamese-network-for-object-Meng-Deng/205caf1d8f8b90d3048ba1dade8f6e89a1a06911",
            "/paper/Learning-the-Model-Update-for-Siamese-Trackers-Zhang-Gonzalez-Garcia/157ff6e216985911cc2f9775155d2a424ba2984b",
            "/paper/Feature-Alignment-and-Aggregation-Siamese-Networks-Fan-Song/10e11dc9dafdca6bedf1327b988971f3cea0c83a"
        ]
    },
    {
        "id": "0c256b5ae76156b8cda006a0abbf7c84efdfcd1b",
        "title": "Compositional 3D Human-Object Neural Animation",
        "abstract": "A new compositional conditional neural radiance field (or CC-NeRF) is devised, which decomposes the interdependence between human and object using latent codes to enable compositionally animation control of novel HOIs. Human-object interactions (HOIs) are crucial for human-centric scene understanding applications such as human-centric visual generation, AR/VR, and robotics. Since existing methods mainly explore capturing HOIs, rendering HOI remains less investigated. In this paper, we address this challenge in HOI animation from a compositional perspective, i.e., animating novel HOIs including novel interaction, novel human and/or novel object driven by a novel pose sequence. Specifically, we adopt neural human-object deformation to model and render HOI dynamics based on implicit neural representations. To enable the interaction pose transferring among different persons and objects, we then devise a new compositional conditional neural radiance field (or CC-NeRF), which decomposes the interdependence between human and object using latent codes to enable compositionally animation control of novel HOIs. Experiments show that the proposed method can generalize well to various novel HOI animation settings. Our project page is https://zhihou7.github.io/CHONA/",
        "publication_year": "2023",
        "authors": [
            "Zhi Hou",
            "Baosheng Yu",
            "Dacheng Tao"
        ],
        "related_topics": [
            "Computer Science",
            "Art"
        ],
        "citation_count": 0,
        "reference_count": "71",
        "references": [
            "/paper/Compositional-Human-Scene-Interaction-Synthesis-Zhao-Wang/9077ada4333c1420009cd12d6b5030ae4eec2f85",
            "/paper/CHORE%3A-Contact%2C-Human-and-Object-REconstruction-a-Xie-Bhatnagar/6c01c2a74a1e98dc7bfa18553b5c786b9fc9342f",
            "/paper/Synthesizing-Long-Term-3D-Human-Motion-and-in-3D-Wang-Xu/c0fb0dec29714658950cc5b69bdea50508cfdf10",
            "/paper/Neural-Actor%3A-Neural-Free-view-Synthesis-of-Human-Liu-Habermann/29da3be81905d577dac9144c1c3cb5c6678f7027",
            "/paper/Neural-Free-Viewpoint-Performance-Rendering-under-Sun-Chen/9cb12a28abe74ab17826c47cd980efdfad94c01d",
            "/paper/Visual-Compositional-Learning-for-Human-Object-Hou-Peng/0ace6a2e9354d9a5ed31e3ce1914cfedf4974ba8",
            "/paper/NeuralHOFusion%3A-Neural-Volumetric-Rendering-under-Jiang-Jiang/af634c312d29f0cf21eefb0c5f1acfd534778fe2",
            "/paper/COUCH%3A-Towards-Controllable-Human-Chair-Zhang-Bhatnagar/2a37b307daef48766eeed8409003066b14955c3b",
            "/paper/D3D-HOI%3A-Dynamic-3D-Human-Object-Interactions-from-Xu-Joo/c25a724a849b489c95846992a42c3e9811eb0e9e",
            "/paper/ARAH%3A-Animatable-Volume-Rendering-of-Articulated-Wang-Schwarz/6d9b0bb216c36aba29a5b11a656a468dc39cad66"
        ]
    },
    {
        "id": "811ffb185bc90ac5d02d6dbfbcdb6173756b52ef",
        "title": "STMTrack: Template-free Visual Tracking with Space-time Memory Networks",
        "abstract": "A novel tracking framework built on top of a space-time memory network that is competent to make full use of historical information related to the target for better adapting to appearance variations during tracking is proposed. Boosting performance of the offline trained siamese trackers is getting harder nowadays since the fixed information of the template cropped from the first frame has been almost thoroughly mined, but they are poorly capable of resisting target appearance changes. Existing trackers with template updating mechanisms rely on time-consuming numerical optimization and complex hand-designed strategies to achieve competitive performance, hindering them from real-time tracking and practical applications. In this paper, we propose a novel tracking framework built on top of a space-time memory network that is competent to make full use of historical information related to the target for better adapting to appearance variations during tracking. Specifically, a novel memory mechanism is introduced, which stores the historical information of the target to guide the tracker to focus on the most informative regions in the current frame. Furthermore, the pixel-level similarity computation of the memory network enables our tracker to generate much more accurate bounding boxes of the target. Extensive experiments and comparisons with many competitive trackers on challenging large-scale benchmarks, OTB-2015, TrackingNet, GOT-10k, LaSOT, UAV123, and VOT2018, show that, without bells and whistles, our tracker outperforms all previous state-of-the-art real-time methods while running at 37 FPS. The code is available at https: //github.com/fzh0917/STMTrack.",
        "publication_year": "2021",
        "authors": [
            "Z. Fu",
            "Qingjie Liu",
            "Zehua Fu",
            "Yunhong Wang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "76",
        "reference_count": "70",
        "references": [
            "/paper/Dynamic-memory-network-with-spatial-temporal-fusion-Zhang-Bao/fb684ae27e8f25b445fc6a270aa6e3e3b12dae53",
            "/paper/Dynamic-Template-Selection-Through-Change-Detection-Kiran-Nguyen-Meidine/2f02729d35704bbec4ccd42fc59e80953f1c3f6b",
            "/paper/Anchor-Free-Tracker-Based-on-Space-Time-Memory-Han-Cao/27e6d0117217f57a9ba8bba860495e4a5147fedd",
            "/paper/Rethink-Object-Tracking-Based-on-Transformer-Jiang-Fan/95602da43843cb1284b8e393f26e096dcbdfa51d",
            "/paper/Target-Aware-Tracking-with-Long-term-Context-He-Zhang/62fc770746f6a283fbc5cfc32275e23bfef82eb7",
            "/paper/Siamese-Template-Diffusion-Networks-for-Robust-Liang-Zhao/e6374b0ee87c8dbd2080cd38d14d2e912be73a9e",
            "/paper/PROCONTEXT%3A-PROGRESSIVE-CONTEXT-TRANSFORMER-FOR-Lan-Cheng/2e9af6dd94959b184d6f2ba7c2b5298853fbcd53",
            "/paper/SiamLight%3A-lightweight-networks-for-object-tracking-Lin-Li/9c648d48e3da2a4b88410b261df746fb5a22136f",
            "/paper/ProContEXT%3A-Exploring-Progressive-Context-for-Lan-Cheng/522c2da0e51f6d3d50493e7a9a2dfedb7f72e649",
            "/paper/Accurate-Object-Tracking-by-Utilizing-Diverse-Prior-Hu-Lin/f067a6ce86b01ee3e06767b53e4d440b83ae1075",
            "/paper/Learning-Dynamic-Memory-Networks-for-Object-Yang-Chan/26c50d272883fc8f72656526f915bb283f772b27",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/GradNet%3A-Gradient-Guided-Network-for-Visual-Object-Li-Chen/47a58f8bec1d34004a7d7cf837e27a26de64f0f7",
            "/paper/Deep-Meta-Learning-for-Real-Time-Target-Aware-Choi-Kwon/6cbc9a49e920231887604e3c3a018a9f2fdbd5e0",
            "/paper/PG-Net%3A-Pixel-to-Global-Matching-Network-for-Visual-Liao-Wang/8dbb2a1939a60f56d0508f597d99c3dc2c27aa61",
            "/paper/Siamese-Box-Adaptive-Network-for-Visual-Tracking-Chen-Zhong/cce1fecc800d2782da638f3060d5b2e887739f74",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Deformable-Siamese-Attention-Networks-for-Visual-Yu-Xiong/5eca15a355b2a9a1e80879e850afe49d3c398c53",
            "/paper/Discriminative-and-Robust-Online-Learning-for-Zhou-Wang/365d7308a10da357009621d22b1548ab464277c3"
        ]
    },
    {
        "id": "d1f1b49e876dae4bb4165d91a59654f4b23bffae",
        "title": "A New Model for Brain Tumor Detection Using Ensemble Transfer Learning and Quantum Variational Classifier",
        "abstract": "Deep features are extracted from the inceptionv3 model, in which score vector is acquired from softmax and supplied to the quantum variational classifier (QVR) for discrimination between glioma, meningiomas, no tumor, and pituitary tumor to prove the proposed model's effectiveness. A brain tumor is an abnormal enlargement of cells if not properly diagnosed. Early detection of a brain tumor is critical for clinical practice and survival rates. Brain tumors arise in a variety of shapes, sizes, and features, with variable treatment options. Manual detection of tumors is difficult, time-consuming, and error-prone. Therefore, a significant requirement for computerized diagnostics systems for accurate brain tumor detection is present. In this research, deep features are extracted from the inceptionv3 model, in which score vector is acquired from softmax and supplied to the quantum variational classifier (QVR) for discrimination between glioma, meningioma, no tumor, and pituitary tumor. The classified tumor images have been passed to the proposed Seg-network where the actual infected region is segmented to analyze the tumor severity level. The outcomes of the reported research have been evaluated on three benchmark datasets such as Kaggle, 2020-BRATS, and local collected images. The model achieved greater than 90% detection scores to prove the proposed model's effectiveness.",
        "publication_year": "2022",
        "authors": [
            "Javeria Amin",
            "M. A. Anjum",
            "M. Sharif",
            "S. Jabeen",
            "Seifedine Kadry",
            "Pablo Moreno Ger"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "17",
        "reference_count": "83",
        "references": [
            "/paper/Classification-of-Brain-Tumors-Using-Ensemble-Saranya-Victoria/b0058c2dc73c301f32c821fc3b15959f081d653a",
            "/paper/Intelligent-Deep-Residual-Network-based-Brain-Tumor-Kartheeban-Kalyani/dcd3a6797fb01fa496c423cd86dba387884837e4",
            "/paper/Classification-and-Detection-of-Brain-Tumors-by-Agrawal/2a7d947e3c3ce3f64d9b85e9100352cac3917ce8",
            "/paper/Ensemble-deep-learning-for-brain-tumor-detection-Alsubai-Khan/0199b48d064010261efbb559ed7c3d6b993d6498",
            "/paper/Brain-Tumor-Detection-using-Machine-Learning-and-Shetty-Fernandes/671d9ba913499063e604eb6d7473a55196cd484b",
            "/paper/Brain-Tumor-Classification-using-Machine-Learning-A-Joshi-Rana/11aeeca80009f0c834045c603198bf1d6c41c7fc",
            "/paper/Detection-of-Brain-Tumors-from-MRI-Images-using-Magboo-Magboo/53882a8a77226705f61296ece3dd1ae570dfb411",
            "/paper/Brain-Tumor-Detection-using-VGG19-model-on-Adadelta-Gill-Sharma/ef2ab54822c1e4f9d046938d7faffe7585a80917",
            "/paper/Brain-Tumor-Segmentation-and-Classification-from-Sahoo-Mishra/696d82671df006913d8533a51c1285714d129acf",
            "/paper/CASS%3A-Cross-Architectural-Self-Supervision-for-Singh-Sizikova/be9a0f74c4143eef825d17499c6dcb1b90cb70e1",
            "/paper/A-distinctive-approach-in-brain-tumor-detection-and-Amin-Sharif/c4830304939b7599abed7d44c50ea4af9d608e68",
            "/paper/An-improved-framework-for-brain-tumor-analysis-MRI-Sharif-Li/9a967f869e545fd1d8f0aa4f69f9fa857f5f1e66",
            "/paper/Multi-Classification-of-Brain-Tumor-Images-Using-Sultan-Salem/abb9b66bc548dbbf25ae242e0996f8a61027cf23",
            "/paper/Brain-tumor-detection-using-fusion-of-hand-crafted-Saba-Mohamed/303bf2cae9becca7e9872e9c21c1767ad3aab6a4",
            "/paper/Brain-tumor-classification-using-deep-CNN-features-Deepak-Ameer/0273decc689f5664a9c44f789ea7e09c4fac859d",
            "/paper/Brain-tumor-classification-using-modified-kernel-Sasank-Venkateswarlu/020ab5accda88be24281300137a9eb157c8212bc",
            "/paper/Deep-CNN-for-Brain-Tumor-Classification-Ayadi-Elhamzi/49eab0f8ea47b8eed71b5c4b88ed6878789ec77e",
            "/paper/Convolutional-Neural-Network-with-Hyperparameter-Minarno-Mandiri/05098ab826219af4221033477f4342fc4eba62c9",
            "/paper/Detection-of-tumors-on-brain-MRI-images-using-the-Cinar-Y%C4%B1ld%C4%B1r%C4%B1m/a1f85d85a9a32bab9a8793421fa2bf7e3502924c",
            "/paper/Deep-learning-for-brain-tumor-classification-Paul-Plassard/33c6a41c1c163856c7fa74e52b6ee24eb7a8bf4f"
        ]
    },
    {
        "id": "a597f9e5586884eaafc3aeb4fbf9164536d3e045",
        "title": "Computer-aided diagnosis of mass-like lesion in breast MRI: Differential analysis of the 3-D morphology between benign and malignant tumors",
        "abstract": "Semantic Scholar extracted view of \"Computer-aided diagnosis of mass-like lesion in breast MRI: Differential analysis of the 3-D morphology between benign and malignant tumors\" by Yan-Hao Huang et al.",
        "publication_year": "2013",
        "authors": [
            "Yan-Hao Huang",
            "Yeun-Chung Chang",
            "Chiun-Sheng Huang",
            "Tsung-Ju Wu",
            "J. Chen",
            "R. Chang"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "40",
        "reference_count": "43",
        "references": [
            "/paper/Improving-the-Accuracy-of-Computer-aided-Diagnosis-Gallego-Ortiz-Martel/c68e2e15dcc87aae31420721d2cad332054d1cd2",
            "/paper/Preliminary-results-of-computer-aided-diagnosis-for-Yu-Huang/a4d0ab50e349e99b28b11910b8277cbc2dbf8c2e",
            "/paper/Tumor-Classification-in-Breast-Magnetic-Resonance-Pashoutan-Ayatollahi/f45f232c5d1e096be06ac4217de739b16aefccfd",
            "/paper/Differentiating-benign-and-malignant-mass-and-in-Ayatollahi-Shokouhi/f435c3f782e98246413f30adcf8f78899650d23f",
            "/paper/Evaluation-of-TP53-PIK3CA-mutations-using-texture-Moon-Chen/c3bf77c652e7bd49ec38014722a7c1e0ea0891e2",
            "/paper/Quantitative-breast-lesion-classification-based-on-Lo-Lai/5393d0af668cda6b123550ff6ecaf7d89254a60f",
            "/paper/Foundation-and-methodologies-in-computer-aided-for-Jalalian-Mashohor/831a1e8106571d82f22cd8dac7725964852b0154",
            "/paper/Breast-cancer-detection-and-classification-in-based-Pak-Kanan/82917b65ec0e7fa1bbdc0ba08642fd8c336196a1",
            "/paper/Automatic-segmentation-algorithm-and-Texture-for-on-Zara/a34ecf16b49c95956444e50e2baba5f53a341d79",
            "/paper/Computer-Aided-Evaluation-of-Malignancy-with-of-the/4e11d87889284076b150c2545b20d9c4b4008378",
            "/paper/Computer-aided-diagnosis-for-the-classification-of-Moon-Shen/c2b582affc0a9f9bde8b1cbb1f2b762cf1c00893",
            "/paper/Cancerous-breast-lesions-on-dynamic-MR-images%3A-for-Bhooshan-Giger/e7e9bb3b20c003e83962a13dfde1ec781589a176",
            "/paper/High-spatial-resolution-MRI-of-non-masslike-breast-Tozaki-Fukuda/777096254ae7348d2405f5bb39d92999a9207c74",
            "/paper/Computer-aided-interpretation-of-dynamic-magnetic-Baltzer-Vag/d59ffe8c1df8236e5316794a3ea9da0d8888a49a",
            "/paper/Characterization-of-breast-lesions-with-CE-MR-and-Vassiou-Kanavou/600180c5a7bdb004648d9e21662d3733bd28965e",
            "/paper/Volumetric-texture-analysis-of-breast-lesions-on-Chen-Giger/73e492d8a44aeb56cad41fff1c0a84aeec26e334",
            "/paper/Computer-aided-classification-system-for-breast-on-Shen-Chang/8ba3a85107c7a530a37041589b61598fb9ac8d8b",
            "/paper/ThyroScreen-system%3A-High-resolution-ultrasound-into-Acharya-Faust/4736a5a85cb4956faa51a2448448f43bd5fb7c60",
            "/paper/Automatic-identification-and-classification-of-of-Chen-Giger/a019ed011a24e96ed7bf33e95bf4ec4450c8eb45",
            "/paper/Correlation-between-High-Resolution-Dynamic-MR-and-Lee-Cho/28ed6c029d849d12e448a556f64bc0cadf9559ef"
        ]
    },
    {
        "id": "a812ebb6761ff453a02d866e97295932153537e2",
        "title": "Boosting Breast Cancer Detection Using Convolutional Neural Network",
        "abstract": "The proposed convolutional neural network method to boost the automatic identification of breast cancer by analyzing hostile ductal carcinoma tissue zones in whole-slide images (WSIs) is found to be successful, achieving results with 87% accuracy, which could reduce human mistakes in the diagnosis process. Breast cancer forms in breast cells and is considered as a very common type of cancer in women. Breast cancer is also a very life-threatening disease of women after lung cancer. A convolutional neural network (CNN) method is proposed in this study to boost the automatic identification of breast cancer by analyzing hostile ductal carcinoma tissue zones in whole-slide images (WSIs). The paper investigates the proposed system that uses various convolutional neural network (CNN) architectures to automatically detect breast cancer, comparing the results with those from machine learning (ML) algorithms. All architectures were guided by a big dataset of about 275,000, 50\u2009\u00d7\u200950-pixel RGB image patches. Validation tests were done for quantitative results using the performance measures for every methodology. The proposed system is found to be successful, achieving results with 87% accuracy, which could reduce human mistakes in the diagnosis process. Moreover, our proposed system achieves accuracy higher than the 78% accuracy of machine learning (ML) algorithms. The proposed system therefore improves accuracy by 9% above results from machine learning (ML) algorithms.",
        "publication_year": "2021",
        "authors": [
            "S. Alanazi",
            "M. Kamruzzaman",
            "Md Nazirul Islam Sarker",
            "Madallah Alruwaili",
            "Yousef Alhwaiti",
            "Nasser O. Alshammari",
            "M. H. Siddiqi"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "45",
        "reference_count": "40",
        "references": [
            "/paper/Classification-of-Breast-Cancer-Cell-Images-using-Tasnim-Shamrat/2aec07bc14d9151a0a25b06b70600b6aa9c0efc9",
            "/paper/Breast-Cancer-Detection-Using-Various-Models-with-Bor-Sivri/8f9f6996a025e44f41dda0a148cf828e4c9aaf69",
            "/paper/Employing-Deep-Learning-Feature-Extraction-Models-Gupta-Panwar/264f9618c1dbf9a5519b8d0aca766bb863d09a33",
            "/paper/A-Combined-Deep-CNN%3A-LSTM-with-a-Random-Forest-for-Begum-Kumar/ada83764622a1ddf4672d7e58650f64749fdc48f",
            "/paper/Comparative-Study-On-Breast-Cancer-Classification-Hossinq-Molla/703e359a0a15545c045967254843b17ceee1467e",
            "/paper/A-CNN-SVM-based-computer-aided-diagnosis-of-breast-Sahu-Tripathi/0155acaf746c70323768f6da043c14a0918b302f",
            "/paper/Detection-of-Breast-Cancer-from-Five-View-Thermal-Mammoottil-Kulangara/94b112de0884bb61cb4d752d55dc6b663070f53d",
            "/paper/Automated-Breast-Cancer-Detection-Models-Based-on-Alruwaili-Gouda/e5672449a04bea6f59d1631058510bd3a42b494d",
            "/paper/Detection-of-Breast-Cancer-Using-Machine-Learning-Bhise-Gadekar/84bac42a05b6ebdd9caa46bf44fd24e0290ffc7f",
            "/paper/Classification-of-Breast-Tumours-Based-on-Images-of-Abbasniya-Sheikholeslamzadeh/38a3511f5ace7871aa6b0a51e8a3ea62dbc24e14",
            "/paper/Multi-Class-Breast-Cancer-Classification-using-Deep-Nawaz-Sewissy/1a19f55bc7f5d3d90bd5cb239c0202b3428b6aa2",
            "/paper/A-Study-on-Automatic-Detection-of-IDC-Breast-Cancer-Wang-Ibrahim/d84bafd1fa5e28f41a1e6b0e7d7a36a61efc2fd8",
            "/paper/Convolutional-neural-network-based-models-for-of-Masud-Rashed/3cc5667b713021d06acb52174ee5f3d0c87e41b3",
            "/paper/Breast-cancer-detection-using-active-contour-and-by-Malathi-Sinthia/465527b0026978f5e37730cdf9529bf347a18981",
            "/paper/End-to-end-improved-convolutional-neural-network-Kumar-Srivastava/5604865ca83cf1bd838bb79a8614b4364f808e8e",
            "/paper/Using-Convolutional-Neural-Network-with-Cheat-Sheet-Ramadan/e3f5688692c955c004b9c7adf1c662777b6c7ca0",
            "/paper/An-anatomization-on-breast-cancer-detection-and-and-Desai-Shah/3010b7a8e99901b61bdbf55aabe89db6f18754c3",
            "/paper/Automatic-detection-of-invasive-ductal-carcinoma-in-Cruz-Roa-Basavanhally/2f11f86fd805807076b22317738c819484a8e21b",
            "/paper/Integrating-segmentation-information-into-CNN-for-Tsochatzidis-Koutla/1b5d4cf4c09acfeb2662ed10ac2ab33aceb3f3ad",
            "/paper/Automatic-Detection-and-Segmentation-of-Breast-on-Zhang-Chan/d2891229c476def083212ab51502090defcc87db"
        ]
    },
    {
        "id": "697b853386ddb894ccf3e45613bced6cb0c63155",
        "title": "Analysis of Histopathological Images for Early Diagnosis of Oral Squamous Cell Carcinoma by Hybrid Systems Based on CNN Fusion Features",
        "abstract": "This study developed several hybrid models based on the fused CNN features for diagnosing oral cancer, which have the ability to analyze medical images with a high level of precision and accuracy and have the potential for early detection of OSCC for treatment success and improved patient outcomes. Oral squamous cell carcinoma (OSCC) is one of the deadliest and most common types of cancer. The incidence of OSCC is increasing annually, which requires early diagnosis to receive appropriate treatment. The biopsy technique is one of the most important techniques for analyzing samples, but it takes a long time to get results. Manual diagnosis is still subject to errors and differences in doctors\u2019 opinions, especially in the early stages. Thus, automated techniques can help doctors and patients to receive appropriate treatment. This study developed several hybrid models based on the fused CNN features for diagnosing OSCC-100x and OSCC-400x datasets for oral cancer, which have the ability to analyze medical images with a high level of precision and accuracy. They can detect subtle patterns, abnormalities, or indicators of diseases that may be difficult to recognize with the naked eye. The systems have the potential to significantly reduce human error and provide more consistent and reliable results, resulting in improved diagnostic accuracy. The systems also have the potential for early detection of OSCC for treatment success and improved patient outcomes. By detecting diseases at an early stage, clinicians can initiate interventions in a timely manner, potentially preventing OSCC progression and improving the chances of successful treatment. The first strategy was based on GoogLeNet, ResNet101, and VGG16 models pretrained, which did not achieve satisfactory results. The second strategy was based on GoogLeNet, ResNet101, and VGG16 models based on the adaptive region growing (ARG) segmentation algorithm. The third strategy is based on a mixed technique between GoogLeNet, ResNet101, and VGG16 models and ANN and XGBoost networks based on the ARG hashing algorithm. The fourth strategy for oral cancer diagnosis by ANN and XGBoost is based on features fused between CNN models. The ANN with fusion features of GoogLeNet-ResNet101-VGG16 yielded an AUC of 98.85%, accuracy of 99.3%, sensitivity of 98.2%, precision of 99.5%, and specificity of 98.35%.",
        "publication_year": "2023",
        "authors": [
            "I. A. Ahmed",
            "Ebrahim Mohammed Senan",
            "Hamzeh Salameh Ahmad Shatnawi"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "36",
        "references": [
            "/paper/Early-Diagnosis-of-Oral-Squamous-Cell-Carcinoma-on-Fati-Senan/57f8e86f2d2ae4a6ae01550a06d3b52db4e994fc",
            "/paper/Histopathology-Based-Diagnosis-of-Oral-Squamous-Yang-Li/19564f39bec7ff5a022b557982ea51b8326b10c6",
            "/paper/An-Enhanced-Histopathology-Analysis%3A-An-AI-Based-of-Musulin-%C5%A0tifani%C4%87/21499cb2ab0c4ac23facee3860b9f59271dadf4a",
            "/paper/An-ensemble-deep-learning-model-with-empirical-for-Deo-Pal/7952a2f8354186182d0c61626b3d6491b48833bb",
            "/paper/Diagnosis-of-Oral-Squamous-Cell-Carcinoma-Using-and-Deif-Attar/18496aa760fac675cb75e5197a85f5d270bde472",
            "/paper/Convolutional-Neural-Network-Based-Clinical-of-Oral-Camalan-Mahmood/eda4b0195fc8d0496be618629c2274e4e9023ba0",
            "/paper/Capsule-network-based-analysis-of-histopathological-Panigrahi-Das/c62a33a886474a78777987c94b84b8a0d996e700",
            "/paper/Automatic-Detection-of-Oral-Squamous-Cell-Carcinoma-Das-Dash/8930a224de45217ece72efa125d192b41e81c00f",
            "/paper/AI-Techniques-of-Dermoscopy-Image-Analysis-for-the-Olayah-Senan/af5dcf1b9215dbfd9de2385952e4f59318b1f0a4",
            "/paper/Histopathological-imaging-database-for-oral-cancer-Rahman-Mahanta/18d2d888d5e1a53b9d9261cd419804ef4205902a"
        ]
    },
    {
        "id": "d8b67c16876d8557ce91485bd68e71cf41792ac1",
        "title": "Performance and Robustness of Regional Image Segmentation Driven by Selected Evolutionary and Genetic Algorithms: Study on MR Articular Cartilage Images",
        "abstract": "This study objectively analyzes the performance of the segmentation strategies upon variable noise with dynamic intensities to report a segmentation\u2019s robustness in various image conditions for a various number of segmentation classes, and suggests that the combination of fuzzy thresholding with an ABC algorithm gives the best performance in the comparison with other methods as from the view of the segmentsation influence of additive dynamic noise influence. The analysis and segmentation of articular cartilage magnetic resonance (MR) images belongs to one of the most commonly routine tasks in diagnostics of the musculoskeletal system of the knee area. Conventional regional segmentation methods, which are based either on the histogram partitioning (e.g., Otsu method) or clustering methods (e.g., K-means), have been frequently used for the task of regional segmentation. Such methods are well known as fast and well working in the environment, where cartilage image features are reliably recognizable. The well-known fact is that the performance of these methods is prone to the image noise and artefacts. In this context, regional segmentation strategies, driven by either genetic algorithms or selected evolutionary computing strategies, have the potential to overcome these traditional methods such as Otsu thresholding or K-means in the context of their performance. These optimization strategies consecutively generate a pyramid of a possible set of histogram thresholds, of which the quality is evaluated by using the fitness function based on Kapur\u2019s entropy maximization to find the most optimal combination of thresholds for articular cartilage segmentation. On the other hand, such optimization strategies are often computationally demanding, which is a limitation of using such methods for a stack of MR images. In this study, we publish a comprehensive analysis of the optimization methods based on fuzzy soft segmentation, driven by artificial bee colony (ABC), particle swarm optimization (PSO), Darwinian particle swarm optimization (DPSO), and a genetic algorithm for an optimal thresholding selection against the routine segmentations Otsu and K-means for analysis and the features extraction of articular cartilage from MR images. This study objectively analyzes the performance of the segmentation strategies upon variable noise with dynamic intensities to report a segmentation\u2019s robustness in various image conditions for a various number of segmentation classes (4, 7, and 10), cartilage features (area, perimeter, and skeleton) extraction preciseness against the routine segmentation strategies, and lastly the computing time, which represents an important factor of segmentation performance. We use the same settings on individual optimization strategies: 100 iterations and 50 population. This study suggests that the combination of fuzzy thresholding with an ABC algorithm gives the best performance in the comparison with other methods as from the view of the segmentation influence of additive dynamic noise influence, also for cartilage features extraction. On the other hand, using genetic algorithms for cartilage segmentation in some cases does not give a good performance. In most cases, the analyzed optimization strategies significantly overcome the routine segmentation methods except for the computing time, which is normally lower for the routine algorithms. We also publish statistical tests of significance, showing differences in the performance of individual optimization strategies against Otsu and K-means method. Lastly, as a part of this study, we publish a software environment, integrating all the methods from this study.",
        "publication_year": "2022",
        "authors": [
            "J. Kub\u00edcek",
            "Alice Varysova",
            "M. Cern\u00fd",
            "Kristyna Hancarova",
            "D. Oczka",
            "M. Augustynek",
            "M. Penhaker",
            "Ondrej Prokop",
            "R. \u0160\u010durek"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "73",
        "references": [
            "/paper/Efficient-Approach-to-Color-Image-Segmentation-on-Rangu-Veramalla/f1a70bc44deabe60ffaadee86d1e314796456158",
            "/paper/State-of-the-Art-Level-Set-Models-and-Their-in-A-Biswas-Hazra/1e169eb424ec17b8fe6cd9d907f98eef09991b02",
            "/paper/Ultrasonic-Image-Segmentation-Algorithm-of-Thyroid-Xiangyu-Huan/32c1d17884c55e3f6c4236037d82b5fcac082dea",
            "/paper/Superpixel-voxel-medical-image-segmentation-based-Fang-Wang/8bf0fd8d14f362173b401d7d1a2472f6daa29496",
            "/paper/Computational-Methods-for-Liver-Vessel-Segmentation-Ciecholewski-Kassja%C5%84ski/f49b07b6dbd815a051e5a350b42c2d8f785b8dad",
            "/paper/Medical-Imaging-using-Signal-Processing%3A-A-Review-Mishra-Aravinda/62db5cb3c89613c9b94539a554489fd943962b10",
            "/paper/Magnetic-resonance-image-based-brain-tumour-A-Bhalodiya-Keung/bda924be4c8f0d087783d6c62cd0687abdb99bbb",
            "/paper/Brain-stroke-computed-tomography-images-analysis-A-Ali-Abdullah/95b1fce2fbe3b8a6fca7ea83825a0bc7945773e7",
            "/paper/Border-Detection-in-Skin-Lesion-Images-Using-an-Jayalakshmi-Dheeba/7b07a0c4f00a1bda5dc34d78ea55c5beac4bce6b",
            "/paper/An-effective-approach-for-CT-lung-segmentation-Yang-Xu/3bf96a19d72fd7eb3fc9404fc02f475f276ca693",
            "/paper/A-Comprehensive-Survey-on-Bone-Segmentation-in-Knee-Ahmed-Mstafa/e67138bc7d675cbe6601b4aef14b1935830f977c"
        ]
    },
    {
        "id": "08e088b3e1187398a0f1edd9b7bffffbe422d5fd",
        "title": "An Efficient Wildfire Detection System for AI-Embedded Applications Using Satellite Imagery",
        "abstract": "The proposed methodology can effectively design an ML application that instantly and efficiently analyses imagery from a spacecraft/weather balloon for the detection of wildfires without the need of an earth control centre. Wildfire risk has globally increased during the past few years due to several factors. An efficient and fast response to wildfires is extremely important to reduce the damaging effect on humans and wildlife. This work introduces a methodology for designing an efficient machine learning system to detect wildfires using satellite imagery. A convolutional neural network (CNN) model is optimized to reduce the required computational resources. Due to the limitations of images containing fire and seasonal variations, an image augmentation process is used to develop adequate training samples for the change in the forest\u2019s visual features and the seasonal wind direction at the study area during the fire season. The selected CNN model (MobileNet) was trained to identify key features of various satellite images that contained fire or without fire. Then, the trained system is used to classify new satellite imagery and sort them into fire or no fire classes. A cloud-based development studio from Edge Impulse Inc. is used to create a NN model based on the transferred learning algorithm. The effects of four hyperparameters are assessed: input image resolution, depth multiplier, number of neurons in the dense layer, and dropout rate. The computational cost is evaluated based on the simulation of deploying the neural network model on an Arduino Nano 33 BLE device, including Flash usage, peak random access memory (RAM) usage, and network inference time. Results supported that the dropout rate only affects network prediction performance; however, the number of neurons in the dense layer had limited effects on performance and computational cost. Additionally, hyperparameters such as image size and network depth significantly impact the network model performance and the computational cost. According to the developed benchmark network analysis, the network model MobileNetV2, with 160 \u00d7 160 pixels image size and 50% depth reduction, shows a good classification accuracy and is about 70% computationally lighter than a full-depth network. Therefore, the proposed methodology can effectively design an ML application that instantly and efficiently analyses imagery from a spacecraft/weather balloon for the detection of wildfires without the need of an earth control centre.",
        "publication_year": "2023",
        "authors": [
            "George L. James",
            "Ryeim B. Ansaf",
            "Sanaa S. Al Samahi",
            "Rebecca D. Parker",
            "Joshua M. Cutler",
            "Rhode V. Gachette",
            "Bahaa I. Ansaf"
        ],
        "related_topics": [
            "Environmental Science",
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "36",
        "references": [
            "/paper/Active-Fire-Detection-Using-a-Novel-Convolutional-Hong-Tang/24351af01bdfbd1912bd53d0374e597eb12dbcf6",
            "/paper/Deep-Learning-Based-Forest-Fire-Classification-and-Priya-Vani/1c08014616f2b7693da71869aeb4cb32c8106b43",
            "/paper/Early-Detection-of-Forest-Fire-Using-Mixed-Learning-Kasyap-Sumathi/983feca034b88978cdcd776b21c5ee392f94a447",
            "/paper/Preliminary-Results-from-a-Wildfire-Detection-Using-Govil-Welch/b6de2c3fed06d838380a501b4d163126307b0000",
            "/paper/Transfer-Learning-for-Wildfire-Identification-in-Wu-Li/c4deaf733e7955e3b0301fad2f589481723258c4",
            "/paper/Real-Time-Wildfire-Detection-and-Alerting-with-a-Zhang-Zhang/190982bb98f985ad145f3f1e3ba460c83966fc59",
            "/paper/Fire-Net%3A-A-Deep-Learning-Framework-for-Active-Fire-Seydi-Saeidi/ed79be00f77eae04d55dac42d0ab5f565e8a0e87",
            "/paper/Wildfire-Detection-Method-Using-DenseNet-and-Data-Park-Tran/7a4a9a7c05b8de396de4ed3ab545236e145fa858",
            "/paper/A-Deep-Learning-Based-Object-Identification-System-Guede-Fern%C3%A1ndez-Martins/8221a67350bbc28917eb33cb6800325979f61544",
            "/paper/Forest-Classification-Method-Based-on-Convolutional-Miranda-Mutiara/8dc641c22388229a2b178a73f9ef32a4e5a2f512"
        ]
    },
    {
        "id": "0eeea4143ffac54d3af9868fe677d164de4f12b7",
        "title": "Ferroelectric Hf0.5Zr0.5O2 Thin Films: A Review of Recent Advances",
        "abstract": "Ferroelectricity in HfO2-based materials, especially Hf0.5Zr0.5O2 (HZO), is today one of the most attractive topics because of its wide range of applications in ferroelectric random-access memory, ferroelectric field-effect transistors, ferroelectric tunneling junctions, steep-slope devices, and synaptic devices. The main reason for this increasing interest is that, when compared with conventional ferroelectric materials, HZO is compatible with complementary metal\u2013oxide\u2013semiconductor flow [even back-end of the line thermal budget] and can exhibit robust ferroelectricity even at extremely thin (<\u200910 nm) thicknesses. In this report, recent advances in the ferroelectric properties of HZO thin films since the first report in 2011, including doping effects, mechanical stress effects, interface effects, and ferroelectric film thickness effects, are comprehensively reviewed.",
        "publication_year": "2018",
        "authors": [
            "S. Kim",
            "Jaidah Mohan",
            "S. Summerfelt",
            "Jiyoung Kim"
        ],
        "related_topics": [
            "Chemistry"
        ],
        "citation_count": 0,
        "reference_count": "4",
        "references": [
            "/paper/Ferroelectric-Hf0.5Zr0.5O2-Thin-Films%3A-A-Review-of-Kim-Mohan/c4b81dbbe80da09b7779d5fce6f267b1be2b8376"
        ]
    },
    {
        "id": "71a161c1450931fda797833655f99969799a18c2",
        "title": "Mechanical Polarization Switching in Hf0.5Zr0.5O2 Thin Film.",
        "abstract": "HfO2-based films with high compatibility with Si and complementary metal-oxide semiconductors (CMOS) have been widely explored in recent years. In addition to ferroelectricity and antiferroelectricity, flexoelectricity, the coupling between polarization and a strain gradient, is rarely reported in HfO2-based films. Here, we demonstrate that the mechanically written out-of-plane domains are obtained in 10 nm Hf0.5Zr0.5O2 (HZO) ferroelectric film at room temperature by generating the stress gradient via the tip of an atomic force microscope. The results of scanning Kelvin force microscopy (SKPM) exclude the possibility of flexoelectric-like mechanisms and prove that charge injection could be avoided by mechanical writing and thus reveal the true polarization state, promoting wider flexoelectric applications and ultrahigh-density storage of HZO thin films.",
        "publication_year": "2022",
        "authors": [
            "Z. Guan",
            "Yun-Kangqi Li",
            "Yifeng Zhao",
            "Yue Peng",
            "G.Vallat han",
            "N. Zhong",
            "P. Xiang",
            "J. Chu",
            "C. Duan"
        ],
        "related_topics": [
            "Materials Science"
        ],
        "citation_count": "5",
        "reference_count": "47",
        "references": [
            "/paper/Effects-of-flexoelectric-polarization-on-surface-of-Xu-Zheng/cd501ab54af7d5f0bbdfcd351f6196c25877ea39",
            "/paper/Understanding-the-Effect-of-Top-Electrode-on-in-Wang-Wen/67f46528f8c55e63641969fcab65a1f97eab332f",
            "/paper/All-inorganic-transparent-Hf0.85Ce0.15O2-thin-films-Mo-Feng/614dd07fe0bef6e52f4c88156db38ee0a199f1d4",
            "/paper/Impact-of-Zr-substitution-on-the-electronic-of-Huang-Mao/e977442ef1157335cad45e155e07a34d7e048100",
            "/paper/Flexible-Film-Bulk-Acoustic-Resonator-Based-on-Film-Yu-Gao/1051f103b8f8582a23e24fa4e8244d4a5037ee38",
            "/paper/Ferroelectric-Hf0.5Zr0.5O2-Thin-Films%3A-A-Review-of-Kim-Mohan/c4b81dbbe80da09b7779d5fce6f267b1be2b8376",
            "/paper/Ferroelectric-Zr0.5Hf0.5O2-thin-films-for-memory-M%C3%BCller-B%C3%B6scke/ba2f7afcf11015cccd094771ea5015a3405788c0",
            "/paper/Strain-induced-low-mechanical-switching-force-in-Guo-Roth/90fb644c047b06709b90a2a1ebded41ae1982bdf",
            "/paper/Flexoelectric-rotation-of-polarization-in-thin-Catal%C3%A1n-Lubk/854de5b7c927a35b79f4208d03942086891f02cb",
            "/paper/Improved-Ferroelectric-Switching-Endurance-of-Thin-Chernikova-Kozodaev/6be7188841668e38a1a000e2db92bea3f9eacd9d",
            "/paper/Ferroelectricity-in-yttrium-doped-hafnium-oxide-M%C3%BCller-Schr%C3%B6der/6f89045f9c713e09d1bd3fd10e041a71588fc4dc",
            "/paper/The-demonstration-of-significant-ferroelectricity-Shimizu-Katayama/ed9908d9d75bd8606d9bca9f1cdbd99ae0240e7d",
            "/paper/Review-and-perspective-on-ferroelectric-HfO_2-based-Park-Lee/803144c27bdee14ab0620c7d034d7244beb046d8",
            "/paper/Nanoscopic-studies-of-domain-structure-dynamics-in-Buragohain-Richter/d0d13950b79d74bb28b01129ebca3199aa621441",
            "/paper/Ferroelastic-Nanodomain-mediated-Mechanical-of-in-Li-Wang/ced1470f30c86afc9f14ac2c52ae2df13df388c8"
        ]
    },
    {
        "id": "df81ef736f77eb7ba96f4a7832ca1820886b9852",
        "title": "A Prediction-based Approach for Anomaly Detection in the Cloud",
        "abstract": "This work aims to study, analyze, propose, develop and evaluate several models and mechanisms to fill the gaps in the trustworthiness of cloud computing environments, and shows that all the proposed mechanisms were able to outperform similar proposals in literature. This document provides an at-a-glance view of the main contributions of my Ph.D. work. This work aims at improving security and trustworthiness of cloud computing environments by developing a model for predicting cloud network traffic, an approach for detecting anomalies in cloud network traffic that relies on traffic prediction, as well as a mechanism for aggregating similar alarms from an IDS in the context of the cloud network traffic. All the benefits and drawbacks of the contributions were demonstrated in realistic simulations using data from real network traces. Furthermore, the evaluations were conducted with well-known metrics and the results show that all the proposed mechanisms were able to outperform similar proposals in literature.",
        "publication_year": "2019",
        "authors": [
            "B. Dalmazo",
            "J. Vilela",
            "M. Curado"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "108",
        "references": [
            "/paper/Predicting-Traffic-in-the-Cloud%3A-A-Statistical-Dalmazo-Vilela/8648599d28b5bf8f88bac203bebc85a4a12b12e7",
            "/paper/Expedite-feature-extraction-for-enhanced-cloud-Dalmazo-Vilela/5bcb66f8e1de86fee9864ec2499af7efda298679",
            "/paper/Online-Traffic-Prediction-in-the-Cloud%3A-A-Dynamic-Dalmazo-Vilela/332df6ad6e60838c3baac8f7e8bc9d9757d35364",
            "/paper/Triple-Similarity-Mechanism-for-alarm-management-in-Dalmazo-Vilela/590b0e3a0de3cbd25f7dee4abfd2275dc7156938",
            "/paper/Online-detection-of-utility-cloud-anomalies-using-Wang-Talwar/205a74010ea0d82ebd2d52bb3e8ec6f3283228a9",
            "/paper/Performance-Analysis-of-Network-Traffic-Predictors-Dalmazo-Vilela/44ff7a652d53d1849bde9606a4a8d0eb0d3d8e57",
            "/paper/A-distributed-approach-to-network-anomaly-detection-Palmieri-Fiore/14c9d0e2afdae2314ac978f49a62a8ffdc041c5c",
            "/paper/Anomaly-secure-detection-methods-by-analyzing-of-in-Xiong-Hu/20663428102ce81b6a8e39d66ee94bca298a70ce",
            "/paper/Detecting-Hidden-Anomalies-Using-Sketch-for-Network-Aiping-Han/5b031bebc736bd44936c94b7c0d8fe7ffe2ce798",
            "/paper/Online-traffic-prediction-in-the-cloud-Dalmazo-Vilela/d16b6598963bab9d58132d46e4b8841f819e55a5"
        ]
    },
    {
        "id": "b00052f831979ac275459a0340fa8d2cf6265490",
        "title": "Texture-Mapping Error Removal Based on the BRIEF Operator in Image-Based Three-Dimensional Reconstruction",
        "abstract": "In image-based three-dimensional (3D) reconstruction, texture-mapping techniques can give the model realistic textures. When the geometric surface in some regions is not reconstructed, such as for moving cars, powerlines, and telegraph poles, the textures in the corresponding image are textured to other regions, resulting in errors. To solve this problem, this letter proposes an image consistency detection method based on the Binary Robust Independent Elementary Features (BRIEF) descriptor. The method is composed of two parts. First, each triangle in the mesh and its neighboring triangles are sampled uniformly to obtain sampling points. Then, these sampled points are projected into the visible image of the triangle, and the corresponding sampled points and their RGB color values are obtained on the corresponding image. Based on the sampled points on these images, a BRIEF descriptor is calculated for each image corresponding to that triangle. In the second step, the Hamming distance between these BRIEF descriptors is calculated, outliers are removed according to the method, and noisy images are also removed. In addition, we propose adding semantic information to Markov energy optimization to reduce errors further. The two methods effectively reduced errors in texture mapping caused by objects not reconstructed, improving the texture quality of 3D models.",
        "publication_year": "2023",
        "authors": [
            "Junxing Yang",
            "Lu Lu",
            "Ge Peng",
            "He Huang",
            "Jian Wang",
            "Fei Deng"
        ],
        "related_topics": [
            "Mathematics"
        ],
        "citation_count": 0,
        "reference_count": "17",
        "references": [
            "/paper/Patch-based-optimization-for-image-based-texture-Bi-Kalantari/414cc069420972c4186dd46ed4e7649f1af86443",
            "/paper/A-Novel-OpenMVS-Based-Texture-Reconstruction-Method-Li-Xiao/ccb6ea77ca61dafadfbfba02647962bf9fcdb3c6",
            "/paper/High-Quality-Texture-Reconstruction-from-Multiple-Bernardini-Boier-Martin/ca7448443241763238bbb53fd09dab714e8495f1",
            "/paper/Efficient-global-color-correction-for-large-scale-Yang-Liu/9868b1a123ec1fd2724f48380c35eb7e12c3ca4b",
            "/paper/Accurate-Multiple-View-3D-Reconstruction-Using-for-Shen/c49f33fa046ce7551673fe4bd75ccdbadc4173d9",
            "/paper/Structure-Aware-Completion-of-Photogrammetric-in-Zhu-Shang/86893029ee91aa34e8554db1c2dba3fab001135c",
            "/paper/Moving-Car-Recognition-and-Removal-for-3D-Urban-Yang-Zhang/b18a249e639a7fad8b093b882bc809e735a8dfc5",
            "/paper/Photometric-Multi-View-Mesh-Refinement-for-Images-Rothermel-Gong/705914b29e3ac29d59f192178bf1532b89d3e19b",
            "/paper/Masked-photo-blending%3A-Mapping-dense-photographic-Callieri-Cignoni/8e522e626b607a95e599361f078c975079b389c0",
            "/paper/Accurate%2C-Dense%2C-and-Robust-Multiview-Stereopsis-Furukawa-Ponce/2097923990ad6d188bd9afc69a1f8abc0580076b"
        ]
    },
    {
        "id": "d47b65b7e39bce84c58ee5fc450935cd4891d4e1",
        "title": "Siamese-Based Attention Learning Networks for Robust Visual Object Tracking",
        "abstract": "This chapter presents an in-depth overview and analysis of attention learning-based siamese trackers and highlights the key findings to provide insights into future visual object tracking developments. Tracking with the siamese network has recently gained enormous popularity in visual object tracking by using the template-matching mechanism. However, using only the template-matching process is susceptible to robust target tracking because of its inability to learn better discrimination between target and background. Several attention-learning are introduced to the underlying siamese network to enhance the target feature representation, which helps to improve the discrimination ability of the tracking framework. The attention mechanism is beneficial for focusing on the particular target feature by utilizing relevant weight gain. This chapter presents an in-depth overview and analysis of attention learning-based siamese trackers. We also perform extensive experiments to compare state-of-the-art methods. Furthermore, we also summarize our study by highlighting the key findings to provide insights into future visual object tracking developments.",
        "publication_year": "2022",
        "authors": [
            "Md. Maklachur Rahman",
            "Soon Ki Jung"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "40",
        "references": [
            "/paper/Efficient-Visual-Tracking-With-Stacked-Attention-Rahman-Fiaz/e085fb462789a8bca7933b5e1c7e9aa0ded8a711",
            "/paper/Siamese-High-Level-Feature-Refine-Network-for-Rahman-Ahmed/057f0dbb8f94d7ec3b5ce3c42a76a492d5ad727e",
            "/paper/Multiple-Context-Features-in-Siamese-Networks-for-Morimitsu/d867a1b599a5a99fa087d3c12a160ffb00dddfa7",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/Learning-Attentions%3A-Residual-Attentional-Siamese-Wang-Teng/6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "/paper/Adaptive-Feature-Selection-Siamese-Networks-for-Fiaz-Rahman/e972436110b4f2c102d938311beff98ece7b6da7",
            "/paper/Initial-Matting-Guided-Visual-Tracking-With-Siamese-Qin-Fan/63b4214451f8a7ba0725298a726928cf04349a29",
            "/paper/Triplet-Loss-in-Siamese-Network-for-Object-Tracking-Dong-Shen/fdb98f5a7015de0956ef8d4e468257dc3079b5e5",
            "/paper/Improving-Object-Tracking-by-Added-Noise-and-Fiaz-Mahmood/93742493c26652d4a95627de699869795698a554",
            "/paper/Visual-Tracking-via-Dynamic-Memory-Networks-Yang-Chan/aaac08ee1b4cae016fdd531b2487794007811d8b"
        ]
    },
    {
        "id": "069ccdbab6ea6ca2d9c3b75c76360ca1e4e9a5e9",
        "title": "Siam R-CNN: Visual Tracking by Re-Detection",
        "abstract": "This work presents Siam R-CNN, a Siamese re-detection architecture which unleashes the full power of two-stage object detection approaches for visual object tracking, and combines this with a novel tracklet-based dynamic programming algorithm to model the full history of both the object to be tracked and potential distractor objects. We present Siam R-CNN, a Siamese re-detection architecture which unleashes the full power of two-stage object detection approaches for visual object tracking. We combine this with a novel tracklet-based dynamic programming algorithm, which takes advantage of re-detections of both the first-frame template and previous-frame predictions, to model the full history of both the object to be tracked and potential distractor objects. This enables our approach to make better tracking decisions, as well as to re-detect tracked objects after long occlusion. Finally, we propose a novel hard example mining strategy to improve Siam R-CNN's robustness to similar looking objects. Siam R-CNN achieves the current best performance on ten tracking benchmarks, with especially strong results for long-term tracking. We make our code and models available at www.vision.rwth-aachen.de/page/siamrcnn.",
        "publication_year": "2019",
        "authors": [
            "P. Voigtlaender",
            "Jonathon Luiten",
            "Philip H. S. Torr",
            "B. Leibe"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "283",
        "reference_count": "118",
        "references": [
            "/paper/Visual-Object-Tracking-With-Discriminative-Filters-Javed-Danelljan/0530cbeb847f5e5002d1183c482759dff5f8c439",
            "/paper/SiamCAN%3A-Real-Time-Visual-Tracking-Based-on-Siamese-Zhou-Wen/aad19f38536f29472d0a78be554a006199445dba",
            "/paper/Siamese-networks-with-distractor-reduction-method-Xuan-Li/3985382474245388bbc73e2c849e783010901775",
            "/paper/Learning-Spatio-Temporal-Transformer-for-Visual-Yan-Peng/72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
            "/paper/RPT%2B%2B%3A-Customized-Feature-Representation-for-Visual-Ma-Zhang/f0eb85690c981f4b185c699616e77f9d3df6aea2",
            "/paper/AFAT%3A-Adaptive-Failure-Aware-Tracker-for-Robust-Xu-Feng/88246e0145e5bffe38772f20c8dcfcec2eb2bea8",
            "/paper/RPT%3A-Learning-Point-Set-Representation-for-Siamese-Ma-Wang/f5ed4f2f4d3614a7186e6e52ca28b248836b9da6",
            "/paper/Reinforced-Similarity-Learning%3A-Siamese-Relation-Zhang-Zheng/f29626d96d4cde815cdc1c0b11346fd34643bd1e",
            "/paper/STMTrack%3A-Template-free-Visual-Tracking-with-Memory-Fu-Liu/811ffb185bc90ac5d02d6dbfbcdb6173756b52ef",
            "/paper/Deep-Learning-for-Visual-Tracking%3A-A-Comprehensive-Marvasti-Zadeh-Cheng/1fbb4201af091aef55360f113ba35814063923e4",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Deep-Meta-Learning-for-Real-Time-Target-Aware-Choi-Kwon/6cbc9a49e920231887604e3c3a018a9f2fdbd5e0",
            "/paper/Meta-Tracker%3A-Fast-and-Robust-Online-Adaptation-for-Park-Berg/50c60583dc0ef09484358deab329f82ee22c2b66",
            "/paper/GradNet%3A-Gradient-Guided-Network-for-Visual-Object-Li-Chen/47a58f8bec1d34004a7d7cf837e27a26de64f0f7",
            "/paper/Tracking-Holistic-Object-Representations-Sauer-Aljalbout/3541fa2a61962981e695ac30bba4efec338222e3",
            "/paper/Learning-regression-and-verification-networks-for-Zhang-Wang/3d372b63020c4d2c9510624f370b50d9f292bcde",
            "/paper/A-Twofold-Siamese-Network-for-Real-Time-Object-He-Luo/a3a4471e82260f573d240cc34aeff431cf236571",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/Triplet-Loss-in-Siamese-Network-for-Object-Tracking-Dong-Shen/fdb98f5a7015de0956ef8d4e468257dc3079b5e5"
        ]
    },
    {
        "id": "ac003f0a7cb8f7feaf1e42a73359bbedfd31940d",
        "title": "Optimal visual tracking using Wasserstein transport proposals",
        "abstract": "Semantic Scholar extracted view of \"Optimal visual tracking using Wasserstein transport proposals\" by JingDong Hong et al.",
        "publication_year": "2022",
        "authors": [
            "JingDong Hong",
            "Junseok Kwon"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "14",
        "references": [
            "/paper/Enhancing-discriminative-appearance-model-for-He-Chen/8822c8ccff68073523b1f9bd60439e5a7c9cc0bb",
            "/paper/Global-optimization-on-non-convex-two-way-truncated-Ju-Rosenberger/1cd3e1d78053c1d41210aab2edd3a9da126f974c",
            "/paper/Robust-Multivehicle-Tracking-With-Wasserstein-in-Zeng-Fu/77a66342856f1273ef90dd2cf01c84dbcbd7a668",
            "/paper/GlobalTrack%3A-A-Simple-and-Strong-Baseline-for-Huang-Zhao/5664e24cacf3f6374c26b5597765099ee9537413",
            "/paper/PROVID%3A-Progressive-and-Multimodal-Vehicle-for-Liu-Liu/6a03f9b4354cbe6bdb4c00eab3521a6463e563f7",
            "/paper/Group-Sensitive-Triplet-Embedding-for-Vehicle-Bai-Lou/864fe713fd082585b67198ad7942a1568e536bd2",
            "/paper/Model-Based-Localization-and-Tracking-Using-Beacons-Dan%C3%AD%C5%9F-Cemgil/fe0fc60840a7d3e673872b8ffb205d9eb7abc7b2",
            "/paper/TMAGIC%3A-A-Model-Free-3D-Tracker-Lebeda-Hadfield/0cf72427a8a9245d368bae614db5b7bff6fee956",
            "/paper/Joint-Tracking-and-Ground-Plane-Estimation-Kwon-Dragon/595ce8e24326658a96cdb471fa14ea1bd89d0e6c",
            "/paper/Highly-Nonrigid-Object-Tracking-via-Patch-Based-Kwon-Lee/191fdf46ecacf152a8fb4bd799b8c5740fb53a41",
            "/paper/Wang-Landau-Monte-Carlo-Based-Tracking-Methods-for-Kwon-Lee/52c7cac35fb27500290c696f1036195000c1b279"
        ]
    },
    {
        "id": "4a2d0374dac0b4b43de606a610dde145054f01ba",
        "title": "AFOD: Adaptive Focused Discriminative Segmentation Tracker",
        "abstract": "A novel Adaptive FOcused Discriminative segmentation tracker with leading performance on two tracking benchmarks including the bounding box annotated VOT2018 and the segmentation mask annotatedVOT2020, while running close to real-time. Visual object tracking is a fundamental task in computer vision which could be integrated into numerous real-world applications. Traditional object tracking methods focus on providing the bounding box as object position, while some recent trackers start to consider the combination of segmentation module to generate the binary segmentation mask, pursuing more accurate localization. However, how to effectively integrate different information for accurate and robust tracking is an open question. In this paper, we propose a novel Adaptive FOcused Discriminative (AFOD) segmentation tracker with the following advanced components. For localization, a more discriminative light weight online target appearance model is employed to provide robust position estimation. For segmentation, leveraging the backbone semantic feature, the coarse segmentation feature, and the localization feature, an offline trained fine segmentation model with IoU optimization is utilized to generate the accurate high resolution masks. The boundary detection further enhances the segmentation quality. For combination, an adaptive prediction strategy is proposed to better integrate the information from two types of predictions, i.e. box and mask. AFOD achieves leading performance on two tracking benchmarks including the bounding box annotated VOT2018 and the segmentation mask annotated VOT2020, while running close to real-time.",
        "publication_year": "2020",
        "authors": [
            "Yiwei Chen",
            "Jingtao Xu",
            "Jiaqian Yu",
            "Qiang Wang",
            "ByungIn Yoo",
            "Jae-Joon Han"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "11",
        "reference_count": "45",
        "references": [
            "/paper/SAFS%3A-Object-Tracking-Algorithm-Based-on-Feature-Guo-Gao/5100a7d3c763973ff3ec49446cd0785a40d35650",
            "/paper/Part-Aware-Framework-for-Robust-Object-Tracking-Li-Zhao/1cd30c9900f73195f9159183406019bb1561e5f3",
            "/paper/Transforming-Model-Prediction-for-Tracking-Mayer-Danelljan/dc47b17250b639d3a89a716c7216ef69b33f9e33",
            "/paper/A-Hybrid-Visual-Tracking-Algorithm-Based-on-SOM-and-Zhang-Huang/cb7fd793641abbac772e25f076da6b7944840f1c",
            "/paper/FEAR%3A-Fast%2C-Efficient%2C-Accurate-and-Robust-Visual-Borsuk-Vei/badd83f91a8f97ba2f3010601af26d8a60edfbba",
            "/paper/QuadTreeCapsule%3A-QuadTree-Capsules-for-Deep-Ma-Wu/d6413eb36ed9f420d42f4bbf35939c21b5238252",
            "/paper/Transforming-Model-Prediction-for-Tracking-Mayer-Danelljan/4e1ee771bf7f9b16cdbfa1f62bc11a0dc1ef22ed",
            "/paper/Gaze-control-system-for-tracking-Quasi-1D-moving-in-Geng-Zhu/c1d3da2463306a94addb3cc38ed9c2fb011ebd4a",
            "/paper/Learning-Generalized-Intersection-Over-Union-for-Yu-Xu/1d7329bc40c4911a24af3e34b62d53d600824cb7",
            "/paper/The-Structured-Abstain-Problem-and-the-Lov%C3%A1sz-Hinge-Finocchiaro-Frongillo/5946dc0077638f6fa0a782f996442da0664edeb5",
            "/paper/Fast-and-Accurate-Online-Video-Object-Segmentation-Cheng-Tsai/12fae9a2c1ed867997e1ca70eba271b3c741c42f",
            "/paper/D3S-%E2%80%93-A-Discriminative-Single-Shot-Segmentation-Luke%C5%BEi%C4%8D-Matas/45512d44f1205bc92775f2e880858b3f23c9f5fd",
            "/paper/Learning-Fast-and-Robust-Target-Models-for-Video-Robinson-Lawin/f4f34b56ef957981cebc3d901f49ddd638007d8d",
            "/paper/Learning-Video-Object-Segmentation-from-Static-Perazzi-Khoreva/1190e0210430e8b743af24cdc43efdeef407b669",
            "/paper/ATOM%3A-Accurate-Tracking-by-Overlap-Maximization-Danelljan-Bhat/d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "/paper/Fast-Online-Object-Tracking-and-Segmentation%3A-A-Wang-Zhang/d58e13f7e5e06440c9470a9101ccbb1bfd91b5a1",
            "/paper/A-Generative-Appearance-Model-for-End-To-End-Video-Johnander-Danelljan/48e52aef87084fa17e6ccb20ad9b3a8ec45934f0",
            "/paper/Learning-Discriminative-Model-Prediction-for-Bhat-Danelljan/2c8315ae713b3e27c6e9f291a158134d9c516166",
            "/paper/VideoMatch%3A-Matching-based-Video-Object-Hu-Huang/8b74008565b575f9ab7a0962ca5f6955d64db045",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602"
        ]
    },
    {
        "id": "d14b5505fec39c88b747c21c1d504140bd0aea99",
        "title": "You Only Need One Detector: Unified Object Detector for Different Modalities based on Vision Transformers",
        "abstract": "This research envisions an application scenario for robotics, where the unified system seamlessly switches between RGB cameras and depth sensors in varying lighting conditions, and introduces a novel inter-modality mixing method that enables the model to achieve significantly better results than previous methods. Traditional systems typically require different models for processing different modalities, such as one model for RGB images and another for depth images. Recent research has demonstrated that a single model for one modality can be adapted for another using cross-modality transfer learning. In this paper, we extend this approach by combining cross/inter-modality transfer learning with a vision transformer to develop a unified detector that achieves superior performance across diverse modalities. Our research envisions an application scenario for robotics, where the unified system seamlessly switches between RGB cameras and depth sensors in varying lighting conditions. Importantly, the system requires no model architecture or weight updates to enable this smooth transition. Specifically, the system uses the depth sensor during low-lighting conditions (night time) and both the RGB camera and depth sensor or RGB caemra only in well-lit environments. We evaluate our unified model on the SUN RGB-D dataset, and demonstrate that it achieves similar or better performance in terms of mAP50 compared to state-of-the-art methods in the SUNRGBD16 category, and comparable performance in point cloud only mode. We also introduce a novel inter-modality mixing method that enables our model to achieve significantly better results than previous methods. We provide our code, including training/inference logs and model checkpoints, to facilitate reproducibility and further research. \\url{https://github.com/liketheflower/UODDM}",
        "publication_year": "2022",
        "authors": [
            "Xiaoke Shen",
            "Zhujun Li",
            "Jaime Canizales",
            "I. Stamos"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "50",
        "references": [
            "/paper/simCrossTrans%3A-A-Simple-Cross-Modality-Transfer-for-Shen-Stamos/a69a702ec8ccf9cbeb7e8574dfedb472ce56b44b",
            "/paper/RGB-Event-Fusion-for-Moving-Object-Detection-in-Zhou-Wu/f067c9e1913b024127885193a1bc108d710a981f",
            "/paper/Semantics-guided-multi-level-RGB-D-feature-fusion-Li-Zhang/ef09b3cbd5ba0bc076d5cbe57752067d03de3dfb",
            "/paper/SUN-RGB-D%3A-A-RGB-D-scene-understanding-benchmark-Song-Lichtenberg/b73e2d40da901ad3da3813670a51c52803d7af7e",
            "/paper/Modality-Adaptive-Mixup-and-Invariant-Decomposition-Huang-Liu/c7dbb5ada2c66911ae727763d9831c469bd48261",
            "/paper/Learning-Rich-Features-from-RGB-D-Images-for-Object-Gupta-Girshick/2c0b510ebf995ef172cb64ed7ce24aa7903dced8",
            "/paper/Robust-RGB-D-Fusion-for-Saliency-Detection-Wu-Gobichettipalayam/1bc08388f83c7a2fccf7b38c97fab8c1d751cc22",
            "/paper/HiDAnet%3A-RGB-D-Salient-Object-Detection-via-Depth-Wu-Allibert/f874d500f82a7d3b49f373fa68abb4453c1a9207",
            "/paper/Frustum-VoxNet-for-3D-object-detection-from-RGB-D-Shen-Stamos/0fdf394a17da68978bb6735d84cc8ff2aae46455",
            "/paper/Multi-view-3D-Object-Detection-Network-for-Driving-Chen-Ma/dc200ab22bf63e10e8b2af328a9e072d82cf75b7"
        ]
    },
    {
        "id": "1081a484fda628cdff9723eb0f9087a9f771baf7",
        "title": "Wissensbasierte probabilistische Modellierung f\u00fcr die Situationsanalyse am Beispiel der maritimen \u00dcberwachung",
        "abstract": "Die Erkennungsqualitat wurde mit einem maritimen Datensatz evaluiert und leistet einen Beitrag zur Situationsanalyse, welche Echtzeitdaten hinsichtlich modellierter Situationen von Interesse mittels eines dynamischen Bayes\u2019schen Netzes auswertet. In heutigen Uberwachungssystemen wird eine Vielzahl an Sensorik eingesetzt, wodurch das Datenvolumen deutlich anwachst. Entsprechend muss der menschlichen Entscheider entlastet werden, indem diese Daten intelligent verarbeitet werden. Die Arbeit leistet einen Beitrag zur Situationsanalyse, welche Echtzeitdaten hinsichtlich modellierter Situationen von Interesse mittels eines dynamischen Bayes\u2019schen Netzes auswertet. Die Erkennungsqualitat wurde mit einem maritimen Datensatz evaluiert.",
        "publication_year": "2016",
        "authors": [
            "Y. Fischer"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "8",
        "reference_count": 0,
        "references": [
            "/paper/Robuste-Detektion%2C-Verfolgung-und-Wiedererkennung-Metzler/a831a3ad3183fdf1f115dc3639be7bf414eeb7c6",
            "/paper/Artificial-Intelligence%3A-Research-Impact-on-Key-the-Christ-Quint/86cceac0078e219cc758d3071f79a11710e5f73c",
            "/paper/Detecting-illegal-diving-and-other-suspicious-in-of-Anneken-Rosa/a717a9587bb7d9da904da35463b5d4ef2eb3107a",
            "/paper/Adaptive-State-%C3%97-Time-Lattices%3A-A-Contribution-to-Petereit/b1d427195ab69bdeacb6c2d67cccb25fb13d2758",
            "/paper/Privacy-Respecting-Smart-Video-Surveillance-Based-Birnstill/7a351aad06c7c2b3d1f799e993ed52eee0a1414b",
            "/paper/Feature-Based-Probabilistic-Data-Association-for-Grinberg/d9a7c97ecd871db83fa0f598908e7f3a6b6e1f4c",
            "/paper/Image-Based-3D-Reconstruction-of-Dynamic-Objects-Bullinger/654f7f942706f17657e9608a42d8907dd0a7b136",
            "/paper/Predictive-energy-efficient-motion-trajectory-of-Guan/57636257378b517b741d13b48b2a2f6c86dc1a6f"
        ]
    },
    {
        "id": "57052d3357cfedf389e3c7b03d9e5f2772d01038",
        "title": "Recurrent Convolutional Network for Video-Based Person Re-identification",
        "abstract": "A novel recurrent neural network architecture for video-based person re-identification that makes use of colour and optical flow information in order to capture appearance and motion information which is useful for video re- identification. In this paper we propose a novel recurrent neural network architecture for video-based person re-identification. Given the video sequence of a person, features are extracted from each frame using a convolutional neural network that incorporates a recurrent final layer, which allows information to flow between time-steps. The features from all timesteps are then combined using temporal pooling to give an overall appearance feature for the complete sequence. The convolutional network, recurrent layer, and temporal pooling layer, are jointly trained to act as a feature extractor for video-based re-identification using a Siamese network architecture. Our approach makes use of colour and optical flow information in order to capture appearance and motion information which is useful for video re-identification. Experiments are conduced on the iLIDS-VID and PRID-2011 datasets to show that this approach outperforms existing methods of video-based re-identification.",
        "publication_year": "2016",
        "authors": [
            "Niall McLaughlin",
            "J. M. D. Rinc\u00f3n",
            "P. Miller"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "502",
        "reference_count": "44",
        "references": [
            "/paper/Video-Person-Re-Identification-for-Wide-Area-Based-McLaughlin-Rinc%C3%B3n/219f3870fc460fc88d34992d5db623cf68a3e2f0",
            "/paper/Learning-Bidirectional-Temporal-Cues-for-Person-Zhang-Yu/8e50abb8460805b9e5cb7a73b306b6be3a86baf1",
            "/paper/Temporal-Enhanced-Convolutional-Network-for-Person-Wu-Qiu/53baabad70915a852477c2191b8bdb005b8cc4a5",
            "/paper/Deep-Recurrent-Convolutional-Networks-for-Person-An-Wu-Shen/2d6130f043e69849fc0443bb489c5d21f933eddd",
            "/paper/Improved-Siamese-Network-for-Video-Based-Person-Wang-Cai/9b103bce635d0eeaeecba6de0bf1005f5d0703a4",
            "/paper/Moving-Average-Recurrent-Neural-Network-Model-for-MuhammedFasil-Chaudhuri/a5454f3fc04c69e23b39181d936a888c8c4c6827",
            "/paper/Video-Based-Person-Re-identification-by-3D-Neural-Kato-Hakozaki/2afd785e8a726750022b591e6e91ed1d54f6b19e",
            "/paper/Non-Local-Attentive-Temporal-Network-for-Person-Rao-Cao/34072999c80e903ac1be3aaf61c0fcb1708a6378",
            "/paper/Video-Based-Person-Re-Identification-by-Re-Ranking-Saha-Ram/eb51b630b194f6161dadeca54b3d21330cde9ebd",
            "/paper/Video-based-Person-Re-Identification-using-Gated-Feng-Wang/60527b85ab7daef01fbad0641a799d2fde853e29",
            "/paper/Beyond-Temporal-Pooling%3A-Recurrence-and-Temporal-in-Pigou-Oord/eae099ca54c9ee12a5763f6347b91f77df2c7bf4",
            "/paper/Video-Sequences-Association-for-People-across-Cong-Achard/03a89b2c497cbf8b41fdfc073a90f1afc4c28d7c",
            "/paper/Beyond-short-snippets%3A-Deep-networks-for-video-Ng-Hausknecht/5418b2a482720e013d487a385c26fae0f017c6a6",
            "/paper/Deep-Metric-Learning-for-Practical-Person-Yi-Lei/afd29ac2de84c8a6d48232477be018ec57d6f564",
            "/paper/Multi-Shot-Human-Re-Identification-Using-Adaptive-Li-Wu/93d1a61b35ead27284bd684e5b1b6daa623a69ee",
            "/paper/Deep-feature-learning-with-relative-distance-for-Ding-Lin/21bae3fcf9f42ba0221234e12df0c3f19d95c088",
            "/paper/DeepReID%3A-Deep-Filter-Pairing-Neural-Network-for-Li-Zhao/6bd36e9fd0ef20a3074e1430a6cc601e6d407fc3",
            "/paper/A-Spatio-Temporal-Appearance-Representation-for-Liu-Ma/f3e73911f213a7ff4f498e678502d4aaf520b25c",
            "/paper/Person-re-identification-by-symmetry-driven-of-Farenzena-Bazzani/ff0a68bcbf72e35c70510ffba07f13ea585e98b4",
            "/paper/Multiple-shot-human-re-identification-by-Mean-Grid-B%C4%85k-Corv%C3%A9e/7c5e33e18ef4fe92907bd1b9d752bcd85a4ec90c"
        ]
    },
    {
        "id": "6ebc40a061433c24a3ea1f305bb6533b8f3dd5f4",
        "title": "Visual Object Tracking for Unmanned Aerial Vehicles: A Benchmark and New Motion Models",
        "abstract": "A simple baselines to model the camera motion by geometric transformation based on background feature points is devised to address the challenging issue of severe camera motion in an unmanned aerial vehicle (UAV) or drone scenario. \n \n Despite recent advances in the visual tracking community, most studies so far have focused on the observation model. As another important component in the tracking system, the motion model is much less well-explored especially for some extreme scenarios. In this paper, we consider one such scenario in which the camera is mounted on an unmanned aerial vehicle (UAV) or drone. We build a benchmark dataset of high diversity, consisting of 70 videos captured by drone cameras. To address the challenging issue of severe camera motion, we devise simple baselines to model the camera motion by geometric transformation based on background feature points. An extensive comparison of recent state-of-the-art trackers and their motion model variants on our drone tracking dataset validates both the necessity of the dataset and the effectiveness of the proposed methods. Our aim for this work is to lay the foundation for further research in the UAV tracking area.\n \n",
        "publication_year": "2017",
        "authors": [
            "Siyi Li",
            "D. Yeung"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "194",
        "reference_count": "38",
        "references": [
            "/paper/A-Unified-Approach-for-Tracking-UAVs-in-Infrared-Zhao-Zhang/5f42754b51d49913cfd73fb33c0c9deca8945099",
            "/paper/Online-Single-Person-Tracking-for-Unmanned-Aerial-Wang-Liu/87043c652fed795a185408d1f93039f3b40f26cf",
            "/paper/Performance-evaluation-of-low-resolution-visual-for-Wang-Wei/41bdc12bbcef9c303e9f77d9a5fa106d23f4c125",
            "/paper/Performance-evaluation-of-low-resolution-visual-for-Wang-Wei/d56329ddecf65edaa67bbdbc2b811a07e873c88e",
            "/paper/Integration-of-the-3D-Environment-for-On-Board-UAV-Vujasinovi'c-Becker/447a42f7868b6306cf558a81ca20893dbedacfcd",
            "/paper/Integration-of-the-3D-Environment-for-UAV-Onboard-Vujasinovi'c-Becker/861503ae0fa8ad271b03ef81c058258e4a2c8fae",
            "/paper/Siamese-Object-Tracking-for-Unmanned-Aerial-A-and-Fu-Lu/1171234cb2f3e1589592e3d04eb10c132fc6a5c8",
            "/paper/All-Day-Object-Tracking-for-Unmanned-Aerial-Vehicle-Li-Fu/f3d7eb617179db9f9621fa2c978dfb9f2c39341f",
            "/paper/Integration-of-3D-Knowledge-for-On-Board-UAV-Visual-Vujasinovic-Becker/fee4b06f76234a8bcdf0a0d4c20e31764b685f75",
            "/paper/High-performance-UAVs-visual-tracking-based-on-Yang-Chen/7b386067a9f1d8aa6f78c58f6574fb0756ad87de",
            "/paper/Tracking-Revisited-Using-RGBD-Camera%3A-Unified-and-Song-Xiao/487eb86379e979a72ebfef67db6eb8f048d1d258",
            "/paper/A-General-Framework-for-Tracking-Multiple-People-a-Choi-Pantofaru/53822d61e829ef02a95a6c89fea082114fd3e16b",
            "/paper/Multi-View-3D-Human-Tracking-in-Crowded-Scenes-Liu/4e71613b0d7117bd158be28fc054b7943b233107",
            "/paper/Visual-Tracking%3A-An-Experimental-Survey-Smeulders-Chu/eda3368a5198ca55768b07b6f5667aea28baf2cd",
            "/paper/Automatic-vehicle-tracking-and-recognition-from-Arandjelovic/92d6362b90e5030de623d5f823675ff9212c3584",
            "/paper/Object-Tracking-Benchmark-Wu-Lim/b7d540cd0de72e984cdec44afa4a4d039cfd5eea",
            "/paper/NUS-PRO%3A-A-New-Visual-Tracking-Challenge-Li-Lin/91f2b2aeb7e65d0b673ed7e782488b3365027979",
            "/paper/Joint-tracking-and-video-registration-by-factorial-Mei-Porikli/ecde35d4d6534bbe8815feb2764169020f37414e",
            "/paper/Robust-Visual-Tracking-using-1-Minimization-Mei-Ling/f317983dfb75ff280d314f9538bb185dc51c6147",
            "/paper/Robust-visual-tracking-using-%26%23x2113%3B1-minimization-Mei-Ling/b5e17a0ed14349d6c4066d2408409751f9595e04"
        ]
    },
    {
        "id": "7eee4087e4087930726eee6b4395fa5e788b8f6a",
        "title": "Robust Visual Tracking via an Improved Background Aware Correlation Filter",
        "abstract": "An improved BACF method of robust visual object tracking is proposed to achieve the location of targets with higher accuracy in complex scenarios allowing scale variation, occlusion, rotation, illumination variation, and so on. Recently, there emerge many excellent algorithms in the field of visual object tracking. Especially, the background aware correlation filter (BACF) has received much attention, owing to its ability to cope with the boundary effect. However, in the related works, there exist two aspects of imperfections: 1) only histograms of oriented gradients (HOG) is extracted, through which the visual information of targets cannot be fully expressed; and 2) the scale estimation strategy is imperfect in terms of scale parameters, which makes it impossible to accurately track the targets with large-scale changes. To overcome the imperfections, an improved BACF method of robust visual object tracking is proposed to achieve the location of targets with higher accuracy in complex scenarios allowing scale variation, occlusion, rotation, illumination variation, and so on. Crucially, a feature fusion strategy based on HOG and color names is integrated to extract a powerful feature of targets, and a modified scale estimation strategy is designed to enhance the ability to track targets with large-scale changes. The effectiveness and robustness of the proposed method are demonstrated through evaluations on OTB2103 and OTB2015 benchmarks. Particularly, compared with other state-of-the-art correlation filter-based trackers and deep learning-based trackers, the proposed method is competitive in terms of accuracy and success rate.",
        "publication_year": "2019",
        "authors": [
            "Xiaoxiao Sheng",
            "Yungang Liu",
            "Huijun Liang",
            "Fengzhong Li",
            "Yongchao Man"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "13",
        "reference_count": "42",
        "references": [
            "/paper/Robust-scale-variation-object-tracking-by-twofold-Zha-Qiu/01b7c93e7d878327cf9d754889d87f4c335e7bae",
            "/paper/An-Optimal-Long-Term-Aerial-Infrared-Object-With-Wang-Zhang/4415ab6adf726075615c34864309c1f1efc5dcd4",
            "/paper/Foreground-Information-Guidance-for-Siamese-Visual-Li-Yu/c19fd532794ee61998404e303cf1f08bc81793db",
            "/paper/Multi-Channel-Feature-Dimension-Adaption-for-Wu-Xu/9e4f67a077a3aad94cdcf5df3a7eb518adba6e12",
            "/paper/Robust-Visual-Tracking-via-Multilayer-CaffeNet-and-Xiao-Pan/e4b914fa07584eb86978dd1a2a0027ec2e3b87ff",
            "/paper/Visual-Object-Multimodality-Tracking-Based-on-for-Yang-Wei/ca87e4f8dcfd57c68115d44a2069a8bb3981b7d7",
            "/paper/Distractor-Aware-Long-Term-Correlation-Tracking-on-Yu-Zhang/49399f7dd28a0ee151bb8344acd09608a7ce743d",
            "/paper/ADT%3A-Object-Tracking-Algorithm-Based-on-Adaptive-Ming-Zhang/9efb94b6ec3ff8ac6662323cfa2bb02a71d3f578",
            "/paper/Visual-Object-Tracking-Based-on-Deep-Neural-Network-Diao-Sun/065a24b0e79cfc9725cd5a0dbfdee542e822872d",
            "/paper/Distractor-Aware-Visual-Tracking-by-Online-Siamese-Zha-Wu/03883930b96cf7386c8708bff6c1e6157ac16689",
            "/paper/Visual-object-tracking-using-adaptive-correlation-Bolme-Beveridge/70c3c9b9a40ca55264e454586dca2a6cf416f6e0",
            "/paper/Correlation-Filter-Based-Scale-Adaptive-Visual-With-Huang-Gu/f919e731aa2e0a86314e981027b71fcc3cea9fef",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "/paper/Hierarchical-Convolutional-Features-for-Visual-Ma-Huang/5c8a6874011640981e4103d120957802fa28f004",
            "/paper/Staple%3A-Complementary-Learners-for-Real-Time-Bertinetto-Valmadre/0f12a3aaf3851078d93a9bba4e3ebece6d4bcfe5",
            "/paper/Real-Time-Compressive-Tracking-Zhang-Zhang/9d57723b4908397654fb1846d37db403d8b2b56a",
            "/paper/Discriminative-Scale-Space-Tracking-Danelljan-H%C3%A4ger/ce8c76bfedc5d86faabf0d49dc42a4924f75876d",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Object-Tracking-With-Multi-View-Support-Vector-Zhang-Yu/ba3f4d05971d3d9ded23725846b864f5a8e8a7a1"
        ]
    },
    {
        "id": "f98be9a91dbf00b52a494720bd36be9c73a1210e",
        "title": "Siamese Cascaded Region Proposal Networks for Real-Time Visual Tracking",
        "abstract": "A multi-stage tracking framework, Siamese Cascaded RPN (C-RPN), which consists of a sequence of RPNs cascaded from deep high-level to shallow low-level layers in aSiamese network, which achieves state-of-the-art results and runs in real-time. Recently, the region proposal networks (RPN) have been combined with the Siamese network for tracking, and shown excellent accuracy with high efficiency. Nevertheless, previously proposed one-stage Siamese-RPN trackers degenerate in presence of similar distractors and large scale variation. Addressing these issues, we propose a multi-stage tracking framework, Siamese Cascaded RPN (C-RPN), which consists of a sequence of RPNs cascaded from deep high-level to shallow low-level layers in a Siamese network. Compared to previous solutions, C-RPN has several advantages: (1) Each RPN is trained using the outputs of RPN in the previous stage. Such process stimulates hard negative sampling, resulting in more balanced training samples. Consequently, the RPNs are sequentially more discriminative in distinguishing difficult background (i.e.,, similar distractors). (2) Multi-level features are fully leveraged through a novel feature transfer block (FTB) for each RPN, further improving the discriminability of C-RPN using both high-level semantic and low-level spatial information. (3) With multiple steps of regressions, C-RPN progressively refines the location and shape of the target in each RPN with adjusted anchor boxes in the previous stage, which makes localization more accurate. C-RPN is trained end-to-end with the multi-task loss function. In inference, C-RPN is deployed as it is, without any temporal adaption, for real-time tracking. In extensive experiments on OTB-2013, OTB-2015, VOT-2016, VOT-2017, LaSOT and TrackingNet, C-RPN consistently achieves state-of-the-art results and runs in real-time.",
        "publication_year": "2018",
        "authors": [
            "Heng Fan",
            "Haibin Ling"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "280",
        "reference_count": "61",
        "references": [
            "/paper/Siamese-Cascaded-Region-Proposal-Networks-With-for-Cui-An/8e911bacef5a5178616cb845b2feccca4129cb6e",
            "/paper/SiamET%3A-a-Siamese-based-visual-tracking-network-Zhou-Zhang/cf1559f1dc37fb77705efee31f01b2dda49e5ced",
            "/paper/Combined-Correlation-Filters-with-Siamese-Region-Cui-Tian/80e666713638aa256f544517ffd6c00bcb0014cc",
            "/paper/Learning-Deep-Lucas-Kanade-Siamese-Network-for-Yao-Han/965c6699b5a4b046090ed5e69a2506acf4a229ad",
            "/paper/A-Deep-Hyper-Siamese-Network-for-Real-Time-Object-Zhao-Yu/0257c05637f5cd5d215c77a6ad3c87f8e41d15dd",
            "/paper/SiamCAR%3A-Siamese-Fully-Convolutional-Classification-Guo-Wang/738165f33c50b059e87b14d8b4a129230e14eacd",
            "/paper/Siamese-Attentive-Graph-Tracking-Zhao-Zhang/e2d889a45acc380d4a10d550ef150fcdf339012f",
            "/paper/IoU-guided-Siamese-region-proposal-network-for-Zhou-He/267a4c62531e3d7d6b344f9e24d8f021a61e82d2",
            "/paper/Towards-real-time-object-tracking-with-deep-Siamese-Yu-Zhao/61f2bdb431f484b26b0ecba43f3803a05714d31e",
            "/paper/Siamese-Networks-with-Distance-IoU-Loss-for-Visual-Liu-Huang/786f4b27a338421420ae9bdeeff001eef75b3450",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/A-Twofold-Siamese-Network-for-Real-Time-Object-He-Luo/a3a4471e82260f573d240cc34aeff431cf236571",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/Faster-R-CNN%3A-Towards-Real-Time-Object-Detection-Ren-He/424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "/paper/SANet%3A-Structure-Aware-Network-for-Visual-Tracking-Fan-Ling/bc4cfc075e406f9f5c621fe27a3e0002eec4a8b3",
            "/paper/Learning-Attentions%3A-Residual-Attentional-Siamese-Wang-Teng/6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "/paper/A-convolutional-neural-network-cascade-for-face-Li-Lin/2e61eb4a5c6fe6c0fdb36cfb84d460ee1524099f",
            "/paper/Structured-Siamese-Network-for-Real-Time-Visual-Zhang-Wang/4b1965a54a064ac9145b1ce404fe33f0120c8ae3",
            "/paper/Towards-a-Better-Match-in-Siamese-Network-Based-He-Luo/01f46bd91e053ce0c92af126bb87d7381a9fbe29"
        ]
    },
    {
        "id": "6d9b0bb216c36aba29a5b11a656a468dc39cad66",
        "title": "ARAH: Animatable Volume Rendering of Articulated Human SDFs",
        "abstract": "This work proposes a model to create animatable clothed human avatars with detailed geometry that generalize well to out-of-distribution poses and proposes a novel joint root-finding algorithm for simultaneous ray-surface intersection search and correspondence search. Combining human body models with differentiable rendering has recently enabled animatable avatars of clothed humans from sparse sets of multi-view RGB videos. While state-of-the-art approaches achieve realistic appearance with neural radiance fields (NeRF), the inferred geometry often lacks detail due to missing geometric constraints. Further, animating avatars in out-of-distribution poses is not yet possible because the mapping from observation space to canonical space does not generalize faithfully to unseen poses. In this work, we address these shortcomings and propose a model to create animatable clothed human avatars with detailed geometry that generalize well to out-of-distribution poses. To achieve detailed geometry, we combine an articulated implicit surface representation with volume rendering. For generalization, we propose a novel joint root-finding algorithm for simultaneous ray-surface intersection search and correspondence search. Our algorithm enables efficient point sampling and accurate point canonicalization while generalizing well to unseen poses. We demonstrate that our proposed pipeline can generate clothed avatars with high-quality pose-dependent geometry and appearance from a sparse set of multi-view RGB videos. Our method achieves state-of-the-art performance on geometry and appearance reconstruction while creating animatable avatars that generalize well to out-of-distribution poses beyond the small number of training poses.",
        "publication_year": "2022",
        "authors": [
            "Shaofei Wang",
            "Katja Schwarz",
            "Andreas Geiger",
            "Siyu Tang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "36",
        "reference_count": "94",
        "references": [
            "/paper/NPC%3A-Neural-Point-Characters-from-Video-Su-Bagautdinov/35e846e0e6824176c58e6147243dc95bf953ed49",
            "/paper/PoseVocab%3A-Learning-Joint-structured-Pose-for-Human-Li-Zheng/23fa05a3c19067e2a6776756c56ad8a791016cfd",
            "/paper/Compositional-3D-Human-Object-Neural-Animation-Hou-Yu/0c256b5ae76156b8cda006a0abbf7c84efdfcd1b",
            "/paper/UVA%3A-Towards-Unified-Volumetric-Avatar-for-View-and-Fan-Zhang/a739f975a6191904a07169a8e2773fbc6990b73a",
            "/paper/Efficient-Meshy-Neural-Fields-for-Animatable-Human-Huang-Cheng/aa958ecb7c7f377c5b2c0bfb306354b13bb57a50",
            "/paper/Fast-SNARF%3A-A-Fast-Deformer-for-Articulated-Neural-Chen-Jiang/6f96b613c8dcb05b3e911aa9ac8e46f64f0b236c",
            "/paper/NCHO%3A-Unsupervised-Learning-for-Neural-3D-of-Humans-Kim-Saito/862dd48a359a44359428bf0b3828b3152fba2d4b",
            "/paper/HumanRF%3A-High-Fidelity-Neural-Radiance-Fields-for-Isik-R%C3%BCnz/91ffe577a053ab0afccc7c031a903d8d40cec512",
            "/paper/AvatarReX%3A-Real-time-Expressive-Full-body-Avatars-Zheng-Zhao/ae1489b912f8d2eb68234b34fcbdb21f73fbd8df",
            "/paper/AG3D%3A-Learning-to-Generate-3D-Avatars-from-2D-Image-Dong-Chen/7841fe0c3e7d9175c960c7cc0121c97cf8bc7072",
            "/paper/The-Unreasonable-Effectiveness-of-Deep-Features-as-Zhang-Isola/c468bbde6a22d961829e1970e6ad5795e05418d1",
            "/paper/I-M-Avatar%3A-Implicit-Morphable-Head-Avatars-from-Zheng-Abrevaya/792d86d09c2fe992d4ebfb5b80d6dd7ff76e1b3c",
            "/paper/MetaAvatar%3A-Learning-Animatable-Clothed-Human-from-Wang-Mihajlovi%C4%87/e4d018da22a15a39ed47fa6ee000d77ab916aaae",
            "/paper/Animatable-Neural-Radiance-Fields-for-Modeling-Peng-Dong/35e940ace815548f620709d9c1803da34c581e86",
            "/paper/SNARF%3A-Differentiable-Forward-Skinning-for-Neural-Chen-Zheng/586feddfa72cc263a17207a0cb73b992e1ee3a71",
            "/paper/A-NeRF%3A-Articulated-Neural-Radiance-Fields-for-and-Su-Yu/74bb19d1ce2ec9fb9605545a750e68281a067e63",
            "/paper/AI-Choreographer%3A-Music-Conditioned-3D-Dance-with-Li-Yang/021bd56b6c95d0196f3b8e818c948ea0702b0836",
            "/paper/SMPL%3A-a-skinned-multi-person-linear-model-Loper-Mahmood/32d3048a4fe4becc7c4638afd05f2354b631cfca",
            "/paper/Neural-Body%3A-Implicit-Neural-Representations-with-Peng-Zhang/af8faec7c0b8f4b2a28d42a86e0e7d499016c560",
            "/paper/Multiview-Neural-Surface-Reconstruction-by-Geometry-Yariv-Kasten/2854af340fffab4f6de0bcdcf21d1d33ece08ae8"
        ]
    },
    {
        "id": "f067a6ce86b01ee3e06767b53e4d440b83ae1075",
        "title": "Accurate Object Tracking by Utilizing Diverse Prior Information",
        "abstract": "A Siamese network with diverse prior information integrated, namely DPINet, is developed by extending two novel blocks to a powerful anchor-free Siamesese network by designing a channel- and space-aware feature enhancement (CSE) block to highlight target-specific feature weights in two aspects (channel and spatial dimensions). Siamese trackers draw continuous attention in the object tracking community due to their proper balance between performance and inference speed. Despite that, it remains unclear how to effectively exploit the target appearance cues and motion cues involved in videos to improve trackers\u2019 performance. To address this problem, we develop a Siamese network with diverse prior information integrated, namely DPINet, by extending two novel blocks to a powerful anchor-free Siamese network. First, we design a channel- and space-aware feature enhancement (CSE) block to highlight target-specific feature weights in two aspects (channel and spatial dimensions). It is devoted to making full use of the target cues in the initial frame by considering them as guidances, in which way target-related representation in feature maps can be improved. It also facilitates the interplay between two input branches. Second, we advance a cross-correlation block with multi-dimensional information fusion (MDI-XCorr). In this block, target motion cues within adjacent frames can be mined and treated as supervisions to refine the response map in the current frame during inference. Hence, both tracking quality and stabilization can be enhanced. Evaluations on four popular benchmarks are conducted, showing that DPINet achieves 0.702 (AUC), 0.474 (EAO), 0.336 (EAO), 0.613 (AO), and 0.527 (AUC) on OTB100, VOT2018, VOT2019, GOT-10k, and LaSOT, respectively.",
        "publication_year": "2023",
        "authors": [
            "Zhaohua Hu",
            "Xiao Lin"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "71",
        "references": [
            "/paper/Siamese-Attentional-Cascade-Keypoints-Network-for-Wang-Wang/45bde350082fe5ee366c8f1b761429d2277fbcca",
            "/paper/Toward-Accurate-Pixelwise-Object-Tracking-via-Zhang-Liu/965dcac204f056bff465247c3efce94e5cb53a7c",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Deformable-Siamese-Attention-Networks-for-Visual-Yu-Xiong/5eca15a355b2a9a1e80879e850afe49d3c398c53",
            "/paper/Siamese-Network-Using-Adaptive-Background-for-Zhu-Chen/6a9857c8b268d295a8716d57cc2f7429fccfc309",
            "/paper/Graph-Attention-Tracking-Guo-Shao/37929e9283d214a1688bd6fa9759cd6e89b2312d",
            "/paper/Siamese-Attentional-Keypoint-Network-for-High-Gao-Ma/fce3655dc22a783b1f82f09190410f070c7bf42c",
            "/paper/Know-Your-Surroundings%3A-Exploiting-Scene-for-Object-Bhat-Danelljan/d1e61fa7824709cae37fb59483dd0772e3101c08",
            "/paper/ATOM%3A-Accurate-Tracking-by-Overlap-Maximization-Danelljan-Bhat/d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "/paper/Siamese-Box-Adaptive-Network-for-Visual-Tracking-Chen-Zhong/cce1fecc800d2782da638f3060d5b2e887739f74"
        ]
    },
    {
        "id": "be9a0f74c4143eef825d17499c6dcb1b90cb70e1",
        "title": "CASS: Cross Architectural Self-Supervision for Medical Image Analysis",
        "abstract": "This paper presents Cross Architectural - Self Supervision (CASS), a novel self-supervised learning approach that leverages Transformer and CNN simultaneously, and empirically shows that CASS is much more robust to changes in batch size and training epochs. Recent advances in deep learning and computer vision have reduced many barriers to automated medical image analysis, allowing algorithms to process label-free images and improve performance. However, existing techniques have extreme computational requirements and drop a lot of performance with a reduction in batch size or training epochs. This paper presents Cross Architectural - Self Supervision (CASS), a novel self-supervised learning approach that leverages Transformer and CNN simultaneously. Compared to the existing state of the art self-supervised learning approaches, we empirically show that CASS-trained CNNs and Transformers across four diverse datasets gained an average of 3.8% with 1% labeled data, 5.9% with 10% labeled data, and 10.13% with 100% labeled data while taking 69% less time. We also show that CASS is much more robust to changes in batch size and training epochs. Notably, one of the test datasets comprised histopathology slides of an autoimmune disease, a condition with minimal data that has been underrepresented in medical imaging. The code is open source and is available on GitHub.",
        "publication_year": "2022",
        "authors": [
            "Pranav Singh",
            "E. Sizikova",
            "J. Cirrone"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "56",
        "references": [
            "/paper/Brain-Tumor-Synthetic-Data-Generation-with-Adaptive-Tariq-Qureshi/7bfa1514a6ffba2fdc5828462f59f7548b094e22",
            "/paper/Self-supervised-Learning-from-100-Million-Medical-Ghesu-Georgescu/e56320ed18e4fef4cbec9a35a1d5e7fe9c05b7fb",
            "/paper/Is-it-Time-to-Replace-CNNs-with-Transformers-for-Matsoukas-Haslum/2b446f63c98e8ec0176866fe66168faf89611ba5",
            "/paper/Big-Self-Supervised-Models-Advance-Medical-Image-Azizi-Mustafa/197ec03481b5e845fb4d34dd99a4b8e844fdabcc",
            "/paper/A-Data-Efficient-Deep-Learning-Framework-for-and-of-Singh-Cirrone/4def25d614027ebb972cf6c491cc449cde3239b6",
            "/paper/Deep-convolutional-neural-network-based-medical-for-Yadav-Jadhav/9eb47e857cdf1f97413de0e3993f8429d7c33503",
            "/paper/Transfusion%3A-Understanding-Transfer-Learning-for-Raghu-Zhang/cff4cb74f4466bd0407977e40ef0be9f444c63ea",
            "/paper/U-Net%3A-Convolutional-Networks-for-Biomedical-Image-Ronneberger-Fischer/6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "/paper/COVID-19-Prognosis-via-Self-Supervised-Learning-and-Sriram-Muckley/63936fe9d0c6c7e87e1f939c926412349ced5716",
            "/paper/COVID-19-Deterioration-Prediction-via-Learning-and-Sriram-Muckley/d232043934430b5cd674b77595d0d6b37211ea50",
            "/paper/Skin-lesion-classification-using-ensembles-of-with-Gessert-Nielsen/2b73f023765f784a5e8bc943875777d2a26e06fa"
        ]
    },
    {
        "id": "4e11d87889284076b150c2545b20d9c4b4008378",
        "title": "Computer-Aided Evaluation of Malignancy with Magnetic Resonance Imaging of the Breast Policy",
        "abstract": "MRI may be used to screen women at high risk of breast cancer or to look for more extensive disease in women diagnosed with breast cancer who are eligible for breast-conserving surgery; it is also being studied to gauge the impact of cancer treatment. Background/Overview The use of CAE is proposed to assist radiologists\u2019 interpretation of contrast-enhanced magnetic resonance imaging (MRI) of the breast. MRI of the breast is suggested as an alternative or adjunct to mammography or other screening and diagnostic tests because of its high sensitivity in detecting breast lesions. However, it has a high false-positive rate because it is difficult to distinguish between benign and malignant lesions. MRI may be used to screen women at high risk of breast cancer or to look for more extensive disease in women diagnosed with breast cancer who are eligible for breast-conserving surgery; it is also being studied to gauge the impact of cancer treatment.",
        "publication_year": "2014",
        "authors": [],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "26",
        "references": [
            "/paper/Computer-aided-detection-of-malignancy-with-imaging-BlueShield/68f197b645c25f68297f27f18aada1b2cc94dc22",
            "/paper/Computer-Aided-Diagnosis-of-Breast-DCE-MRI-Images-Yang-Li/b6b784f957af20523a9b1ed94d90b44a44061824",
            "/paper/Can-breast-MRI-computer-aided-detection-(CAD)-for-a-Arazi-Kleinman-Causer/67a0b81c3b147ffb1e4b937a70a96af818949db4",
            "/paper/3.0T-MR-CAD%3A-Clinical-Value-in-Diagnosis-of-Breast-Liu-Xu/503da21f48bd4e1cc576beb0241a813f4f8cde05",
            "/paper/Breast-MR-imaging%3A-computer-aided-evaluation-for-Williams-DeMartini/db0fe3d29d01a48b3ea406f0d5d96aea4e7890dc",
            "/paper/Computer-aided-detection-(CAD)-system-for-breast-in-Song-Seo/c49958ee3c26aec1cffd8e28af269003098a256f",
            "/paper/Contralateral-lesions-detected-by-preoperative-MRI-Cho-Kim/6b80b780f92427f0a34b3e59a83dd96dd35cac26",
            "/paper/Magnetic-resonance-imaging-of-the-breast%3A-from-the-Sardanelli-Boetes/7fcfd6603f2382e166238145c6eebf4cde6cb30a",
            "/paper/Differentiation-of-benign-and-metastatic-axillary-Yun-Sohn/153052f9e9e1dcf048160360bb188a7baaf2d463",
            "/paper/Cancerous-breast-lesions-on-dynamic-MR-images%3A-for-Bhooshan-Giger/e7e9bb3b20c003e83962a13dfde1ec781589a176"
        ]
    },
    {
        "id": "38a3511f5ace7871aa6b0a51e8a3ea62dbc24e14",
        "title": "Classification of Breast Tumours Based on Histopathology Images Using Deep Features and Ensemble of Gradient Boosting Methods",
        "abstract": "Semantic Scholar extracted view of \"Classification of Breast Tumours Based on Histopathology Images Using Deep Features and Ensemble of Gradient Boosting Methods\" by Mohammad Reza Abbasniya et al.",
        "publication_year": "2022",
        "authors": [
            "Mohammad Reza Abbasniya",
            "Sayed Ali Sheikholeslamzadeh",
            "H. Nasiri",
            "S. Emami"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "17",
        "reference_count": "66",
        "references": [
            "/paper/Automated-carcinoma-classification-using-efficient-Dhivya-Mohanavalli/de96c4ca0e695dc6c1a491aef8db479d8d668e46",
            "/paper/Impact-of-Image-Preprocessing-Methods-and-Deep-for-Murcia-G%C3%B3mez-Rojas-Valenzuela/fb52c9004eb8f2bef2d58df6febdba2f60d70b4f",
            "/paper/Accuracy-Analysis-of-Deep-Learning-Methods-in-A-Yusoff-Haryanto/993e1de15a1bd2e04881f7bde53bf32156827088",
            "/paper/Multi-objective-hyperparameter-optimization-on-for-Singh-Gupta/e75e7d0e62cf3056e40a6ea66ec54ef29418d63b",
            "/paper/GC-EnC%3A-A-Copula-based-ensemble-of-CNNs-for-in-and-Dey-Mitra/2c2af7a83c68deffa698e1f9e411b1f6480ef3b7",
            "/paper/Evolution-of-LiverNet-2.x%3A-Architectures-for-liver-Chanchal-Lal/54105731747d055b248c1baae44233b55e81309e",
            "/paper/ETECADx%3A-Ensemble-Self-Attention-Transformer-for-Al-Hejri-Al-Tam/dc9e589f4c576a963d5cd2cc57c7378a8ec91178",
            "/paper/Towards-More-Transparent-and-Accurate-Cancer-with-Tabatabaei-Colomer/fda2ac4f337db91ac3eb1e6892d5497eb171cb53",
            "/paper/Hyperparameter-Optimizer-with-Deep-Learning-Based-Obayya-Maashi/fcdded046102348cb1f17da49e810090d3299549",
            "/paper/Detection-of-Covid-19-and-other-pneumonia-cases-CT-%C3%87elik/b054a4fabe26056d0ed0186abc4c8b5ed8681c3e",
            "/paper/Breast-Cancer-Classification-Using-Deep-Learning-A-Shahidi-Daud/9e505d09d3df9f1786d86c7817f2c3d1a86d6cdd",
            "/paper/Classification-on-Digital-Pathological-Images-of-on-Li-Li/8312c8ad7afe11bb8e30981bf06738ac2e4966c0",
            "/paper/Automated-detection-and-grading-of-Invasive-Ductal-Barsha-Rahman/d8dc0be93f6fd389a5593deb52ba8620ac6a6833",
            "/paper/Boosting-Breast-Cancer-Detection-Using-Neural-Alanazi-Kamruzzaman/a812ebb6761ff453a02d866e97295932153537e2",
            "/paper/Breast-cancer-detection-from-biopsy-images-using-George-Faziludeen/32aa3271a38fca4051854bfccbef5f4eb4f89da1",
            "/paper/Histopathological-breast-image-classification-with-Nahid-Mikaelian/021778497ed06c1252aeb6817e21395758b38436",
            "/paper/Representation-learning-based-unsupervised-domain-Alirezazadeh-Hejrati/6672903ac7a4bb160a0ca0104e49ef02c9bc29f4",
            "/paper/Multiple-instance-learning-for-histopathological-Sudharshan-Petitjean/7bfc41af5653255df067395928f164210f71d7f9",
            "/paper/Breast-Cancer-Histopathological-Image-via-Deep-and-Du-Qi/f39224b7fd314f6db33d02065940ec4c7c2551be",
            "/paper/Deep-features-for-breast-cancer-histopathological-Spanhol-Oliveira/9e338dd767703ab0a14edeb2a784798f4e169975"
        ]
    },
    {
        "id": "18d2d888d5e1a53b9d9261cd419804ef4205902a",
        "title": "Histopathological imaging database for oral cancer analysis",
        "abstract": "Semantic Scholar extracted view of \"Histopathological imaging database for oral cancer analysis\" by T. Y. Rahman et al.",
        "publication_year": "2020",
        "authors": [
            "T. Y. Rahman",
            "L. Mahanta",
            "A. Das",
            "J. Sarma"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "21",
        "reference_count": "4",
        "references": [
            "/paper/NDB-UFES%3A-An-oral-cancer-and-leukoplakia-dataset-of-Assis-Soares/784fc7611f2df38c5bb6fa317ba987c9e96d5fd8",
            "/paper/Analysis-of-Histopathological-Images-for-Early-of-Ahmed-Senan/697b853386ddb894ccf3e45613bced6cb0c63155",
            "/paper/Stain-color-translation-of-multi-domain-OSCC-images-Barua-Bora/b67af898cc021c95186ff2ca94bf5b19acc87aa8",
            "/paper/Histopathological-Image-Analysis-for-Oral-Squamous-Amin-Zamir/629c3a99f0f62a12f8aee93aea9934bc61509335",
            "/paper/An-ensemble-deep-learning-model-with-empirical-for-Deo-Pal/7952a2f8354186182d0c61626b3d6491b48833bb",
            "/paper/Noninvasive-Imaging-Methods-to-Improve-the-of-Oral-Romano-Stasio/93c80affd4447c89ca34dc3ccba8fd1cb1bf1a1a",
            "/paper/Computer-Assisted-Recognition-of-Oral-Squamous-Cell-S.-RejuV./4b7b426bce83db16f1c3f65fec1df2405176b566",
            "/paper/Automatic-Detection-of-Oral-Squamous-Cell-Carcinoma-Das-Dash/8930a224de45217ece72efa125d192b41e81c00f",
            "/paper/Intelligent-Deep-Learning-Enabled-Oral-Squamous-and-Alanazi-Khayyat/81eacfbe64d362346b311f85a1f7f95df8d8edce",
            "/paper/Duck-Pack-Optimization-With-Deep-Transfer-Oral-Cell-Shetty-Patil/c6cf22e6def0eeef5df3e5ff96878b67d4ea8139",
            "/paper/Automated-oral-squamous-cell-carcinoma-using-shape%2C-Rahman-Mahanta/0335094921c450f96c64564a60bd3793d29fb0de",
            "/paper/Textural-pattern-classification-for-oral-squamous-Rahman-Mahanta/79bc5f414713f68681aaf414cb66b5f714735636",
            "/paper/Observer-variation-in-histopathological-diagnosis-Ismail-Colclough/3fb3523c11a270db015add7911c86cd280b1e0f1",
            "/paper/Malignant-mesothelioma-of-the-pleura%3A-interobserver-Andrion-Magnani/4a3332efa5078664791087ab6690aa184c177b14"
        ]
    },
    {
        "id": "3bf96a19d72fd7eb3fc9404fc02f475f276ca693",
        "title": "An effective approach for CT lung segmentation using region growing",
        "abstract": "A region growing algorithm based on threshold presegmentation is selected for lung segmentation, which integrates image enhancement, threshold segmentsation, seed point selection and morphological post-processing, etc., to improve the segmentation effect, which also has certain reference value for other medical image processing. X-ray is an important means of detecting lung diseases. With the increasing incidence of lung diseases, computer-aided diagnosis technology is of great significance in clinical treatment. It has become a hot research direction to use computer-aided diagnosis to recognize chest radiography images, which can alleviate the uneven status of regional medical level. For clinical diagnosis, medical image segmentation can enable users to timely obtain the target region they are interested in and analyze it, which is significant to be used as an important basis for auxiliary research and judgment. In this case, a region growing algorithm based on threshold presegmentation is selected for lung segmentation, which integrates image enhancement, threshold segmentation, seed point selection and morphological post-processing, etc., to improve the segmentation effect, which also has certain reference value for other medical image processing.",
        "publication_year": "2021",
        "authors": [
            "Xi Yang",
            "Guanyu Xu",
            "Teng Zhou"
        ],
        "related_topics": [
            "Computer Science",
            "Medicine"
        ],
        "citation_count": "2",
        "reference_count": "13",
        "references": [
            "/paper/Performance-and-Robustness-of-Regional-Image-Driven-Kub%C3%ADcek-Varysova/d8b67c16876d8557ce91485bd68e71cf41792ac1",
            "/paper/Chinese-rubbing-document-image-binarization-image-Yu-Sun/74117fa25b9acde0a935bff6312ba151f7646415",
            "/paper/Segmentation-of-Lung-Cancer-from-CT-Image-A-Indumathi-Vasuki/fba02f47eb12e487748dc517cb92295fbf26a94d",
            "/paper/A-simplified-cluster-model-and-a-tool-adapted-for-Morozov-Gombolevsky/02730d70a602091e3fe2c4cd0fd1a11a189e3dd7",
            "/paper/Deep-learning-for-COVID-19-chest-CT-(computed-image-Jiang-Tang/b94558c5d82ea6b42f512c6eab046460f20f9f74",
            "/paper/Compound-image-segmentation-of-published-biomedical-Li-Jiang/f04ad7f58454fea5ad7ecc5acbd796774a14f1cc",
            "/paper/Segmentation-of-cDNA-Microarray-Spots-Using-Markov-Demirkaya-Asyali/94a60e432ead75ec5189d77f8ac18de6d2638bb6",
            "/paper/2dSpAn%3A-semiautomated-2-d-segmentation%2C-and-of-Basu-Plewczy%C5%84ski/fa49c1ccbf4733c31772d35089ded57bc395a822",
            "/paper/New-approach-for-segmentation-and-quantification-of-Anjos-M%C3%B8ller/02ec43c470666ff4d3366df490514781763181b7",
            "/paper/A-spectral-graph-theoretic-approach-to-and-of-in-Lin-Lin/8fe176f3ffecb63cfbf581529714f805a132c206",
            "/paper/Stratifying-tumour-subtypes-based-on-copy-number-Gusnanto-Tcherveniakov/b4defd66762e7055b26bfcb991808bd7a3e6848d",
            "/paper/Multi-platform-segmentation-for-joint-detection-of-Mei-Pawitan/8838cc82fb39d429140ba7b08e79f226b6f0271a"
        ]
    },
    {
        "id": "8dc641c22388229a2b178a73f9ef32a4e5a2f512",
        "title": "Forest Classification Method Based on Convolutional Neural Networks and Sentinel-2 Satellite Imagery",
        "abstract": "The objective of this study is to develop a classi\ufb01cation method based on convolutional neural network (CNN) and Sentinel-2 satellite imagery including the spectral feature, spectral index and spatial feature together as an input to answer forest monitoring problem. This research also used contextual information on Indonesia National Standard Agency\u2019s document for Land cover classi\ufb01cation as a baseline for feature extraction to get the appropriate classi\ufb01er feature. The test set was located in Semarang, Central Java, Indonesia. The research work\ufb02ow consists of de\ufb01ning forest class based on Indonesia National Standard Agency for Land cover classi\ufb01cation, extracting optical image features based on contextual information of the forest class de\ufb01nition, extracting image features from the Sentinel-2 satellite image, and classifying image object features using CNN classi\ufb01er. Image segmentation produced 1,211 segments/objects by using eCognition software. Subsequently, these objects were used as a dataset. Overall accuracy was used to evaluate the performance of the classi\ufb01cation result. The result showed the classi\ufb01cation method results in this study yielded high overall accuracy (97.66%) when using CNN with the image features like NDVI, Brightness, GLCM homogeneity and Rectangular \ufb01t. Small improvement of overall accuracy was also achieved when it was compared to GBT with an overall accuracy of 95.50%.",
        "publication_year": "2019",
        "authors": [
            "Eka Miranda",
            "A. Mutiara",
            "Ernastuti",
            "W. Wibowo"
        ],
        "related_topics": [
            "Environmental Science",
            "Mathematics"
        ],
        "citation_count": "7",
        "reference_count": "21",
        "references": [
            "/paper/Early-Detection-of-Deforestation-through-Satellite-Pratiwi-Fu%E2%80%99adah/0406e19307d7e92e8a327346eea60e14232f70df",
            "/paper/An-Efficient-Wildfire-Detection-System-for-Using-James-Ansaf/08e088b3e1187398a0f1edd9b7bffffbe422d5fd",
            "/paper/The-Effect-of-Batch-Size-and-Epoch-on-Performance-Sari-Arifin/3ef9c282b0cd7cb9c8f3521ffc44fa52327c2414",
            "/paper/Forest-Land-Cover-Mapping-at-a-Regional-Scale-Using-Alonso-Picos/f877286d0e30cf842f4e95f51a2ea874883c1b0b",
            "/paper/The-Potential-of-Sentinel-2-Satellite-Images-for-A-Isbaex-Coelho/812355b63fb14bd7adf41856a35e2b5632ebdc61",
            "/paper/Analysis-of-Capabilities-of-the-Multispectral-in-Belov-Belov/2b5e6789c7a8e4380bb86298e4d7295b6491ea6f",
            "/paper/Approach-to-Obstacle-Localization-for-Robot-in-ksamentov-Astapova/fe496a1d32daad0de93863f4957a030d6ac5b657",
            "/paper/Land-use-scene-classification-based-on-a-CNN-using-Weng-Mao/db07b4813b1a691d60f4df40910f89bf0e26f40d",
            "/paper/Deep-Learning-Classification-of-Land-Cover-and-Crop-Kussul-Lavreniuk/7a9e471e31ac156cf22a5e2a5c1463697df866ab",
            "/paper/Object-Based-Convolutional-Neural-Network-for-Zhao-Du/dd9a8643ef80fd3fb95060f9b02e0233759639aa",
            "/paper/Land-use-classification-using-convolutional-neural-Zhu-Newsam/5e0530ab9025757a0ae22c1b357bf2be2d258218",
            "/paper/Band-selection-in-sentinel-2-satellite-for-Zhang-Su/f4e09b00869ff846a91dfb9f8ccd92de2904e920",
            "/paper/Use-of-Sentinel-2-for-forest-classification-in-Puletti-Chianucci/1bdbb25306c323934a80f6d772d45e1d9b1d299c",
            "/paper/Forest-Condition-Monitoring-Using-Satellite-Imagery-Uddin-Gilani/6f6c5a38a9fd653aee02519b4f46bcfa702c1acc",
            "/paper/Ontology-based-classification-of-remote-sensing-Andr%C3%A9s-Arvor/fc2764aa0c0ee9ee34bbe4db6fe4332a2f9b5033",
            "/paper/Deep-Learning-LeCun-Bengio/a4cec122a08216fe8a3bc19b22e78fbaea096256",
            "/paper/Beyond-the-Normalized-Difference-Vegetation-Index-a-Rugel-Henderson/ad4d2f6e88687a56200275374df202844e7cb7a7"
        ]
    },
    {
        "id": "854de5b7c927a35b79f4208d03942086891f02cb",
        "title": "Flexoelectric rotation of polarization in ferroelectric thin films.",
        "abstract": "This work has studied the strain distribution inside epitaxial films of the archetypal ferroelectric PbTiO(3), where the mismatch with the substrate is relaxed through the formation of domains (twins). Strain engineering enables modification of the properties of thin films using the stress from the substrates on which they are grown. Strain may be relaxed, however, and this can also modify the properties thanks to the coupling between strain gradient and polarization known as flexoelectricity. Here we have studied the strain distribution inside epitaxial films of the archetypal ferroelectric PbTiO(3), where the mismatch with the substrate is relaxed through the formation of domains (twins). Synchrotron X-ray diffraction and high-resolution scanning transmission electron microscopy reveal an intricate strain distribution, with gradients in both the vertical and, unexpectedly, the horizontal direction. These gradients generate a horizontal flexoelectricity that forces the spontaneous polarization to rotate away from the normal. Polar rotations are a characteristic of compositionally engineered morphotropic phase boundary ferroelectrics with high piezoelectricity; flexoelectricity provides an alternative route for generating such rotations in standard ferroelectrics using purely physical means.",
        "publication_year": "2011",
        "authors": [
            "G. Catal\u00e1n",
            "A. Lubk",
            "A. Vlooswijk",
            "E. Snoeck",
            "C. Mag\u00e9n",
            "A. Janssens",
            "G. Rispens",
            "G. Rijnders",
            "D. Blank",
            "B. Noheda"
        ],
        "related_topics": [
            "Physics",
            "Materials Science"
        ],
        "citation_count": "477",
        "reference_count": "57",
        "references": [
            "/paper/Atomic-Scale-Tunable-Flexoelectric-Couplings-in-Geng-Wang/c342f82d15794fe29f641108adf7dbc20b029ce4",
            "/paper/Role-of-flexoelectric-coupling-in-polarization-at-Cao-Chen/090c4ad2ec9620ef70123d3641688f51695ef0af",
            "/paper/Influence-of-flexoelectric-effects-on-domain-in-Zou-Tang/1c0a2cf10a93c30a604ee6c6cd20cc1fe63eeb1a",
            "/paper/Polar-Graded-Multiferroic-SrMnO3-Thin-Films.-Guzm%C3%A1n-Maurel/bce831fc663527ac1196a54c1e6fc5ee445e8cab",
            "/paper/Mechanical-Writing-of-Ferroelectric-Polarization-Lu-Bark/218e3eab5a69cfbe63e001a9ebeb104d11c4117a",
            "/paper/Flexopiezoelectricity-at-ferroelastic-domain-walls-Yun-Song/5e287d25e4722b37ee19702318cf2c7f6d621034",
            "/paper/Giant-Flexoelectric-Polarization-in-a-Micromachined-Wang-Zhang/4c8c44de09e2ce937390080a14cbe781d59886ee",
            "/paper/Flexoelectricity-induced-retention-failure-in-films-Zou-Tang/aea657c9ebbe7c05ab79bed25fcce7e44cc43193",
            "/paper/Phase-field-modeling-of-flexoelectric-effects-in-Chen-Soh/64527aca066f496631154bd536b4528c62afce12",
            "/paper/Phase-field-modeling-of-flexoelectric-effects-in-Chen-Soh/9fc021cac59043cc23a431e94a86d8c22379b705",
            "/paper/The-effect-of-flexoelectricity-on-the-dielectric-of-Catal%C3%A1n-Sinnamon/9ee3b6b81481cd42bb838aee055ef60a9b8a1bac",
            "/paper/Strain-gradient-induced-polarization-in-SrTiO3-Zubko-Catal%C3%A1n/7bc97bce46a770f87d13b80073ab329cab077038",
            "/paper/Strain-gradients-in-epitaxial-ferroelectrics-Catal%C3%A1n-Noheda/8753b220e65a9a0a2b47939fc5e8b8b0d3b3a7f4",
            "/paper/A-Strain-Driven-Morphotropic-Phase-Boundary-in-Zeches-Rossell/2da82f668261edccccef1fd5e14cf5cd7b384868",
            "/paper/Giant-flexoelectric-effect-in-ferroelectric-thin-Lee-Yoon/1061e73eb65e8093c90f1e45316b1f8640afdd20",
            "/paper/Dynamics-of-ferroelastic-domains-in-ferroelectric-Nagarajan-Roytburd/8ff563f730cf86bb2e48bc444e931c8dcd2139ec",
            "/paper/Atomistic-determination-of-flexoelectric-properties-Maranganti-Sharma/f4e82e232e592a9c3a4d44de37706abaf30d3ff1",
            "/paper/Flexoelectric-effects%3A-Charge-separation-in-solids-Cross/91bda599121b3f019b0fbd57cd2a713e22f360d2",
            "/paper/Room-temperature-ferroelectricity-in-strained-Haeni-Irvin/ec5a1822525df038b054daf335ce8a8ca1cb5047",
            "/paper/Polar-domains-in-lead-titanate-films-under-tensile-Catal%C3%A1n-Janssens/2334b7682de305ad87ddd6c4bb1d77c7df4b783d"
        ]
    },
    {
        "id": "5b031bebc736bd44936c94b7c0d8fe7ffe2ce798",
        "title": "Detecting Hidden Anomalies Using Sketch for High-speed Network Data Stream Monitoring",
        "abstract": "This paper proposed an IP address traceability network anomaly detection method at right time based on the summary data structure that takes up little computing and memory resources and is suitable for anomaly detection under the high-speed network traffic. Monitoring network data streams in real-time to check security event become more and more important along with the rapid growth of Internet applications. The detection typically treats the traffic as a collection of flows that need to be examined for significant changes in traffic pattern (e.g., volume, number of connections). However, as link speeds and the number of flows increase, keeping per- flow state is either too expensive or too slow. We propose building compact summaries of the traffic data using the notion of sketches.In this paper, we proposed an IP address traceability network anomaly detection method at right time based on the summary data structure. In this method, the network traffic information is recorded into sketch online in every circle which is used to detect anomalies. By using EWMA forecasting model to get each circle forecast value, it computes the error sketch between the recoded value and forecast value and detects heavy network traffic change based on Mean-Standard deviation in the error sketch. The method is effective in detecting DDoS attack, scan attack. And it can trace the IP address of victim host. Evaluated by the experiment, the results show that this method takes up little computing and memory resources and is suitable for anomaly detection under the high-speed network traffic.",
        "publication_year": "2012",
        "authors": [
            "Liu Aiping",
            "Yi Han",
            "Zhou Bin",
            "Weihong Han",
            "Y. Jia"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "13",
        "reference_count": "16",
        "references": [
            "/paper/Promising-techniques-for-anomaly-detection-on-Tian-Liu/305bd745fdc9a44686cc67482be5c18fe022f46f",
            "/paper/Network-backbone-anomaly-detection-using-double-on-Yin-Yao/b2dc3957685a71f20a7edec78f39ee270af08a7f",
            "/paper/Online-traffic-prediction-in-the-cloud-Dalmazo-Vilela/d16b6598963bab9d58132d46e4b8841f819e55a5",
            "/paper/Predicting-Traffic-in-the-Cloud%3A-A-Statistical-Dalmazo-Vilela/8648599d28b5bf8f88bac203bebc85a4a12b12e7",
            "/paper/Performance-Analysis-of-Network-Traffic-Predictors-Dalmazo-Vilela/44ff7a652d53d1849bde9606a4a8d0eb0d3d8e57",
            "/paper/Online-Traffic-Prediction-in-the-Cloud%3A-A-Dynamic-Dalmazo-Vilela/332df6ad6e60838c3baac8f7e8bc9d9757d35364",
            "/paper/Network-traffic-fusion-and-analysis-against-DDoS-a-Jing-Yan/0743c6b547ca835d06b7d2fe2a20caba07719f3a",
            "/paper/Enhancing-Network-Intrusion-Detection-by-of-Hashed-Drasar-Jirs%C3%ADk/516165fa051c6976fb1d638f3f5734710702eb0c",
            "/paper/A-Prediction-based-Approach-for-Anomaly-Detection-Dalmazo-Vilela/df81ef736f77eb7ba96f4a7832ca1820886b9852",
            "/paper/An-Efficient-Approach-on-Answering-Top-k-Queries-Li-Xu/d0c3af88cfc14d63c6f94f3a29c0f8f78b7fffc3",
            "/paper/Reversible-sketches-for-efficient-and-accurate-over-Schweller-Gupta/ec16f48079432902f62ed8f98604f7ab0e0bba58",
            "/paper/Sketch-based-change-detection%3A-methods%2C-evaluation%2C-Krishnamurthy-Sen/35a25052bf9fc89e0b28499236322c5786fc58a9",
            "/paper/Extracting-hidden-anomalies-using-sketch-and-non-Dewaele-Fukuda/a2c6af1e7517ff9a0583ccd7996ae9ca08885278",
            "/paper/Bro%3A-a-system-for-detecting-network-intruders-in-Paxson/ca2949898b9a139f7b9129d414b3c1cafe915b61",
            "/paper/Data-streams%3A-algorithms-and-applications-Muthukrishnan/0233eabf848e9bbc6b6debfaa9ec3cf5a195b23d",
            "/paper/Snort%3A-Lightweight-Intrusion-Detection-for-Networks-Roesch/363d109c3f00026f9ef904dd8cc3c935ee463b65",
            "/paper/Remote-data-checking-for-network-coding-based-Chen-Curtmola/6067a1688418fe32a0681972af722d002b5776d6",
            "/paper/Deflating-the-big-bang%3A-fast-and-scalable-deep-with-Smith-Estan/0f121a53edd7be2504cc46b2d2774157ff883469",
            "/paper/Universal-Classes-of-Hash-Functions-Carter-Wegman/feb061b699a2249f803baf159a991d63c64f9c99",
            "/paper/Universal-classes-of-hash-functions-(Extended-Carter-Wegman/a1b9f637796f7366669f3c68dc7459596d1f7fad"
        ]
    },
    {
        "id": "8e522e626b607a95e599361f078c975079b389c0",
        "title": "Masked photo blending: Mapping dense photographic data set on high-resolution sampled 3D models",
        "abstract": "Semantic Scholar extracted view of \"Masked photo blending: Mapping dense photographic data set on high-resolution sampled 3D models\" by M. Callieri et al.",
        "publication_year": "2008",
        "authors": [
            "M. Callieri",
            "Paolo Cignoni",
            "M. Corsini",
            "Roberto Scopigno"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "147",
        "reference_count": "33",
        "references": [
            "/paper/MeshLab-as-a-complete-open-tool-for-the-integration-Callieri-Dellepiane/7c944eed0aef745655906cc22e0bfbfe390fe4c6",
            "/paper/Geometric-Processing-for-Image-based-3D-Object-Qin-Huang/b3014666f9419c43a54730c65a79b10731db7f3d",
            "/paper/LARGE-SCALE-TEXTURED-MESH-RECONSTRUCTION-FROM-AND-Boussaha-Vallet/77cac9ddf8596b14bcb6e826026f7e4a0f4ba820",
            "/paper/Texturing-3D-Models-with-Low-Geometric-Features-Marroquim-Pfeiffer/2143a587c4b94b7be81ffb13bd72b8744d1149b9",
            "/paper/Realistic-Texture-Reconstruction-Incorporating-Papachristou-Dimitriou/3b5b122daabd79f42cc999ad935855d2463c44ae",
            "/paper/Improved-color-acquisition-and-mapping-on-3D-models-Dellepiane-Callieri/ea6942a2b4c301291accc63e2acf558ccd40c47c",
            "/paper/Large-Scale-Point-Cloud-Visualization-through-Arikan-Preiner/98116c31785f0216abbff2c71ce12680fc09d980",
            "/paper/Fully-Automatic-Registration-of-Image-Sets-on-Corsini-Dellepiane/97f95a03b8abf8faff0968883e23abbbab3b1dc7",
            "/paper/Robust-Texture-Mapping-Using-RGB-D-Cameras-Oliveira-Lim/e46b23ba2f6fce4fdecbe677b807d7a5300aa415",
            "/paper/Texture-Mapping-for-3D-Reconstruction-with-RGB-D-Fu-Yan/fd8f3bc244999e003c799aba9e2bf642f570faa7",
            "/paper/High-Quality-Texture-Reconstruction-from-Multiple-Bernardini-Boier-Martin/ca7448443241763238bbb53fd09dab714e8495f1",
            "/paper/Blending-Images-for-Texturing-3D-Models-Baumberg/0221b9a1c671ca8cbfb9ac6dcddd723c17c310e1",
            "/paper/Reconstructing-Textured-Meshes-from-Multiple-Range-Callieri-Cignoni/2e17630fd79058d4d4e1628eb81a2f3ccaa20296",
            "/paper/Texturing-3D-Models-of-Real-World-Objects-from-Heugebauer-Klein/edab6e4b7b967c62e56234ce73269c2ae7543946",
            "/paper/GoLD%3A-interactive-display-of-huge-colored-and-Borgeat-Godin/4de901b88b0fb8eccf07edc299df8a807ae469eb",
            "/paper/Minimizing-user-intervention-in-registering-2D-to-Franken-Dellepiane/d89f636e506f41b7b2a7e0d6e01e3a0430d94c44",
            "/paper/Multiple-Texture-Stitching-and-Blending-on-3D-Rocchini-Cignoni/3a63e87c77a4732d3a92491f70954c39146d7640",
            "/paper/Automatic-relighting-of-overlapping-textures-of-a-Beauchesne-Roy/cde95be199ab74059f9cc8b7c7d5e5b5fc416e90",
            "/paper/Inverse-global-illumination%3A-recovering-reflectance-Yu-Debevec/7c54700f193a0b842e1e684e0e6858646f94eb57",
            "/paper/Automated-texture-registration-and-stitching-for-Lensch-Heidrich/193901d2a9df89f9f41a1e41f0cfac417a48bf07"
        ]
    },
    {
        "id": "93742493c26652d4a95627de699869795698a554",
        "title": "Improving Object Tracking by Added Noise and Channel Attention",
        "abstract": "This paper proposes an Input-Regularized Channel Attentional Siamese (IRCA-Siam) tracker which exhibits improved generalization compared to the current state-of-the-art trackers and proposes feature fusion from noisy and clean input channels which improves the target localization. CNN-based trackers, especially those based on Siamese networks, have recently attracted considerable attention because of their relatively good performance and low computational cost. For many Siamese trackers, learning a generic object model from a large-scale dataset is still a challenging task. In the current study, we introduce input noise as regularization in the training data to improve generalization of the learned model. We propose an Input-Regularized Channel Attentional Siamese (IRCA-Siam) tracker which exhibits improved generalization compared to the current state-of-the-art trackers. In particular, we exploit offline learning by introducing additive noise for input data augmentation to mitigate the overfitting problem. We propose feature fusion from noisy and clean input channels which improves the target localization. Channel attention integrated with our framework helps finding more useful target features resulting in further performance improvement. Our proposed IRCA-Siam enhances the discrimination of the tracker/background and improves fault tolerance and generalization. An extensive experimental evaluation on six benchmark datasets including OTB2013, OTB2015, TC128, UAV123, VOT2016 and VOT2017 demonstrate superior performance of the proposed IRCA-Siam tracker compared to the 30 existing state-of-the-art trackers.",
        "publication_year": "2020",
        "authors": [
            "Mustansar Fiaz",
            "A. Mahmood",
            "Ki Yeol Baek",
            "S. Farooq",
            "Soon Ki Jung"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "5",
        "reference_count": "81",
        "references": [
            "/paper/Siamese-High-Level-Feature-Refine-Network-for-Rahman-Ahmed/057f0dbb8f94d7ec3b5ce3c42a76a492d5ad727e",
            "/paper/Siamese-Based-Attention-Learning-Networks-for-Rahman-Jung/d47b65b7e39bce84c58ee5fc450935cd4891d4e1",
            "/paper/Learning-Soft-Mask-Based-Feature-Fusion-with-and-Fiaz-Mahmood/9fe233b41ddb40188187dc7c4122aa59f6809ae8",
            "/paper/Robust-appearance-modeling-for-object-detection-and-Mumuni-Mumuni/579966818f54843c5c2c2607132bb7378f7210e3",
            "/paper/AttTrack%3A-Online-Deep-Attention-Transfer-for-Nalaie-Zheng/512750c78600a045be5665102eb12899f12efba0",
            "/paper/Efficient-Visual-Tracking-With-Stacked-Attention-Rahman-Fiaz/e085fb462789a8bca7933b5e1c7e9aa0ded8a711",
            "/paper/Triplet-Loss-in-Siamese-Network-for-Object-Tracking-Dong-Shen/fdb98f5a7015de0956ef8d4e468257dc3079b5e5",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Learning-Attentions%3A-Residual-Attentional-Siamese-Wang-Teng/6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "/paper/Convolutional-neural-network-with-structural-input-Fiaz-Mahmood/5cc27ba9ea61c3240eb249fc9b56dd42b7fb86e3",
            "/paper/Object-Adaptive-LSTM-Network-for-Visual-Tracking-Du-Yan/deaaa383bb9291bc77a70a22b70d2683673ba76f",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Adaptive-Feature-Selection-Siamese-Networks-for-Fiaz-Rahman/e972436110b4f2c102d938311beff98ece7b6da7",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763"
        ]
    },
    {
        "id": "1cd3e1d78053c1d41210aab2edd3a9da126f974c",
        "title": "Global optimization on non-convex two-way interaction truncated linear multivariate adaptive regression splines using mixed integer quadratic programming",
        "abstract": "Semantic Scholar extracted view of \"Global optimization on non-convex two-way interaction truncated linear multivariate adaptive regression splines using mixed integer quadratic programming\" by Xinglong Ju et al.",
        "publication_year": "2022",
        "authors": [
            "Xinglong Ju",
            "J. Rosenberger",
            "Victoria C. P. Chen",
            "Feng Liu"
        ],
        "related_topics": [
            "Mathematics"
        ],
        "citation_count": "12",
        "reference_count": "38",
        "references": [
            "/paper/Feature-screening-strategy-for-non-convex-sparse-Yuan-Xu/4dc3007f2f9f147705d96f8b7a947685d10d830c",
            "/paper/A-comparative-study-on-effect-of-news-sentiment-on-Dahal-Pokhrel/e8a576eb7ffeb9ff4bd518956596e9018299d735",
            "/paper/On-constrained-smoothing-and-out-of-range-using-A-Navarro-Garc%C3%ADa-Guerrero/837698d20218c756a35212b1fd470e6c60899a76",
            "/paper/Anomaly-and-Cyber-Attack-Detection-Technique-Based-Kotenko-Saenko/380376f79c38f11d4a804b86ac32e8952627929a",
            "/paper/A-Proactive-Protection-of-Smart-Power-Grids-against-Kotenko-Saenko/be43f48e523e05dd295873cb8130644794c0b755",
            "/paper/Statistical-characteristics-and-complexity-of-wind-Xiao-Shi/c6d8bab474fd12d397e6c49beef327a358b05fc5",
            "/paper/Analysis-of-financial-pressure-impacts-on-the-care-Weng-Zhu/909cc27a5f5a2ed8a11c6ab26f551d9f3b888568",
            "/paper/Predicting-Nepse-Index-Price-Using-Deep-Learning-Pokhrel-Dahal/5233ab646e6a8ff68240215558f91e78995e9fb4",
            "/paper/Optimal-visual-tracking-using-Wasserstein-transport-Hong-Kwon/ac003f0a7cb8f7feaf1e42a73359bbedfd31940d",
            "/paper/Application-of-the-VNS-heuristic-for-feature-in-Helder-Filomena/f02030b30323146f51f76af0a70cbf8fbddfb88b",
            "/paper/Wind-farm-layout-optimization-using-adaptive-with-Bai-Ju/487e0b8457ec4b67384b8f4153ed2e0c09ccec78",
            "/paper/A-multi-stage-evolutionary-algorithm-for-with-Ma-Wei/076237179a6d4602951ab1bef36c29cce9ac4772",
            "/paper/Incremental-classifier-in-crime-prediction-using-Das-Das/54f1512fffd4395e0882b74deafe37caf767548a",
            "/paper/GGA%3A-A-modified-genetic-algorithm-with-local-search-D%E2%80%99Angelo-Palmieri/0fbd4562644a83e8422c1628c4445013a2d10515",
            "/paper/Sentiment-analysis-with-genetic-programming-Junior-Silva/9b90d5cb47e1046332c99608a4c87dfbdbd790ff",
            "/paper/A-problem-specific-non-dominated-sorting-genetic-Zhou-Zhang/465ad459fed5a7e5e51257e10a7abd648279557c",
            "/paper/Evolutionary-Algorithm-and-Multifactorial-Algorithm-Hanh-Thanh/93c7ce7f9fa51c13410b1f78aefa1b035a570618",
            "/paper/Wind-farm-macro-siting-optimization-with-insightful-Liu-Liu/fb61778c0f066420f6476893a4a7e4c5909a8159",
            "/paper/Wind-farm-layout-optimization-based-on-support-with-Ju-Liu/80e68b734ba40e2b02eb725ddb45582a1cb096f6",
            "/paper/Wind-farm-layout-optimization-using-self-informed-Ju-Liu/17cb2deac9383ddc9ad86ada3c7e395516924a20"
        ]
    },
    {
        "id": "595ce8e24326658a96cdb471fa14ea1bd89d0e6c",
        "title": "Joint Tracking and Ground Plane Estimation",
        "abstract": "A novel framework is proposed that jointly estimates the ground plane and a target's motion trajectory and infers them jointly, which reduces sampling errors drastically and outperforms several state-of-the-art tracking methods. We propose a novel framework that jointly estimates the ground plane and a target's motion trajectory. This results in improvements for both. Estimating their joint posterior is based on Particle Markov Chain Monte Carlo (Particle MCMC). In Particle MCMC, the best target state is inferred by a particle filter and the best ground plane is obtained by MCMC. Compared with conventional sampling methods that iteratively infer the best target states and ground plane parameters, our method infers them jointly. This reduces sampling errors drastically. Experimental results demonstrate that our method outperforms several state-of-the-art tracking methods, while the ground plane accuracy is also improved.",
        "publication_year": "2016",
        "authors": [
            "Junseok Kwon",
            "R. Dragon",
            "L. Gool"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "15",
        "reference_count": "28",
        "references": [
            "/paper/Particle-Filter-for-Reliable-Estimation-of-the-from-Owczarek-Skulimowski/f90bbd661997011f82fb59b7efdf318de07a65d4",
            "/paper/Particle-swarm-optimization-Markov-Chain-Monte-for-Kwon/35d87ae4b1f5801dcf77bf9209976e9de1759ce1",
            "/paper/Robust-visual-tracking-based-on-variational-Markov-Kwon/5e19699e957f6f2db17a63153c7651e12d598bec",
            "/paper/Visual-Tracking-Using-Wang%E2%80%93Landau-Reinforcement-Kwon-Kwon/961aa999329500c053f04793641cd431b60ed1f8",
            "/paper/Zero%E2%80%90variance-minibatch-Monte-Carlo-for-pixel%E2%80%90wise-Park-Kwon/7201bb19396117f247608ace198d0dd03dd2caf7",
            "/paper/Fast-Ground-Detection-for-Range-Cameras-on-Road-a-Crombrugge-Azza/e7cec5ab18f779331aebe1825ada28c43dfa1428",
            "/paper/Extended-Object-Tracking-and-Classification-Using-Cao-Lan/930dcf84f99b2e5cd3707d6a50606001f036ecbd",
            "/paper/Analysis-of-a-nonlinear-importance-sampling-scheme-Mguez-Mario/ccc8759bf39b64bca8d70721eed0e3df68fdbc5d",
            "/paper/A-Unique-Method-for-Detecting-Grounds-in-the-Indoor-Chen-Song/2a638fd0debeb8ff64fe34354cabefdfd5a8e97a",
            "/paper/Rejection-Sampling-Based-Ancestor-Sampling-for-Hostettler-S%C3%A4rkk%C3%A4/c944c984d2e6bd86536573bd30307f9b2a197e87",
            "/paper/Ground-Plane-Estimation-Using-a-Hidden-Markov-Model-Dragon-Gool/635905ecdb84dacf2703e06a28d23eb797430fd7",
            "/paper/Visual-tracking-using-the-joint-inference-of-target-Roh-Park/cf6c674113bc112a9c7bc6f0d76d91949fb1026e",
            "/paper/Tracking-by-switching-state-space-models-Kwon-Dragon/1bf0fb043f96904c9f8bc38c4556e69b11d349ae",
            "/paper/Robust-Real-Time-Tracking-of-Multiple-Objects-by-Possegger-Sternig/13c98a2b99e009af2023af16a7ad6c07b2b9d6ca",
            "/paper/Robust-Multiperson-Tracking-from-a-Mobile-Platform-Ess-Leibe/0dfdb4d55430b88d01c0a6d7d6e20df0f6a932fc",
            "/paper/MCMC-based-particle-filtering-for-tracking-a-number-Khan-Balch/4831bafed66417a353530e3cd6d9dbee6516499c",
            "/paper/Robust-People-Tracking-with-Global-Trajectory-Berclaz-Fleuret/d0ff97aa54e19035ef4b118be828cb7cdd00fcd4",
            "/paper/Monocular-Visual-Scene-Understanding%3A-Understanding-Wojek-Walk/ef6bb6b4a5949d0aba98f06f1c8698d8f5aa907f",
            "/paper/Object-Tracking-by-Oversampling-Local-Features-Pernici-Bimbo/c559e4099a6351837753b0a413f9bafed90f5dcd",
            "/paper/Rao-Blackwellised-Particle-Filtering-for-Dynamic-Doucet-Freitas/b9cbf264042dcbb04ac421697620268ccd382727"
        ]
    },
    {
        "id": "1d7329bc40c4911a24af3e34b62d53d600824cb7",
        "title": "Learning Generalized Intersection Over Union for Dense Pixelwise Prediction",
        "abstract": "PixIoU is proposed, a generalized IoU for pixelwise prediction that is sensitive to the distance for nonoverlapping cases and the locations in prediction and a loss function that is proved to be submodular is proposed that can be applied for learning this loss. Intersection over union (IoU) score, also named Jaccard Index, is one of the most fundamental evaluation methods in machine learning. The original IoU computation cannot provide non-zero gradients and thus cannot be directly optimized by nowadays deep learning methods. Several recent works generalized IoU for bounding box regression, but they are not straightforward to adapt for pixelwise prediction. In particular, the original IoU fails to provide effective gradients for the nonoverlapping and location-sensitive cases, which results in performance plateau. In this paper, we propose PixIoU, a generalized IoU for pixelwise prediction that is sensitive to the distance for nonoverlapping cases and the locations in prediction. We provide proofs that PixIoU holds nice properties as the original IoU. To optimize the PixIoU, we also propose a loss function that is proved to be submodular, hence we can apply the Lov\u00e1sz functions, the efficient surrogates for submodular functions for learning this loss. Experimental results show consistent performance improvements by learning PixIoU over the original IoU for several different pixelwise prediction tasks on Pascal VOC, VOT-2020 and Cityscapes.",
        "publication_year": "2021",
        "authors": [
            "Jiaqian Yu",
            "Jingtao Xu",
            "Yiwei Chen",
            "Weiming Li",
            "Qiang Wang",
            "ByungIn Yoo",
            "Jae-Joon Han"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "6",
        "reference_count": "32",
        "references": [
            "/paper/Jaccard-Metric-Losses%3A-Optimizing-the-Jaccard-Index-Wang-Blaschko/c13cc96c717333e2854d4a2d0cee84794282d5be",
            "/paper/Dice-Semimetric-Losses%3A-Optimizing-the-Dice-Score-Wang-Popordanoska/a4ad1f58595c56e615f6f1ac426c961ddd454631",
            "/paper/Pytri%3A-A-multi-weight-detection-system-for-entities-Mehdi-Vu/5490d526c312d3fd9a36d27f487037886baec27b",
            "/paper/Improving-CNN-roof-segment-detection-by-dataset-3D-Faltermeier/263fd894cf1be3246b54a7b969765e5f45bb8711",
            "/paper/Kidney-Tumor-Semantic-Segmentation-Using-Deep-A-of-Abdelrahman-Viriri/ad7e29cfb9e73dea681e503bf0e5f04ad5514633",
            "/paper/Image2Lego%3A-Customized-LEGO-Set-Generation-from-Lennon-Fransen/b3831283a2f12d81105820fa7d29e610cd27621d",
            "/paper/Generalized-Intersection-Over-Union%3A-A-Metric-and-a-Rezatofighi-Tsoi/889c81b4d7b7ed43a3f69f880ea60b0572e02e27",
            "/paper/Distance-IoU-Loss%3A-Faster-and-Better-Learning-for-Zheng-Wang/63a243afcb133569a962c41e9db956c076c5c4f3",
            "/paper/The-Lovasz-Softmax-Loss%3A-A-Tractable-Surrogate-for-Berman-Triki/c7afd747b5c6b77dc22eaa87a8b22888243842b6",
            "/paper/Instance-Segmentation-by-Jointly-Optimizing-Spatial-Neven-Brabandere/e8b3f70d99b47a83fd0c8f7ce589a7a4ae3f2f25",
            "/paper/Optimizing-Intersection-Over-Union-in-Deep-Neural-Rahman-Wang/64dc671107cfdf98e02e76cbc993732d47210549",
            "/paper/Auto-Seg-Loss%3A-Searching-Metric-Surrogates-for-Li-Tao/79112ae62a5f8ce03829d3c77e218269d4c0d35c",
            "/paper/UnitBox%3A-An-Advanced-Object-Detection-Network-Yu-Jiang/22264e60f1dfbc7d0b52549d1de560993dd96e46",
            "/paper/Learning-Submodular-Losses-with-the-Lovasz-Hinge-Yu-Blaschko/285b47c29e8a3667fab0156150632b74319e1658",
            "/paper/The-Lov%C3%A1sz-Hinge%3A-A-Novel-Convex-Surrogate-for-Yu-Blaschko/c399c0089fb134d1476fadf5f0426e0e8b70eebd",
            "/paper/Learning-What-to-Learn-for-Video-Object-Bhat-Lawin/79304c6c8f8689ff2e8fd37383d0691c991f7181"
        ]
    },
    {
        "id": "0fdf394a17da68978bb6735d84cc8ff2aae46455",
        "title": "Frustum VoxNet for 3D object detection from RGB-D or Depth images",
        "abstract": "Results on SUN RGB-D dataset show that the new 3D object detection system, which is based on a small network, can process 20 frames per second with comparable detection results to the state-of-the-art, achieving a 2\u00d7 speedup. Recently, there have been a plethora of classification and detection systems from RGB as well as 3D images. In this work, we describe a new 3D object detection system from an RGB-D or depth-only point cloud. Our system first detects objects in 2D (either RGB, or pseudo-RGB constructed from depth). The next step is to detect 3D objects within the 3D frustums these 2D detections define. This is achieved by voxelizing parts of the frustums (since frustums can be really large), instead of using the whole frustums as done in earlier work. The main novelty of our system has to do with determining which parts (3D proposals) of the frustums to voxelize, thus allowing us to provide high resolution representations around the objects of interest. It also allows our system to have reduced memory requirements. These 3D proposals are fed to an efficient ResNet-based 3D Fully Convolutional Network (FCN). Our 3D detection system is fast, and can be integrated into a robotics platform. With respect to systems that do not perform voxelization (such as PointNet), our methods can operate without the requirement of subsampling of the datasets. We have also introduced a pipelining approach that further improves the efficiency of our system. Results on SUN RGB-D dataset show that our system, which is based on a small network, can process 20 frames per second with comparable detection results to the state-of-the-art [16], achieving a 2\u00d7 speedup.",
        "publication_year": "2019",
        "authors": [
            "Xiaoke Shen",
            "I. Stamos"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "17",
        "reference_count": "33",
        "references": [
            "/paper/simCrossTrans%3A-A-Simple-Cross-Modality-Transfer-for-Shen-Stamos/a69a702ec8ccf9cbeb7e8574dfedb472ce56b44b",
            "/paper/Efficient-and-accurate-object-detection-for-3D-in-Li-Wang/e01e541ada9ba8e40d48a7767083f80dbf44fa35",
            "/paper/3D-Object-Detection-and-Instance-Segmentation-from-Shen-Stamos/3dabed77ff01ab7fa186dd95ddf36b88d76b6cbb",
            "/paper/You-Only-Need-One-Detector%3A-Unified-Object-Detector-Shen-Li/d14b5505fec39c88b747c21c1d504140bd0aea99",
            "/paper/Survey-and-Systematization-of-3D-Object-Detection-Drobnitzky-Friederich/ecbcc92adcc5c6ddef06223e4d284cc337284538",
            "/paper/A-survey-of-Object-Classification-and-Detection-on-Shen/b2c768b7ddc9a038111a1361ef25828adfdcbdf8",
            "/paper/CountNet3D%3A-A-3D-Computer-Vision-Approach-to-Infer-Jenkins-Armstrong/8a1513f76a6b8aaecbadafdb03f313cb600d6685",
            "/paper/Semantically-Accurate-Super-Resolution-Generative-Frizza-Dansereau/6182df8abf9733a351fd115b59c3406748b6d4de",
            "/paper/FCAF3D%3A-Fully-Convolutional-Anchor-Free-3D-Object-Rukhovich-Vorontsova/ab91110608f6df4c676a5c41573172fe71b97e71",
            "/paper/Recent-advances-in-3D-object-detection-based-on-A-Wang-Wang/37355144c99c30f547df3f16c69e91fe2951f3fa",
            "/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "/paper/Frustum-PointNets-for-3D-Object-Detection-from-Data-Qi-Liu/526cf249c2760b7bdbb28f2a2a7c85851d3c2727",
            "/paper/2D-Driven-3D-Object-Detection-in-RGB-D-Images-Lahoud-Ghanem/f4a9e26a38a7b05baccc38fd563a3c24b80eb56f",
            "/paper/PointNet%3A-Deep-Learning-on-Point-Sets-for-3D-and-Qi-Su/d997beefc0922d97202789d2ac307c55c2c52fba",
            "/paper/CNN-Based-Object-Segmentation-in-Urban-LIDAR-with-Zelener-Stamos/2c49852c0fbb1d883a926d0fcf00d9afe3a0524b",
            "/paper/Deep-Sliding-Shapes-for-Amodal-3D-Object-Detection-Song-Xiao/dc0d60f6e8bb7a04ba8a77d51d10b6dba8117480",
            "/paper/You-Only-Look-Once%3A-Unified%2C-Real-Time-Object-Redmon-Divvala/f8e79ac0ea341056ef20f2616628b3e964764cfd",
            "/paper/A-survey-of-Object-Classification-and-Detection-on-Shen/b2c768b7ddc9a038111a1361ef25828adfdcbdf8",
            "/paper/Deep-Hough-Voting-for-3D-Object-Detection-in-Point-Qi-Litany/e60da8d3a79801a3ccbf1abcdd001bb6e001b267",
            "/paper/Group-Normalization-Wu-He/d08b35243edc5be07387a9ed218070b31e502901"
        ]
    },
    {
        "id": "219f3870fc460fc88d34992d5db623cf68a3e2f0",
        "title": "Video Person Re-Identification for Wide Area Tracking Based on Recurrent Neural Networks",
        "abstract": "A video-based person re-identification system for wide area tracking based on a recurrent neural network architecture using a Siamese network architecture to give an overall appearance feature for the complete sequence. In this paper, we propose a video-based person re-identification system for wide area tracking based on a recurrent neural network architecture. Given short video sequences of a person, generated by a tracking algorithm, our video re-identification algorithm links these tracklets in full trajectories across a network of non-overlapping cameras in an open-world scenario. In our system, features are first extracted from each frame using a convolutional neural network. Then, a recurrent layer combines information across time-steps. The features from all time-steps are finally combined using temporal pooling to give an overall appearance feature for the complete sequence. Our system is trained to perform re-identification using a Siamese network architecture. Experiments are conducted on the iLIDS-VID and PRID-2011 video re-identification data sets as well as in the DukeMTMC multi-camera tracking data set.",
        "publication_year": "2019",
        "authors": [
            "Niall McLaughlin",
            "Jes\u00fas Mart\u00ednez del Rinc\u00f3n",
            "P. Miller"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "11",
        "reference_count": "66",
        "references": [
            "/paper/Local-and-global-aligned-spatiotemporal-attention-Cheng-Jing/df9f020392bba629347f092b49b08db61d84c43a",
            "/paper/Novel-Approach-of-Video-Tracking-System-Using-over-Kumar-Kavya/571130f7733a5e472912b2444bfb9116c1a7f20f",
            "/paper/Person-Re-Identification-using-Deep-Learning-A-Yadav-Vishwakarma/663359a693e16827849aa82ea48e3d7324474fa9",
            "/paper/ID-aware-Quality-for-Set-based-Person-Wang-Kodirov/2d8df36c67e8a9278d1f1afdac0ce9f4b92f05fc",
            "/paper/Video-Saliency-Forecasting-Transformer-Ma-Sun/63c933ae795c63e6acd475f875f199da4bf93ac1",
            "/paper/ANL%3A-Anti-Noise-Learning-for-Cross-Domain-Person-Zhang-Han/fb72079742b0a7601030993399da2f135b9b76bb",
            "/paper/Video-guided-Camera-Control-for-Target-Tracking-and-Gemerek-Ferrari/837b2d26087ed62a2c2968a48c7f1cecccd7653a",
            "/paper/Example-weighting-for-deep-representation-learning-Wang/4ae8642f0f7d36f5f85dd152d661e30909d69212",
            "/paper/2019-Index-IEEE-Transactions-on-Circuits-and-for-29-see/59b8abd4d8ee57e7fe0857496203f6394e70677e",
            "/paper/Deep-Metric-Learning-by-Online-Soft-Mining-and-Wang-Hua/681a83c9b55c3d901bc04baca858b7fcb14838ce",
            "/paper/Recurrent-Convolutional-Network-for-Video-Based-McLaughlin-Rinc%C3%B3n/57052d3357cfedf389e3c7b03d9e5f2772d01038",
            "/paper/Convolutional-LSTM-Networks-for-Video-based-Person-Wu-Shen/589d06db45e2319b29fc96582ea6c8be369f57ed",
            "/paper/Video-Based-Person-Re-Identification-With-Motion-Liu-Jie/93288d536a90fbf7f78e09cbc164345bca0ec2c7",
            "/paper/Deep-Recurrent-Convolutional-Networks-for-Person-An-Wu-Shen/2d6130f043e69849fc0443bb489c5d21f933eddd",
            "/paper/Person-Re-identification-via-Recurrent-Feature-Yan-Ni/732fb168d378866515026abc4b5eea59bf19e514",
            "/paper/MARS%3A-A-Video-Benchmark-for-Large-Scale-Person-Zheng-Bie/c0387e788a52f10bf35d4d50659cfa515d89fbec",
            "/paper/Video-Sequences-Association-for-People-across-Cong-Achard/03a89b2c497cbf8b41fdfc073a90f1afc4c28d7c",
            "/paper/Person-Re-identification-by-Video-Ranking-Wang-Gong/9a828410bcbe584d2503a8efb36f756a240a0425",
            "/paper/Person-re-identification-in-multi-camera-system-by-Hamdoun-Moutarde/6f2971ec99e9d640fd184cfe07b94b384053d831",
            "/paper/Multi-Shot-Human-Re-Identification-Using-Adaptive-Li-Wu/93d1a61b35ead27284bd684e5b1b6daa623a69ee"
        ]
    },
    {
        "id": "5f42754b51d49913cfd73fb33c0c9deca8945099",
        "title": "A Unified Approach for Tracking UAVs in Infrared",
        "abstract": "A unified framework, including a local tracker, camera motion estimation module, bounding box refinement module, re-detection module and model updater is designed, which achieves motion compensation for the local tracker. With complex camera and object movement, the tracked object often suffers camera motion, out of view, dramatic scale variation, etc., which severely influence tracking performance. Due to the fast speed and tiny size of unmanned aerial vehicles(UAV), it is crucial to design a robust framework for tracking UAVs. This paper carefully designs a unified framework, including a local tracker, camera motion estimation module, bounding box refinement module, re-detection module and model updater. The camera motion estimation module achieves motion compensation for the local tracker. Then, the bounding box refinement module aims to measure an accurate bounding box. If the target is missing, we switch to the re-detection module to re-localize the target when it reappears. We also adopt a model updater to control the updating process and filter out unreliable samples. Numerous experimental results on 9 visual/thermal datasets show the effectiveness and generalization of our framework.",
        "publication_year": "2021",
        "authors": [
            "Jinjian Zhao",
            "Xiaohan Zhang",
            "Pengyu Zhang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "48",
        "references": [
            "/paper/Recent-Advances-on-Thermal-Infrared-Target-A-Survey-Yuan-Shu/c643ef83e1a9fb8420b26faadebdc122c500268d",
            "/paper/Object-tracking-in-infrared-images-using-a-deep-and-Parhizkar-Karamali/caf9bc9df8cdfd3f7878ed35bdcf2ccf6f554f12",
            "/paper/Visual-Object-Tracking-for-Unmanned-Aerial-A-and-Li-Yeung/6ebc40a061433c24a3ea1f305bb6533b8f3dd5f4",
            "/paper/A-Benchmark-and-Simulator-for-UAV-Tracking-Mueller-Smith/27850781e39df9f750e05409b8072261124068e8",
            "/paper/A-Real-time-Robust-Approach-for-Tracking-UAVs-in-Wu-Li/adafe2972c2bbc47f12b3013f2330756dfdb87f6",
            "/paper/Real-time-Tracking-with-Stabilized-Frame-Wang-Zhao/e6472ac0b5de36ce3b0c5bea35b9f91f62379472",
            "/paper/Alpha-Refine%3A-Boosting-Tracking-Performance-by-Box-Yan-Wang/2c1228e79722728fe292f2e8bdc7b7fb9ac9c454",
            "/paper/Need-for-Speed%3A-A-Benchmark-for-Higher-Frame-Rate-Galoogahi-Fagg/703505a00579c0aa67712836acc41d94fa6d6edc",
            "/paper/ADTrack%3A-Target-Aware-Dual-Filter-Learning-for-UAV-Li-Fu/689b230b228c7ff5e2bb5d500c5349f54bcc6d3c",
            "/paper/Tracking-Learning-Detection-Kalal/c63a34ac6a4e049118070e707ca7679fbb132d33",
            "/paper/Kernel-Based-Object-Tracking-Comaniciu-Ramesh/dd4bdfc5d3944e3e9573b887635487d4c5f5330f",
            "/paper/LaSOT%3A-A-High-quality-Large-scale-Single-Object-Fan-Bai/e284bc13c2b76d0d0c7ad61d976f8a9d3eef8461"
        ]
    },
    {
        "id": "01b7c93e7d878327cf9d754889d87f4c335e7bae",
        "title": "Robust scale-variation object tracking by twofold weighted phase correlation with the kernel",
        "abstract": "The proposed twofold weighted phase correlation with the kernel to simultaneously estimate the translation and scale for the object during tracking has superior performance gains over several state-of-the-art trackers in terms of accuracy, robustness, and speed. Abstract. Tracking objects with the scale variations quickly is a challenging problem in visual tracking. Most existing methods estimate the scale of the object with an exhaustive search strategy, which needs large calculations but with less improvement. We propose to use twofold weighted phase correlation with the kernel (WPCK) to simultaneously estimate the translation and scale for the object during tracking. Besides, from the linear phase correlation, a weighted kernel phase correlator is first introduced to improve tracking performance. It extends the filter into a nonlinear space, which is more robust to signal noises and distortions. To efficiently locate the object in a large three-dimensional space for real-time running, we formulate tracking problem into two independent subproblems: the translation offset and scale offset. The mechanism is: through the log-polar transform of the amplitude spectrum, the scale estimation can be performed with phase correlation, which is also utilized to solve the translation offset. Thus, the accurate object status can be achieved effectively by twofold WPCK. Extensive experimental results on OTB2013, OTB100, and UAV123 datasets reveal that the proposed method has superior performance gains over several state-of-the-art trackers in terms of accuracy, robustness, and speed.",
        "publication_year": "2020",
        "authors": [
            "Yufei Zha",
            "Zhuling Qiu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "36",
        "references": [
            "/paper/Learning-adaptive-weighted-response-consistency-for-Zhang-Ma/14ce7801cb459436f378ffbff746e6d340074703",
            "/paper/Robust-and-fast-visual-tracking-via-spatial-kernel-Zhang-Bi/44c18fc9582862185a96f7a79f63a788abc856bc",
            "/paper/Robust-Estimation-of-Similarity-Transformation-for-Li-Zhu/68c8d13f396272424f4bcba93c2ce34d052b6511",
            "/paper/Discriminative-Scale-Space-Tracking-Danelljan-H%C3%A4ger/ce8c76bfedc5d86faabf0d49dc42a4924f75876d",
            "/paper/Robust-Visual-Tracking-via-an-Improved-Background-Sheng-Liu/7eee4087e4087930726eee6b4395fa5e788b8f6a",
            "/paper/High-Speed-Tracking-with-Kernelized-Correlation-Henriques-Caseiro/65c9b4b1d49f46b3f8f64a5f617acfc14f85d031",
            "/paper/Discriminative-Correlation-Filter-Tracker-with-and-Luke%C5%BEi%C4%8D-Voj%C3%ADr/8cc3651488e02d51fa5ae8d3563b346e9e370f5a",
            "/paper/A-Scale-Adaptive-Kernel-Correlation-Filter-Tracker-Li-Zhu/0cae491292feccbc9ad1d864cf8b7144923ce6de",
            "/paper/Adaptive-Correlation-Filters-with-Long-Term-and-for-Ma-Huang/7a078987c929b1aef72f4ca283dddef187c48132",
            "/paper/Visual-object-tracking-using-adaptive-correlation-Bolme-Beveridge/70c3c9b9a40ca55264e454586dca2a6cf416f6e0",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763"
        ]
    },
    {
        "id": "8e911bacef5a5178616cb845b2feccca4129cb6e",
        "title": "Siamese Cascaded Region Proposal Networks With Channel-Interconnection-Spatial Attention for Visual Tracking",
        "abstract": "A Siamese based tracker is proposed, which integrates three attention mechanisms and a novel Cascaded Region Proposal Networks (RPN) architecture, for improving the feature extraction ability, adaptability and discrimination ability in complex scenes. Trackers based on Siamese networks show great potential in tracking accuracy and speed. However, it is still challenging to adapt offline training model to online tracking. In this paper, a Siamese based tracker (SCRPN-CISA) is proposed, which integrates three attention mechanisms and a novel Cascaded Region Proposal Networks (RPN) architecture, for improving the feature extraction ability, adaptability and discrimination ability in complex scenes. Firstly, the deep network VGG-Net-D is adopted as the backbone network in the Siamese framework to increase the feature extraction capability. Then, a Channel-Interconnection-Spatial Attention module is constructed to enhance the adaptive and discriminative capability of the model. Next, a Deconvolution Adjust Block is built to fusion cross-layer features. Finally, a Three-Layer Cascaded RPN is conceived to acquire the foreground-background classification and bounding box regression by correlation calculation, and moreover, a proposal region screening strategy is presented to obtain more accurate tracking results. Experiments on OTB-2015, UAV123, VOT2016, and VOT2019 benchmarks demonstrate that, the proposed tracker (SCRPN-CISA) achieves competitive performance compared with the state-of-the-art trackers.",
        "publication_year": "2020",
        "authors": [
            "Zhoujuan Cui",
            "Junshe An",
            "Qing Ye",
            "Tianshu Cui"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "64",
        "references": [
            "/paper/Cross-scale-content-based-full-Transformer-network-Fan-Chen/31512af54ab956ee6c2ec3cfa104899e974b39a1",
            "/paper/Mask-Guided-Self-Distillation-For-Visual-Tracking-Li-Chen/f497a4af8e5689e0688aff85d316bac9ce9f4e80",
            "/paper/Siamese-Cascaded-Region-Proposal-Networks-for-Fan-Ling/f98be9a91dbf00b52a494720bd36be9c73a1210e",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Visual-object-tracking-with-adaptive-structural-Yuan-Li/8bd38ba1c0a2154269d6a85b575ffa60b4090f7c",
            "/paper/Learning-Attentions%3A-Residual-Attentional-Siamese-Wang-Teng/6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Deeper-and-Wider-Siamese-Networks-for-Real-Time-Zhang-Peng/2e713509e96daf06e65e10bdc438d00a827c914c",
            "/paper/Siamese-Box-Adaptive-Network-for-Visual-Tracking-Chen-Zhong/cce1fecc800d2782da638f3060d5b2e887739f74",
            "/paper/A-Twofold-Siamese-Network-for-Real-Time-Object-He-Luo/a3a4471e82260f573d240cc34aeff431cf236571",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3"
        ]
    },
    {
        "id": "35e846e0e6824176c58e6147243dc95bf953ed49",
        "title": "NPC: Neural Point Characters from Video",
        "abstract": "This work proposes a hybrid point-based representation for reconstructing animatable characters that does not require an explicit surface model, while being generalizable to novel poses, and demonstrates on established benchmarks that this representation overcomes limitations of prior work operating in either canonical or in observation space. High-fidelity human 3D models can now be learned directly from videos, typically by combining a template-based surface model with neural representations. However, obtaining a template surface requires expensive multi-view capture systems, laser scans, or strictly controlled conditions. Previous methods avoid using a template but rely on a costly or ill-posed mapping from observation to canonical space. We propose a hybrid point-based representation for reconstructing animatable characters that does not require an explicit surface model, while being generalizable to novel poses. For a given video, our method automatically produces an explicit set of 3D points representing approximate canonical geometry, and learns an articulated deformation model that produces pose-dependent point transformations. The points serve both as a scaffold for high-frequency neural features and an anchor for efficiently mapping between observation and canonical space. We demonstrate on established benchmarks that our representation overcomes limitations of prior work operating in either canonical or in observation space. Moreover, our automatic point extraction approach enables learning models of human and animal characters alike, matching the performance of the methods using rigged surface templates despite being more general. Project website: https://lemonatsu.github.io/npc/",
        "publication_year": "2023",
        "authors": [
            "Shih-Yang Su",
            "Timur M. Bagautdinov",
            "Helge Rhodin"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "51",
        "references": [
            "/paper/Neural-Actor%3A-Neural-Free-view-Synthesis-of-Human-Liu-Habermann/29da3be81905d577dac9144c1c3cb5c6678f7027",
            "/paper/Neural-Body%3A-Implicit-Neural-Representations-with-Peng-Zhang/af8faec7c0b8f4b2a28d42a86e0e7d499016c560",
            "/paper/ARAH%3A-Animatable-Volume-Rendering-of-Articulated-Wang-Schwarz/6d9b0bb216c36aba29a5b11a656a468dc39cad66",
            "/paper/A-NeRF%3A-Articulated-Neural-Radiance-Fields-for-and-Su-Yu/74bb19d1ce2ec9fb9605545a750e68281a067e63",
            "/paper/NeuMan%3A-Neural-Human-Radiance-Field-from-a-Single-Jiang-Yi/17df7e87a5d25e6e83d773e6f686eb0a85f6827e",
            "/paper/Unsupervised-Learning-of-Efficient-Geometry-Aware-Noguchi-Sun/33bd7d7fba56e7b74691b286e0d148803b709b2a",
            "/paper/HumanNeRF%3A-Free-viewpoint-Rendering-of-Moving-from-Weng-Curless/f763a59644e27a2215095943224f2564e670a504",
            "/paper/COAP%3A-Compositional-Articulated-Occupancy-of-People-Mihajlovi%C4%87-Saito/866453e5fdc7ebab40d02eae59e8823d54f6e37e",
            "/paper/TAVA%3A-Template-free-Animatable-Volumetric-Actors-Li-Tanke/5a365ad81138028dacf989317f34f46852e3e88e",
            "/paper/SNARF%3A-Differentiable-Forward-Skinning-for-Neural-Chen-Zheng/586feddfa72cc263a17207a0cb73b992e1ee3a71"
        ]
    },
    {
        "id": "45bde350082fe5ee366c8f1b761429d2277fbcca",
        "title": "Siamese Attentional Cascade Keypoints Network for Visual Object Tracking",
        "abstract": "A novel Siamese Attentional Cascade Keypoints Tracking Network named SiamACN is proposed to exactly track the object by using keypoints prediction instead of anchors to improve the tracking efficiency. Visual object tracking is urgent yet challenging work since it requires the simultaneous and effective classification and estimation of a target. Thus, research on tracking has been attracting a considerable amount of attention despite the limitations of existing trackers owing to deformation, occlusion and motion. For most current tracking methods, researchers have proposed various ways to adopt a multi-scale search or anchors for estimation, but these methods always need prior knowledge and too many hyperparameters. To address these issues, we proposed a novel Siamese Attentional Cascade Keypoints Tracking Network named SiamACN to exactly track the object by using keypoints prediction instead of anchors. Compared to complex target prediction, the anchor-free method is performed to avoid plaguy hyperparameters, and a simplified hourglass network with global attention is considered the backbone to improve the tracking efficiency. Further, our framework uses keypoints prediction around the target with cascade corner pooling to simplify the model. To certificate the superiority of our framework, extensive tests are conducted on five tracking benchmarks, including OTB-2015, VOT-2016, VOT-2018, LaSOT and UAV123. Our method achieves the leading performance with an accuracy of 61.2% on VOT2016 and favorably runs at 32 FPS against other competing algorithms, which confirms its effectiveness in real-time applications.",
        "publication_year": "2021",
        "authors": [
            "Ershen Wang",
            "Donglei Wang",
            "Yufeng Huang",
            "Gang Tong",
            "Song Xu",
            "Tao Pang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "78",
        "references": [
            "/paper/Visual-Tracking-With-Siamese-Network-Based-on-Fast-Qin-Yang/1e9f70ac4c3c45b83bd3213a2a93acd1cbeafd8b",
            "/paper/Accurate-Object-Tracking-by-Utilizing-Diverse-Prior-Hu-Lin/f067a6ce86b01ee3e06767b53e4d440b83ae1075",
            "/paper/Visual-Tracking-by-Adaptive-Continual-Meta-Learning-Choi-Baik/3c7ea85b4d6c588bca023aa0ca59d202dc9ddf2d",
            "/paper/Adaptive-chaotic-sampling-particle-filter-to-handle-Firouznia-Koupaei/7ce6afa1fbee5fa7edd489f40be1c4892052371f",
            "/paper/Siamese-Attentional-Keypoint-Network-for-High-Gao-Ma/fce3655dc22a783b1f82f09190410f070c7bf42c",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Learning-Attentions%3A-Residual-Attentional-Siamese-Wang-Teng/6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "/paper/ATOM%3A-Accurate-Tracking-by-Overlap-Maximization-Danelljan-Bhat/d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "/paper/Structured-Siamese-Network-for-Real-Time-Visual-Zhang-Wang/4b1965a54a064ac9145b1ce404fe33f0120c8ae3",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "/paper/TRBACF%3A-Learning-temporal-regularized-correlation-Yuan-Shu/93874b197f48562fc410b3c351edb9f26da8c123",
            "/paper/Deep-Learning-for-Visual-Tracking%3A-A-Comprehensive-Marvasti-Zadeh-Cheng/1fbb4201af091aef55360f113ba35814063923e4"
        ]
    },
    {
        "id": "7bfa1514a6ffba2fdc5828462f59f7548b094e22",
        "title": "Brain Tumor Synthetic Data Generation with Adaptive StyleGANs",
        "abstract": "A method to generate brain tumor MRI images using generative adversarial networks using StyleGAN2 with ADA methodology to generate high-quality brain MRI with tumors while using a significantly smaller amount of training data when compared to the existing approaches. Generative models have been very successful over the years and have received significant attention for synthetic data generation. As deep learning models are getting more and more complex, they require large amounts of data to perform accurately. In medical image analysis, such generative models play a crucial role as the available data is limited due to challenges related to data privacy, lack of data diversity, or uneven data distributions. In this paper, we present a method to generate brain tumor MRI images using generative adversarial networks. We have utilized StyleGAN2 with ADA methodology to generate high-quality brain MRI with tumors while using a significantly smaller amount of training data when compared to the existing approaches. We use three pre-trained models for transfer learning. Results demonstrate that the proposed method can learn the distributions of brain tumors. Furthermore, the model can generate high-quality synthetic brain MRI with a tumor that can limit the small sample size issues. The approach can addresses the limited data availability by generating realistic-looking brain MRI with tumors. The code is available at: ~\\url{https://github.com/rizwanqureshi123/Brain-Tumor-Synthetic-Data}.",
        "publication_year": "2022",
        "authors": [
            "Usama Tariq",
            "Rizwan Qureshi",
            "Ana Zafar",
            "Danyal Aftab",
            "Jia Wu",
            "Tanvirul Alam",
            "Zubair Shah",
            "Hazrat Ali"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "36",
        "references": [
            "/paper/Transferring-GANs%3A-generating-images-from-limited-Wang-Wu/fac36fa1b809b71756c259f2c5db20add0cb0da0",
            "/paper/The-role-of-generative-adversarial-networks-in-MRI%3A-Ali-Biswas/1f1c4153224370a8e3ee0d78c9948e163c273d65",
            "/paper/Generative-Adversarial-Network-for-Medical-Images-Iqbal-Ali/5443e8aca3deb9dae313e7796435023727102130",
            "/paper/Generative-Adversarial-Network-in-Medical-Imaging%3A-Yi-Walia/acbeebdfd9dd3456628604eefcd53f50f974b132",
            "/paper/Compound-Frechet-Inception-Distance-for-Quality-of-Nunn-Khadivi/9aa7155f2e197bd9801633d47e03dec20fd95b58",
            "/paper/A-new-generative-adversarial-network-for-medical-Ahmad-Ali/c5427eab1a236e4529dc004307e921677fde304b",
            "/paper/Training-Generative-Adversarial-Networks-with-Data-Karras-Aittala/29858b40a15704398aecdca6bd2820f2fcc99891",
            "/paper/CASS%3A-Cross-Architectural-Self-Supervision-for-Singh-Sizikova/be9a0f74c4143eef825d17499c6dcb1b90cb70e1",
            "/paper/An-Adaptive-Control-Algorithm-for-Stable-Training-Ma-Jin/6ea6c39593853a5382ee975a0b7bb8222227a6cc",
            "/paper/MineGAN%3A-Effective-Knowledge-Transfer-From-GANs-to-Wang-Gonzalez-Garcia/b23cc14ba33e4bfbf4f8774cb0b10a034167e9b2"
        ]
    },
    {
        "id": "68f197b645c25f68297f27f18aada1b2cc94dc22",
        "title": "Computer-aided detection of malignancy with magnetic resonance imaging of the breast.",
        "abstract": "Comparing the sensitivity, specificity, and recall rate of MRI with and without the use of commercially available CAD systems in detecting malignant lesions, evaluating the extent of disease in women with cancer, or gauging the impact of treatment is compared. Authors' objectives The aim of this review was to assess the evidence on the use of computer-aided detection (CAD) with magnetic resonance imaging (MRI) of the breast by comparing the sensitivity, specificity, and recall rate1 of MRI with and without the use of commercially available CAD systems in detecting malignant lesions, evaluating the extent of disease in women with cancer, or gauging the impact of treatment.",
        "publication_year": "2006",
        "authors": [
            "BlueCross BlueShield"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "8",
        "reference_count": 0,
        "references": [
            "/paper/Computer-Aided-Evaluation-of-Malignancy-with-of-the/4e11d87889284076b150c2545b20d9c4b4008378",
            "/paper/Computer-Aided-Evaluation-of-Malignancy-With-of-the/6b2cc97992644a36d26bffe02d15aac3b4dff683",
            "/paper/BREAST-Computer-aided-detection-in-breast-MRI-%3A-a-Dorrius-Weide/0afa4be2884e1a78efb4b998899f51731ff62944",
            "/paper/Computer-aided-detection-in-breast-MRI%3A-a-review-Dorrius-Weide/9aa6afcfb84172c9d23535ee7941c016ef5c0ce6",
            "/paper/Detection-and-Segmentation-of-Breast-Tumor-from-MRI-Rahman-Hussain/15aedc33c1b47df9531be92cc58b1a130c9389de",
            "/paper/Breast-Imaging-for-Screening-and-Diagnosing-Cancer/6110c176a88c55d48b8eb28b3bacf885c767fbe1",
            "/paper/Breast-Imaging-for-Screening-and-Diagnosing-Cancer/fe123724fb4841f1fa30e23ad1f55ad1400f9785",
            "/paper/New-diagnostic-developments-to-prevent-unnecessary-Dorrius/38be2e905b4690ad5a1605e659795f5292e12ae2"
        ]
    },
    {
        "id": "153052f9e9e1dcf048160360bb188a7baaf2d463",
        "title": "Differentiation of benign and metastatic axillary lymph nodes in breast cancer: additive value of MRI computer-aided evaluation.",
        "abstract": "Semantic Scholar extracted view of \"Differentiation of benign and metastatic axillary lymph nodes in breast cancer: additive value of MRI computer-aided evaluation.\" by S. J. Yun et al.",
        "publication_year": "2016",
        "authors": [
            "S. J. Yun",
            "Yu-Mee Sohn",
            "M. Seo"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "10",
        "reference_count": 0,
        "references": [
            "/paper/A-Nomogram-Combined-Radiomics-and-Kinetic-Curve-as-Shan-Xu/6cb124bd46fe728a9165c039ed0c12929e6fd6ea",
            "/paper/The-Diagnostic-Performance-of-Machine-Radiomics-of-Zhang-Li/f22857746a4c1faec90d5ebdc81dbfb82063569d",
            "/paper/Diagnostic-performance-of-T2-weighted-imaging-and-Liu-Luo/e1afa50976569a6ed548ce71ec3369480961a41f",
            "/paper/Pretherapeutic-Imaging-for-Axillary-Staging-in-A-of-Boulc%E2%80%99h-Gilhodes/087e00cf47ae0b7dbe9576decc61c124e089c25f",
            "/paper/Machine-learning-prediction-of-axillary-lymph-node-Arefan-Chai/5162bc3cd8c5df9495f0b53d6264afa754b72adb",
            "/paper/Preoperative-Axillary-Lymph-Node-Evaluation-in-and-Choi-Park/eef6275dbbbf3934805c5ff3c7e3865a37600f83",
            "/paper/Computer-Aided-Evaluation-of-Malignancy-with-of-the/4e11d87889284076b150c2545b20d9c4b4008378",
            "/paper/Computer-Aided-Evaluation-of-Malignancy-With-of-the/6b2cc97992644a36d26bffe02d15aac3b4dff683",
            "/paper/Differentiating-axillary-lymph-node-metastasis-in-A-Chai-Ma/4594a3b34198a89202f591eb9aef335f25bba23d",
            "/paper/Lobar-Approach-to-Breast-Ultrasound-Amy/96fc87241d32a909336f2e679b234c119f5e01e8"
        ]
    },
    {
        "id": "fcdded046102348cb1f17da49e810090d3299549",
        "title": "Hyperparameter Optimizer with Deep Learning-Based Decision-Support Systems for Histopathological Breast Cancer Diagnosis",
        "abstract": "This comparative study states that the AOADL-HBCC technique displays better performance than other recent methodologies, with a maximum accuracy of 96.77%. Simple Summary This study develops an arithmetic optimization algorithm with deep-learning-based histopathological breast cancer classification (AOADL-HBCC) technique for healthcare decision making. The AOADL-HBCC technique employs noise removal based on median filtering (MF) and a contrast enhancement process. In addition, the presented AOADL-HBCC technique applies an AOA with a SqueezeNet model to derive feature vectors. Finally, a deep belief network (DBN) classifier with an Adamax hyperparameter optimizer is applied for the breast cancer classification process. Abstract Histopathological images are commonly used imaging modalities for breast cancer. As manual analysis of histopathological images is difficult, automated tools utilizing artificial intelligence (AI) and deep learning (DL) methods should be modelled. The recent advancements in DL approaches will be helpful in establishing maximal image classification performance in numerous application zones. This study develops an arithmetic optimization algorithm with deep-learning-based histopathological breast cancer classification (AOADL-HBCC) technique for healthcare decision making. The AOADL-HBCC technique employs noise removal based on median filtering (MF) and a contrast enhancement process. In addition, the presented AOADL-HBCC technique applies an AOA with a SqueezeNet model to derive feature vectors. Finally, a deep belief network (DBN) classifier with an Adamax hyperparameter optimizer is applied for the breast cancer classification process. In order to exhibit the enhanced breast cancer classification results of the AOADL-HBCC methodology, this comparative study states that the AOADL-HBCC technique displays better performance than other recent methodologies, with a maximum accuracy of 96.77%.",
        "publication_year": "2023",
        "authors": [
            "M. Obayya",
            "M. Maashi",
            "N. Nemri",
            "Heba Mohsen",
            "Abdelwahed Motwakel",
            "Azza Elneil Osman",
            "Amani A. Alneil",
            "Mohamed Ibrahim Alsaid"
        ],
        "related_topics": [
            "Computer Science",
            "Medicine"
        ],
        "citation_count": "5",
        "reference_count": "22",
        "references": [
            "/paper/BC2NetRF%3A-Breast-Cancer-Classification-from-Images-Jabeen-Khan/b0925e7744cb305c4a6b1fcf012b66eec249fc69",
            "/paper/Multi-Method-Diagnosis-of-Histopathological-Images-Al-Jabbar-Alshahrani/85772c72e504869299d53819c8a58e6d58da6550",
            "/paper/Enhancing-Ductal-Carcinoma-Classification-Using-3D-Khalil-Nawaz/62d998297f7f1d6ea565ab1cabbbb554d6398b66",
            "/paper/Development-of-Deep-Learning-with-RDA-U-Net-Network-Lee-Wang/36d63c0a5639cca8ae825398a8fb7130e2a65bc8",
            "/paper/Deep-learning-based-ensemble-model-for-of-breast-Nemade-Pathak/73707e8143c0ca11467f0686ea254c3ea91a8906",
            "/paper/Chaotic-Sparrow-Search-Algorithm-with-Deep-Transfer-Shankar-Dutta/49e422ecba78c392f15adbc37f392afffb1f49b4",
            "/paper/Transfer-learning-assisted-multi-resolution-breast-Ahmad-Asghar/473887bdb528322f2dda72fb6517b59d2c8fb062",
            "/paper/A-Transfer-Learning-Architecture-Based-on-a-Support-Fan-Lee/98db1a282f09615d8bbb21b69396f9065e7111fe",
            "/paper/Classification-of-breast-cancer-using-a-manta-ray-Baghdadi-Malki/cec32bf0f5cb95219b1e87775006d291aafb64da",
            "/paper/Optimizing-the-Performance-of-Breast-Cancer-by-the-Alzubaidi-Al-Shamma/2b18e8af1b9c007f1c04d93c0f0b1642c1c4e4f9",
            "/paper/Deep-Learning-Models-Combining-for-Breast-Cancer-Elmannai-Hamdi/4333af581e45025885312224b9607ebbba7a3524",
            "/paper/A-new-transfer-learning-based-approach-to-dependent-Boumaraf-Liu/b73a73d74ad301c7558a2e6284a91c91b7b7d948",
            "/paper/Classification-of-Breast-Tumours-Based-on-Images-of-Abbasniya-Sheikholeslamzadeh/38a3511f5ace7871aa6b0a51e8a3ea62dbc24e14",
            "/paper/Classification-of-breast-cancer-histology-images-Vesal-Ravikumar/5e0b34c8a371dcd7d623169c079f56ac75ffcd1e",
            "/paper/Transfer-learning-based-histopathologic-image-for-Deniz-%C5%9Eeng%C3%BCr/f4008deb11f6a2ec3d52dde72be48cd53c898a45"
        ]
    },
    {
        "id": "81eacfbe64d362346b311f85a1f7f95df8d8edce",
        "title": "Intelligent Deep Learning Enabled Oral Squamous Cell Carcinoma Detection and Classification Using Biomedical Images",
        "abstract": "An intelligent deep learning enabled oral squamous cell carcinoma detection and classification (IDL-OSCDC) technique using biomedical images is developed and its promising performance over the other methods is highlighted. Oral cancer is one of the lethal diseases among the available malignant tumors globally, and it has become a challenging health issue in developing and low-to-middle income countries. The prognosis of oral cancer remains poor because over 50% of patients are recognized at advanced stages. Earlier detection and screening models for oral cancer are mainly based on experts' knowledge, and it necessitates an automated tool for oral cancer detection. The recent developments of computational intelligence (CI) and computer vision-based approaches help to accomplish enhanced performance in medical-image-related tasks. This article develops an intelligent deep learning enabled oral squamous cell carcinoma detection and classification (IDL-OSCDC) technique using biomedical images. The presented IDL-OSCDC model involves the recognition and classification of oral cancer on biomedical images. The proposed IDL-OSCDC model employs Gabor filtering (GF) as a preprocessing step to eliminate noise content. In addition, the NasNet model is exploited for the generation of high-level deep features from the input images. Moreover, an enhanced grasshopper optimization algorithm (EGOA)-based deep belief network (DBN) model is employed for oral cancer detection and classification. The hyperparameter tuning of the DBN model is performed using the EGOA algorithm which in turn boosts the classification outcomes. The experimentation outcomes of the IDL-OSCDC model using a benchmark biomedical imaging dataset highlighted its promising performance over the other methods with maximum accuy, precn, recal, and Fscore of 95%, 96.15%, 93.75%, and 94.67% correspondingly.",
        "publication_year": "2022",
        "authors": [
            "A. Alanazi",
            "Manal M. Khayyat",
            "Mashael M. Khayyat",
            "Bushra M Elamin Elnaim",
            "S. Abdel-Khalek"
        ],
        "related_topics": [
            "Computer Science",
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "24",
        "references": [
            "/paper/Automated-Detection-and-Classification-of-Oral-Deep-Welikala-Remagnino/230b8255925d56b72444df3f358f5851b3158b59",
            "/paper/Automated-detection-of-oral-pre-cancerous-tongue-of-Shamim-Syed/395cc086fa298cf161bcf51a4bf8be02a647bc3f",
            "/paper/Automated-Grading-of-Oral-Squamous-Cell-Carcinoma-Musulin-%C5%A0tifani%C4%87/a590c65e518a742d5b9c32be5731deb3ceefa03b",
            "/paper/IoMT-Cloud-Based-Intelligent-Prediction-of-Breast-Siddiqui-Haider/a903854fb65eee6f81900687b75d151e9b6683b5",
            "/paper/Optimal-deep-learning-based-fusion-model-for-image-Mansour-Alfaer/b457749b96d6bb905aa98ffbca1f1ccdbaeb66e3",
            "/paper/A-Deep-Learning-based-Pipeline-for-Efficient-Oral-Lu-Sladoje/1ceacbd639d50872887723c9d573370886d45a32",
            "/paper/Deep-learning-neural-network-for-texture-feature-in-Bhandari-Alsadoon/f2d66eede760722f5b19e815ee01bfbb79d13794",
            "/paper/Histopathological-imaging-database-for-oral-cancer-Rahman-Mahanta/18d2d888d5e1a53b9d9261cd419804ef4205902a",
            "/paper/D'OraCa%3A-Deep-Learning-Based-Classification-of-Oral-Lim-Tan/d155a3313f92a3b6c739fff8b46b2ed1f8e65e1c",
            "/paper/Automatic-classification-of-dual-modalilty%2C-oral-Song-Sunny/eeebfa2343ef9180d647d7bbec008e249d250d88"
        ]
    },
    {
        "id": "02ec43c470666ff4d3366df490514781763181b7",
        "title": "New approach for segmentation and quantification of two-dimensional gel electrophoresis images",
        "abstract": "It is shown that the proposed approach for segmentation and quantification of 2-DE images can compete with the available commercial and academic software packages. MOTIVATION\nDetection of protein spots in two-dimensional gel electrophoresis images (2-DE) is a very complex task and current approaches addressing this problem still suffer from significant shortcomings. When quantifying a spot, most of the current software applications include a lot of background due to poor segmentation. Other software applications use a fixed window for this task, resulting in omission of part of the protein spot, or including background in the quantification. The approach presented here for the segmentation and quantification of 2-DE aims to minimize these problems.\n\n\nRESULTS\nFive sections from different gels are used to test the performance of the presented method concerning the detection of protein spots, and three gel sections are used to test the quantification of sixty protein spots. Comparisons with a state-of-the-art commercial software and an academic state-of-the-art approach are presented. It is shown that the proposed approach for segmentation and quantification of 2-DE images can compete with the available commercial and academic software packages.\n\n\nAVAILABILITY\nA command-line prototype may be downloaded, for non-commercial use, from http://w3.ualg.pt/~aanjos/prototypes.html.",
        "publication_year": "2011",
        "authors": [
            "A. Anjos",
            "A. M\u00f8ller",
            "B. Ersb\u00f8ll",
            "C. Finnie",
            "H. Shahbazkia"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "30",
        "reference_count": "37",
        "references": [
            "/paper/Spot-quantification-in-two-dimensional-gel-image-of-Brauner-Groemer/5576b0d7a5ee8a2c9d0eae8f20bfe34d905ac923",
            "/paper/An-effective-approach-for-detection-and-of-protein-Kostopoulou-Zacharia/7c92fad59e00611ed03be1dfa5db6eeedac9e591",
            "/paper/SEGMENTATION-OF-2D-GEL-ELECTROPHORESIS-IMAGES-Kostopoulou-Zacharia/ff151223e331297bbbf1fcdce8e205073df44ed9",
            "/paper/2D-gel-spot-detection-and-segmentation-based-on-and-Kostopoulou-Katsigiannis/265dbaad6472c987ffbc8287bbf7c377a2b1f570",
            "/paper/SpotDSQ%3A-A-2D-Gel-Image-Analysis-Tool-for-Protein-Kostopoulou-Katsigiannis/e1a898c136cb2c544b02de186f2f9a3fbd63d836",
            "/paper/A-Novel-Gaussian-Extrapolation-Approach-for-2-D-Gel-Natale-Caiazzo/0fffa9a4407586c660e7474224cd452c1942634b",
            "/paper/Mixture-Modeling-of-2-D-Gel-Electrophoresis-Spots-Marczyk/1e2dadc12c0dc6d7574aa830221abe08114810f4",
            "/paper/2D-GE-image-analysis-focusing-on-elimination-of-Kostopoulou-Zacharia/a8dad2cf9a865bcf459ab8344318b371fa7d115c",
            "/paper/Image-Analysis-Workflow-for-2-D-Electrophoresis-on-Natale-Maresca/3fd7cb53920b411ce0e8b6ccaf517a66ee58678f",
            "/paper/A-Novel-Gaussian-Extrapolation-Approach-for-2D-Gel-Natale-Caiazzo/f7fd49f4e7a4c24e082d5a82e97fe11c8e846770",
            "/paper/A-new-method-for-assigning-common-spot-boundaries-Rye-F%C3%A6rgestad/8c69c02b81761e55801bcb74b2ba8326f0942718",
            "/paper/Statistical-models-of-shape-for-the-analysis-of-in-Rogers-Graham/6ebf231010a6f7b970885c71d96e7d362eabf42d",
            "/paper/Influence-of-image%E2%80%90analysis-software-on-of-gel-data-Stessl-Noe/2105aeaa1c60e29f9c4094007ec2f70dd059396a",
            "/paper/Pinnacle%3A-a-fast%2C-automatic-and-accurate-method-for-Morris-Clark/f645c20def86c783dc66e6e045a30acc64ef6632",
            "/paper/Elsie-4%3A-quantitative-computer-analysis-of-sets-of-Olson-Miller/572c096aeac8b4b1ef8824814711b2d78f9ffbed",
            "/paper/A-new-method-for-2D-gel-spot-alignment%3A-application-P%C3%A9r%C3%A8s-Molina/b85f3733938e92c607032a71137b67def51634c9",
            "/paper/High-resolution-two-dimensional-electrophoresis-of-O%E2%80%99Farrell/02322f38aa70f0a42954b4bcc9923f0e2f48186b",
            "/paper/Comparative-evaluation-of-two-two-dimensional-gel-Arora-Yamagiwa/76b608add550ba78e002835dc2335aa01d4fab00",
            "/paper/Melanie-II-%E2%80%93-a-third%E2%80%90generation-software-package-of-Appel-Vargas/40e63a69990c2ca204f5ee204f9a5be8f39871f1",
            "/paper/Automatic-marker-detection-for-blob-images-Anjos-Shahbazkia/56673775e442a24c99bfe2a3403d724e539b0ac2"
        ]
    },
    {
        "id": "7a9e471e31ac156cf22a5e2a5c1463697df866ab",
        "title": "Deep Learning Classification of Land Cover and Crop Types Using Remote Sensing Data",
        "abstract": "A multilevel DL architecture that targets land cover and crop type classification from multitemporal multisource satellite imagery outperforms the one with MLPs allowing us to better discriminate certain summer crop types. Deep learning (DL) is a powerful state-of-the-art technique for image processing including remote sensing (RS) images. This letter describes a multilevel DL architecture that targets land cover and crop type classification from multitemporal multisource satellite imagery. The pillars of the architecture are unsupervised neural network (NN) that is used for optical imagery segmentation and missing data restoration due to clouds and shadows, and an ensemble of supervised NNs. As basic supervised NN architecture, we use a traditional fully connected multilayer perceptron (MLP) and the most commonly used approach in RS community random forest, and compare them with convolutional NNs (CNNs). Experiments are carried out for the joint experiment of crop assessment and monitoring test site in Ukraine for classification of crops in a heterogeneous environment using nineteen multitemporal scenes acquired by Landsat-8 and Sentinel-1A RS satellites. The architecture with an ensemble of CNNs outperforms the one with MLPs allowing us to better discriminate certain summer crop types, in particular maize and soybeans, and yielding the target accuracies more than 85% for all major crops (wheat, maize, sunflower, soybeans, and sugar beet).",
        "publication_year": "2017",
        "authors": [
            "N. Kussul",
            "M. Lavreniuk",
            "S. Skakun",
            "A. Shelestov"
        ],
        "related_topics": [
            "Environmental Science"
        ],
        "citation_count": "1,012",
        "reference_count": "40",
        "references": [
            "/paper/A-Framework-for-Evaluating-Land-Use-and-Land-Cover-Carranza-Garc%C3%ADa-Garc%C3%ADa-Guti%C3%A9rrez/fa0a7b4988b593112464cf06845f5fa0430421cf",
            "/paper/Application-of-a-convolutional-neural-network-to-on-Yu-Du/6e56281f434a4dee62d1b9e3dc2701a7636fae8c",
            "/paper/Seasonal-Multi-temporal-Pixel-Based-Crop-Types-and-Laban-Abdellatif/8b182b5224155d2bc41f603eab377ad2dcddc5d4",
            "/paper/A-Comparative-Analysis-of-Deep-Learning-Techniques-Castro-Feitosa/a0bc0f9386362e80874ab10e4aa8b462847634f0",
            "/paper/Land-Cover-Classification-Based-on-Sentinel-2-Using-Heryadi-Miranda/2c303628b5d7ae32fd7a7d5619e7dfd34c9b9200",
            "/paper/BENCHMARKING-OF-CONVOLUTIONAL-NEURAL-NETWORK-FOR-Carpentier-Masse/516cce0479e5ecd44f06a810f93a095adb596bf8",
            "/paper/Improvement-in-Land-Cover-and-Crop-Classification-Mazzia-Khaliq/005bb7b6c6dbbbb3f2da59ba9c82da3fd3cf522f",
            "/paper/Deep-Learning-for-Land-Use-and-Land-Cover-Based-on-Vali-Comai/d9331c4fb24f87bd8da6d8dda3df730643f1b9b9",
            "/paper/An-Hybrid-Recurrent-Convolutional-Neural-Network-on-Castro-Feitosa/a110318b83917b3e6a1338be5ff1faa191fab9c8",
            "/paper/Evaluation-of-Deep-Learning-CNN-Model-for-Land-Use-Bhosle-Musande/18d21f34580be52b9919147005f8d301e6f52f22",
            "/paper/Regional-scale-crop-mapping-using-multi-temporal-Kussul-Skakun/b138b3c406e0a2e2e9460251b814e68fc1a8bed2",
            "/paper/Learning-multiscale-and-deep-representations-for-Zhao-Du/49595205981e115d85f1445429694909a1a25040",
            "/paper/Scene-Classification-via-a-Gradient-Boosting-Random-Zhang-Du/729723fad4703821a33554d9d4d803adb351c620",
            "/paper/Spectral%E2%80%93Spatial-Classification-of-Hyperspectral-on-Chen-Zhao/f9d119346b0773ea83251598fa5305bc75bac8ab",
            "/paper/Deep-Learning-Based-Classification-of-Hyperspectral-Chen-Lin/ef8ae1effca9cd45677086034d8c7b06a69c03e5",
            "/paper/Efficiency-Assessment-of-Multitemporal-C-Band-and-Skakun-Kussul/ac1b1ed4097a97f9dbb06628324f83475a3bc01e",
            "/paper/High-Resolution-SAR-Image-Classification-via-Deep-Geng-Fan/e1a977d97cc108afc9d6811d9bdcff3e8f39846c",
            "/paper/Remote-sensing-image-classification-based-on-neural-Han-Zhu/f9375efb0ab9b799fd24cdb4f25ed526de71e166",
            "/paper/Parcel-Based-Crop-Classification-in-Ukraine-Using-Kussul-Lemoine/9a7ef86ad80457b32773b09323db89163bdd1259",
            "/paper/Hyperspectral-Imagery-Classification-Using-Sparse-Liang-Li/9472ed1440694e9be67b6f7168a656d15651a442"
        ]
    },
    {
        "id": "64527aca066f496631154bd536b4528c62afce12",
        "title": "Phase field modeling of flexoelectric effects in ferroelectric epitaxial thin films",
        "abstract": "The flexoelectric effect which is defined as the coupling between strain gradient and polarization has long been neglected because it is insignificant in bulk ferroelectrics. However, at nanoscale, the strain gradient can be dramatically increased leading to giant flexoelectric effects. In the present study, the flexoelectric effects in epitaxial nano thin films of a 180\u00b0 multi-domain structure, which are subjected to a compressive in-plane misfit strain, are investigated by the phase field method. Unlike the case of a single domain structure where the strain gradient is mainly attributed to the formation of dislocation which relaxes the misfit strain, in a multi-domain structure, it is attributed to many factors, such as surface and interface effects, misfit relaxation and domain wall structure. The results obtained show that relatively large flexoelectricity-induced electric fields are produced near the domain wall region. The induced field will not only influence the domain structure of the thin film, but also the hysteresis loops when it is under an applied electric field.",
        "publication_year": "2014",
        "authors": [
            "H. T. Chen",
            "A. Soh",
            "Y. Ni"
        ],
        "related_topics": [
            "Materials Science",
            "Physics"
        ],
        "citation_count": "28",
        "reference_count": "34",
        "references": [
            "/paper/Phase-field-modeling-of-flexoelectricity-in-solid-Chen-Zhang/32be513855e6129ef0c7610365caf0b2ea74061a",
            "/paper/The-effect-of-flexoelectricity-on-domain-switching-Zhao-Soh/708239289f9ff6f514fbccdac5a38e1aeb406182",
            "/paper/A-meshfree-formulation-for-large-deformation-of-for-Zhuang-Nanthakumar/7a7542ae9a5914a51fabdae6537c2fe387f6e918",
            "/paper/Design-of-super-elastic-freestanding-ferroelectric-Guo-Huang/708e23788c8026c0ba66466a395292abc61a5992",
            "/paper/Phase-Field-Modeling-of-Relaxor-Ferroelectrics-and-Wang/5d13ff6f634c832704b65a84649fef3698ab1acb",
            "/paper/Coupling-of-electrical-and-mechanical-switching-in-Cao-Li/28e79ec0b02bd5f7a24839d76eb13d9071c36962",
            "/paper/Utilizing-mechanical-loads-and-flexoelectricity-to-Chen-Zheng/470ef919d5e48028104539e13449cc44fb62fa47",
            "/paper/Nanograins-in-ferroelectric-films-Su-Ouyang/b7ab93cdfb14ad6161cc6f410064f14f913a9ae4",
            "/paper/Simulations-of-local-mechanical-stress-induced-by-a-Jiang-Tang/725daf1c9bfcf3b683b997828dbd2bd491ad31cb",
            "/paper/Electromechanical-analysis-of-direct-and-converse-a-Liu-Deng/7947046c32398692296f912bbce21f67f6881ce1",
            "/paper/External-uniform-electric-field-removing-the-effect-Zhou-Hong/be32779b55929b0ae216f751542a76428bbee2d7",
            "/paper/The-effect-of-flexoelectricity-on-the-dielectric-of-Catal%C3%A1n-Sinnamon/9ee3b6b81481cd42bb838aee055ef60a9b8a1bac",
            "/paper/Flexoelectric-rotation-of-polarization-in-thin-Catal%C3%A1n-Lubk/854de5b7c927a35b79f4208d03942086891f02cb",
            "/paper/Influence-of-flexoelectric-effects-on-multiferroic-Chen-Soh/ce7756691bdd1417a7add8fe7cffa46ff4394f5f",
            "/paper/Atomistic-determination-of-flexoelectric-properties-Maranganti-Sharma/f4e82e232e592a9c3a4d44de37706abaf30d3ff1",
            "/paper/Interface-and-surface-effects-on-ferroelectric-Hong-Soh/a6e9f09315408809ef9daa0bdbbdb766d0956786",
            "/paper/Strain-gradient-induced-polarization-in-SrTiO3-Zubko-Catal%C3%A1n/7bc97bce46a770f87d13b80073ab329cab077038",
            "/paper/Effects-of-film-thickness-and-mismatch-strains-on-Chen-Hong/7cd90304f877dd0598bf058074717e14cd99b177",
            "/paper/Enhanced-size-dependent-piezoelectricity-and-in-due-Majdoub-Sharma/0bdeff278763d194147019c1e9712f41b826c4a3",
            "/paper/Flexoelectric-effect-in-ceramic-lead-zirconate-Ma-Cross/932888f000963343d34115316aaf70132e07eaeb"
        ]
    },
    {
        "id": "d0c3af88cfc14d63c6f94f3a29c0f8f78b7fffc3",
        "title": "An Efficient Approach on Answering Top-k Queries with Grid Dominant Graph Index",
        "abstract": "Analytical and experimental evidences display that GDG index approach performs better on both storage of index and efficiency of queries than traditional dominant graph (DG) structure. The top-k queries are well used in processing spatial data and dimensional data stream to recognize important objects. To address the problem of top-k queries with preferable function, a grid dominant graph (GDG) index based on reserve data points set (RDPS) is presented. Because of the correlation between top-k and skyline computation, when the count of one point\u2019s RDPS is bigger than or equals to the value of k, it is for sure to be pruned the point\u2019s dominant set. This approach in used to prune a lot of free points of top-k queries. GDG used grid index to calculate all RDPS approximately and efficiently. When construction GDG, grid index figures out k-max calculating region to prune free points that all top-k function would not visit, which decreases the amount of index dramatically. When travelling GDG, grid index figures out k-max search region by top-k function which avoids travelling those free points of ad-hoc top-k function. Because GDG uses grid index to rank those data points in the same layer approximately by k-max search region by top-k function, this index structure make less visited points than traditional dominant graph (DG) structure. What\u2019s more, grid index needs less additional computation and storage that make the GDG index more adaptive for top-k queries. Analytical and experimental evidences display that GDG index approach performs better on both storage of index and efficiency of queries.",
        "publication_year": "2013",
        "authors": [
            "Aiping Li",
            "Jinghu Xu",
            "Liang Gan",
            "Bin Zhou",
            "Yan Jia"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "10",
        "references": [
            "/paper/Pareto-Based-Dominant-Graph%3A-An-Efficient-Indexing-Zou-Chen/a0378320ca434a40c392df7a964d020afd047869",
            "/paper/Answering-top-k-queries-with-multi-dimensional-the-Xin-Han/f7af2736c3a2a01e47600dce1596d8c35da82ecc",
            "/paper/Answering-top-k-queries-using-views-Das-Gunopulos/0f7e7d9add82a37b44764bd706b60fddd97b13fd",
            "/paper/Towards-robust-indexing-for-ranked-queries-Xin-Chen/6156864e6fd0836726fc7e7ce1d5bdcff112fe27",
            "/paper/The-onion-technique%3A-indexing-for-linear-queries-Chang-Bergman/279c6d90821d46703fe6c17daa9c64064cc2044a",
            "/paper/PREFER%3A-a-system-for-the-efficient-execution-of-Hristidis-Koudas/b47283c7d69d0116e656beb6ddcc64e97dbaef57",
            "/paper/The-Skyline-operator-B%C3%B6rzs%C3%B6nyi-Kossmann/a9f60496014fc31ade0edbbee6d6479ca0c4de76",
            "/paper/Query-processing-issues-in-image-(multimedia)-Nepal-Ramakrishna/3e7d5a7c8f6d87be47fc170fc62e0949331352bc",
            "/paper/Structured-databases-on-the-web%3A-observations-and-Chang-He/08273b526c161aa93b4734889fe26d1e70aa4b16",
            "/paper/Detecting-Hidden-Anomalies-Using-Sketch-for-Network-Aiping-Han/5b031bebc736bd44936c94b7c0d8fe7ffe2ce798"
        ]
    },
    {
        "id": "e46b23ba2f6fce4fdecbe677b807d7a5300aa415",
        "title": "Robust Texture Mapping Using RGB-D Cameras",
        "abstract": "It is argued that use of the depth data from RGB-D images can be an invaluable help to confer such robustness to the texture mapping process, and the complete texture mapping procedure proposed in this paper is able to significantly improve the quality of the produced textured 3D meshes. The creation of a textured 3D mesh from a set of RGD-D images often results in textured meshes that yield unappealing visual artifacts. The main cause is the misalignments between the RGB-D images due to inaccurate camera pose estimations. While there are many works that focus on improving those estimates, the fact is that this is a cumbersome problem, in particular due to the accumulation of pose estimation errors. In this work, we conjecture that camera poses estimation methodologies will always display non-neglectable errors. Hence, the need for more robust texture mapping methodologies, capable of producing quality textures even in considerable camera misalignments scenarios. To this end, we argue that use of the depth data from RGB-D images can be an invaluable help to confer such robustness to the texture mapping process. Results show that the complete texture mapping procedure proposed in this paper is able to significantly improve the quality of the produced textured 3D meshes.",
        "publication_year": "2021",
        "authors": [
            "Miguel Oliveira",
            "G. H. Lim",
            "Tiago Madeira",
            "Paulo Dias",
            "V\u00edtor M. F. Santos"
        ],
        "related_topics": [
            "Mathematics"
        ],
        "citation_count": "3",
        "reference_count": "66",
        "references": [
            "/paper/A-Robust-3D-Based-Color-Correction-Approach-for-Coelho-Dal%E2%80%99Col/7aea5ee6a11ca4666aeedea9001490e710c9a0fb",
            "/paper/A-Sequential-Color-Correction-Approach-for-Texture-Dal%E2%80%99Col-Coelho/49c2f3de221ed0a688731b6812540cc11d5a0300",
            "/paper/Robot-Localization-and-Scene-Modeling-Based-on-Guo/57cb5e31213a3e4f7176f9a8cc540f9c28fd4aaf",
            "/paper/Texture-Mapping-for-3D-Reconstruction-with-RGB-D-Fu-Yan/fd8f3bc244999e003c799aba9e2bf642f570faa7",
            "/paper/Automatic-Registration-of-Images-to-Untextured-Pl%C3%B6tz-Roth/e17aa75482175bc09ade7067ce105bd48518e602",
            "/paper/Masked-photo-blending%3A-Mapping-dense-photographic-Callieri-Cignoni/8e522e626b607a95e599361f078c975079b389c0",
            "/paper/Seamless-image-based-texture-atlases-using-blending-All%C3%A8ne-Pons/9f7be8afe4662fd01e850ebcc70576c520538edc",
            "/paper/A-Fast-and-Robust-Extrinsic-Calibration-for-RGB-D-%E2%80%A0-Su-Shen/ba93ea308864b8828139c630113912a912e9cb35",
            "/paper/Intrinsic3D%3A-High-Quality-3D-Reconstruction-by-and-Maier-Kim/6fed4cd5847dd199ef56c369e3fb48a6efab47de",
            "/paper/Color-map-optimization-for-3D-reconstruction-with-Zhou-Koltun/20035f36d8d21207170ca2a6c15623ca056a8641",
            "/paper/Seamless-Mosaicing-of-Image-Based-Texture-Maps-Lempitsky-Ivanov/8f7ff9e9474a5eb1bac32c0a649d6ccd152a3674",
            "/paper/State-of-the-Art-on-3D-Reconstruction-with-RGB%E2%80%90D-Zollh%C3%B6fer-Stotko/484024a91c1cb97b358c2cabd9fa427fcc6d52e3",
            "/paper/Deep-Depth-Completion-of-a-Single-RGB-D-Image-Zhang-Funkhouser/9766552d5d25519ed50bcafaf88b7701d2533477"
        ]
    },
    {
        "id": "320d05db95ab42ade69294abe46cd1aca6aca602",
        "title": "High Performance Visual Tracking with Siamese Region Proposal Network",
        "abstract": "The Siamese region proposal network (Siamese-RPN) is proposed which is end-to-end trained off-line with large-scale image pairs for visual object tracking and consists of SiAMESe subnetwork for feature extraction and region proposal subnetwork including the classification branch and regression branch. Visual object tracking has been a fundamental topic in recent years and many deep learning based trackers have achieved state-of-the-art performance on multiple benchmarks. However, most of these trackers can hardly get top performance with real-time speed. In this paper, we propose the Siamese region proposal network (Siamese-RPN) which is end-to-end trained off-line with large-scale image pairs. Specifically, it consists of Siamese subnetwork for feature extraction and region proposal subnetwork including the classification branch and regression branch. In the inference phase, the proposed framework is formulated as a local one-shot detection task. We can pre-compute the template branch of the Siamese subnetwork and formulate the correlation layers as trivial convolution layers to perform online tracking. Benefit from the proposal refinement, traditional multi-scale test and online fine-tuning can be discarded. The Siamese-RPN runs at 160 FPS while achieving leading performance in VOT2015, VOT2016 and VOT2017 real-time challenges.",
        "publication_year": "2018",
        "authors": [
            "Bo Li",
            "Junjie Yan",
            "Wei Wu",
            "Zheng Zhu",
            "Xiaolin Hu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "1,565",
        "reference_count": "41",
        "references": [
            "/paper/Siamese-Centerness-Prediction-Network-for-Real-Time-Wu-Cai/93593630a30eeb44905cd1f3d512ea814fbf1f62",
            "/paper/SiamAtt%3A-Siamese-attention-network-for-visual-Yang-He/238764091b1aff3ede9ec16c5ca1db168bac7ad3",
            "/paper/SiamCAR%3A-Siamese-Fully-Convolutional-Classification-Guo-Wang/738165f33c50b059e87b14d8b4a129230e14eacd",
            "/paper/Combined-Correlation-Filters-with-Siamese-Region-Cui-Tian/80e666713638aa256f544517ffd6c00bcb0014cc",
            "/paper/The-Multi-task-Fully-Convolutional-Siamese-Network-Xuan-Li/a36447134c21bde4f6de39e42444e105c7659089",
            "/paper/SiamVGG%3A-Visual-Tracking-using-Deeper-Siamese-Li-Zhang/dedafafc0f7845d163a9c7d90c48c75dd24aa67f",
            "/paper/Siamese-Attentive-Graph-Tracking-Zhao-Zhang/e2d889a45acc380d4a10d550ef150fcdf339012f",
            "/paper/Siamese-Visual-Tracking-With-Deep-Features-and-Li-Wang/5766d1b3a5a176dc1fef891c54545f74620be0c9",
            "/paper/Joint-Classification-and-Regression-for-Visual-with-Cui-Guo/d3245bf2e156d0c40fc3cdf45c055380b596e2a6",
            "/paper/Siamese-Region-Proposal-Networks-and-Attention-for-Dong-Zeng/ae6c283c5fd060da76f9a554c8210ddea6d11248",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/UCT%3A-Learning-Unified-Convolutional-Networks-for-Zhu-Huang/1131c53b9baaa740a4deef4c1282821b23d18687",
            "/paper/End-to-End-Flow-Correlation-Tracking-with-Attention-Zhu-Wu/7ccbb845829234548bfa9b24c61297b4f0cd678e",
            "/paper/DCFNet%3A-Discriminant-Correlation-Filters-Network-Wang-Gao/5404718135548b01516a668e0c022c5cb22b422e",
            "/paper/CREST%3A-Convolutional-Residual-Learning-for-Visual-Song-Ma/c2046fc4744a9d358ea7a8e9c21c92fd58df7a64",
            "/paper/The-Visual-Object-Tracking-VOT2016-Challenge-Kristan-Leonardis/966aad492f75b17f698e981e008b73b51816c6aa",
            "/paper/Siamese-Instance-Search-for-Tracking-Tao-Gavves/c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "/paper/Faster-R-CNN%3A-Towards-Real-Time-Object-Detection-Ren-He/424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "/paper/Re3-%3A-Real-Time-Recurrent-Regression-Networks-for-Gordon-Farhadi/dda27eb7ddc4510f94cac0e5134b5d56aa77b075",
            "/paper/Learning-to-Track-at-100-FPS-with-Deep-Regression-Held-Thrun/5f0850ec47a17f22ba2611a5cb67a30cb02cf306"
        ]
    },
    {
        "id": "f02030b30323146f51f76af0a70cbf8fbddfb88b",
        "title": "Application of the VNS heuristic for feature selection in credit scoring problems",
        "abstract": "Semantic Scholar extracted view of \"Application of the VNS heuristic for feature selection in credit scoring problems\" by Victor Gomes Helder et al.",
        "publication_year": "2022",
        "authors": [
            "Victor Gomes Helder",
            "T. Filomena",
            "Luciano Ferreira",
            "G. Kirch"
        ],
        "related_topics": [
            "Education"
        ],
        "citation_count": 0,
        "reference_count": "37",
        "references": [
            "/paper/Time-Optimal-Trajectory-Planning-for-the-Based-on-Hou-Du/82a86882dc6e03648fa0399fb6e705999fe5223c",
            "/paper/Global-optimization-on-non-convex-two-way-truncated-Ju-Rosenberger/1cd3e1d78053c1d41210aab2edd3a9da126f974c",
            "/paper/Assessing-feature-selection-method-performance-with-Matharaarachchi-Domaratzki/12055e6e7be178f218e6f13af44f9262c6e79ae6",
            "/paper/Ensemble-Based-Filter-Feature-Selection-with-Swarm-Hamid-Sallehuddin/9035e4f665ab6608da0c3f5902030d25fbd967b2",
            "/paper/Feature-selection-and-deep-neural-networks-for-Peng-Albuquerque/2e0918089c9a4f808d59cd1856b731a9c0b273fa",
            "/paper/Machine-learning-algorithms-for-fraud-prediction-in-Severino-Peng/bdb1b221d7c969c3e9f0772b3a32da761261bb2e",
            "/paper/Analysis-of-Dimensionality-Reduction-Techniques-on-Reddy-Reddy/b34fc78de28be598e21118d7cb9d84d63374addc",
            "/paper/Variable-selection-using-statistical-non-parametric-Beuren-Anzanello/e88aa4ef85a707709494d0f5736ecdd67dca7228",
            "/paper/Integration-of-unsupervised-and-supervised-machine-Bao-Ning/32ff2fba49066a963d3f132e19de40e47d86a30a",
            "/paper/A-novel-multi-stage-hybrid-model-with-enhanced-An-Zhang-He/acc09c9fe31232a1ac6f02b55b84af520a78eb79",
            "/paper/Three-local-search-based-methods-for-feature-in-Boughaci-Alkhawaldeh/b7fbdae6c540a016d8981c169929d9f7e4a96607"
        ]
    },
    {
        "id": "2a638fd0debeb8ff64fe34354cabefdfd5a8e97a",
        "title": "A Unique Method for Detecting Grounds in the Indoor Environment",
        "abstract": "This method combines clustering and recognition to estimate the planes show in the depth image, and is ready to be used in many other situations like path planning, target following, map reconstructing, etc. In this paper, a unique method for detecting and distinguishing the grounds, walls, and ceilings in the indoor environment using single depth image is presented. Our method combines clustering and recognition to estimate the planes show in the depth image. A depth image correction method is proposed. Clustering process using k-means algorithm. The number of clusters is determined by elbow method. Recognition process using HOD (histogram of distance)to classify the planes. The classification of grounds walls and ceilings is very useful in mobile robot navigation, people detection and many other situations. Different from other existing methods, our method is fully automated, and only need one single depth image, no need to know the position and posture of the camera. On the basis of a certain number of experiments, we demonstrate the usefulness and robustness of our method. Our method is ready to be used in many other situations like path planning, target following, map reconstructing, etc.",
        "publication_year": "2018",
        "authors": [
            "Xiaoxiao Chen",
            "Ping Song",
            "Yayu Zhai"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "13",
        "references": [
            "/paper/Homography-based-ground-plane-detection-for-mobile-Conrad-DeSouza/c9867b36f37387cedafcdcab91899bbe2eae1b39",
            "/paper/Road-Detection-Based-on-Illuminant-Invariance-%C3%81lvarez-L%C3%B3pez/37e99517a808c8768e590c77a9e06744e0f1d3b7",
            "/paper/An-iterative-convergence-algorithm-for-single-multi-Lang-Wu/5bea3851a0a13c410cb4e75b5ad86c36517cf480",
            "/paper/Ground-Plane-Estimation-Using-a-Hidden-Markov-Model-Dragon-Gool/635905ecdb84dacf2703e06a28d23eb797430fd7",
            "/paper/Camera-Self-Calibration%3A-Theory-and-Experiments-Faugeras-Luong/80d088324bf9326815b50496fed3cd401572cbff",
            "/paper/Joint-Tracking-and-Ground-Plane-Estimation-Kwon-Dragon/595ce8e24326658a96cdb471fa14ea1bd89d0e6c",
            "/paper/Camera-Calibration-with-Distortion-Models-and-Weng-Cohen/d19c274cb0e8b70d6cbeb138e13269a89ebe7889",
            "/paper/Improved-Research-to-K-means-Initial-Cluster-Zhang-Duan/a2c56e2017ef570d9b1b1cadb08a293cac02d23e",
            "/paper/Improved-Research-to-K-means-Initial-Cluster-Min-Kai-fei/a311875bd5c23edecb8ffb8aca2334132b05b598",
            "/paper/Review-on-determining-number-of-Cluster-in-K-Means-Kodinariya-Makwana/1a34936bffe558a380168b790dc37956813514ba"
        ]
    },
    {
        "id": "c7afd747b5c6b77dc22eaa87a8b22888243842b6",
        "title": "The Lovasz-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks",
        "abstract": "This work presents a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex Lov\u00c3\u00a1sz extension of submodular losses. The Jaccard index, also referred to as the intersection-over-union score, is commonly employed in the evaluation of image segmentation results given its perceptual qualities, scale invariance - which lends appropriate relevance to small objects, and appropriate counting of false negatives, in comparison to per-pixel losses. We present a method for direct optimization of the mean intersection-over-union loss in neural networks, in the context of semantic image segmentation, based on the convex Lov\u00c3\u00a1sz extension of submodular losses. The loss is shown to perform better with respect to the Jaccard index measure than the traditionally used cross-entropy loss. We show quantitative and qualitative differences between optimizing the Jaccard index per image versus optimizing the Jaccard index taken over an entire dataset. We evaluate the impact of our method in a semantic segmentation pipeline and show substantially improved intersection-over-union segmentation scores on the Pascal VOC and Cityscapes datasets using state-of-the-art deep learning segmentation architectures.",
        "publication_year": "2017",
        "authors": [
            "Maxim Berman",
            "A. Triki",
            "Matthew B. Blaschko"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "527",
        "reference_count": "33",
        "references": [
            "/paper/Jaccard-Metric-Losses%3A-Optimizing-the-Jaccard-Index-Wang-Blaschko/c13cc96c717333e2854d4a2d0cee84794282d5be",
            "/paper/Learning-Generalized-Intersection-Over-Union-for-Yu-Xu/1d7329bc40c4911a24af3e34b62d53d600824cb7",
            "/paper/Distribution-Aware-Margin-Calibration-for-Semantic-Yu-Li/56a2fef6d1ce3cdf21a1aab769b24d6320d07103",
            "/paper/Distribution-Aware-Margin-Calibration-for-Semantic-Yu-Li/ffed77447244cc86f5dbad9542db20c0b44b71e8",
            "/paper/Auto-Seg-Loss%3A-Searching-Metric-Surrogates-for-Li-Tao/79112ae62a5f8ce03829d3c77e218269d4c0d35c",
            "/paper/AUTO-SEG-LOSS%3A-SEARCHING-METRIC-SURROGATES-Tao-Zhu/62e94e7a1dcf9a705908dd75f85c524e6072bd34",
            "/paper/Generalized-Intersection-Over-Union%3A-A-Metric-and-a-Rezatofighi-Tsoi/889c81b4d7b7ed43a3f69f880ea60b0572e02e27",
            "/paper/RankSEG%3A-A-Consistent-Ranking-based-Framework-for-Dai-Li/ca9cf3add7ce16272aeb3e5993d034d892b1ff3b",
            "/paper/MetricOpt%3A-Learning-to-Optimize-Black-Box-Metrics-Huang-Zhai/3e9fa14b16c3092718f3f9468553e106aa4ac818",
            "/paper/J-Regularization-Improves-Imbalanced-Multiclass-Guerrero-Pe%C3%B1a-Marrero-Fern%C3%A1ndez/7d78b5bfb749eca31a37340e1d6e55360e877914",
            "/paper/Optimizing-Expected-Intersection-Over-Union-with-Ahmed-Tarlow/8a596dd5bc68b73c1579300e283eb44d7c6b8ed4",
            "/paper/Optimizing-Intersection-Over-Union-in-Deep-Neural-Rahman-Wang/64dc671107cfdf98e02e76cbc993732d47210549",
            "/paper/The-Lov%C3%A1sz-Hinge%3A-A-Convex-Surrogate-for-Submodular-Yu-Blaschko/794dbf68bae49bb571d1b2461289a6bb629de875",
            "/paper/Learning-Submodular-Losses-with-the-Lovasz-Hinge-Yu-Blaschko/285b47c29e8a3667fab0156150632b74319e1658",
            "/paper/Optimal-Decisions-from-Probabilistic-Models%3A-The-Nowozin/ca736f2c509631b7226957dbab2188f842468c8f",
            "/paper/The-Lov%C3%A1sz-Hinge%3A-A-Novel-Convex-Surrogate-for-Yu-Blaschko/c399c0089fb134d1476fadf5f0426e0e8b70eebd",
            "/paper/DeepLab%3A-Semantic-Image-Segmentation-with-Deep-and-Chen-Papandreou/cab372bc3824780cce20d9dd1c22d4df39ed081a",
            "/paper/Microsoft-COCO%3A-Common-Objects-in-Context-Lin-Maire/71b7178df5d2b112d07e45038cb5637208659ff7",
            "/paper/Adam%3A-A-Method-for-Stochastic-Optimization-Kingma-Ba/a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "/paper/Learning-to-Localize-Objects-with-Structured-Output-Blaschko-Lampert/8483da1bc3302c34b437baf8e329390da8f2c9bf"
        ]
    },
    {
        "id": "ab91110608f6df4c676a5c41573172fe71b97e71",
        "title": "FCAF3D: Fully Convolutional Anchor-Free 3D Object Detection",
        "abstract": "FCAF3D is a first-in-class fully convolutional anchor-free indoor 3D object detection method that uses a voxel representation of a point cloud and processes voxels with sparse convolutions and proposes a novel parametrization of oriented bounding boxes that allows obtaining better results in a purely data-driven way. Recently, promising applications in robotics and augmented reality have attracted considerable attention to 3D object detection from point clouds. In this paper, we present FCAF3D - a first-in-class fully convolutional anchor-free indoor 3D object detection method. It is a simple yet effective method that uses a voxel representation of a point cloud and processes voxels with sparse convolutions. FCAF3D can handle large-scale scenes with minimal runtime through a single fully convolutional feed-forward pass. Existing 3D object detection methods make prior assumptions on the geometry of objects, and we argue that it limits their generalization ability. To get rid of any prior assumptions, we propose a novel parametrization of oriented bounding boxes that allows obtaining better results in a purely data-driven way. The proposed method achieves state-of-the-art 3D object detection results in terms of mAP@0.5 on ScanNet V2 (+4.5), SUN RGB-D (+3.5), and S3DIS (+20.5) datasets. The code and models are available at https://github.com/samsunglabs/fcaf3d.",
        "publication_year": "2021",
        "authors": [
            "D. Rukhovich",
            "Anna Vorontsova",
            "A. Konushin"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "31",
        "reference_count": "37",
        "references": [
            "/paper/OctFormer%3A-Octree-based-Transformers-for-3D-Point-Wang/ee21089c6ef2a683b19407b76bf4ed631cb4633b",
            "/paper/Autoregressive-Uncertainty-Modeling-for-3D-Bounding-Liu-Mishra/e745e69fffd0c84780037409c8ca84c8a65c7f1a",
            "/paper/Boosting-3D-Object-Detection-via-Object-Focused-Yang-Shi/65dbae93c738f1e5b92f2c0df5ce730e0e380c12",
            "/paper/DSPDet3D%3A-Dynamic-Spatial-Pruning-for-3D-Small-Xu-Sun/984b4e278fab04e73f34d2aad6285b6a58d2629c",
            "/paper/DQS3D%3A-Densely-matched-Quantization-aware-3D-Gao-Tian/dcde2e099b2ad2ce40c8a3edace11dd8d0275c9a",
            "/paper/Top-Down-Beats-Bottom-Up-in-3D-Instance-Kolodiazhnyi-Rukhovich/8ee9b854d5ddfb43064ea8339ca0d27c3182e8e5",
            "/paper/TR3D%3A-Towards-Real-Time-Indoor-3D-Object-Detection-Rukhovich-Vorontsova/3dbcbd019f60a414ee0ba04691f49eef6a4b62d3",
            "/paper/CAGroup3D%3A-Class-Aware-Grouping-for-3D-Object-on-Wang-Ding/f37c9cbf7b09885a3ec686d8967c84a755077f63",
            "/paper/Learning-from-Multi-View-Representation-for-Yan-Song/43d2366ec0996e4036ad83bee84d44a1a847b20f",
            "/paper/A-Lightweight-Model-for-3D-Point-Cloud-Object-Li-Li/ca802e9c2011ced6a092a94a9adfac6aa0426fb5",
            "/paper/Deep-Hough-Voting-for-3D-Object-Detection-in-Point-Qi-Litany/e60da8d3a79801a3ccbf1abcdd001bb6e001b267",
            "/paper/ImVoxelNet%3A-Image-to-Voxels-Projection-for-and-3D-Rukhovich-Vorontsova/2f9465d95975fd61b7bc2af95d803f2a43bd7d44",
            "/paper/Group-Free-3D-Object-Detection-via-Transformers-Liu-Zhang/c2cf5e5f21cca7045be719afce6bf66cc2934370",
            "/paper/SMOKE%3A-Single-Stage-Monocular-3D-Object-Detection-Liu-Wu/c0a640915e50983de7294a5127020695eaee30b9",
            "/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "/paper/An-End-to-End-Transformer-Model-for-3D-Object-Misra-Girdhar/03a2befad038a9f29859295fdfcdbfa52c564622",
            "/paper/Generative-Sparse-Detection-Networks-for-3D-Object-Gwak-Choy/fa2189b9ba9b4679be99cbfa4e1b8bc21d0f5c6c",
            "/paper/Bridging-the-Gap-Between-Anchor-Based-and-Detection-Zhang-Chi/448529da2bf004cf79084401ad3cbd6b511e4969",
            "/paper/FCOS%3A-Fully-Convolutional-One-Stage-Object-Tian-Shen/e2751a898867ce6687e08a5cc7bdb562e999b841",
            "/paper/VENet%3A-Voting-Enhancement-Network-for-3D-Object-Xie-Lai/f32a16a0d30ce68dd3d80a9ec7ffbaa75786e47d"
        ]
    },
    {
        "id": "59b8abd4d8ee57e7fe0857496203f6394e70677e",
        "title": "2019 Index IEEE Transactions on Circuits and Systems for Video Technology Vol. 29",
        "abstract": "This index covers all technical items\u2014papers, correspondence, reviews, etc.\u2014that appeared in this periodical during 2019, and items from previous years that were commented upon or corrected in 2019. Departments and other items may also be covered if they have been judged to have archival value. The Author Index contains the primary entry for each item, listed under the first author\u2019s name. The primary entry includes the coauthors\u2019 names, the title of the paper or other item, and its location, specified by the publication abbreviation, year, month, and inclusive pagination. The Subject Index contains entries describing the item under all appropriate subject headings, plus the first author\u2019s name, the publication abbreviation, month, and year, and inclusive pages. Note that the item title is found only under the primary entry in the Author Index.",
        "publication_year": "2019",
        "authors": [
            "see"
        ],
        "related_topics": [
            "Psychology"
        ],
        "citation_count": 0,
        "reference_count": "262",
        "references": [
            "/paper/Context-Driven-Optimized-Perceptual-Video-and-Thomas-Gupta/9b4335a255d9ea4b5dfa19f5c134f56119046769",
            "/paper/A-Motion-Based-Partitioning-Algorithm-for-HEVC-a-Cebri%C3%A1n-M%C3%A1rquez-Mart%C3%ADnez/76d39256e2d37775f32fa37b369359a53793cd81",
            "/paper/IR-Feature-Embedded-BOF-Indexing-Method-for-Video-Liao-Lei/3fa4d3b0212e397989da31daee9db78e08d4d834",
            "/paper/Reference-Clip-for-Inter-Prediction-in-Video-Coding-Ma-Liu/aa80760f07f551478a953c473a9ca4ce8c5d6e72",
            "/paper/VLSI-Design-of-an-Efficient-Flicker-Free-Video-for-Shiau-Kuo/14276f7f5bd42266e430fbb6b6e397aea19c8a79",
            "/paper/A-Novel-Joint-Rate-Allocation-Scheme-of-Multiple-Fan-Ding/344fd22f745f13ac09b3bb96a4a5fd0b6fae0a55",
            "/paper/Fast-Coding-Unit-Partition-Decision-for-HEVC-Using-Grellert-Zatt/c7ce99a9a22b2d23c4561b2b3ad484e4b4bce3e6",
            "/paper/Uniform-Color-Space-Based-High-Dynamic-Range-Video-Mukherjee-Debattista/bd8cefc15a85fbcba9e264af6a1b78c42aa5778f",
            "/paper/Multi-Script-Oriented-Text-Detection-and-in-Video-Raghunandan-Shivakumara/76ddcaf5d4837d02370dd80b7c358719c142dc90",
            "/paper/Interactive-Hierarchical-Object-Proposals-Chen-Zhang/a8ebbc184a31176eb46c17c61061a88efb574917"
        ]
    },
    {
        "id": "689b230b228c7ff5e2bb5d500c5349f54bcc6d3c",
        "title": "ADTrack: Target-Aware Dual Filter Learning for Real-Time Anti-Dark UAV Tracking",
        "abstract": "A novel tracker with anti-dark function (ADTrack), which integrates an efficient and effective low-light image enhancer into a CF-based tracker and adopts dual regression, where the context filter and the target-focused filter restrict each other for dual filter learning. Prior correlation filter (CF)-based tracking methods for unmanned aerial vehicles (UAVs) have virtually focused on tracking in the daytime. However, when the night falls, the trackers will encounter more harsh scenes, which can easily lead to tracking failure. In this regard, this work proposes a novel tracker with anti-dark function (ADTrack). The proposed method integrates an efficient and effective low-light image enhancer into a CF-based tracker. Besides, a target-aware mask is simultaneously generated by virtue of image illumination variation. The target-aware mask can be applied to jointly train a target-focused filter that assists the context filter for robust tracking. Specifically, ADTrack adopts dual regression, where the context filter and the target-focused filter restrict each other for dual filter learning. Exhaustive experiments are conducted on typical dark sceneries benchmark, consisting of 37 typical night sequences from authoritative benchmarks, i.e., UAVDark, and our newly constructed benchmark UAVDark70. The results have shown that ADTrack favorably outperforms other state-of-the-art trackers and achieves a real-time speed of 34 frames/s on a single CPU, greatly extending robust UAV tracking to night scenes.",
        "publication_year": "2021",
        "authors": [
            "Bowen Li",
            "Changhong Fu",
            "Fangqiang Ding",
            "Junjie Ye",
            "Fuling Lin"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "15",
        "reference_count": "39",
        "references": [
            "/paper/DarkLighter%3A-Light-Up-the-Darkness-for-UAV-Tracking-Ye-Fu/a0e544f6db3b6659a5f77c419908238f91b188bc",
            "/paper/PVT%2B%2B%3A-A-Simple-End-to-End-Latency-Aware-Visual-Li-Huang/8fa3221af7f6d3008e00cc0c459ecbde6f1014da",
            "/paper/Towards-tiny-object-tracking-for-low-illumination-Xie-Jia/7e99499f0c268c18929bda8987bc9bbbd7babdfb",
            "/paper/Learning-Spatial-Regularization-Correlation-Filters-An-Wang/74c6afcf9884244075701252d86a2c9de724513e",
            "/paper/Unsupervised-Domain-Adaptation-for-Nighttime-Aerial-Ye-Fu/32db2d409384575aeae453acc45220b51fe96301",
            "/paper/A-Unified-Approach-for-Tracking-UAVs-in-Infrared-Zhao-Zhang/5f42754b51d49913cfd73fb33c0c9deca8945099",
            "/paper/Occlusion-and-Deformation-Handling-Visual-Tracking-Bai-Song/04c01178be3bd85aad2332a4eac24499ba0cb02d",
            "/paper/Mutation-detection-dual-correlation-filter-with-an-Cao-Wu/42bc16ff1f5b76048ce1b82ac69d4d617462d453",
            "/paper/Siamese-Object-Tracking-for-Unmanned-Aerial-A-and-Fu-Lu/1171234cb2f3e1589592e3d04eb10c132fc6a5c8",
            "/paper/Robust-visual-tracking-for-UAVs-with-dynamic-weight-An-Wang/e63797ded1611e60fd55cd3f4616a3d5be68a2d4",
            "/paper/AutoTrack%3A-Towards-High-Performance-Visual-Tracking-Li-Fu/0619650ae0f698bcc38244a6858cc270df9dfaad",
            "/paper/Disruptor-Aware-Interval-Based-Response-for-Filters-Fu-Ye/5f5f11c405ce58fe378eaeff048cd11a7c540099",
            "/paper/Learning-Aberrance-Repressed-Correlation-Filters-Huang-Fu/1b3a7e6ebd89a1fb03c992110952d7bddae3345a",
            "/paper/The-Unmanned-Aerial-Vehicle-Benchmark%3A-Object-and-Du-Qi/a213bbf9740854a276fdf71dad8f30cfbe3ea4d4",
            "/paper/Object-Saliency-Aware-Dual-Regularized-Correlation-Fu-Xu/bb1e5031d0b5f905fa88ed248c44aa7165f10714",
            "/paper/Correlation-Filter-for-UAV-Based-Aerial-Tracking%3A-A-Fu-Li/f6883f7ba0380a65323be7a10c09ae44e9f1b8fe",
            "/paper/Pose-Estimate-Based-Target-Tracking-for-Remote-with-McArthur-An/e459d74e356aadc6ac228f50f358d9c77ad8104c",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "/paper/Visual-Tracking-via-Adaptive-Spatially-Regularized-Dai-Wang/e5820233ea8abd27c81c0a83b1b782a9d4ca8db2",
            "/paper/Automatic-Failure-Recovery-and-Re-Initialization-Ding-Fu/bc8b8bbbf023f9e594ff624dbf0aa5aa57a2b97f"
        ]
    },
    {
        "id": "7a078987c929b1aef72f4ca283dddef187c48132",
        "title": "Adaptive Correlation Filters with Long-Term and Short-Term Memory for Object Tracking",
        "abstract": "This paper proposes to learn multiple adaptive correlation filters with both long-term and short-term memory of target appearance for robust object tracking and demonstrates that the proposed algorithm performs favorably against the state-of-the-art methods in terms of efficiency, accuracy, and robustness. Object tracking is challenging as target objects often undergo drastic appearance changes over time. Recently, adaptive correlation filters have been successfully applied to object tracking. However, tracking algorithms relying on highly adaptive correlation filters are prone to drift due to noisy updates. Moreover, as these algorithms do not maintain long-term memory of target appearance, they cannot recover from tracking failures caused by heavy occlusion or target disappearance in the camera view. In this paper, we propose to learn multiple adaptive correlation filters with both long-term and short-term memory of target appearance for robust object tracking. First, we learn a kernelized correlation filter with an aggressive learning rate for locating target objects precisely. We take into account the appropriate size of surrounding context and the feature representations. Second, we learn a correlation filter over a feature pyramid centered at the estimated target position for predicting scale changes. Third, we learn a complementary correlation filter with a conservative learning rate to maintain long-term memory of target appearance. We use the output responses of this long-term filter to determine if tracking failure occurs. In the case of tracking failures, we apply an incrementally learned detector to recover the target position in a sliding window fashion. Extensive experimental results on large-scale benchmark datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods in terms of efficiency, accuracy, and robustness.",
        "publication_year": "2017",
        "authors": [
            "Chao Ma",
            "Jia-Bin Huang",
            "Xiaokang Yang",
            "Ming-Hsuan Yang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "143",
        "reference_count": "69",
        "references": [
            "/paper/Learning-passive%E2%80%93aggressive-correlation-filter-for-Zhang-Gao/012b6d96840abb0f11b27353a3d033317678ae0a",
            "/paper/Robust-Scale-Adaptive-Visual-Tracking-with-Filters-Li-Yang/fc3289307fe4c48ce0ad6066f7d0d09ed5a23a71",
            "/paper/Spatial-Temporal-Aware-Long-Term-Object-Tracking-Zhang-Kang/d9f05707991a9544a40cd01d13be058062595951",
            "/paper/Dual-template-adaptive-correlation-filter-for-Yan-Zhong/59a53795a1dc8f6fecfe2bb9aa5f5c21ffb1272b",
            "/paper/Dual-template-adaptive-correlation-filter-for-Yan-Zhong/cec5a11906dcaefb54c3ffb80cb212ccfae0fd76",
            "/paper/Improving-model-drift-for-robust-object-tracking-Dong-He/7ff2890afa1692da846cc0211d61c4fbb6f3a0b7",
            "/paper/Learning-salient-features-to-prevent-model-drift-Zhang-Gao/acf44a3dce49a747b96f92fbd9eec61e72864ead",
            "/paper/Multi-stage-analysis-for-long-term-tracking-Bai-Zhang/b662a32644182184ab232b8ffdcd2510b4f52f36",
            "/paper/Long-Term-Object-Tracking-Method-Based-on-Reduction-Zhao-Kang/66c321cc3f66279d5d965a772a7770e1646242a5",
            "/paper/Robust-visual-tracking-via-spatio-temporal-adaptive-Liang-Liu/5dc3b582ebd95f029b1daa8622cf56bb192ef018",
            "/paper/Visual-object-tracking-using-adaptive-correlation-Bolme-Beveridge/70c3c9b9a40ca55264e454586dca2a6cf416f6e0",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Real-time-part-based-visual-tracking-via-adaptive-Liu-Wang/6410c97ae03d356e14544c8e95f5367fb7ebb6e6",
            "/paper/Long-term-correlation-tracking-Ma-Yang/754504cf01ef3846259783e748b1d3ea52fa2c81",
            "/paper/Incremental-Learning-for-Robust-Visual-Tracking-Ross-Lim/505f48d8236eb25f871da272c2ac2fe4b41ea289",
            "/paper/MUlti-Store-Tracker-(MUSTer)%3A-A-cognitive-inspired-Hong-Chen/e73590fdfd6dab391111bb734053ae24207e2c71",
            "/paper/Self-Paced-Learning-for-Long-Term-Tracking-Supan%C4%8Di%C4%8D-Ramanan/1c721511e4c0e21bd264ca71c0d909528511b7ad",
            "/paper/Real-Time-Compressive-Tracking-Zhang-Zhang/9d57723b4908397654fb1846d37db403d8b2b56a",
            "/paper/Struck%3A-Structured-output-tracking-with-kernels-Hare-Saffari/5b9ace65f7368f6dc6907c8f6f7c3b0c248d9bc4",
            "/paper/A-Scale-Adaptive-Kernel-Correlation-Filter-Tracker-Li-Zhu/0cae491292feccbc9ad1d864cf8b7144923ce6de"
        ]
    },
    {
        "id": "cce1fecc800d2782da638f3060d5b2e887739f74",
        "title": "Siamese Box Adaptive Network for Visual Tracking",
        "abstract": "SiamBAN views the visual tracking problem as a parallel classification and regression problem, and thus directly classifies objects and regresses their bounding boxes in a unified FCN, making SiamB Ban more flexible and general. Most of the existing trackers usually rely on either a multi-scale searching scheme or pre-defined anchor boxes to accurately estimate the scale and aspect ratio of a target. Unfortunately, they typically call for tedious and heuristic configurations. To address this issue, we propose a simple yet effective visual tracking framework (named Siamese Box Adaptive Network, SiamBAN) by exploiting the expressive power of the fully convolutional network (FCN). SiamBAN views the visual tracking problem as a parallel classification and regression problem, and thus directly classifies objects and regresses their bounding boxes in a unified FCN. The no-prior box design avoids hyper-parameters associated with the candidate boxes, making SiamBAN more flexible and general. Extensive experiments on visual tracking benchmarks including VOT2018, VOT2019, OTB100, NFS, UAV123, and LaSOT demonstrate that SiamBAN achieves state-of-the-art performance and runs at 40 FPS, confirming its effectiveness and efficiency. The code will be available at https://github.com/hqucv/siamban.",
        "publication_year": "2020",
        "authors": [
            "Zedu Chen",
            "Bineng Zhong",
            "Guorong Li",
            "Shengping Zhang",
            "Rongrong Ji"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "360",
        "reference_count": "52",
        "references": [
            "/paper/SiamBAN%3A-Target-Aware-Tracking-With-Siamese-Box-Chen-Zhong/1cd5d920d19d08d2eb1b67c01648a33369b0cfaf",
            "/paper/Siamese-Centerness-Prediction-Network-for-Real-Time-Wu-Cai/93593630a30eeb44905cd1f3d512ea814fbf1f62",
            "/paper/SiamDA%3A-distribution-aware-Siamese-network-for-Ji-Shi/0bc6b7c8e3d2efdb6cf35a15192029c078bebbc6",
            "/paper/SiamCorners%3A-Siamese-Corner-Networks-for-Visual-Yang-He/4b6a190bb897a626639849c09426d30d5a1462c8",
            "/paper/SiamPCF%3A-siamese-point-regression-with-coarse-fine-Zeng-Zeng/2f80275f6e7c146c2eee4ea9e13544e07b85e305",
            "/paper/Visual-Object-Tracking-With-Discriminative-Filters-Javed-Danelljan/0530cbeb847f5e5002d1183c482759dff5f8c439",
            "/paper/SiamCAN%3A-Real-Time-Visual-Tracking-Based-on-Siamese-Zhou-Wen/aad19f38536f29472d0a78be554a006199445dba",
            "/paper/SiamRCR%3A-Reciprocal-Classification-and-Regression-Peng-Jiang/7bdfc488a2d64e34066df04255a741636ac156ee",
            "/paper/SiamET%3A-a-Siamese-based-visual-tracking-network-Zhou-Zhang/cf1559f1dc37fb77705efee31f01b2dda49e5ced",
            "/paper/Att-SiamMask%3A-Attention-based-Network-For-Enhanced-Elsaid-Fouad/e96f09ed9e769ad6bee9b977cf21e5106fa18df3",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "/paper/GradNet%3A-Gradient-Guided-Network-for-Visual-Object-Li-Chen/47a58f8bec1d34004a7d7cf837e27a26de64f0f7",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/Siamese-Cascaded-Region-Proposal-Networks-for-Fan-Ling/f98be9a91dbf00b52a494720bd36be9c73a1210e",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Structured-Siamese-Network-for-Real-Time-Visual-Zhang-Wang/4b1965a54a064ac9145b1ce404fe33f0120c8ae3",
            "/paper/Deeper-and-Wider-Siamese-Networks-for-Real-Time-Zhang-Peng/2e713509e96daf06e65e10bdc438d00a827c914c",
            "/paper/Learning-Attentions%3A-Residual-Attentional-Siamese-Wang-Teng/6683442ae358ae4261fdcde0164f83dd1ccd621b"
        ]
    },
    {
        "id": "5a365ad81138028dacf989317f34f46852e3e88e",
        "title": "TAVA: Template-free Animatable Volumetric Actors",
        "abstract": "This paper proposes TAVA, a method to create T emplate-free Animatable Volumetric Actors, based on neural representations that relies solely on multi-view data and a tracked skeleton to create a volumetric model of an actor, which can be animated at the test time given novel pose. Coordinate-based volumetric representations have the potential to generate photo-realistic virtual avatars from images. However, virtual avatars also need to be controllable even to a novel pose that may not have been observed. Traditional techniques, such as LBS, provide such a function; yet it usually requires a hand-designed body template, 3D scan data, and limited appearance models. On the other hand, neural representation has been shown to be powerful in representing visual details, but are under explored on deforming dynamic articulated actors. In this paper, we propose TAVA, a method to create T emplate-free Animatable Volumetric Actors, based on neural representations. We rely solely on multi-view data and a tracked skeleton to create a volumetric model of an actor, which can be animated at the test time given novel pose. Since TAVA does not require a body template, it is applicable to humans as well as other creatures such as animals. Furthermore, TAVA is designed such that it can recover accurate dense correspondences, making it amenable to content-creation and editing tasks. Through extensive experiments, we demonstrate that the proposed method generalizes well to novel poses as well as unseen views and showcase basic editing capabilities.",
        "publication_year": "2022",
        "authors": [
            "Ruilong Li",
            "Julian Tanke",
            "Minh Vo",
            "Michael Zollhofer",
            "Jurgen Gall",
            "Angjoo Kanazawa",
            "Christoph Lassner"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "42",
        "reference_count": "51",
        "references": [
            "/paper/MonoHuman%3A-Animatable-Human-Neural-Field-from-Video-Yu-Cheng/b5fb909d436856ba7c4d5e15bfdb83a847e7ff8a",
            "/paper/ARAH%3A-Animatable-Volume-Rendering-of-Articulated-Wang-Schwarz/6d9b0bb216c36aba29a5b11a656a468dc39cad66",
            "/paper/PointAvatar%3A-Deformable-Point-based-Head-Avatars-Zheng-Wang/65619218cddc1c3371d86d074e568da5036d4193",
            "/paper/CloSET%3A-Modeling-Clothed-Humans-on-Continuous-with-Zhang-Lin/38f1d91d135343d339874dae6583466c3a1ff496",
            "/paper/AvatarReX%3A-Real-time-Expressive-Full-body-Avatars-Zheng-Zhao/ae1489b912f8d2eb68234b34fcbdb21f73fbd8df",
            "/paper/X-Avatar%3A-Expressive-Human-Avatars-Shen-Guo/d763a2af9af6e7dec0fa5c9cf3f89d6d362e305d",
            "/paper/NPC%3A-Neural-Point-Characters-from-Video-Su-Bagautdinov/35e846e0e6824176c58e6147243dc95bf953ed49",
            "/paper/CAMM%3A-Building-Category-Agnostic-and-Animatable-3D-Kuai-Karthikeyan/672ef260934caae5ac3ebb91ea151165e2de52ec",
            "/paper/HQ3DAvatar%3A-High-Quality-Controllable-3D-Head-Teotia-MallikarjunB./25936b2b0030e604a753a18b92099b6d1baf36e3",
            "/paper/Efficient-Meshy-Neural-Fields-for-Animatable-Human-Huang-Cheng/aa958ecb7c7f377c5b2c0bfb306354b13bb57a50",
            "/paper/ARCH%3A-Animatable-Reconstruction-of-Clothed-Humans-Huang-Xu/0ff2c939d136df8988f845ae5cdfb725939a82ab",
            "/paper/SCANimate%3A-Weakly-Supervised-Learning-of-Skinned-Saito-Yang/42bf65a6eb1cb5fda999373c3ce7d79690133c92",
            "/paper/Animatable-Neural-Radiance-Fields-for-Modeling-Peng-Dong/35e940ace815548f620709d9c1803da34c581e86",
            "/paper/SCAPE%3A-shape-completion-and-animation-of-people-Anguelov-Srinivasan/adddc04ec7f00f3c3823b66c3369f668df15f957",
            "/paper/Unsupervised-Shape-and-Pose-Disentanglement-for-3D-Zhou-Bhatnagar/73da8ee8bd3370c110051e53b94db78bc742aad6",
            "/paper/PIFu%3A-Pixel-Aligned-Implicit-Function-for-Clothed-Saito-Huang/343da6d4cff7ce8c04270487a1f7a037ea0572d6",
            "/paper/SNARF%3A-Differentiable-Forward-Skinning-for-Neural-Chen-Zheng/586feddfa72cc263a17207a0cb73b992e1ee3a71",
            "/paper/HumanNeRF%3A-Free-viewpoint-Rendering-of-Moving-from-Weng-Curless/f763a59644e27a2215095943224f2564e670a504",
            "/paper/ANR%3A-Articulated-Neural-Rendering-for-Virtual-Raj-Tanke/e850d04207d87734991a8441d6fbe5f8cc3b18ce",
            "/paper/Neural-Actor%3A-Neural-Free-view-Synthesis-of-Human-Liu-Habermann/29da3be81905d577dac9144c1c3cb5c6678f7027"
        ]
    },
    {
        "id": "6683442ae358ae4261fdcde0164f83dd1ccd621b",
        "title": "Learning Attentions: Residual Attentional Siamese Network for High Performance Online Visual Tracking",
        "abstract": "A Residual Attentional Siamese Network (RASNet) for high performance object tracking that mitigates the over-fitting problem in deep network training, but also enhances its discriminative capacity and adaptability due to the separation of representation learning and discriminator learning. Offline training for object tracking has recently shown great potentials in balancing tracking accuracy and speed. However, it is still difficult to adapt an offline trained model to a target tracked online. This work presents a Residual Attentional Siamese Network (RASNet) for high performance object tracking. The RASNet model reformulates the correlation filter within a Siamese tracking framework, and introduces different kinds of the attention mechanisms to adapt the model without updating the model online. In particular, by exploiting the offline trained general attention, the target adapted residual attention, and the channel favored feature attention, the RASNet not only mitigates the over-fitting problem in deep network training, but also enhances its discriminative capacity and adaptability due to the separation of representation learning and discriminator learning. The proposed deep architecture is trained from end to end and takes full advantage of the rich spatial temporal information to achieve robust visual tracking. Experimental results on two latest benchmarks, OTB-2015 and VOT2017, show that the RASNet tracker has the state-of-the-art tracking accuracy while runs at more than 80 frames per second.",
        "publication_year": "2018",
        "authors": [
            "Qiang Wang",
            "Zhu Teng",
            "Junliang Xing",
            "Jin Gao",
            "Weiming Hu",
            "S. Maybank"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "391",
        "reference_count": "55",
        "references": [
            "/paper/Learning-attention-modules-for-visual-tracking-Wang-Meng/f0e93ad223e82f16f2d6fa22a713744614ed8ee3",
            "/paper/Siamese-Attentional-Keypoint-Network-for-High-Gao-Ma/fce3655dc22a783b1f82f09190410f070c7bf42c",
            "/paper/Siamese-Region-Proposal-Networks-and-Attention-for-Dong-Zeng/ae6c283c5fd060da76f9a554c8210ddea6d11248",
            "/paper/Distractor-Aware-Visual-Tracking-by-Online-Siamese-Zha-Wu/03883930b96cf7386c8708bff6c1e6157ac16689",
            "/paper/Discriminative-and-Robust-Online-Learning-for-Zhou-Wang/365d7308a10da357009621d22b1548ab464277c3",
            "/paper/SiamAtt%3A-Siamese-attention-network-for-visual-Yang-He/238764091b1aff3ede9ec16c5ca1db168bac7ad3",
            "/paper/Channel-Attention-Based-Generative-Network-for-Hu-Xuan/77308d202d17f7597d8581d50e7c3598a1724c32",
            "/paper/Efficient-Visual-Tracking-With-Stacked-Attention-Rahman-Fiaz/e085fb462789a8bca7933b5e1c7e9aa0ded8a711",
            "/paper/Visual-Tracking-with-Attentional-Convolutional-Tan-Wei/9189d19304764c819b6e07dfb78f969f1db35628",
            "/paper/An-Enhanced-Visual-Attention-Siamese-Network-That-Zhu-Zou/678f0cdb6b8e59aa2ef6c409b97fecc529059703",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/DeepTrack%3A-Learning-Discriminative-Feature-Online-Li-Li/084bd219dd239dc4c9a02621a5333d3bc1446566",
            "/paper/Good-Features-to-Correlate-for-Visual-Tracking-Gundogdu-Alatan/388d29f001411ff80650f80cf197afc440d98b51",
            "/paper/Attentional-Correlation-Filter-Network-for-Adaptive-Choi-Chang/f2c050fa106b2e6f27d32e21e75ecbdb0cc75f67",
            "/paper/Learning-to-Track-at-100-FPS-with-Deep-Regression-Held-Thrun/5f0850ec47a17f22ba2611a5cb67a30cb02cf306",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Online-Tracking-by-Learning-Discriminative-Saliency-Hong-You/c46b08850b9c458704a3ca69172e6a0d40a6cb7f",
            "/paper/CREST%3A-Convolutional-Residual-Learning-for-Visual-Song-Ma/c2046fc4744a9d358ea7a8e9c21c92fd58df7a64",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/Visual-Tracking-with-Fully-Convolutional-Networks-Wang-Ouyang/bf94906f0d7a8ca9da5f6b86e2a476fde1a34dd0"
        ]
    },
    {
        "id": "6ea6c39593853a5382ee975a0b7bb8222227a6cc",
        "title": "An Adaptive Control Algorithm for Stable Training of Generative Adversarial Networks",
        "abstract": "An Adaptive Adaptive GAN designed to mitigate the impact of instability and saturation in the original by dynamically adjusting the ratio of the training steps of both the generator and discriminator is proposed. Generative adversarial networks (GANs) have shown significant progress in generating high-quality visual samples, however they are still well known both for being unstable to train and for the problem of mode collapse, particularly when trained on data collections containing a diverse set of visual objects. In this paper, we propose an Adaptive <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula>-step Generative Adversarial Network (<inline-formula> <tex-math notation=\"LaTeX\">$\\text{A}k$ </tex-math></inline-formula>-GAN), which is designed to mitigate the impact of instability and saturation in the original by dynamically adjusting the ratio of the training steps of both the generator and discriminator. To accomplish this, we track and analyze stable training curves of relatively narrow datasets and use them as the target fitting lines when training more diverse data collections. Furthermore, we conduct experiments on the proposed procedure using several optimization techniques (e.g., supervised guiding from previous stable learning curves with and without momentum) and compare their performance with that of state-of-the-art models on the task of image synthesis from datasets consisting of diverse images. Empirical results demonstrate that <inline-formula> <tex-math notation=\"LaTeX\">$\\text{A}k$ </tex-math></inline-formula>-GAN works well in practice and exhibits more stable behavior than regular GANs during training. A quantitative evaluation has been conducted on the <inline-formula> <tex-math notation=\"LaTeX\">$Inception~Score$ </tex-math></inline-formula> (<inline-formula> <tex-math notation=\"LaTeX\">$IS$ </tex-math></inline-formula>) and the <inline-formula> <tex-math notation=\"LaTeX\">$relative~inverse~Inception~Score$ </tex-math></inline-formula> (<inline-formula> <tex-math notation=\"LaTeX\">$RIS$ </tex-math></inline-formula>); compared with regular GANs, the former has been improved by 61% and 83%, and the latter by 21% and 60%, on the CelebA and the Anime datasets, respectively.",
        "publication_year": "2019",
        "authors": [
            "Xiaohan Ma",
            "Rize Jin",
            "Kyung-ah Sohn",
            "Joon-Young Paik",
            "Tae-Sun Chung"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "27",
        "references": [
            "/paper/Brain-Tumor-Synthetic-Data-Generation-with-Adaptive-Tariq-Qureshi/7bfa1514a6ffba2fdc5828462f59f7548b094e22",
            "/paper/Lung-cancer-diagnosis-using-Hessian-adaptive-in-Thirumagal-Saruladha/769291ce3ea1824a804d7d67957781bb37732c82",
            "/paper/A-Novel-Cross-Channel-Self-Attention-based-Approach-Xu-Jin/87d542afefd8dace3ac48f98a46833f953b5f591",
            "/paper/A-Machine-Learning-Method-to-Synthesize-Channel-in-Siddiqi-Sait/656dcf2f6614471e8d54d41ff0a5f21813fc23d7",
            "/paper/Improving-Generative-Adversarial-Networks-with-Ma-Jin/b875ddc6a9ebad08ec30b61ecd2b899fc6467025",
            "/paper/Improved-Techniques-for-Training-GANs-Salimans-Goodfellow/571b0750085ae3d939525e62af510ee2cee9d5ea",
            "/paper/Evolutionary-Generative-Adversarial-Networks-Wang-Xu/cbca46c24c800bee41b21ac0258651db54892e80",
            "/paper/ABC-GAN-%3A-Adaptive-Blur-and-Control-for-improved-of-Susmelj-Agustsson/8326d3e57796dad294ab1c14a0688221550098b6",
            "/paper/Self-Attention-Generative-Adversarial-Networks-Zhang-Goodfellow/a8f3dc53e321fbb2565f5925def4365b9f68d1af",
            "/paper/Energy-based-Generative-Adversarial-Network-Zhao-Mathieu/2ba23d9b46027e47b4483243871760e315213ffe",
            "/paper/AdaGAN%3A-Boosting-Generative-Models-Tolstikhin-Gelly/90c10cc419420e18ed8649967c2cf3ae45c42057",
            "/paper/TAC-GAN-Text-Conditioned-Auxiliary-Classifier-Dash-Gamboa/8fb82c76d3a8522a30b443c95facc66d30ef5999",
            "/paper/An-empirical-study-on-evaluation-metrics-of-Xu-Huang/a476f45d867c9a2f953c44e14fec35a6e6af27a0",
            "/paper/Improving-Generative-Adversarial-Networks-with-Warde-Farley-Bengio/6ee38873273aead30c8e561ba807a3032204f870"
        ]
    },
    {
        "id": "6cb124bd46fe728a9165c039ed0c12929e6fd6ea",
        "title": "A Nomogram Combined Radiomics and Kinetic Curve Pattern as Imaging Biomarker for Detecting Metastatic Axillary Lymph Node in Invasive Breast Cancer",
        "abstract": "A nomogram model integrated the Radscore and the kinetic curve pattern could serve as a biomarker for detecting metastatic ALN in patients with invasive breast cancer preoperatively. Objective: To construct and validate a nomogram model integrating the magnetic resonance imaging (MRI) radiomic features and the kinetic curve pattern for detecting metastatic axillary lymph node (ALN) in invasive breast cancer preoperatively. Materials and Methods: A total of 145 ALNs from two institutions were classified into negative and positive groups according to the pathologic or surgical results. One hundred one ALNs from institution I were taken as the training cohort, and the other 44 ALNs from institution II were taken as the external validation cohort. The kinetic curve was computed using dynamic contrast-enhanced MRI software. The preprocessed images were used for radiomic feature extraction. The LASSO regression was applied to identify optimal radiomic features and construct the Radscore. A nomogram model was constructed combining the Radscore and the kinetic curve pattern. The discriminative performance was evaluated by receiver operating characteristic analysis and calibration curve. Results: Five optimal features were ultimately selected and contributed to the Radscore construction. The kinetic curve pattern was significantly different between negative and positive lymph nodes. The nomogram model showed a better performance in both training cohort [area under the curve (AUC) = 0.91, 95% CI = 0.83\u20130.96] and external validation cohort (AUC = 0.86, 95% CI = 0.72\u20130.94); the calibration curve indicated a better accuracy of the nomogram model for detecting metastatic ALN than either Radscore or kinetic curve pattern alone. Conclusion: A nomogram model integrated the Radscore and the kinetic curve pattern could serve as a biomarker for detecting metastatic ALN in patients with invasive breast cancer.",
        "publication_year": "2020",
        "authors": [
            "Yanna Shan",
            "Wen Xu",
            "Rong Wang",
            "Wei Wang",
            "P. Pang",
            "Qijun Shen"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "9",
        "reference_count": "51",
        "references": [
            "/paper/Radiomics-nomogram-for-predicting-axillary-lymph-in-Zhang-Zhang/e4fd861a5ff7daf756f5681955da20197ccf5e6e",
            "/paper/A-Clinical%E2%80%93Radiomics-Model-for-Predicting-Axillary-Gan-Ma/b59b2a1c9af5b06dd93a8ce18d42274403c1a168",
            "/paper/Construction-and-validation-of-a-risk-prediction-in-Luo-Wen/5932be31d37930d3eac7f32fa400102491887cc6",
            "/paper/Radiomics-MRI-for-lymph-node-status-prediction-in-Calabrese-Santucci/eacb1a8594286f277ccdcdfce1c52d947f66f4ad",
            "/paper/Radiomics-Based-on-DCE-MRI-Improved-Diagnostic-to-Ruan-Ding/baaa6f79d271e6c86abd0c61e07f1fd9477916f0",
            "/paper/Radiomic-Signatures-Derived-from-Hybrid-Ultrasound-Bene-Ciurea/9faa7cc6521d46588e25379d4cf2912a4f885c0e",
            "/paper/Beyond-N-Staging-in-Breast-Cancer%3A-Importance-of-Paola-Mazzotta/a47620d406da6b789a57b62217f8ccf7c72d824e",
            "/paper/Radiomics%3A-A-Primer-for-Breast-Radiologists-Grimm/d054c721d7c41d6b0113ae6fdba15d5a8efc8e2f",
            "/paper/Preoperative-Prediction-of-Axillary-Lymph-Node-in-Wang-Sun/255efe5e871378449c93476fe8a0fdba1dd5f927",
            "/paper/Radiomics-Signature-on-Computed-Tomography-Imaging%3A-Jiang-Wang/021e1c95f68f89c097623af597466cebcb08fec4",
            "/paper/Radiomic-nomogram-for-prediction-of-axillary-lymph-Han-Zhu/1fe8ce3b7818426d9a826ea3135acaa6887957e1",
            "/paper/Using-quantitative-image-analysis-to-classify-lymph-Schacht-Drukker/8fab13fe0902ad7e5546eb11ae12ed0815551e6b",
            "/paper/Radiomics-of-Multiparametric-MRI-for-Pretreatment-A-Liu-Li/52fbaa733d540b64f1dbe7fd39f820a6874417cd",
            "/paper/Differentiation-of-benign-and-metastatic-axillary-Yun-Sohn/153052f9e9e1dcf048160360bb188a7baaf2d463",
            "/paper/Contribution-of-kinetic-characteristics-of-axillary-Org%C3%BC%C3%A7-Ba%C5%9Fara/0e56926568aa0d8190e58b2018d95784590762e8",
            "/paper/Radiomic-analysis-for-preoperative-prediction-of-in-Lu-Zhong/3445bd766701fe9da1fd3008ed1a50f6e88c4bf3",
            "/paper/A-new%2C-preoperative%2C-MRI-based-scoring-system-for-He-Xie/3dece049061121a59cf68cdfcb9fa10a06606727",
            "/paper/Diagnostic-accuracy-of-metastatic-axillary-lymph-in-Arslan-Altintoprak/a06a7bed4df35440511e1f41fa2e1c89bdc29a56",
            "/paper/Ex-vivo-MRI-of-axillary-lymph-nodes-in-breast-Luciani-Pigneur/776b49637805aaff567c4938ba9354186512767a"
        ]
    },
    {
        "id": "b0925e7744cb305c4a6b1fcf012b66eec249fc69",
        "title": "BC2NetRF: Breast Cancer Classification from Mammogram Images Using Enhanced Deep Learning Features and Equilibrium-Jaya Controlled Regula Falsi-Based Features Selection",
        "abstract": "A comparison with state-of-the-art (SOTA) technology shows that the obtained proposed framework improved the accuracy, and the confidence interval-based analysis shows consistent results of the proposed framework. One of the most frequent cancers in women is breast cancer, and in the year 2022, approximately 287,850 new cases have been diagnosed. From them, 43,250 women died from this cancer. An early diagnosis of this cancer can help to overcome the mortality rate. However, the manual diagnosis of this cancer using mammogram images is not an easy process and always requires an expert person. Several AI-based techniques have been suggested in the literature. However, still, they are facing several challenges, such as similarities between cancer and non-cancer regions, irrelevant feature extraction, and weak training models. In this work, we proposed a new automated computerized framework for breast cancer classification. The proposed framework improves the contrast using a novel enhancement technique called haze-reduced local-global. The enhanced images are later employed for the dataset augmentation. This step aimed at increasing the diversity of the dataset and improving the training capability of the selected deep learning model. After that, a pre-trained model named EfficientNet-b0 was employed and fine-tuned to add a few new layers. The fine-tuned model was trained separately on original and enhanced images using deep transfer learning concepts with static hyperparameters\u2019 initialization. Deep features were extracted from the average pooling layer in the next step and fused using a new serial-based approach. The fused features were later optimized using a feature selection algorithm known as Equilibrium-Jaya controlled Regula Falsi. The Regula Falsi was employed as a termination function in this algorithm. The selected features were finally classified using several machine learning classifiers. The experimental process was conducted on two publicly available datasets\u2014CBIS-DDSM and INbreast. For these datasets, the achieved average accuracy is 95.4% and 99.7%. A comparison with state-of-the-art (SOTA) technology shows that the obtained proposed framework improved the accuracy. Moreover, the confidence interval-based analysis shows consistent results of the proposed framework.",
        "publication_year": "2023",
        "authors": [
            "Kiran Jabeen",
            "Muhammad Attique Khan",
            "Jamel Balili",
            "M. Alhaisoni",
            "Nouf Abdullah Almujally",
            "Huda Alrashidi",
            "U. Tariq",
            "Jaehyuk Cha"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "5",
        "reference_count": "49",
        "references": [
            "/paper/A-novel-framework-of-multiclass-skin-lesion-from-AI-Ahmad-Shah/bed89a835cf8b203f3ca57fc62f01e60c0fa0b82",
            "/paper/Neuroevolution-of-Convolutional-Neural-Networks-for-Llaguno-Roque-Barrientos-Mart%C3%ADnez/1732a276e62c63801b46d8f0ee2e62ae7148de22",
            "/paper/Classification-of-skin-cancer-stages-using-a-AHP-of-Samiei-Hassani/92bf3b325d8ccb0b5223d7637ded11e560e9d448",
            "/paper/Multi-classification-deep-neural-networks-for-of-Malik-Naeem/4ad698fc5f6699c9f5798c9f89a31f108a6c5dd3",
            "/paper/Deep-learning-based-ensemble-model-for-of-breast-Nemade-Pathak/73707e8143c0ca11467f0686ea254c3ea91a8906",
            "/paper/Automated-Breast-Cancer-Detection-Models-Based-on-Alruwaili-Gouda/e5672449a04bea6f59d1631058510bd3a42b494d",
            "/paper/A-Hybrid-Deep-Transfer-Learning-of-CNN-Based-LR-PCA-Samee-Alhussan/53dbfdb4d3bf8d498586b309a98c048b6da3e422",
            "/paper/Breast-Cancer-Classification-from-Ultrasound-Images-Jabeen-Khan/34a91f535f841db560062e7266ac87069f282615",
            "/paper/Deep-Learning-RN-BCNN-Model-for-Breast-Cancer-Siddeeq-Li/dd692fed303438bbf94ab4d2f4e7ed4a850a1cea",
            "/paper/Vision-Transformer-Based-Transfer-Learning-for-Ayana-Dese/e7b94521b38c05f66f6a337c80be5956084585ac",
            "/paper/Transfer-Learning-in-Breast-Mammogram-Abnormalities-Falcon%C3%AD-P%C3%A9rez/e4cbb9a30b37f606da624bd33d83788f413f4946",
            "/paper/Malignant-and-nonmalignant-classification-of-breast-Houby-Yassin/0677d83fbc116d6a9eb71ab3e73d45350720033d",
            "/paper/Computer-aided-diagnosis-for-breast-cancer-using-Aljuaid-Alturki/688bc5b6b171019afb7c56887c374a77253d5782",
            "/paper/Automated-diagnosis-of-breast-cancer-using-A-deep-Muduli-Dash/9718aeeb15294208b03f610dc3bdd6badc6df346",
            "/paper/Breast-Tumor-Detection-and-Classification-in-Images-Mohiyuddin-Basharat/0bac967d54ecbca93cadeca63edf273e9ee9b6e6"
        ]
    },
    {
        "id": "230b8255925d56b72444df3f358f5851b3158b59",
        "title": "Automated Detection and Classification of Oral Lesions Using Deep Learning for Early Detection of Oral Cancer",
        "abstract": "Two deep learning based computer vision approaches were assessed for the automated detection and classification of oral lesions for the early detection of oral cancer, these were image classification with ResNet-101 and object detection with the Faster R-CNN. Oral cancer is a major global health issue accounting for 177,384 deaths in 2018 and it is most prevalent in low- and middle-income countries. Enabling automation in the identification of potentially malignant and malignant lesions in the oral cavity would potentially lead to low-cost and early diagnosis of the disease. Building a large library of well-annotated oral lesions is key. As part of the MeMoSA\u00ae(Mobile Mouth Screening Anywhere) project, images are currently in the process of being gathered from clinical experts from across the world, who have been provided with an annotation tool to produce rich labels. A novel strategy to combine bounding box annotations from multiple clinicians is provided in this paper. Further to this, deep neural networks were used to build automated systems, in which complex patterns were derived for tackling this difficult task. Using the initial data gathered in this study, two deep learning based computer vision approaches were assessed for the automated detection and classification of oral lesions for the early detection of oral cancer, these were image classification with ResNet-101 and object detection with the Faster R-CNN. Image classification achieved an F1 score of 87.07% for identification of images that contained lesions and 78.30% for the identification of images that required referral. Object detection achieved an F1 score of 41.18% for the detection of lesions that required referral. Further performances are reported with respect to classifying according to the type of referral decision. Our initial results demonstrate deep learning has the potential to tackle this challenging task.",
        "publication_year": "2020",
        "authors": [
            "R. Welikala",
            "P. Remagnino",
            "Jian Han Lim",
            "Chee Seng Chan",
            "Senthilmani Rajendran",
            "T. G. Kallarakkal",
            "R. Zain",
            "R. Jayasinghe",
            "J. Rimal",
            "A. Kerr",
            "R. Amtha",
            "K. Patil",
            "W. Tilakaratne",
            "J. Gibson",
            "S. Cheong",
            "S. Barman"
        ],
        "related_topics": [
            "Medicine",
            "Computer Science"
        ],
        "citation_count": "73",
        "reference_count": "44",
        "references": [
            "/paper/Automated-Detection-and-Classification-of-Oral-Deep-Tanriver-Tekke%C5%9Fin/bad52faef74960f458f3dfab35f7714335d334c0",
            "/paper/Automated-Oral-Cancer-Detection-using-Deep-Muqeet-Mohammad/bfa71958d8b449e86cd14768d6396fe6d8056b79",
            "/paper/Fine-Tuning-Deep-Learning-Architectures-for-Early-Welikala-Remagnino/48d8a6abcab78cb8cd0dd336af942629f8d506de",
            "/paper/Oral-Cancer-Detection-using-Machine-Learning-and-GeethaKiranASanathkumarM/9ce1044010ac3577f42c00e67bf13bbd6a91a4fd",
            "/paper/Paper%E2%80%94An-Ensemble-Deep-Neural-Network-Approach-for-Kiran/ebccf83698193a4e7c4642d3e1732bcfc5c2ad49",
            "/paper/Intelligent-Deep-Learning-Enabled-Oral-Squamous-and-Alanazi-Khayyat/81eacfbe64d362346b311f85a1f7f95df8d8edce",
            "/paper/Automatic-classification-and-detection-of-oral-in-Warin-Limprasert/a932aa4621106478255a5b1defd17236ca2e6c35",
            "/paper/Detection-of-oral-squamous-cell-carcinoma-in-using-Fl%C3%BCgge-Gaudin/092ffdedfb93eda21e55bd4faa8c8bc51ecfa31b",
            "/paper/Oral-Cancer-Detection-Using-Deep-Learning-Approach-Yadav-Ujjainkar/e116b8836388ef6a84624b4542136fdd28751d75",
            "/paper/Convolutional-Neural-Network-Based-Clinical-of-Oral-Camalan-Mahmood/eda4b0195fc8d0496be618629c2274e4e9023ba0",
            "/paper/Tissue-Level-Based-Deep-Learning-Framework-for-of-Gupta-Kaur/357808ea06135fc8f954f4721c4798d8b7dcfaf7",
            "/paper/Automated-oral-cancer-identification-using-images%3A-Krishnan-Venkatraghavan/8571c603c489c1beaec60fa3d14a2474dba0f248",
            "/paper/Automatic-Classification-of-Cancerous-Tissue-in-of-Aubreville-Knipfer/6d23efb8c59592b29f876f310ab480f303c20d8a",
            "/paper/Utilizing-Mask-R-CNN-for-Detection-and-Segmentation-Anantharaman-Velazquez/400ab4404ac1b2701db7622e1456a90257de92d1",
            "/paper/Computer-assisted-medical-image-classification-for-Jeyaraj-Nadar/11ded8e630c3d52bf3563c30b72cc06f6e66af7f",
            "/paper/Active-deep-learning%3A-Improved-training-efficiency-Folmsbee-Liu/4645e7882f3196213eb86187cbf88504521a051c",
            "/paper/Detecting-and-classifying-lesions-in-mammograms-Ribli-Horv%C3%A1th/3978804d0824c3bf1e17a1d0cd2e9734060fa058",
            "/paper/Automatic-classification-of-dual-modalilty%2C-oral-Song-Sunny/eeebfa2343ef9180d647d7bbec008e249d250d88",
            "/paper/An-Early-Diagnosis-of-Oral-Cancer-based-on-Neural-Xu-Liu/9755711bf7dbe47e2ff1457641850c2b5e8c2bad",
            "/paper/m-Health-for-Early-Detection-of-Oral-Cancer-in-Low-Haron-Zain/dc0597b1856c3bd4e44b1015f4db3e90c5f995da"
        ]
    },
    {
        "id": "5576b0d7a5ee8a2c9d0eae8f20bfe34d905ac923",
        "title": "Spot quantification in two dimensional gel electrophoresis image analysis: comparison of different approaches and presentation of a novel compound fitting algorithm",
        "abstract": "A ready-to-use algorithm that uses fitting of two dimensional Gaussian function curves for the extraction of data from two dimensional gel electrophoresis (2-DE) images is proposed that shows higher precision and accuracy than other approaches when applied to exposure time series and standard gels. BackgroundVarious computer-based methods exist for the detection and quantification of protein spots in two dimensional gel electrophoresis images. Area-based methods are commonly used for spot quantification: an area is assigned to each spot and the sum of the pixel intensities in that area, the so-called volume, is used a measure for spot signal. Other methods use the optical density, i.e. the intensity of the most intense pixel of a spot, or calculate the volume from the parameters of a fitted function.ResultsIn this study we compare the performance of different spot quantification methods using synthetic and real data. We propose a ready-to-use algorithm for spot detection and quantification that uses fitting of two dimensional Gaussian function curves for the extraction of data from two dimensional gel electrophoresis (2-DE) images. The algorithm implements fitting using logical compounds and is computationally efficient. The applicability of the compound fitting algorithm was evaluated for various simulated data and compared with other quantification approaches. We provide evidence that even if an incorrect bell-shaped function is used, the fitting method is superior to other approaches, especially when spots overlap. Finally, we validated the method with experimental data of urea-based 2-DE of A\u03b2 peptides andre-analyzed published data sets. Our methods showed higher precision and accuracy than other approaches when applied to exposure time series and standard gels.ConclusionCompound fitting as a quantification method for 2-DE spots shows several advantages over other approaches and could be combined with various spot detection methods.The algorithm was scripted in MATLAB (Mathworks) and is available as a supplemental file.",
        "publication_year": "2014",
        "authors": [
            "J. Brauner",
            "T. Groemer",
            "Armin Stroebel",
            "Simon Grosse-Holz",
            "T. Oberstein",
            "J. Wiltfang",
            "J. Kornhuber",
            "J. Maler"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "20",
        "reference_count": "32",
        "references": [
            "/paper/Mixture-Modeling-of-2-D-Gel-Electrophoresis-Spots-Marczyk/1e2dadc12c0dc6d7574aa830221abe08114810f4",
            "/paper/SpotDSQ%3A-A-2D-Gel-Image-Analysis-Tool-for-Protein-Kostopoulou-Katsigiannis/e1a898c136cb2c544b02de186f2f9a3fbd63d836",
            "/paper/Preprocessing-of-2-Dimensional-Gel-Electrophoresis-Goez-Torres-Madronero/a43d6a662ed51d3c6d9c9fd803111c619abdb6d5",
            "/paper/Improved-Detection-of-2D-Gel-Electrophoresis-Spots-Marczyk/aeecbda3acfeedbf7daf540d8493dfe443e3a022",
            "/paper/UWS-Academic-Portal-SpotDSQ%3A-A-2D-gel-image-tool-Kostopoulou/66f8911dbdb3e45a9eb131f9469e7693f80cceeb",
            "/paper/Spot-counting-on-fluorescence-in-situ-hybridization-Liu-Sa/9711ca9ef5245ed41965b7faa715dddba2112a34",
            "/paper/Impulse-noise-removal-in-two-dimensional-images-on-Ou-Zhang/4a7548fb04a4a0b5862b9cafbd4b7ebbd49eb06b",
            "/paper/Two-Dimensional-Zymography-of-Proteases-from-Duck-Wilkesman-Padr%C3%B3n/d281ef55ad918bdeaaa1325a2d8c2a156f8b1e73",
            "/paper/Comparative-phosphoproteomic-analysis-of-developing-Cao-Zhou/de9f8da0d2cdb07571a6f5b78c71f50a831cd891",
            "/paper/Proteomic-consequences-of-TDA1-deficiency-in-kinase-M%C3%BCller-Lesur/70fb0054565c097ee4f4ac8bafd6bd3cc407aa76",
            "/paper/Spot-quantification-in-two-dimensional-gel-image-of-Brauner-Groemer/05494d0bfff5df9d456d4b879fa748c5cfe69637",
            "/paper/New-approach-for-segmentation-and-quantification-of-Anjos-M%C3%B8ller/02ec43c470666ff4d3366df490514781763181b7",
            "/paper/An-image-analysis-suite-for-spot-detection-and-spot-Srinark-Kambhamettu/8c008860d6864d0dd17883c18f03f3926338f62f",
            "/paper/Quantitative-comparison-and-evaluation-of-two-image-Raman-Cheung/2f1622dc8c3dbbe3a005511d9da63fc3099810cb",
            "/paper/Statistical-models-of-shape-for-the-analysis-of-in-Rogers-Graham/6ebf231010a6f7b970885c71d96e7d362eabf42d",
            "/paper/A-Novel-Gaussian-Extrapolation-Approach-for-2D-Gel-Natale-Caiazzo/f7fd49f4e7a4c24e082d5a82e97fe11c8e846770",
            "/paper/The-role-of-bioinformatics-in-two%E2%80%90dimensional-gel-Dowsey-Dunn/773c13a7a2086dc8e1e8f3d08b2b4d9f1668bb1c",
            "/paper/Image-Analysis-Workflow-for-2-D-Electrophoresis-on-Natale-Maresca/3fd7cb53920b411ce0e8b6ccaf517a66ee58678f",
            "/paper/Protein-spot-detection-and-quantification-in-2%E2%80%90DE-Tsakanikas-Manolakos/981f7367347008ab3c67604835324b095fddda01",
            "/paper/The-QUEST-system-for-quantitative-analysis-of-gels.-Garrels/726463c85aa3ed8518dac08738f8cefe8a31e5ee"
        ]
    },
    {
        "id": "fa0a7b4988b593112464cf06845f5fa0430421cf",
        "title": "A Framework for Evaluating Land Use and Land Cover Classification Using Convolutional Neural Networks",
        "abstract": "A general CNN is proposed, with a fixed architecture and parametrization, to achieve high accuracy on LULC classification over RS data from different sources such as radar and hyperspectral, and demonstrates that the CNN outperforms the rest of techniques. Analyzing land use and land cover (LULC) using remote sensing (RS) imagery is essential for many environmental and social applications. The increase in availability of RS data has led to the development of new techniques for digital pattern classification. Very recently, deep learning (DL) models have emerged as a powerful solution to approach many machine learning (ML) problems. In particular, convolutional neural networks (CNNs) are currently the state of the art for many image classification tasks. While there exist several promising proposals on the application of CNNs to LULC classification, the validation framework proposed for the comparison of different methods could be improved with the use of a standard validation procedure for ML based on cross-validation and its subsequent statistical analysis. In this paper, we propose a general CNN, with a fixed architecture and parametrization, to achieve high accuracy on LULC classification over RS data from different sources such as radar and hyperspectral. We also present a methodology to perform a rigorous experimental comparison between our proposed DL method and other ML algorithms such as support vector machines, random forests, and k-nearest-neighbors. The analysis carried out demonstrates that the CNN outperforms the rest of techniques, achieving a high level of performance for all the datasets studied, regardless of their different characteristics.",
        "publication_year": "2019",
        "authors": [
            "Manuel Carranza-Garc\u00eda",
            "Jorge Garc\u00eda-Guti\u00e9rrez",
            "Jos\u00e9 Crist\u00f3bal Riquelme Santos"
        ],
        "related_topics": [
            "Computer Science",
            "Environmental Science"
        ],
        "citation_count": "106",
        "reference_count": "49",
        "references": [
            "/paper/Using-Deep-Learning-Approach-for-Land-Use-and-based-Agarwal-Goel/44607c13992295ad450b69ab940a38b634929b48",
            "/paper/Multiclass-Land-Use-and-Land-Cover-Classification-Arrechea-Castillo-Correa/6ee3743bee387bff38c2768defd28e69e6a63d6c",
            "/paper/Deep-Learning-Methods-for-Land-Cover-and-Land-Use-A-Alem-Kumar/e6aa969ce981efb298d7e936a195fddfe71a25fd",
            "/paper/Automatic-land-cover-classification-of-dualpol-data-Memon-Parikh/cbd3f3e1fb36500819c3ed1b08185b6d1ee0bc9a",
            "/paper/Detecting-Land-Abandonment-in-%C5%81%C3%B3d%C5%BA-Voivodeship-Krysiak-Papi%C5%84ska/2511a4b449d55b1b2d578b000e2c543f1259b838",
            "/paper/Different-Spectral-Domain-Transformation-for-Land-Lee-Han/52f99f4154b810ea7208b5e5c88b60b50bc58234",
            "/paper/Medium-Resolution-Satellite-Image-Classification-in-Obianuju-Agwu/7799ee408d8f7b615b565c2fade0e09f1b0f6beb",
            "/paper/Land-Use-and-Land-Cover-Classification-in-Semi-Arid-Ali-Johnson/f75af20216daa2fc1198e1145c384e50c38d69cb",
            "/paper/Application-of-Different-CNN-Models-for-Land-Use-Akeboshi-Ducut/c470a7ce6dd7a662f86c311bd7ebac8bc64d454a",
            "/paper/Development-of-Land-Cover-Classification-Model-AI-Park-Jang/3ffb489760625bdfab3a2b2ba12557c8b03f6508",
            "/paper/Deep-Learning-Classification-of-Land-Cover-and-Crop-Kussul-Lavreniuk/7a9e471e31ac156cf22a5e2a5c1463697df866ab",
            "/paper/Very-Deep-Convolutional-Neural-Networks-for-Complex-Mahdianpari-Salehi/55ae79ca10937e3921003b4f1172467ae0be9065",
            "/paper/A-patch-based-convolutional-neural-network-for-Sharma-Liu/2e9500fbbf38bce4196db460516664b44fbe301a",
            "/paper/Land-Use-Classification-in-Remote-Sensing-Images-by-Castelluccio-Poggi/caa866d068dfcbc410e6b71bf564f32f6470a871",
            "/paper/Stacked-Autoencoder-based-deep-learning-for-image-a-Li-Fu/7a46bd1e11087838235ba5174567e15d3f6d063d",
            "/paper/Deep-learning-for-remote-sensing-image-A-survey-Li-Zhang/fdb7c847a947c2249882cabf0ba4bf070c6c667e",
            "/paper/Deep-Feature-Extraction-and-Classification-of-Based-Chen-Jiang/10fb16414324a5db44f5d830adcb4810af59eed0",
            "/paper/Spectral-Spatial-Classification-of-Hyperspectral-3D-Li-Zhang/8d81ebf99a7d8eec942503acf219c970bf532825",
            "/paper/Unsupervised-Deep-Feature-Extraction-for-Remote-Romero-Gatta/3ede3ddf60740270d33b52327f5857d6ff481d41",
            "/paper/Spectral%E2%80%93Spatial-Classification-of-Hyperspectral-on-Chen-Zhao/f9d119346b0773ea83251598fa5305bc75bac8ab"
        ]
    },
    {
        "id": "32be513855e6129ef0c7610365caf0b2ea74061a",
        "title": "Phase field modeling of flexoelectricity in solid dielectrics",
        "abstract": "A phase field model is developed to study the flexoelectricity in nanoscale solid dielectrics, which exhibit both structural and elastic inhomogeneity. The model is established for an elastic homogeneous system by taking into consideration all the important non-local interactions, such as electrostatic, elastic, polarization gradient, as well as flexoelectric terms. The model is then extended to simulate a two-phase system with strong elastic inhomogeneity. Both the microscopic domain structures and the macroscopic effective piezoelectricity are thoroughly studied using the proposed model. The results obtained show that the largest flexoelectric induced polarization exists at the interface between the matrix and the inclusion. The effective piezoelectricity is greatly influenced by the inclusion size, volume fraction, elastic stiffness, and the applied stress. The established model in the present study can provide a fundamental framework for computational study of flexoelectricity in nanoscale solid dielectrics, since various boundary conditions can be easily incorporated into the phase field model.",
        "publication_year": "2015",
        "authors": [
            "Haitao Chen",
            "Shudao Zhang",
            "A. Soh",
            "W. Yin"
        ],
        "related_topics": [
            "Physics"
        ],
        "citation_count": "8",
        "reference_count": "52",
        "references": [
            "/paper/A-meshfree-formulation-for-large-deformation-of-for-Zhuang-Nanthakumar/7a7542ae9a5914a51fabdae6537c2fe387f6e918",
            "/paper/An-isogeometric-approach-to-flexoelectric-effect-in-Liu-Wang/c10e6ff2c9fb3a8f5d2550b70f5d4ce769de614c",
            "/paper/Phase-Field-Modeling-of-Relaxor-Ferroelectrics-and-Wang/5d13ff6f634c832704b65a84649fef3698ab1acb",
            "/paper/Apparent-Flexoelectricity-Due-to-Heterogeneous-Yvonnet-Chen/799b84dafc70ac4c93d2b7b8f4de98e176ae716a",
            "/paper/Flexoelectricity-in-solids%3A-Progress%2C-challenges%2C-Wang-Gu/4146cc88daa24992927c096f8191d1942a3426cd",
            "/paper/Topology-optimization-of-flexoelectric-composites-Chen-Yvonnet/9701969be5463ce2088e3d6dd463cbe253704fbd",
            "/paper/Flexoelectricity-in-polycrystalline-TiO2-thin-films-Maier-Schneider/b607035004d7188bcbe3547474317f177b31d185",
            "/paper/Bending-and-Buckling-Analysis-of-Functionally-Using-Ren-Qing/e92a835d96d5456db090230fe39ad9944ebd87fb",
            "/paper/Phase-field-modeling-of-flexoelectric-effects-in-Chen-Soh/64527aca066f496631154bd536b4528c62afce12",
            "/paper/A-theory-of-flexoelectricity-with-surface-effect-Shen-Hu/95494bc33cecf7b96c5fe430f4dc88f602b1023d",
            "/paper/Size-effects-on-electromechanical-coupling-fields-a-Zhang-Jiang/01e54b5dfc403eebaab35560bd10e44e54e4fefe",
            "/paper/Fundamentals-of-flexoelectricity-in-solids-Yudin-Tagantsev/cf52271a594390db50e5bd441460db7de6288542",
            "/paper/Effects-of-surface-and-flexoelectricity-on-a-Liang-Hu/29b2a3ce89702a3f653222de4743c52515101360",
            "/paper/Phase-field-microelasticity-theory-and-modeling-of-Wang-Jin/89b55fc7bbf5f7e113eb21319b89b869da4ea2b4",
            "/paper/A-study-of-flexoelectric-coupling-associated-field-Ma/6dfad59df028001e5ab82422c302d6044f2484d0",
            "/paper/Phase-transitions-and-domain-structures-of-Phase-Wang-Ma/e716e20c2e64cdc5e90c417a04082e9a284cce84",
            "/paper/External-uniform-electric-field-removing-the-effect-Zhou-Hong/be32779b55929b0ae216f751542a76428bbee2d7",
            "/paper/Influence-of-flexoelectric-effects-on-multiferroic-Chen-Soh/ce7756691bdd1417a7add8fe7cffa46ff4394f5f"
        ]
    },
    {
        "id": "a0378320ca434a40c392df7a964d020afd047869",
        "title": "Pareto-Based Dominant Graph: An Efficient Indexing Structure to Answer Top-K Queries",
        "abstract": "This paper investigates the intrinsic connection between top-k queries and dominant relationships between records, and based on which, an efficient layer-based indexing structure, Pareto-Based Dominant Graph (DG), is proposed to improve the query efficiency. Given a record set D and a query score function F, a top-k query returns k records from D, whose values of function F on their attributes are the highest. In this paper, we investigate the intrinsic connection between top-k queries and dominant relationships between records, and based on which, we propose an efficient layer-based indexing structure, Pareto-Based Dominant Graph (DG), to improve the query efficiency. Specifically, DG is built offline to express the dominant relationship between records and top-k query is implemented as a graph traversal problem, i.e., Traveler algorithm. We prove theoretically that the size of search space (that is the number of retrieved records from the record set to answer top-k query) in our algorithm is directly related to the cardinality of skyline points in the record set (see Theorem 3). Considering I/O cost, we propose cluster-based storage schema to reduce I/O cost in Traveler algorithm. We also propose the cost estimation methods in this paper. Based on cost analysis, we propose an optimization technique, pseudorecord, to further improve the search efficiency. In order to handle the top-k query in the high-dimension record set, we also propose N-Way Traveler algorithm. In order to handle DG maintenance efficiently, we propose \u201cInsertion\u201d and \u201cDeletion\u201d algorithms for DG. Finally, extensive experiments demonstrate that our proposed methods have significant improvement over its counterparts, including both classical and state art of top-k algorithms.",
        "publication_year": "2008",
        "authors": [
            "Lei Zou",
            "Lei Chen"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "83",
        "reference_count": "34",
        "references": [
            "/paper/Close-Dominance-Graph%3A-An-Efficient-Framework-for-Santoso-Chiu/7ec1909b3c7b29c4a7b09f56f6449c0ccb7d08e6",
            "/paper/An-Efficient-Approach-on-Answering-Top-k-Queries-Li-Xu/d0c3af88cfc14d63c6f94f3a29c0f8f78b7fffc3",
            "/paper/Reverse-top-k-queries-Vlachou-Doulkeridis/a17bdc32e9a964bb20969f82dec7a805631e7957",
            "/paper/Querying-Improvement-Strategies-Yang-Cai/118df597733525bd4ea767aa44fb5d0f918cdf62",
            "/paper/Efficient-processing-of-exact-top-k-queries-over-Pang-Ding/0eaeea8842d4d4f820ec6f406fe31fc87bea1ca3",
            "/paper/Toward-Scalable-Indexing-for-Top-k-Queries-Lee-Cho/a8f9b80796e739bbdf22844748f6598c1fe29196",
            "/paper/Efficient-Processing-of-Exact-Top-k-Queries-over-Pang-Ding/5b72ef8a3f015f378e92ffbc0816d929b524abf3",
            "/paper/Distributed-top-k-query-processing-on-data-with-Amagata-Hara/a7ff658bb74de488597748c31c7ca7be7ee5b81d",
            "/paper/Indexing-Reverse-Top-k-Queries-Chester-Thomo/b095b3d25e251cb8bce19fc583804cec67ac132b",
            "/paper/An-Efficient-Massive-Data-Retrieval-Algorithm-Based-Peng/08d469c8a92b1d9f098a95329c078359083a7dde",
            "/paper/Answering-top-k-queries-with-multi-dimensional-the-Xin-Han/f7af2736c3a2a01e47600dce1596d8c35da82ecc",
            "/paper/Answering-top-k-queries-using-views-Das-Gunopulos/0f7e7d9add82a37b44764bd706b60fddd97b13fd",
            "/paper/Supporting-top-kjoin-queries-in-relational-Ilyas-Aref/a4bfe4bc37cbf976bc414a6e93e241844f839e9e",
            "/paper/RankSQL%3A-query-algebra-and-optimization-for-top-k-Li-Chang/47ef171b092a4cbb8e9e11122887b5c3e7c6eee5",
            "/paper/Towards-robust-indexing-for-ranked-queries-Xin-Chen/6156864e6fd0836726fc7e7ce1d5bdcff112fe27",
            "/paper/IO-Top-k%3A-index-access-optimized-top-k-query-Bast-Majumdar/520ab7c9bc7dd96698fcedca2fde6f1aa52e002c",
            "/paper/Efficient-Processing-of-Ranked-Queries-with-Jin-Ester/c4b5c9a174d76da48aa542d0355637e25133f9dc",
            "/paper/Efficient-Aggregation-of-Ranked-Inputs-Mamoulis-Cheng/c9637e53479dd5d6466440b88bfdcd43f8d8b7dc",
            "/paper/Top-k-selection-queries-over-relational-databases%3A-Bruno-Chaudhuri/10927ae8a906b482330a95b411285e2e4e407ca6",
            "/paper/Branch-and-bound-processing-of-ranked-queries-Tao-Hristidis/6de2e907d57d03aa0bca9c6eff1b93a7656df0c5"
        ]
    },
    {
        "id": "7aea5ee6a11ca4666aeedea9001490e710c9a0fb",
        "title": "A Robust 3D-Based Color Correction Approach for Texture Mapping Applications",
        "abstract": "The main goal of this work is to produce textured meshes free of texture seams through a process of color correcting all images of the scene, and shows that the proposed approach outperforms all others, both in qualitative and quantitative metrics. Texture mapping of 3D models using multiple images often results in textured meshes with unappealing visual artifacts known as texture seams. These artifacts can be more or less visible, depending on the color similarity between the used images. The main goal of this work is to produce textured meshes free of texture seams through a process of color correcting all images of the scene. To accomplish this goal, we propose two contributions to the state-of-the-art of color correction: a pairwise-based methodology, capable of color correcting multiple images from the same scene; the application of 3D information from the scene, namely meshes and point clouds, to build a filtering procedure, in order to produce a more reliable spatial registration between images, thereby increasing the robustness of the color correction procedure. We also present a texture mapping pipeline that receives uncorrected images, an untextured mesh, and point clouds as inputs, producing a final textured mesh and color corrected images as output. Results include a comparison with four other color correction approaches. These show that the proposed approach outperforms all others, both in qualitative and quantitative metrics. The proposed approach enhances the visual quality of textured meshes by eliminating most of the texture seams.",
        "publication_year": "2022",
        "authors": [
            "Daniel Coelho",
            "Lucas Dal\u2019Col",
            "Tiago Madeira",
            "Paulo Dias",
            "Miguel Oliveira"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "56",
        "references": [
            "/paper/A-Sequential-Color-Correction-Approach-for-Texture-Dal%E2%80%99Col-Coelho/49c2f3de221ed0a688731b6812540cc11d5a0300",
            "/paper/Robust-Texture-Mapping-Using-RGB-D-Cameras-Oliveira-Lim/e46b23ba2f6fce4fdecbe677b807d7a5300aa415",
            "/paper/Seamless-Texture-Optimization-for-RGB-D-Fu-Yan/d2b89d018b8423473f7478261508c70022c279d6",
            "/paper/Seamless-image-based-texture-atlases-using-blending-All%C3%A8ne-Pons/9f7be8afe4662fd01e850ebcc70576c520538edc",
            "/paper/Seamless-Mosaicing-of-Image-Based-Texture-Maps-Lempitsky-Ivanov/8f7ff9e9474a5eb1bac32c0a649d6ccd152a3674",
            "/paper/Color-correction-for-stereoscopic-image-based-on-Zheng-Niu/35c99e18afa16026de8d02f1c03277661403214a",
            "/paper/Fast-color-correction-using-principal-regions-in-Zhang-Georganas/322f2f0a9eab458bc965b98b10062e6dcc6435b9",
            "/paper/Seamless-Montage-for-Texturing-Models-Gal-Wexler/c692e0d5c2ec09c97d5768361541aade6addee9b",
            "/paper/A-Probabilistic-Approach-for-Color-Correction-in-Oliveira-Sappa/281bd853b01005efae2e4bf667cd97a05a1325ba",
            "/paper/Masked-photo-blending%3A-Mapping-dense-photographic-Callieri-Cignoni/8e522e626b607a95e599361f078c975079b389c0",
            "/paper/Probabilistic-moving-least-squares-with-spatial-for-Hwang-Lee/d716dc2d6f524cf204c33570b0a763f9c33484e2"
        ]
    },
    {
        "id": "93593630a30eeb44905cd1f3d512ea814fbf1f62",
        "title": "Siamese Centerness Prediction Network for Real-Time Visual Object Tracking",
        "abstract": "The proposed framework has achieved highly competitive and better performance compared with the state-of-the-art trackers and is simple and relatively efficient due to the fact that it avoids the need for complicated hyper-parameters of the anchor boxes. Siamese network has been proven to achieve excellent results for visual object tracking where the SiamFC(Fully-Convolutional)is among the most well-known seminar work. Recently, with the successful application of the Region Proposal Network (RPN) in object detection, siamese networks combined with RPN have achieved good performance in visual tracking tasks. However, RPN requires the selection of the number, aspect ratio and size of the anchor boxes and these anchor-related parameters more often than not, need manual intervention and tuning. In this work, we first add a channel-aware module in the siamese network to obtain the more discriminative features. Thereafter, we propose an anchor-free strategy to replace the RPN module. The proposed framework consists of two networks, namely, the Siamese network and the Centerness Prediction network (CPN). We call the proposed method SiamCPN. In the Siamese network, Resnet50 is used as the backbone. SiamCPN is simple and relatively efficient due to the fact that it avoids the need for complicated hyper-parameters of the anchor boxes. Extensive experimental results on four visual tracking benchmark datasets, OTB100, VOT2016, UAV123 and LaSOT show that the proposed framework has achieved highly competitive and better performance compared with the state-of-the-art trackers. SiamCPN can run at 60 frames per second (FPS) on an AMD processor with 2 RTX3090.",
        "publication_year": "2022",
        "authors": [
            "Yue Wu",
            "Chengtao Cai",
            "C. Yeo"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "43",
        "references": [
            "/paper/SiamADT%3A-Siamese-Attention-and-Deformable-Features-Wang-Cao/bacf3f214aff330949cc4d1be6773d9c1e52f6aa",
            "/paper/A-Novel-Matching-Operator-for-Visual-Object-Abbasi-Rezaeian/01bcd685a212682c1b2b889280c75335c97c4630",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/SiamAtt%3A-Siamese-attention-network-for-visual-Yang-He/238764091b1aff3ede9ec16c5ca1db168bac7ad3",
            "/paper/Siamese-Box-Adaptive-Network-for-Visual-Tracking-Chen-Zhong/cce1fecc800d2782da638f3060d5b2e887739f74",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/Faster-R-CNN%3A-Towards-Real-Time-Object-Detection-Ren-He/424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "/paper/Learning-Attentions%3A-Residual-Attentional-Siamese-Wang-Teng/6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "/paper/SiamFC%2B%2B%3A-Towards-Robust-and-Accurate-Visual-with-Xu-Wang/be412c7c7128cf91455233b652d6c94a6001a7c8"
        ]
    },
    {
        "id": "82a86882dc6e03648fa0399fb6e705999fe5223c",
        "title": "Time-Optimal Trajectory Planning for the Manipulator Based on Improved Non-Dominated Sorting Genetic Algorithm II",
        "abstract": "A novel solution that generates a time-optimal path for a manipulator while adhering to its kinematic limitations is introduced, which has reduced the total running time by 19.26%, effectively improving the working efficiency of the manipulator. To address the issues of low efficiency and lengthy running time associated with trajectory planning for 6-degree-of-freedom manipulators, this paper introduces a novel solution that generates a time-optimal path for a manipulator while adhering to its kinematic limitations. The proposed method comprises several stages. Firstly, the kinematics of the manipulator are analyzed. Secondly, the manipulator\u2019s joint-space path points are interpolated via the quintic B-spline curve. Subsequently, the non-dominated sorting genetic algorithm II (NSGA-II) is improved by applying reinforcement learning to optimize its crossover and mutation probabilities, and the variable neighborhood search (VNS) algorithm is integrated to enhance its local search capability. Finally, the joint increments and running time of the manipulator are optimized using the improved NSGA-II, and the time-optimal trajectory is then determined by simulation on MATLAB. Furthermore, compared with other conventional optimization methods, the proposed approach has reduced the total running time by 19.26%, effectively improving the working efficiency of the manipulator.",
        "publication_year": "2023",
        "authors": [
            "Jie Hou",
            "Juan Du",
            "Zhi Chen"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "32",
        "references": [
            "/paper/A-Novel-Bat-Algorithm-with-Asymmetrical-Weighed-in-Cao-Wang/e94d50fb4ca56ebe03d3956ae212306249a06409",
            "/paper/Time-optimal-trajectory-planning-of-serial-based-on-Zhang-Wang/aa8fce83ee08bd429f3854bb35d80c2bc2244012",
            "/paper/Multi-objective-optimal-trajectory-planning-for-in-Zhang-Shi/f628a70d1c2e8443ca6e3699e9787bc6d336206a",
            "/paper/Online-Time-Optimal-Trajectory-Planning-for-Robotic-Liu-Guo/9cad3721e9b500cd8846f9ffebd0f4a53b7cacd3",
            "/paper/Optimal-trajectory-planning-for-robotic-using-Gao-Mu/a0ee93f4b11d75816fca0b78aa821f7f295a6c97",
            "/paper/A-multi-objective-trajectory-planning-method-based-Chen-Li/d904ec635f76ce85e51a033337987e8445aae0d9",
            "/paper/Jerk-Optimized-Motion-Planning-of-Redundant-Space-Shrivastava-Dalla/f23506f19cefa0182ddfdb3e302ba3031e3e0c8b",
            "/paper/Direct-Trajectory-Planning-Method-Based-on-IEPSO-Lv-Yu/0cdac63e2580ff92c0e8d34407edcf7a45412c45",
            "/paper/Optimal-trajectory-generation-method-to-find-a-on-Nadir-Mohammed/c443a90c9d429e03940fbad6bac67d09a0751fa6",
            "/paper/A-multi-objective-approach-for-the-trajectory-of-a-Zesheng-Li/3a376e4920afaaf45a333739106b2bb8fbcb71de",
            "/paper/Time-Optimal-Trajectory-Planning-With-Interaction-Petrone-Ferrentino/133642639ecca247e8af3393553d99d41b7479c4"
        ]
    },
    {
        "id": "c9867b36f37387cedafcdcab91899bbe2eae1b39",
        "title": "Homography-based ground plane detection for mobile robot navigation using a Modified EM algorithm",
        "abstract": "The usefulness and robustness of the homography-based approach for determining the ground plane using image pairs is demonstrated by applying it to a target following algorithm and it achieves an almost perfect detection rate. In this paper, a homography-based approach for determining the ground plane using image pairs is presented. Our approach is unique in that it uses a Modified Expectation Maximization algorithm to cluster pixels on images as belonging to one of two possible classes: ground and non-ground pixels. This classification is very useful in mobile robot navigation because, by segmenting out the ground plane, we are left with all possible objects in the scene, which can then be used to implement many mobile robot navigation algorithms such as obstacle avoidance, path planning, target following, landmark detection, etc. Specifically, we demonstrate the usefulness and robustness of our approach by applying it to a target following algorithm. As the results section shows, the proposed algorithm for ground plane detection achieves an almost perfect detection rate (over 99%) despite the relatively higher number of errors in pixel correspondence from the feature matching algorithm used: SIFT.",
        "publication_year": "2010",
        "authors": [
            "D. Conrad",
            "G. DeSouza"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "44",
        "reference_count": "19",
        "references": [
            "/paper/Ground-plane-feature-detection-in-mobile-inertial-Panahandeh-Mohammadiha/97149c2ba68f1d86b042fc72b11ac9ee11c4e692",
            "/paper/A-Robust-and-Efficient-Homography-Based-Approach-Mittal-Sofat/445b3d1fb3793af3d7734aff56835c4ea5846b7a",
            "/paper/Robust-ground-plane-region-detection-using-multiple-Lin-Song/47169505ba8013e309b63f408a546f25b28f8fd8",
            "/paper/Reviews-on-Planar-Region-Detection-for-Visual-of-Wang-Cai/f903e1e285effd67fb141a55c6eb2b48bbbc4f86",
            "/paper/A-Monocular-Vision-Sensor-Based-Obstacle-Detection-Lee-Yi/d8bfaf2e3e9e7f68a50c1b9439b7f9ae9a4a2bc5",
            "/paper/Homography-based-ground-area-detection-for-indoor-Ke-Meng/594742cf6a6b81d81f6be7dc9387315cd1da305c",
            "/paper/Image-based-floor-segmentation-in-visual-inertial-Barcelo-Panahandeh/bf52c45e15cf1739d2a0d341169262d8d7821c87",
            "/paper/Planar-flow-based-planar-region-extraction-for-a-Wang-Yi/e6c033666c1e803921405b92af522ed87126d5a1",
            "/paper/A-Unique-Method-for-Detecting-Grounds-in-the-Indoor-Chen-Song/2a638fd0debeb8ff64fe34354cabefdfd5a8e97a",
            "/paper/Cast-Shadow-Resistant-Ground-Plane-Detection-in-Xu-Liu/8eb1b16a6275cb931b0e1b68d916d7fea41a9869",
            "/paper/Homography-based-ground-detection-for-a-mobile-a-Zhou-Li/22ddd206729d219a2191f71aa64c91c578d237a8",
            "/paper/Layered-ground-floor-detection-for-vision-based-Kim-Kim/ceea410c95792740cb31f9e9ae4c7fa37453ac9f",
            "/paper/Robust-Ground-Plane-Detection-with-Normalized-in-a-Zhou-Li/80eb2e42b496215016f8122224ee689b69c8d6e7",
            "/paper/Accurate-3D-ground-plane-estimation-from-a-single-Cherian-Morellas/2523be05acafac5246763c09206e617079d34ae5",
            "/paper/Vision-based-obstacle-detection-for-wheeled-robots-Wekel-Kroll-Peters/0074213afb14ae5b82d528bac914c5c25e2affaf",
            "/paper/Global-Correlation-Based-Ground-Plane-Estimation-Zhao-Katupitiya/80c31589ad32e63d880e6998fb11b58537fd00e5",
            "/paper/Unified-stereovision-for-ground%2C-road%2C-and-obstacle-Lombardi-Zanin/cf7c7384f557c43548ba53844965292ab765dd18",
            "/paper/Visual-sonar%3A-fast-obstacle-avoidance-using-vision-Lenser-Veloso/29eeb3ca76c1d10180a8b087be97a259d344d8f1",
            "/paper/Instataneous-geo-location-of-multiple-targets-from-Han-DeSouza/2f7a1ab4d91eaf12f5749424f6a66d6de8488c49",
            "/paper/Spatio-Temporal-RANSAC-for-Robust-Estimation-of-in-Mufti-Mahony/498e6c02f1c30128bcdf9dfedbb7d1be4d59aa83"
        ]
    },
    {
        "id": "c13cc96c717333e2854d4a2d0cee84794282d5be",
        "title": "Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels",
        "abstract": "Jaccard metric losses are proposed, which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels and study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-art knowledge distillation methods by a large margin. Code is available at: \\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.",
        "publication_year": "2023",
        "authors": [
            "Zifu Wang",
            "Matthew B. Blaschko"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "67",
        "references": [
            "/paper/Dice-Semimetric-Losses%3A-Optimizing-the-Dice-Score-Wang-Popordanoska/a4ad1f58595c56e615f6f1ac426c961ddd454631",
            "/paper/ChatGPT-Crawler%3A-Find-out-if-ChatGPT-really-knows-Rangapur-Wang/c2074a186f143f6a8f082747d431ab85f90a4fb5",
            "/paper/The-Lovasz-Softmax-Loss%3A-A-Tractable-Surrogate-for-Berman-Triki/c7afd747b5c6b77dc22eaa87a8b22888243842b6",
            "/paper/Auto-Seg-Loss%3A-Searching-Metric-Surrogates-for-Li-Tao/79112ae62a5f8ce03829d3c77e218269d4c0d35c",
            "/paper/Optimization-for-Medical-Image-Segmentation%3A-Theory-Eelbode-Bertels/143b05998faa4c01280fe42a1363edf9fcab091f",
            "/paper/Learning-Generalized-Intersection-Over-Union-for-Yu-Xu/1d7329bc40c4911a24af3e34b62d53d600824cb7",
            "/paper/Generalised-Dice-overlap-as-a-deep-learning-loss-Sudre-Li/2d8edc4e38bf9907170238726ec902cb3739393b",
            "/paper/The-Lov%C3%A1sz-Hinge%3A-A-Novel-Convex-Surrogate-for-Yu-Blaschko/c399c0089fb134d1476fadf5f0426e0e8b70eebd",
            "/paper/Theoretical-analysis-and-experimental-validation-of-Bertels-Robben/dfab01c27e94064837384dd86a4d422bbc1681d7",
            "/paper/Perturbed-and-Strict-Mean-Teachers-for-Semantic-Liu-Tian/4ff8f9424917d9743282e77386818a3480843a1a",
            "/paper/Rethinking-Soft-Labels-for-Knowledge-Distillation%3A-Zhou-Song/be51e9141ae2af4daf3a1ba745ad3ff66a5990f3",
            "/paper/Optimizing-Intersection-Over-Union-in-Deep-Neural-Rahman-Wang/64dc671107cfdf98e02e76cbc993732d47210549"
        ]
    },
    {
        "id": "ee21089c6ef2a683b19407b76bf4ed631cb4633b",
        "title": "OctFormer: Octree-based Transformers for 3D Point Clouds",
        "abstract": "A novel octree attention is proposed, which leverages sorted shuffled keys of octrees to partition point clouds into local windows containing a fixed number of points while permitting shapes of windows to change freely. We propose octree-based transformers, named OctFormer, for 3D point cloud learning. OctFormer can not only serve as a general and effective backbone for 3D point cloud segmentation and object detection but also have linear complexity and is scalable for large-scale point clouds. The key challenge in applying transformers to point clouds is reducing the quadratic, thus overwhelming, computation complexity of attentions. To combat this issue, several works divide point clouds into non-overlapping windows and constrain attentions in each local window. However, the point number in each window varies greatly, impeding the efficient execution on GPU. Observing that attentions are robust to the shapes of local windows, we propose a novel octree attention, which leverages sorted shuffled keys of octrees to partition point clouds into local windows containing a fixed number of points while permitting shapes of windows to change freely. And we also introduce dilated octree attention to expand the receptive field further. Our octree attention can be implemented in 10 lines of code with open-sourced libraries and runs 17 times faster than other point cloud attentions when the point number exceeds 200k. Built upon the octree attention, OctFormer can be easily scaled up and achieves state-of-the-art performances on a series of 3D segmentation and detection benchmarks, surpassing previous sparse-voxel-based CNNs and point cloud transformers in terms of both efficiency and effectiveness. Notably, on the challenging ScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNs by 7.3 in mIoU. Our code and trained models are available at https://wang-ps.github.io/octformer.",
        "publication_year": "2023",
        "authors": [
            "Peng Wang"
        ],
        "related_topics": [
            "Computer Science",
            "Environmental Science"
        ],
        "citation_count": 0,
        "reference_count": "84",
        "references": [
            "/paper/SWFormer%3A-Sparse-Window-Transformer-for-3D-Object-Sun-Tan/e702f6034063e0227e4d413a90ba375d5e181f09",
            "/paper/PointConv%3A-Deep-Convolutional-Networks-on-3D-Point-Wu-Qi/5ee147684b06ffc4db0f6326e0cba017d12ceff3",
            "/paper/Stratified-Transformer-for-3D-Point-Cloud-Lai-Liu/e259cfabd07b65d703bf8b768c667bb50796954d",
            "/paper/Octree-Guided-CNN-With-Spherical-Kernels-for-3D-Lei-Akhtar/9c042383bd8eb39eac442da96812e6b2273036bf",
            "/paper/Voxel-Transformer-for-3D-Object-Detection-Mao-Xue/813b03e123d448d53d93a087a2d34a04dfe70c5c",
            "/paper/Deep-Hough-Voting-for-3D-Object-Detection-in-Point-Qi-Litany/e60da8d3a79801a3ccbf1abcdd001bb6e001b267",
            "/paper/Embracing-Single-Stride-3D-Object-Detector-with-Fan-Pang/babf62e12510a025dad4a602bd2e9f3a8331314d",
            "/paper/Point-Transformer-V2%3A-Grouped-Vector-Attention-and-Wu-Lao/3cd0823eabd32c062a56d4e1cef7de21f8283230",
            "/paper/RandLA-Net%3A-Efficient-Semantic-Segmentation-of-Hu-Yang/587110558b1744b912108dd0b74970f131317e0d",
            "/paper/PointNet%3A-Deep-Learning-on-Point-Sets-for-3D-and-Qi-Su/d997beefc0922d97202789d2ac307c55c2c52fba"
        ]
    },
    {
        "id": "9b4335a255d9ea4b5dfa19f5c134f56119046769",
        "title": "Context Driven Optimized Perceptual Video Summarization and Retrieval",
        "abstract": "A single frame-based indexing of the video database instead of multi-frame indexing as it is an interesting and potentially promising approach and the experiments demonstrate that the video summarization produced by the approach gives a realistic view of the events in the video, while the reduction in computational complexity and memory requirement is high. Video summarization is an economical way of representing video contents and is useful for effective and quick browsing of the relevant activity present in the video. Most current video summarization approaches do not provide the correct insight of the events occurring in the video, such as moving the objects in a nonchronological order and tampering with the background and size of the objects. In this paper, we present an approach to create a video summarization that is a precise representation of the video content. First, our approach finds out the salient activities that are taking place in the video using an optimization framework for static and dynamic scenes. Second, the frames with the salient activities are stitched using alpha matting to form a single frame. Third, the summarized frame over multiple video shots obtained by our approach gives superior retrieval performance with image queries while reducing retrieval latency and memory requirement. This paper proposes a single frame-based indexing of the video database instead of multi-frame indexing as it is an interesting and potentially promising approach. Finally, several experiments are carried out to evaluate the proposed approach. The experiments demonstrate that the video summarization produced by our approach gives a realistic view of the events in the video, while the reduction in computational complexity and memory requirement is high.",
        "publication_year": "2019",
        "authors": [
            "S. S. Thomas",
            "Sumana Gupta",
            "V. Subramanian"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "16",
        "reference_count": "64",
        "references": [
            "/paper/A-survey-of-recent-work-on-video-summarization%3A-and-Tiwari-Bhatnagar/effe764766d222d5da406997a01c05fb37f30111",
            "/paper/Keyframe-Generation-Method-via-Improved-Clustering-Wang-Chen/c08b31eeab61843564a06b46f240e25c299d283b",
            "/paper/Machine-learning-based-fusion-algorithm-to-perform-Harakannanavar-Kanabur/fb88e70549a1519de43def4666089038155060fd",
            "/paper/Local-Optimal-Oriented-Pattern-and-Exponential-Deep-Jimson.-Ananth/1039e8bfbcacfeb9f8941f09e827efa8badb58eb",
            "/paper/RETRACTED-ARTICLE%3A-Content-based-video-retrieval-on-Prathiba-Kumari/56d1256e6fc7cb4a40e14c9e4976c00357ce561e",
            "/paper/Content-based-video-retrieval-system-based-on-by-to-Prathiba-Kumari/f549f6ee024302d9fe18396cd60c7b4f55c5b8bb",
            "/paper/LTC-SUM%3A-Lightweight-Client-Driven-Personalized-2D-Mujtaba-Malik/af6ffa0113d64548ad923043c49a906a7605b69c",
            "/paper/Multi-Turn-Video-Question-Generation-via-Reinforced-Guo-Zhao/fe5d9dfb99000827192cc86c91689fe6d4113382",
            "/paper/AMDFNet%3A-Adaptive-multi-level-deformable-fusion-for-Li-Zheng/7fec2cef6276c4da23bc4496581204ea40448232",
            "/paper/Multiscale-multilevel-context-and-multimodal-fusion-Wu-Zhou/bd31bc8d71785feff60ceb1983be97f40cb0581a",
            "/paper/Keyframe-Based-Video-Summary-Using-Visual-Attention-Jiang-Qin/861fbc1587bdbf93c398226091ef70bc4441f7ae",
            "/paper/InsightVideo%3A-toward-hierarchical-video-content-for-Zhu-Elmagarmid/a5742ca260f541d72c6729f63a58cdbf0695aadb",
            "/paper/Semantic-Based-Surveillance-Video-Retrieval-Hu-Xie/d6b7a7b3535f510683bbb3c9014d2d10db4dccc0",
            "/paper/Context-Aware-Surveillance-Video-Summarization-Zhang-Zhu/9e308d6902d15c98d264f9e592fb358f25244cdf",
            "/paper/A-new-key-frame-representation-for-video-segment-Sze-Lam/5b0e9f52f3feb2ea5fc2400370bc10b9ee8caf3f",
            "/paper/A-generic-framework-of-user-attention-model-and-its-Ma-Hua/4541e6605c73c2499d145fb3c8621b91fddf3a78",
            "/paper/Object-centered-narratives-for-video-surveillance-Fu-Wang/a1977ebd3b4eadda8d78d0bf2ae75e4c5d06db04",
            "/paper/Motion-State-Adaptive-Video-Summarization-via-Zhang-Tao/7f93bff0d796f866f41bec712a73c5bbe2e112d8",
            "/paper/Speeded-Up-Video-Summarization-Based-on-Local-Iparraguirre-Delrieux/7ebfcd738fd38c81d2a6d45acad78efcfd37f035",
            "/paper/TVSum%3A-Summarizing-web-videos-using-titles-Song-Vallmitjana/cbf89cb4e107fb59e119ae619bcfe48e1964e033"
        ]
    },
    {
        "id": "a0e544f6db3b6659a5f77c419908238f91b188bc",
        "title": "DarkLighter: Light Up the Darkness for UAV Tracking",
        "abstract": "This work proposes a low-light image enhancer namely DarkLighter, which dedicates to alleviate the impact of poor illumination and noise iteratively, and is implemented on a typical UAV system. Recent years have witnessed the fast evolution and promising performance of the convolutional neural network (CNN)-based trackers, which aim at imitating biological visual systems. However, current CNN-based trackers can hardly generalize well to low-light scenes that are commonly lacked in the existing training set. In indistinguishable night scenarios frequently encountered in unmanned aerial vehicle (UAV) tracking-based applications, the robustness of the state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial tracking in the dark through a general fashion, this work proposes a low-light image enhancer namely DarkLighter, which dedicates to alleviate the impact of poor illumination and noise iteratively. A lightweight map estimation network, i.e., ME-Net, is trained to efficiently estimate illumination maps and noise maps jointly. Experiments are conducted with several SOTA trackers on numerous UAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability and universality of DarkLighter, with high efficiency. Moreover, DarkLighter has further been implemented on a typical UAV system. Real-world tests at night scenes have verified its practicability and dependability.",
        "publication_year": "2021",
        "authors": [
            "Junjie Ye",
            "Changhong Fu",
            "Guang-Zheng Zheng",
            "Ziang Cao",
            "Bowen Li"
        ],
        "related_topics": [
            "Computer Science",
            "Environmental Science"
        ],
        "citation_count": "8",
        "reference_count": "32",
        "references": [
            "/paper/Tracker-Meets-Night%3A-A-Transformer-Enhancer-for-UAV-Ye-Fu/9e5d4094cbe46de2879ce588748083e1b52aa3be",
            "/paper/Cascaded-Denoising-Transformer-for-UAV-Nighttime-Lu-Fu/6a86342bc85a21f7169e03b7a5425c3ad95cb166",
            "/paper/HighlightNet%3A-Highlighting-Low-Light-Potential-for-Fu-Dong/7d0a977179bfe592a29d6aac84d309d3e532e2a9",
            "/paper/Unsupervised-Domain-Adaptation-for-Nighttime-Aerial-Ye-Fu/32db2d409384575aeae453acc45220b51fe96301",
            "/paper/Siamese-Object-Tracking-for-Unmanned-Aerial-A-and-Fu-Lu/1171234cb2f3e1589592e3d04eb10c132fc6a5c8",
            "/paper/Low-Illumination-Lane-Detection-by-Fusion-of-Zhao-Tian/34c162cbf327a2ae2973277d13ebb93f563303fd",
            "/paper/Blind-Multimodal-Quality-Assessment%3A-A-Brief-Survey-Wang-Xu/0dd643ce93b69657c274b05049deaf7d3785c705",
            "/paper/Low-light-image-enhancement-model-based-on-improved-Cheng-Yang/950ae222e701479f8186077a7f1a96c40ee7e009",
            "/paper/All-Day-Object-Tracking-for-Unmanned-Aerial-Vehicle-Li-Fu/f3d7eb617179db9f9621fa2c978dfb9f2c39341f",
            "/paper/ADTrack%3A-Target-Aware-Dual-Filter-Learning-for-UAV-Li-Fu/689b230b228c7ff5e2bb5d500c5349f54bcc6d3c",
            "/paper/Siamese-Anchor-Proposal-Network-for-High-Speed-Fu-Cao/827ee2cdc56ea65ef644bd8ca085f4274b106f03",
            "/paper/Mutation-Sensitive-Correlation-Filter-for-Real-Time-Zheng-Fu/8f1b398b289388d4a6c726053887a12d34eb9fdb",
            "/paper/Kindling-the-Darkness%3A-A-Practical-Low-light-Image-Zhang-Zhang/28168e2c182e5456ad4712dae479dd44423b2ed6",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "/paper/EnlightenGAN%3A-Deep-Light-Enhancement-Without-Paired-Jiang-Gong/228a801801abd28b58db1de75cfe6885f7d47bb7",
            "/paper/LIME%3A-Low-Light-Image-Enhancement-via-Illumination-Guo-Li/f1be34d5711811475be97dc137f1de07602cba44",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/HiFT%3A-Hierarchical-Feature-Transformer-for-Aerial-Cao-Fu/9916ed982600be133ed2d185b70fe721809a3096"
        ]
    },
    {
        "id": "012b6d96840abb0f11b27353a3d033317678ae0a",
        "title": "Learning passive\u2013aggressive correlation filter for long-term and short-term visual tracking",
        "abstract": "A two-stage cascaded framework for accurate object modeling with a passive\u2013aggressive correlation filter tracker to reduce error accumulation and an online refinement algorithm used to calibrate the tracking model by exploiting both long-term and short-term cues is proposed. Abstract. Correlation filter (CF) has received increasing attention in online visual object tracking. By accelerating the correlation in the frequency domain, CF trackers have achieved superior performance. However, existing CF trackers have a common shortcoming in that tracking models are prone to drift due to error accumulation. To address this problem, we propose a two-stage cascaded framework for accurate object modeling. In the first stage, we propose a passive\u2013aggressive correlation filter (PACF) tracker to reduce error accumulation. In the subsequent stage, an online refinement algorithm is used to calibrate the tracking model by exploiting both long-term and short-term cues. In order to achieve high efficiency, our scheme reuses the PACF tracking response in the following stage. Extensive experiments were conducted on both long-term and short-term visual tracking benchmarks. The experimental results demonstrate that our tracker outperforms the state-of-the-art online tracking schemes in both long-term and short-term settings. Finally, we present a comprehensive analysis to validate the efficacy of our proposed method.",
        "publication_year": "2019",
        "authors": [
            "Yu Zhang",
            "Xingyu Gao",
            "Zhenyu Chen",
            "Huicai Zhong"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "64",
        "references": [
            "/paper/A-new-near-lossless-image-information-hiding-with-Zhang-Zhou/96cf8ae11cd12fb4d57b014b941a4f0e56cf1260",
            "/paper/Multi-directional-broad-learning-system-for-the-Zihao-Ying/8ba6f4dca48bd9e7a6c3d249d650dbfda88a6378",
            "/paper/An-Electromagnetic-Field-Detection-Module-of-Wang-Gong/6ce50d00ff8bfa51dc89521f6317704b37bb3800",
            "/paper/Cross-Media-and-Multilingual-Image-Understanding-on-Gao-Chen/26f36554b6895af2dd7c4a74cf9021972d7afd52",
            "/paper/Adaptive-Correlation-Filters-with-Long-Term-and-for-Ma-Huang/7a078987c929b1aef72f4ca283dddef187c48132",
            "/paper/Learning-regression-and-verification-networks-for-Zhang-Wang/3d372b63020c4d2c9510624f370b50d9f292bcde",
            "/paper/Long-term-Correlation-Tracking-using-Multi-layer-in-Baisa-Bhowmik/8d099a209216c06033cef5a35615a61696cc13bd",
            "/paper/Learning-Spatial-Temporal-Regularized-Correlation-Li-Tian/9f45b55af027503fab557f55f70e81e43c6c1db7",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "/paper/Target-Response-Adaptation-for-Correlation-Filter-Bibi-Mueller/f5eae0841111c814c977dd691c072a3aa57e6ad8",
            "/paper/PROST%3A-Parallel-robust-online-simple-tracking-Santner-Leistner/16e36a4b59e214786737aa4ebc3ba86075b61e49",
            "/paper/Long-term-correlation-tracking-Ma-Yang/754504cf01ef3846259783e748b1d3ea52fa2c81",
            "/paper/Multi-Complementary-Model-for-Long-Term-Tracking-Zhang-Zhang/50bd231f813f1e6a53bb6d3784854b4b3e98d8fe"
        ]
    },
    {
        "id": "1cd5d920d19d08d2eb1b67c01648a33369b0cfaf",
        "title": "SiamBAN: Target-Aware Tracking With Siamese Box Adaptive Network",
        "abstract": "This work proposes a simple yet effective tracker to learn a target-aware scale handling schema in a data-driven manner and divides the tracking problem into classification and regression tasks, which directly predict objectiveness and regress bounding boxes, respectively. Variation of scales or aspect ratios has been one of the main challenges for tracking. To overcome this challenge, most existing methods adopt either multi-scale search or anchor-based schemes, which use a predefined search space in a handcrafted way and therefore limit their performance in complicated scenes. To address this problem, recent anchor-free based trackers have been proposed without using prior scale or anchor information. However, an inconsistency problem between classification and regression degrades the tracking performance. To address the above issues, we propose a simple yet effective tracker (named Siamese Box Adaptive Network, SiamBAN) to learn a target-aware scale handling schema in a data-driven manner. Our basic idea is to predict the target boxes in a per-pixel fashion through a fully convolutional network, which is anchor-free. Specifically, SiamBAN divides the tracking problem into classification and regression tasks, which directly predict objectiveness and regress bounding boxes, respectively. A no-prior box design is proposed to avoid tuning hyper-parameters related to candidate boxes, which makes SiamBAN more flexible. SiamBAN further uses a target-aware branch to address the inconsistency problem. Experiments on benchmarks including VOT2018, VOT2019, OTB100, UAV123, LaSOT and TrackingNet show that SiamBAN achieves promising performance and runs at 35 FPS.",
        "publication_year": "2022",
        "authors": [
            "Zedu Chen",
            "Bineng Zhong",
            "Guorong Li",
            "Shengping Zhang",
            "Rongrong Ji",
            "Zhenjun Tang",
            "Xianxian Li"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "7",
        "reference_count": "65",
        "references": [
            "/paper/Real-Time-Siamese-Multiple-Object-Tracker-with-Vaquero-Brea/e0b64af1def66378e37fa33d3795d18e4b3171aa",
            "/paper/Learning-to-Propose-and-Refine-for-Accurate-and-via-Mo-Li/72dbd39df1e46671ae50e015ea6c8575c28720e7",
            "/paper/Siamese-Network-Tracking-Based-on-Feature-Huang-Yang/eae6cbca7eef199b1995d29ee9d5b6617016e30f",
            "/paper/Multiple-Object-Tracking-With-Appearance-Feature-Li-Chen/8c4ced538ca85ce29d45c31f856056e318a764e4",
            "/paper/Feature-Complement-for-Visual-Tracking-Based-on-He-Chen/9e9ecc8e8624fe612b8ac56961b7cacc81292b60",
            "/paper/A-Moving-Target-Tracking-Framework-Based-on-a-Set-Zeng-Min/fd7c8d474387e4098cd9aa9d184eead59c285ff9",
            "/paper/Tracking-vision-transformer-with-class-and-tokens-Nardo-Ciaramella/4f7902ded75425a6e65d89e07b35a4a90f2603d9",
            "/paper/Siamese-Box-Adaptive-Network-for-Visual-Tracking-Chen-Zhong/cce1fecc800d2782da638f3060d5b2e887739f74",
            "/paper/SiamCAR%3A-Siamese-Fully-Convolutional-Classification-Guo-Wang/738165f33c50b059e87b14d8b4a129230e14eacd",
            "/paper/Ocean%3A-Object-aware-Anchor-free-Tracking-Zhang-Peng/27d52bf3265bea0f9929980f6ffb4c2009eecfee",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/Siamese-Cascaded-Region-Proposal-Networks-for-Fan-Ling/f98be9a91dbf00b52a494720bd36be9c73a1210e",
            "/paper/Triplet-Loss-in-Siamese-Network-for-Object-Tracking-Dong-Shen/fdb98f5a7015de0956ef8d4e468257dc3079b5e5",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/ATOM%3A-Accurate-Tracking-by-Overlap-Maximization-Danelljan-Bhat/d74169a8fd2f90a06480d1d583d0ae5e980ea951"
        ]
    },
    {
        "id": "b5fb909d436856ba7c4d5e15bfdb83a847e7ff8a",
        "title": "MonoHuman: Animatable Human Neural Field from Monocular Video",
        "abstract": "This paper proposes a novel framework MonoHuman, which robustly renders view-consistent and high-fidelity avatars under arbitrary novel poses and explicitly leverage the off-the-peg keyframe information to reason the feature correlations for coherent results. Animating virtual avatars with free-view control is crucial for various applications like virtual reality and digital entertainment. Previous studies have attempted to utilize the representation power of the neural radiance field (NeRF) to reconstruct the human body from monocular videos. Recent works propose to graft a deformation network into the NeRF to further model the dynamics of the human neural field for animating vivid human motions. However, such pipelines either rely on pose-dependent representations or fall short of motion coherency due to frame-independent optimization, making it difficult to generalize to unseen pose sequences realistically. In this paper, we propose a novel framework MonoHuman, which robustly renders view-consistent and high-fidelity avatars under arbitrary novel poses. Our key insight is to model the deformation field with bi-directional constraints and explicitly leverage the off-the-peg keyframe information to reason the feature correlations for coherent results. Specifically, we first propose a Shared Bidirectional Deformation module, which creates a pose-independent generalizable deformation field by disentangling backward and forward deformation correspondences into shared skeletal motion weight and separate non-rigid motions. Then, we devise a Forward Correspondence Search module, which queries the correspondence feature of keyframes to guide the rendering network. The rendered results are thus multi-view consistent with high fidelity, even under challenging novel pose settings. Extensive experiments demonstrate the superiority of our proposed MonoHuman over state-of-the-art methods.",
        "publication_year": "2023",
        "authors": [
            "Zhengming Yu",
            "W. Cheng",
            "Xian Liu",
            "Wayne Wu",
            "Kwan-Yee Lin"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "58",
        "references": [
            "/paper/DreamWaltz%3A-Make-a-Scene-with-Complex-3D-Animatable-Huang-Wang/8c626745013558a83103f3a19afae174cd72f455",
            "/paper/Neural-Capture-of-Animatable-3D-Human-from-Video-Te-Li/a422900dbccc0b01bbbcd735ea54001a44f67615",
            "/paper/HumanNeRF%3A-Free-viewpoint-Rendering-of-Moving-from-Weng-Curless/f763a59644e27a2215095943224f2564e670a504",
            "/paper/Animatable-Neural-Radiance-Fields-for-Modeling-Peng-Dong/35e940ace815548f620709d9c1803da34c581e86",
            "/paper/Neural-Actor%3A-Neural-Free-view-Synthesis-of-Human-Liu-Habermann/29da3be81905d577dac9144c1c3cb5c6678f7027",
            "/paper/NeuMan%3A-Neural-Human-Radiance-Field-from-a-Single-Jiang-Yi/17df7e87a5d25e6e83d773e6f686eb0a85f6827e",
            "/paper/Vid2Actor%3A-Free-viewpoint-Animatable-Person-from-in-Weng-Curless/a6167229bac2ff9206fde21c443b7cadde9e8a5a",
            "/paper/TAVA%3A-Template-free-Animatable-Volumetric-Actors-Li-Tanke/5a365ad81138028dacf989317f34f46852e3e88e",
            "/paper/MonoPerfCap%3A-Human-Performance-Capture-from-Video-Xu-Chatterjee/46882d15fb3f6449b0021ac5980676d858590f02",
            "/paper/Neural-Body%3A-Implicit-Neural-Representations-with-Peng-Zhang/af8faec7c0b8f4b2a28d42a86e0e7d499016c560",
            "/paper/BANMo%3A-Building-Animatable-3D-Neural-Models-from-Yang-Vo/e230950f1a784982df3bff6b4f753e50f9fbfb69"
        ]
    },
    {
        "id": "f0e93ad223e82f16f2d6fa22a713744614ed8ee3",
        "title": "Learning attention modules for visual tracking",
        "abstract": "A novel backbone network based on CNN model and attention mechanism in the Siamese framework with excellent performances against state-of-the-art trackers is presented. Siamese networks have been widely used in visual tracking. However, it is difficult to deal with complex appearance variations when the discriminative background information is ignored and an offline training strategy is adopted. In this paper, we present a novel backbone network based on CNN model and attention mechanism in the Siamese framework. The attention mechanism is composed of a channel attention module and a spatial attention module. The channel attention module uses the learned global information to selectively focus on the convolution features, which enhances a network representation ability. Besides, the spatial attention module obtains more contextual information and semantic features of target candidates. The designed attention mechanism-based backbone is lightweight and has a real-time tracking performance. We utilize GOT-10K as a training set to offline adjust trained model parameters. The extensive experimental evaluations on OTB2015, VOT2016, VOT2018, GOT-10k and UAV123 datasets demonstrate that the proposed algorithm has excellent performances against state-of-the-art trackers.",
        "publication_year": "2022",
        "authors": [
            "Jun Wang",
            "Chenchen Meng",
            "Chengzhi Deng",
            "Yuanyun Wang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "37",
        "references": [
            "/paper/SGAT%3A-Shuffle-and-graph-attention-based-Siamese-for-Wang-Zhang/63def7abe9b09a847500351d167d179c7b0026ba",
            "/paper/Siamese-network-with-a-depthwise-over-parameterized-Wang-Zhang/632db41e735e67a5507bcb77acee948a554bff1e",
            "/paper/Learning-convolutional-self-attention-module-for-Wang-Meng/10976c9444ef01085f02343a87d91c7de70f6524",
            "/paper/Adaptive-temporal-feature-modeling-for-visual-via-Wang-Zhang/07b87af5fb9cbb6624305b1d414078c62bba5869",
            "/paper/Learning-Attentions%3A-Residual-Attentional-Siamese-Wang-Teng/6683442ae358ae4261fdcde0164f83dd1ccd621b",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Deformable-Siamese-Attention-Networks-for-Visual-Yu-Xiong/5eca15a355b2a9a1e80879e850afe49d3c398c53",
            "/paper/Online-Multi-object-Tracking-Using-CNN-Based-Single-Chu-Ouyang/794ed0316647a7fb1d334b7dafc38b7a2b2a5b76",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Learning-Multi-domain-Convolutional-Neural-Networks-Nam-Han/2ce63d77eecc35faef85a3b752a314c93a077ac9",
            "/paper/Siamese-Box-Adaptive-Network-for-Visual-Tracking-Chen-Zhong/cce1fecc800d2782da638f3060d5b2e887739f74",
            "/paper/A-Twofold-Siamese-Network-for-Real-Time-Object-He-Luo/a3a4471e82260f573d240cc34aeff431cf236571"
        ]
    },
    {
        "id": "769291ce3ea1824a804d7d67957781bb37732c82",
        "title": "Lung cancer diagnosis using Hessian adaptive learning optimization in generative adversarial networks",
        "abstract": "The experimental outcomes prove that the GANs with HAL optimization technique yields better performance compared to SGD and GN models, and the experimental results assured that theGANs converge fast and eliminate mode collapse problems using HAL optimization. Lung cancer is the most frequent cancer and the reason for cancer death, with high morbidity and mortality. Computed tomography is one of the efficient medical imaging tools for lung cancer diagnosis, which offers internal lung details. However, as there is limited availability of datasets and requires large number of images for interpretation, it is hard for radiologists to diagnose lung cancer. The generative adversarial network (GAN) is a significant generative model employed for data augmentation, which has the benefit of simulating data distribution without the explicit modeling of potential probability density functions. Despite the benefits of GAN, training process remains challenging due to high convergence time and mode collapse problems. To resolve these issues, in this paper, a Hessian Adaptive Learning (HAL) Optimization technique. The proposed HL technique uses gradient and curvature data to eliminate the mode collapse problem and to improve the dataset size via a generation of diverse images. The experiments were conducted on Vanilla GAN, Wasserstein Generative Adversarial Network (WGAN), Conditional Generative Adversarial Network (CGAN), Wasserstein, and Deep Convolutional Generative Adversarial Network (DCGAN). Each GAN is tested using stochastic Gradient descent (SGD), Gauss\u2013Newton (GN) Second-order learning, and proposed HAL optimization techniques. The experimental outcomes prove that the GANs with HAL optimization technique yields better performance compared to SGD and GN models. The experimental results assured that the GANs converge fast and eliminate mode collapse problems using HAL optimization.",
        "publication_year": "2023",
        "authors": [
            "E. Thirumagal",
            "K. Saruladha"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "33",
        "references": [
            "/paper/Combination-of-generative-adversarial-network-and-Wang-Zhou/52e6f2053d11771641b8ad6ba424d6dd71ea7f42",
            "/paper/Synthetic-CT-image-generation-of-shape-controlled-Toda-Teramoto/a8fe2db9481fffece7a1b5de7d9ca64ae9a4c776",
            "/paper/Automated-Pulmonary-Nodule-Classification-in-Images-Onishi-Teramoto/df85275c4effa766aea480abfa480f5a92e9c851",
            "/paper/Segmentation-of-Lungs-in-Chest-X-Ray-Image-Using-Munawar-Azmat/da959782ef230a5653967585090ea31e67390ce1",
            "/paper/Automatic-Nodule-Segmentation-Method-for-CT-Images-Shi-Hu/9946d335ae1b556ff3c15b7500c043a879b8fdca",
            "/paper/Using-Deep-Learning-to-Locate-Lung-Tumor-from-Chest-Chan-Liu/ab18da03a2b1703d73db62ff480e9138afc7424e",
            "/paper/Research-on-Computer-Aided-Detection-and-of-Nodules-Wang-Sun/cbe0020707822de6421fb0cdb36904348104da0d",
            "/paper/Deep-learning-approach-to-classification-of-lung-by-Teramoto-Tsukamoto/864cb6101067f72f96e960464c4d1aef8559563d",
            "/paper/A-survey-of-computer-aided-diagnosis-of-lung-from-Gu-Chi/994c2dc6d814c6787e574a6414ed4649b318a6cd",
            "/paper/Knowledge-based-Collaborative-Deep-Learning-for-on-Xie-Xia/09abb9f27320e291003036a8a5b2ff15a9d7f1e2"
        ]
    },
    {
        "id": "e4fd861a5ff7daf756f5681955da20197ccf5e6e",
        "title": "Radiomics nomogram for predicting axillary lymph node metastasis in breast cancer based on DCE-MRI: A multicenter study.",
        "abstract": "The radiomics nomogram combined with clinical risk factors and DCE-MRI-based radiomics signature may be used to predict ALN metastasis in a noninvasive manner. OBJECTIVES\nThis study aims to develop and validate a radiomics nomogram based on dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) to noninvasively predict axillary lymph node (ALN) metastasis in breast cancer.\n\n\nMETHODS\nThis retrospective study included 263 patients with histologically proven invasive breast cancer and who underwent DCE-MRI examination before surgery in two hospitals. All patients had a defined ALN status based on pathological examination results. Regions of interest (ROIs) of the primary tumor and ipsilateral ALN were manually drawn. A total of 1,409 radiomics features were initially computed from each ROI. Next, the low variance threshold, SelectKBest, and least absolute shrinkage and selection operator (LASSO) algorithms were used to extract the radiomics features. The selected radiomics features were used to establish the radiomics signature of the primary tumor and ALN. A radiomics nomogram model, including the radiomics signature and the independent clinical risk factors, was then constructed. The predictive performance was evaluated by the receiver operating characteristic (ROC) curves, calibration curve, and decision curve analysis (DCA) by using the training and testing sets.\n\n\nRESULTS\nALNM rates of the training, internal testing, and external testing sets were 43.6%, 44.3% and 32.3%, respectively. The nomogram, including clinical risk factors (tumor diameter) and radiomics signature of the primary tumor and ALN, showed good calibration and discrimination with areas under the ROC curves of 0.884, 0.822, and 0.813 in the training, internal and external testing sets, respectively. DCA also showed that radiomics nomogram displayed better clinical predictive usefulness than the clinical or radiomics signature alone.\n\n\nCONCLUSIONS\nThe radiomics nomogram combined with clinical risk factors and DCE-MRI-based radiomics signature may be used to predict ALN metastasis in a noninvasive manner.",
        "publication_year": "2023",
        "authors": [
            "Jiwen Zhang",
            "Zhongsheng Zhang",
            "N. Mao",
            "Haicheng Zhang",
            "Jing Gao",
            "Bin Wang",
            "Jianlin Ren",
            "Xin Liu",
            "Binyue Zhang",
            "Tingyao Dou",
            "Wenjuan Li",
            "Yanhong Wang",
            "H. Jia"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "60",
        "references": [
            "/paper/Radiomics-Nomogram-of-DCE-MRI-for-the-Prediction-of-Mao-Dai/8b0038c63b6deecef09265f6d28c762f14f9d99b",
            "/paper/Radiomics-nomogram-of-contrast-enhanced-spectral-of-Mao-Yin/0515758bba43914a893610d81a02c94207e03db3",
            "/paper/A-Nomogram-Combined-Radiomics-and-Kinetic-Curve-as-Shan-Xu/6cb124bd46fe728a9165c039ed0c12929e6fd6ea",
            "/paper/Magnetic-resonance-imaging-radiomics-predicts-lymph-Yu-He/4c95603a9611faa53c3ca851f324ac0ca6c95a17",
            "/paper/Radiomic-nomogram-for-prediction-of-axillary-lymph-Han-Zhu/1fe8ce3b7818426d9a826ea3135acaa6887957e1",
            "/paper/Development-and-Validation-of-a-Preoperative-to-and-Yu-Tan/211783a923ac1e38941247f65c256ec21a393045",
            "/paper/Contrast-enhanced-CT-radiomics-for-predicting-lymph-Li-Yao/47ea2feab4ecc277fccdfad1680e8c3f6a8f3aa4",
            "/paper/3T-MRI-Radiomic-Approach-to-Predict-for-Lymph-Node-Santucci-Faiella/52aa405d161c991fc1e5310b696877cb249896ce",
            "/paper/Radiomics-MRI-for-lymph-node-status-prediction-in-Calabrese-Santucci/eacb1a8594286f277ccdcdfce1c52d947f66f4ad",
            "/paper/Pharmacokinetic-parameters-and-radiomics-model-on-Liu-Mao/0899612e71e48278241f623a91b7f943da708698"
        ]
    },
    {
        "id": "bed89a835cf8b203f3ca57fc62f01e60c0fa0b82",
        "title": "A novel framework of multiclass skin lesion recognition from dermoscopic images using deep learning and explainable AI",
        "abstract": "This work presented a new framework for skin lesion recognition using data augmentation, deep learning, and explainable artificial intelligence and developed an improved Butterfly Optimization Algorithm. Skin cancer is a serious disease that affects people all over the world. Melanoma is an aggressive form of skin cancer, and early detection can significantly reduce human mortality. In the United States, approximately 97,610 new cases of melanoma will be diagnosed in 2023. However, challenges such as lesion irregularities, low-contrast lesions, intraclass color similarity, redundant features, and imbalanced datasets make improved recognition accuracy using computerized techniques extremely difficult. This work presented a new framework for skin lesion recognition using data augmentation, deep learning, and explainable artificial intelligence. In the proposed framework, data augmentation is performed at the initial step to increase the dataset size, and then two pretrained deep learning models are employed. Both models have been fine-tuned and trained using deep transfer learning. Both models (Xception and ShuffleNet) utilize the global average pooling layer for deep feature extraction. The analysis of this step shows that some important information is missing; therefore, we performed the fusion. After the fusion process, the computational time was increased; therefore, we developed an improved Butterfly Optimization Algorithm. Using this algorithm, only the best features are selected and classified using machine learning classifiers. In addition, a GradCAM-based visualization is performed to analyze the important region in the image. Two publicly available datasets\u2014ISIC2018 and HAM10000\u2014have been utilized and obtained improved accuracy of 99.3% and 91.5%, respectively. Comparing the proposed framework accuracy with state-of-the-art methods reveals improved and less computational time.",
        "publication_year": "2023",
        "authors": [
            "Naveed Ahmad",
            "J. H. Shah",
            "M. A. Khan",
            "J. Baili",
            "Ghulam Jillani Ansari",
            "U. Tariq",
            "Y. Kim",
            "Jaehyuk Cha"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "64",
        "references": [
            "/paper/A-Transfer-Learning-Based-Novel-Convolution-Neural-Qureshi-Umar/7ad948d07a88a3c339f342384c415cbf7c2cef9c",
            "/paper/An-enhanced-technique-of-skin-cancer-classification-Ali-Miah/3043d8b8e03c9f8c1f3879fae6191ed9eb100255",
            "/paper/Skin-Lesion-Analysis-towards-Melanoma-Detection-Li-Shen/4ceb12d01dadadcef4e6233a1fe1792cca1c8c16",
            "/paper/Multiclass-skin-lesion-classification-in-images-Ayas/0d754abea13b6d99938d3aeb8d72073c89e69021",
            "/paper/Transfer-learning-using-a-multi-scale-and-ensemble-Mahbod-Schaefer/0ac46d88c9b883802dfda25aa12c068ef3b7c570",
            "/paper/Single-Model-Deep-Learning-on-Imbalanced-Small-for-Yao-Shen/80fb28fedc0d0a512fe7956655c50f862f23dbf1",
            "/paper/Skin-Lesions-Classification-Into-Eight-Classes-for-Kassem-Hosny/d93c65722d2e43ea9bc4e0ccd70d9357985fc145",
            "/paper/Deep-Learning-Classifier-with-Patient%E2%80%99s-Metadata-of-Ningrum-Yuan/83cbd1fbf0deabd20e6e2275e8c03b5839e90497",
            "/paper/Skin-cancer-disease-images-classification-using-Mijwil/f8ee1b6def17383e216897c35705e73c51a7ac07",
            "/paper/An-Improved-Framework-by-Mapping-Salient-Features-Javed/4603b36b2cf9483c01fee4033fc6b97dad2433ab"
        ]
    },
    {
        "id": "bad52faef74960f458f3dfab35f7714335d334c0",
        "title": "Automated Detection and Classification of Oral Lesions Using Deep Learning to Detect Oral Potentially Malignant Disorders",
        "abstract": "A two-stage model was proposed to detect oral lesions with a detector network and classify the detected region into three categories (benign, OPMD, carcinoma) with a second-stage classifier network and preliminary results demonstrate the feasibility of deep learning-based approaches for the automated detection and classification of oral lesions in real time. Simple Summary Oral cancer is the most common type of head and neck cancer worldwide. The detection of oral potentially malignant disorders, which carry a risk of developing into cancer, often provides the best chances for curing the disease and is therefore crucial for improving morbidity and mortality outcomes from oral cancer. In this study, we explored the potential applications of computer vision and deep learning techniques in the oral cancer domain within the scope of photographic images and investigated the prospects of an automated system for identifying oral potentially malignant disorders with a two-stage pipeline. Our preliminary results demonstrate the feasibility of deep learning-based approaches for the automated detection and classification of oral lesions in real time. The proposed model offers great potential as a low-cost and non-invasive tool that can support screening processes and improve the detection of oral potentially malignant disorders. Abstract Oral cancer is the most common type of head and neck cancer worldwide, leading to approximately 177,757 deaths every year. When identified at early stages, oral cancers can achieve survival rates of up to 75\u201390%. However, the majority of the cases are diagnosed at an advanced stage mainly due to the lack of public awareness about oral cancer signs and the delays in referrals to oral cancer specialists. As early detection and treatment remain to be the most effective measures in improving oral cancer outcomes, the development of vision-based adjunctive technologies that can detect oral potentially malignant disorders (OPMDs), which carry a risk of cancer development, present significant opportunities for the oral cancer screening process. In this study, we explored the potential applications of computer vision techniques in the oral cancer domain within the scope of photographic images and investigated the prospects of an automated system for detecting OPMD. Exploiting the advancements in deep learning, a two-stage model was proposed to detect oral lesions with a detector network and classify the detected region into three categories (benign, OPMD, carcinoma) with a second-stage classifier network. Our preliminary results demonstrate the feasibility of deep learning-based approaches for the automated detection and classification of oral lesions in real time. The proposed model offers great potential as a low-cost and non-invasive tool that can support screening processes and improve detection of OPMD.",
        "publication_year": "2021",
        "authors": [
            "G. Tanriver",
            "M. Soluk Tekke\u015fin",
            "O. Ergen"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "21",
        "reference_count": "44",
        "references": [
            "/paper/Automated-Oral-Cancer-Detection-using-Deep-Muqeet-Mohammad/bfa71958d8b449e86cd14768d6396fe6d8056b79",
            "/paper/An-Image-Recognition-Framework-for-Oral-Cancer-Zhang-Li/84a1605ca9f8fcd84912331d330fae7ad39790f2",
            "/paper/Detection-of-oral-squamous-cell-carcinoma-in-using-Fl%C3%BCgge-Gaudin/092ffdedfb93eda21e55bd4faa8c8bc51ecfa31b",
            "/paper/Deep-convolutional-neural-network-algorithm-for-the-%C3%9Cnsal-Chaurasia/8863394d788287653f9b206dc5b00f277f235f5d",
            "/paper/AI-based-analysis-of-oral-lesions-using-novel-deep-Warin-Limprasert/5c7503a0c4366fe056cd968ea70342e5d5f49973",
            "/paper/A-Review-of-Deep-Learning-Application-in-Oral-Begum-Vidyullatha/f5f45ddcb30463fd3160e93b9123c3888568722c",
            "/paper/A-Current-Review-of-Machine-Learning-and-Deep-in-Dixit-Kumar/fb5fcf878c1f390c0eb1b81e30a8672589459117",
            "/paper/The-Comparison-of-Deep-Learning-Model-Efficiency-of-Phosri-Treebupachatsakul/f10951d0a1306baca353c369097e4cbd1d62afa4",
            "/paper/Efficacy-of-Artificial-Intelligence-Assisted-of-on-Kim-Kim/4d645cab3ab7db16a129b83d947e8fee7b3c5c6c",
            "/paper/Use-of-Artificial-Intelligence-in-the-of-Elementary-Gomes-Schmith/99fa75de709fa077e619b2385f378b5c590e3071",
            "/paper/Automated-Detection-and-Classification-of-Oral-Deep-Welikala-Remagnino/230b8255925d56b72444df3f358f5851b3158b59",
            "/paper/Automated-detection-of-oral-pre-cancerous-tongue-of-Shamim-Syed/395cc086fa298cf161bcf51a4bf8be02a647bc3f",
            "/paper/A-deep-learning-algorithm-for-detection-of-oral-A-Fu-Chen/9244964828f717dc94ad4adb24dc16d8dd899542",
            "/paper/Evaluation-of-a-low-cost%2C-portable-imaging-system-Rahman-Ingole/0c03f338e44c6785f75db8f1aea3cba298857fd7",
            "/paper/Computer-assisted-medical-image-classification-for-Jeyaraj-Nadar/11ded8e630c3d52bf3563c30b72cc06f6e66af7f",
            "/paper/Objective-Detection-and-Delineation-of-Oral-Using-Roblyer-Kurachi/4de3e58f4261d51ca1ebede64e09b55a32a3769f",
            "/paper/Utilizing-Mask-R-CNN-for-Detection-and-Segmentation-Anantharaman-Velazquez/400ab4404ac1b2701db7622e1456a90257de92d1",
            "/paper/Automatic-classification-of-dual-modalilty%2C-oral-Song-Sunny/eeebfa2343ef9180d647d7bbec008e249d250d88",
            "/paper/Optical-Coherence-Tomography-as-an-Oral-Cancer-in-a-Heidari-Sunny/34155efc413d7fa86ad0783b9e1f35f73a4fe74b",
            "/paper/Oral-cancer%3A-current-and-future-diagnostic-Scully-Bagan/e1a17166c35a4e4c19f818e79e286e995305f483"
        ]
    },
    {
        "id": "1e2dadc12c0dc6d7574aa830221abe08114810f4",
        "title": "Mixture Modeling of 2-D Gel Electrophoresis Spots Enhances the Performance of Spot Detection",
        "abstract": "A three-step algorithm based on mixture of 2-D normal distribution functions is introduced to improve the efficiency of spot detection performed by the existing algorithms, namely Pinnacle software and watershed segmentation method. 2-D gel electrophoresis is the most commonly used method in biomedicine to separate even thousands of proteins in a complex sample on a single gel. Even though the technique is quite known, there is still a need to find an efficient and reliable method for detection of protein spots on gel image. In this paper, a three-step algorithm based on mixture of 2-D normal distribution functions is introduced to improve the efficiency of spot detection performed by the existing algorithms, namely Pinnacle software and watershed segmentation method. Comparison of methods is based on using simulated and real data sets with known true spot positions and different number of spots. Fitting a mixture of components to gel image allows for achieving higher sensitivity in detecting spots, regardless the method used to find initial conditions for the model parameters, and it leads to better overall performance of spot detection. By using mixture model, location of spot centers can be estimated with higher accuracy than using the Pinnacle method. An application of spot shape modeling gives higher sensitivity of obtaining low-intensity spots than the watershed method, which is crucial in the discovery of novel biomarkers.",
        "publication_year": "2017",
        "authors": [
            "M. Marczyk"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "5",
        "reference_count": "30",
        "references": [
            "/paper/Processing-2D-Gel-Electrophoresis-Images-for-Marczyk/62efaab94aca60429afedac1c2d41ec6ce9f7453",
            "/paper/Preliminary-study-for-a-fully-automated-pre-gating-Suwalska-Pola%C5%84ska/206b62190f89c07bb23688847517b5397be0bd78",
            "/paper/POLCOVID%3A-a-multicenter-multiclass-chest-X-ray-Suwalska-Tobiasz/6c814dcca13812e254990aa26ad77a64cf3fe714",
            "/paper/CIRCA%3A-comprehensible-online-system-in-support-of-Prazuch-Suwalska/a53e428eec311e7b1880f2d009792b7798c975d2",
            "/paper/Computational-Methods-for-Proteome-Analysis-Navakauskien%C4%97-Navakauskas/e064f3e5d984dc8761b3410628cb0bdb3b26813a",
            "/paper/Improved-Detection-of-2D-Gel-Electrophoresis-Spots-Marczyk/aeecbda3acfeedbf7daf540d8493dfe443e3a022",
            "/paper/Spot-quantification-in-two-dimensional-gel-image-of-Brauner-Groemer/5576b0d7a5ee8a2c9d0eae8f20bfe34d905ac923",
            "/paper/2D-gel-spot-detection-and-segmentation-based-on-and-Kostopoulou-Katsigiannis/265dbaad6472c987ffbc8287bbf7c377a2b1f570",
            "/paper/New-approach-for-segmentation-and-quantification-of-Anjos-M%C3%B8ller/02ec43c470666ff4d3366df490514781763181b7",
            "/paper/The-state-of-the-art-in-the-analysis-of-gel-images-Berth-Moser/cdcfc10f61bef8b922b5fe5be5c6ade539e15ff8",
            "/paper/An-improved-pixel%E2%80%90based-approach-for-analyzing-in-Rye-F%C3%A6rgestad/65471e955bf5f7c6a2b7ae9a8e8df2c377bd17cb",
            "/paper/Computer-analysis-of-two%E2%80%90dimensional-gels%3A-A-new-Bettens-Scheunders/6caf6563964991b1983e446832c9625576485c0a",
            "/paper/A-novel-multi-scale-Hessian-based-spot-enhancement-Shamekhi-Baygi/33402e0ceb6d4cd73202ef6ae46987e579bb110b",
            "/paper/Protein-spot-detection-and-quantification-in-2%E2%80%90DE-Tsakanikas-Manolakos/981f7367347008ab3c67604835324b095fddda01",
            "/paper/An-image-analysis-suite-for-spot-detection-and-spot-Srinark-Kambhamettu/8c008860d6864d0dd17883c18f03f3926338f62f"
        ]
    },
    {
        "id": "44607c13992295ad450b69ab940a38b634929b48",
        "title": "Using Deep Learning Approach for Land-Use and Land-Cover Classification based on Satellite images",
        "abstract": "The land cover is the apparent (bio)physical cover, and land use alludes to how the actual land type is being utilized. This research is fundamental to survey the degree to which social, monetary, and natural factors influence urbanization. This will likewise assist with urban planning. As laborious process of handcrafted feature extraction has not helped obtain high accuracies, this paper proposes use of Deep Learning approach that explores different Image Recognition Models using various ML classifiers on remote sensing images classifying the images from large Landsat satellite dataset into 9 different classes. It was observed that the highest accuracy of 97.4% was achieved by the Logistic Regression algorithm coupled with Inceptionv3 model. The proposed model shows the capability of increasing the accuracy of existing state-of-art-algorithms low resolution land classification maps. Thus, the improved results will contribute to better land maps helping with the growing demand of LULC information concerning climate change and sustainable development.",
        "publication_year": "2022",
        "authors": [
            "Rashi Agarwal",
            "Silky Goel",
            "Rahul Nijhawan"
        ],
        "related_topics": [
            "Environmental Science"
        ],
        "citation_count": 0,
        "reference_count": "39",
        "references": [
            "/paper/A-Futuristic-Deep-Learning-Framework-Approach-for-Nijhawan-Joshi/e295de834977fb75d509d5f5e15a16a04d347d2f",
            "/paper/A-Framework-for-Evaluating-Land-Use-and-Land-Cover-Carranza-Garc%C3%ADa-Garc%C3%ADa-Guti%C3%A9rrez/fa0a7b4988b593112464cf06845f5fa0430421cf",
            "/paper/EuroSAT%3A-A-Novel-Dataset-and-Deep-Learning-for-Land-Helber-Bischke/9c88c2357abcd58cc330179c1965fe0a8c067ebc",
            "/paper/Segmentation-of-Satellite-Imagery-using-U-Net-for-Ulmas-Liiv/867ddab0cf818baaa8c88d6bd983b2569755ce46",
            "/paper/Land-Use-and-Land-Cover-Classification-Using-CNN%2C-%26-Dewangkoro-Arymurthy/88bcbd65634c529a0646281e98619fb000821d18",
            "/paper/Differences-of-image-classification-techniques-for-Mahmon-Ya'acob/2a2c207409678a8fae0a2150940cb75aa183767e",
            "/paper/Improving-the-Accuracy-of-Land-Use-and-Land-Cover-Manandhar-Odeh/8d58953f980a20ea5727826e30b46078646dbd08",
            "/paper/Land-Use-Land-Cover-Classification-by-Machine-for-A-Talukdar-Singha/20966ec8b64dd7b713873c3a13e3d8e25d1d833f",
            "/paper/CLASSIFICATION-OF-LAND-COVER-AND-LAND-USE-BASED-ON-Yang-Rottensteiner/d5bb684bbd39ee0444908ce0144d0ca543c36dfe",
            "/paper/Land-Use-and-Land-Cover-Classification-Using-Neural-Vignesh-Thyagharajan/5a9067902b7657abfe42a2aadec06821cd78dc63"
        ]
    },
    {
        "id": "7a7542ae9a5914a51fabdae6537c2fe387f6e918",
        "title": "A meshfree formulation for large deformation analysis of flexoelectric structures accounting for the surface effects",
        "abstract": "Semantic Scholar extracted view of \"A meshfree formulation for large deformation analysis of flexoelectric structures accounting for the surface effects\" by X. Zhuang et al.",
        "publication_year": "2019",
        "authors": [
            "X. Zhuang",
            "S. Nanthakumar",
            "T. Rabczuk"
        ],
        "related_topics": [
            "Physics",
            "Engineering"
        ],
        "citation_count": "14",
        "reference_count": "40",
        "references": [
            "/paper/A-Numerical-Framework-for-Geometrically-Nonlinear-Kim/38dd3edbb031762038042ee55d29e2ca075dfae0",
            "/paper/Modeling-flexoelectricity-in-soft-dielectrics-at-Codony-Gupta/5aabc786503034531edb8198151dbf2dfbe6d79a",
            "/paper/Topology-optimization-of-flexoelectric-with-Greco-Codony/246c5e0d6049944135d762718c69856614bde728",
            "/paper/Free-vibration-analysis-of-rotating-piezoelectric-Hosseini-Beni/3f9f38f6929504d4430f88288d8eea8cd23c7bc6",
            "/paper/Static-and-dynamic-flexoelectric-effects-on-wave-in-Gupta-Singh/0838593d58c37d32f9e6886b6a2c8c489a1e4884",
            "/paper/Weak-enforcement-of-interface-continuity-and-in-Barcel%C3%B3-Mercader-Codony/8e2f62d387381415309d1c218197f61d0e16feb6",
            "/paper/Mathematical-and-computational-modeling-of-Codony-Mocci/18e970a769a00befcf7aff17ee0099c160fc3b36",
            "/paper/The-existence-and-uniqueness-theorem-for-linear-and-Deng-Yu/cc8232c02df94aa9544328d4f9950066da157394",
            "/paper/A-New-Meshless-Fragile-Points-Method-(FPM)-With-at-Guan-Dong/3391d8c453e625c18eabb83aa1f4331be35a3bd3",
            "/paper/Analytical-and-meshless-numerical-approaches-to-%C5%BBur-Faghidian/358273e2c120f107b0dec61c7a8dc34d3248abbb",
            "/paper/A-theory-of-flexoelectricity-with-surface-effect-Shen-Hu/95494bc33cecf7b96c5fe430f4dc88f602b1023d",
            "/paper/Phase-field-modeling-of-flexoelectricity-in-solid-Chen-Zhang/32be513855e6129ef0c7610365caf0b2ea74061a",
            "/paper/Computational-evaluation-of-the-flexoelectric-in-Abdollahi-Peco/2bd021f91661db2c2e23f6c07e80f3ecaee8a8a3",
            "/paper/A-numerical-framework-for-modeling-flexoelectricity-Yvonnet-Liu/e53d4c541a4f6b1e4c22d39649955f59449e5095",
            "/paper/Mixed-finite-element-formulations-in-and-Mao-Purohit/35230e2c7b0751fef615abea462fbaf572421a29",
            "/paper/Flexoelectric-effect-on-the-electroelastic-of-Yan-Jiang/f80e8663373e96a8e6e39d5d64980e0e2a156613",
            "/paper/Topology-optimization-of-flexoelectric-structures-Nanthakumar-Zhuang/279ff91f5222645f0f0aa744bf5974143d55ceb5",
            "/paper/Phase-field-modeling-of-flexoelectric-effects-in-Chen-Soh/64527aca066f496631154bd536b4528c62afce12",
            "/paper/Enhanced-size-dependent-piezoelectricity-and-in-due-Majdoub-Sharma/0bdeff278763d194147019c1e9712f41b826c4a3",
            "/paper/An-extended-finite-element-level-set-method-to-on-Farsad-Vernerey/cd1ab3008ba8d9ba7ac186b5addfdf98bd65c790"
        ]
    },
    {
        "id": "7ec1909b3c7b29c4a7b09f56f6449c0ccb7d08e6",
        "title": "Close Dominance Graph: An Efficient Framework for Answering Continuous Top- $k$  Dominating Queries",
        "abstract": "This work proposes a unique indexing structure, called a Close Dominance Graph (CDG), to support the processing of a cTKDQ and shows that this scheme is able to offer much better performance when compared with existing solutions. There are two preference-based queries commonly used in database systems: (1) top-k query and (2) skyline query. By combining the ranking rule used in top-\\(k\\) query and the notion of dominance relationships utilized in the skyline query, a top-\\(k\\) dominating query emerges, providing a new perspective on data processing. This query returns the \\(k\\) records with the highest domination scores from the dataset. However, the processing of the top-\\(k\\) dominating query is complex when the dataset operates under a streaming model. With new data being continuously generated while stale data being removed from the database, a continuous top-\\(k\\) dominating query (cTKDQ) requires that updated results can be returned to users at any time. This work explores the cTKDQ problem and proposes a unique indexing structure, called a Close Dominance Graph (CDG), to support the processing of a cTKDQ. The CDG provides comprehensive information regarding the dominance relationship between records, which is vital in answering a cTKDQ with a limited search space. The update process for a cTKDQ is then converted to a simple update affecting a small portion of the CDG. Experimental results show that this scheme is able to offer much better performance when compared with existing solutions.",
        "publication_year": "2014",
        "authors": [
            "B. J. Santoso",
            "Ge-Ming Chiu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "29",
        "reference_count": "41",
        "references": [
            "/paper/Dictionary-Compression%3A-An-Optimal-Answering-for-Parkavi/1c2426ec1980487107d8d9397e814bad141ac899",
            "/paper/Top-k-Dominating-Queries-on-Incomplete-Data-Miao-Gao/a747dbfe0f35784b119fd5e9e257b17b0232cc12",
            "/paper/Space-Filling-Approach-for-Distributed-Processing-Amagata-Hara/8b570e0573ff7ad0eb53d0a994735b72e9ece29f",
            "/paper/Efficient-Processing-of-Reverse-Top-k-Dominating-Jiang-Zhang/1195108ab46bd6c00ea714ff1549a5e10d8cf9eb",
            "/paper/Efficient-processing-of-top-k-group-skyline-queries-Yang-Zhou/cc2c4771a02086bed5c70cb8f996d6ad955d6947",
            "/paper/Continuous-Top-k-Dominating-Query-of-Incomplete-Santoso-Permadi/04da76e3d554549d36d3120a15116fc490b790fc",
            "/paper/Probabilistic-Top-k-Dominating-Queries-in-Uncertain-Rai-Lian/5dd6150e76fb2334bbe662e96f901ff71aa7eef2",
            "/paper/Efficient-Top-k-Dominating-Computation-on-Massive-Han-Li/0816fab0ea3f1e8846b6fa826c45dbd489012904",
            "/paper/Efficient-Approaches-to-k-Representative-G-Skyline-Zhou-Li/c0389a2bcab0e90f95dfe7bf420a6e48dff05853",
            "/paper/Multi-keyword-score-threshold-and-B%2B-tree-indexing-Lekshmi-Prem/cb9eca5b786710cf2291cb624ce5209dc60ac684",
            "/paper/Pareto-Based-Dominant-Graph%3A-An-Efficient-Indexing-Zou-Chen/a0378320ca434a40c392df7a964d020afd047869",
            "/paper/Multi-dimensional-top-k-dominating-queries-Yiu-Mamoulis/5118b57f47915d49e9a78e41496383fd9649525e",
            "/paper/Continuous-Top-k-Dominating-Queries-Kontaki-Papadopoulos/caf62f1c21a671e7226078e8287995012a92c14e",
            "/paper/Continuous-Top-k-Dominating-Queries-in-Subspaces-Kontaki-Papadopoulos/0693c6b9323a59d1890dec9fad454ef66994f79d",
            "/paper/Top-k-Exploration-of-Query-Candidates-for-Efficient-Tran-Wang/a9ecf2ae6b009e80ad5b4a9f81b116c2ee1face7",
            "/paper/Continuous-monitoring-of-top-k-queries-over-sliding-Mouratidis-Bakiras/f2032d4c266421cf1f120ddce0ed539fa234bf02",
            "/paper/On-efficient-top-k-query-processing-in-highly-Vlachou-Doulkeridis/6f804e0a515cc8a61b92f70c339ed4afdc169a6a",
            "/paper/A-Generic-Framework-for-Top-k-Pairs-and-Top-k-over-Shen-Cheema/9ecfcf61ae36acf9c43704f681429ac2a836fc57",
            "/paper/Top-k-dominating-queries-in-uncertain-databases-Lian-Chen/3b7e4d1767da71035e790e2eed7c53005daf9cca",
            "/paper/A-Generic-Framework-for-Top-%24%7B%5Cschmi-k%7D%24-Pairs-and-Shen-Cheema/371899e946f2015fedfa1cfb825a47a44cc4e239"
        ]
    },
    {
        "id": "49c2f3de221ed0a688731b6812540cc11d5a0300",
        "title": "A Sequential Color Correction Approach for Texture Mapping of 3D Meshes",
        "abstract": "This work proposes a novel color-correction approach, called sequential pairwise color correction, capable of color correcting multiple images from the same scene, using a pairwise-based method, and shows that the proposed approach outperforms several state-of-the-art color-Correction algorithms, both in qualitative and quantitative evaluations. Texture mapping can be defined as the colorization of a 3D mesh using one or multiple images. In the case of multiple images, this process often results in textured meshes with unappealing visual artifacts, known as texture seams, caused by the lack of color similarity between the images. The main goal of this work is to create textured meshes free of texture seams by color correcting all the images used. We propose a novel color-correction approach, called sequential pairwise color correction, capable of color correcting multiple images from the same scene, using a pairwise-based method. This approach consists of sequentially color correcting each image of the set with respect to a reference image, following color-correction paths computed from a weighted graph. The color-correction algorithm is integrated with a texture-mapping pipeline that receives uncorrected images, a 3D mesh, and point clouds as inputs, producing color-corrected images and a textured mesh as outputs. Results show that the proposed approach outperforms several state-of-the-art color-correction algorithms, both in qualitative and quantitative evaluations. The approach eliminates most texture seams, significantly increasing the visual quality of the textured meshes.",
        "publication_year": "2023",
        "authors": [
            "Lucas Dal\u2019Col",
            "Daniel Coelho",
            "Tiago Madeira",
            "Paulo Dias",
            "Miguel Oliveira"
        ],
        "related_topics": [
            "Computer Science",
            "Art"
        ],
        "citation_count": 0,
        "reference_count": "66",
        "references": [
            "/paper/A-Robust-3D-Based-Color-Correction-Approach-for-Coelho-Dal%E2%80%99Col/7aea5ee6a11ca4666aeedea9001490e710c9a0fb",
            "/paper/Color-Correction-for-Image-Based-Modeling-in-the-Shen-Wang/488dc439af342a702bfcc5ab50e5e35272e4a4b1",
            "/paper/Robust-Texture-Mapping-Using-RGB-D-Cameras-Oliveira-Lim/e46b23ba2f6fce4fdecbe677b807d7a5300aa415",
            "/paper/Seamless-Texture-Optimization-for-RGB-D-Fu-Yan/d2b89d018b8423473f7478261508c70022c279d6",
            "/paper/Seamless-image-based-texture-atlases-using-blending-All%C3%A8ne-Pons/9f7be8afe4662fd01e850ebcc70576c520538edc",
            "/paper/Efficient-global-color-correction-for-large-scale-Yang-Liu/9868b1a123ec1fd2724f48380c35eb7e12c3ca4b",
            "/paper/A-closed-form-solution-for-multi-view-color-with-Xia-Yao/84a259fcb9268c82403a76e6db897d67e84c572d",
            "/paper/Seamless-Mosaicing-of-Image-Based-Texture-Maps-Lempitsky-Ivanov/8f7ff9e9474a5eb1bac32c0a649d6ccd152a3674",
            "/paper/Fast-color-correction-using-principal-regions-in-Zhang-Georganas/322f2f0a9eab458bc965b98b10062e6dcc6435b9",
            "/paper/Jointly-optimizing-global-and-local-color-for-image-Li-Menghan/cc752b73595183142f4d6b839fb2af06febbd6ee"
        ]
    },
    {
        "id": "bacf3f214aff330949cc4d1be6773d9c1e52f6aa",
        "title": "SiamADT: Siamese Attention and Deformable Features Fusion Network for Visual Object Tracking",
        "abstract": "Semantic Scholar extracted view of \"SiamADT: Siamese Attention and Deformable Features Fusion Network for Visual Object Tracking\" by Fasheng Wang et al.",
        "publication_year": "2023",
        "authors": [
            "Fasheng Wang",
            "Ping Cao",
            "Xing Wang",
            "B. He",
            "Fuming Sun"
        ],
        "related_topics": "",
        "citation_count": 0,
        "reference_count": "7",
        "references": [
            "/paper/Siamese-Centerness-Prediction-Network-for-Real-Time-Wu-Cai/93593630a30eeb44905cd1f3d512ea814fbf1f62",
            "/paper/Learning-Saliency-Aware-Correlation-Filters-for-Wang-Wang/44b094a8adc16ccf7be558cbb653b2ae4c0edd18",
            "/paper/Overview-and-methods-of-correlation-filter-in-Liu-Liu/b1c2549c94cd0070c1725c2a5306970aa34ad2bc",
            "/paper/Distance-IoU-Loss%3A-Faster-and-Better-Learning-for-Zheng-Wang/63a243afcb133569a962c41e9db956c076c5c4f3",
            "/paper/Deep-Learning-and-Preference-Learning-for-Object-A-Pang-Coz/256246194cfac55e16a96e081310b0547a8108f1",
            "/paper/UnitBox%3A-An-Advanced-Object-Detection-Network-Yu-Jiang/22264e60f1dfbc7d0b52549d1de560993dd96e46",
            "/paper/Into-the-woods%3A-visual-surveillance-of-and-targets-Boult-Micheals/94aa37507a816f1fe2afc30eabd8ecb4fa180b8e"
        ]
    },
    {
        "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
        "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "abstract": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features. State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available",
        "publication_year": "2015",
        "authors": [
            "Shaoqing Ren",
            "Kaiming He",
            "Ross B. Girshick",
            "Jian Sun"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "43,837",
        "reference_count": "45",
        "references": [
            "/paper/R-FCN%3A-Object-Detection-via-Region-based-Fully-Dai-Li/b724c3f7ff395235b62537203ddeb710f0eb27bb",
            "/paper/Revisiting-Faster-R-CNN%3A-A-Deeper-Look-at-Region-Han-Zhang/f249c266321d661ae398c26ddb8c7409f6455ba1",
            "/paper/Relief-R-CNN%3A-Utilizing-Convolutional-Feature-for-Li-Liu/a5ae7d662ed086bc5b0c9a2c1dc54fcb23635000",
            "/paper/R-FCN%2B%2B%3A-Towards-Accurate-Region-Based-Fully-for-Li-Chen/f4a2732d4051b9c4b5d1f057aaa7935be390f51e",
            "/paper/BackNet%3A-An-Enhanced-Backbone-Network-for-Accurate-Hossain-Teng/847554335d38454d5a761c5c9b785bf1fa591257",
            "/paper/Relief-R-CNN%3A-Utilizing-Convolutional-Features-for-Li-Liu/7c2f6424b0bb2c28f282fbc0b4e98bf85d5584eb",
            "/paper/Rich-Features-and-Precise-Localization-with-Region-Chu-Wu/1c07e9a6395d352819f4800d7ae13c5e0fcb3a4c",
            "/paper/Multi-scale-Region-Proposal-Network-Trained-by-for-Fang-Ko/5f75318e37add49f810c00299498d6fa80fb7b9d",
            "/paper/Atrous-Faster-R-CNN-for-Small-Scale-Object-Guan-Zhu/9b7af646fc817fa1aca2fc6655d64686a65ab17a",
            "/paper/Real-Time-Object-Detection-With-Reduced-Region-via-Shih-Chiu/ccb572d3be31406a148a750475f1f96bd66725de",
            "/paper/Fast-R-CNN-Girshick/7ffdbc358b63378f07311e883dddacc9faeeaf4b",
            "/paper/R-CNN-minus-R-Lenc-Vedaldi/a0afd098e7f56ae5e68ee2c95c2566af28064d54",
            "/paper/Scalable-Object-Detection-Using-Deep-Neural-Erhan-Szegedy/67fc0ec1d26f334b05fe66d2b7e0767b60fb73b6",
            "/paper/Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue/2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "/paper/Object-Detection-Networks-on-Convolutional-Feature-Ren-He/f075f89b4f4026748cbf2fb9f989a9934c42ee8f",
            "/paper/Convolutional-feature-masking-for-joint-object-and-Dai-He/3ad998a9b2c071c4a1971048f8a2d754530f08e8",
            "/paper/Learning-to-Segment-Object-Candidates-Pinheiro-Collobert/6b8d0df903496699e52b4daee5d1815b7b784cf7",
            "/paper/DeePM%3A-A-Deep-Part-Based-Model-for-Object-Detection-Zhu-Chen/b07f1d518d08012b9d12eaead0fdb6234e71a680",
            "/paper/Instance-Aware-Semantic-Segmentation-via-Multi-task-Dai-He/1e9b1f6061ef779e3ad0819c2832a29168eaeb9d",
            "/paper/Scalable%2C-High-Quality-Object-Detection-Szegedy-Reed/4328ec9d98eff5d7eb70997f76d81b27849f3220"
        ]
    },
    {
        "id": "0cdac63e2580ff92c0e8d34407edcf7a45412c45",
        "title": "Direct Trajectory Planning Method Based on IEPSO and Fuzzy Rewards and Punishment Theory for Multi-Degree-of Freedom Manipulators",
        "abstract": "A direct trajectory planning method based on an improved particle swarm optimization (PSO) algorithm and the fuzzy rewards and punishment theory is proposed, which can not only ensure the accuracy of manipulator trajectory planning but also effectively reduce the calculation amount of the trajectory planning for the multi-degree-of-freedom manipulator to improve the optimization efficiency. Manipulator is a kind of commonly used multi-degree-of-freedom nonlinear system. There is a strong coupling between the links, and the movement of each link constraints and affects each other, which increases the difficulty of motion analysis of the system and reduces its trajectory planning efficiency under specific task targets. To solve this problem, a direct trajectory planning method based on an improved particle swarm optimization (PSO) algorithm, called IEPSO, and the fuzzy rewards and punishment theory is proposed in this paper. First, on the basis of preserving the local search ability of PSO, the global search ability of the population is improved by increasing a population exchange item. At the same time, in order to avoid the population falling into the local optimal value, the last elimination principle is incorporated into the standard PSO algorithm. Second, the fuzzy rewards and punishment theory is introduced to reduce the redundant decoupling operation, which can not only ensure the accuracy of manipulator trajectory planning but also effectively reduce the calculation amount of the trajectory planning for the multi-degree-of-freedom manipulator, to improve the optimization efficiency. Finally, the direct trajectory planning method of the multi-degree-of-freedom manipulator is compared and tested. It can be seen that the efficiency scalar and accuracy of the proposed direct trajectory planning method are significantly higher than those of other optimization methods.",
        "publication_year": "2019",
        "authors": [
            "Xueying Lv",
            "Zhaoxia Yu",
            "Mingyang Liu",
            "Guanyu Zhang",
            "Liu Zhang"
        ],
        "related_topics": [
            "Computer Science",
            "Engineering"
        ],
        "citation_count": "6",
        "reference_count": 0,
        "references": [
            "/paper/Time-Optimal-Trajectory-Planning-for-the-Based-on-Hou-Du/82a86882dc6e03648fa0399fb6e705999fe5223c",
            "/paper/Online-Time-Optimal-Trajectory-Planning-for-Robotic-Liu-Guo/9cad3721e9b500cd8846f9ffebd0f4a53b7cacd3",
            "/paper/Design-and-Modeling-of-9-Degrees-of-Freedom-Robotic-Anjum-Samo/5e663712f91f88198f0fa21df8ef8b56b36860af",
            "/paper/Opposition-Based-Constriction-Factor-Particle-Swarm-S.-Babu/2e396e5857ca61e877ce2fee0110a79eb297ebfb",
            "/paper/Improved-Eliminate-Particle-Swarm-Optimization-on-Yulianto-Arifando/337b3ee73507a2da3c388977dfefd5c058e50f37",
            "/paper/Correction%3A-Zhang-G.-et-al.-Autonomous-Operation-of-Fan-Lv/c7cc61f350f2c9735a8b02e4e8267458d81204c4"
        ]
    },
    {
        "id": "f903e1e285effd67fb141a55c6eb2b48bbbc4f86",
        "title": "Reviews on Planar Region Detection for Visual Navigation of Mobile Robot under Unknown Environment",
        "abstract": "A brief review about state-of-art method for planar region detection is presented, that is stereo-based, monocular-based and multi-sensor fusion based method according to the sensor they used. Dominant plane detection is an essential task for an autonomous navigation of mobile robots equipped with a vision system, as we assume that the robot moves on the dominant plane. In this paper, a brief review about state-of-art method for planar region detection is presented. All these methods are classified into three types, that is stereo-based, monocular-based and multi-sensor fusion based method according to the sensor they used. And the monocular-based method is further divided into three kinds, projection invariance, homography based and optical flow based method. For each kind of the method, the correspondent references and its main process are summarized. Finally, the challenges for plane detection with vision are presented in authors view. We hope that this paper can be a start point to probe deep insight in this field.",
        "publication_year": "2011",
        "authors": [
            "Zhong-li Wang",
            "B. Cai",
            "F. Yi",
            "M. Li"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "50",
        "references": [
            "/paper/Planar-flow-based-planar-region-extraction-for-a-Wang-Yi/e6c033666c1e803921405b92af522ed87126d5a1",
            "/paper/An-Embedded-Classifier-for-Mobile-Robot-Using-and-Sampaio-Silva/719189a4d2915275425de821f63b4a53cd9ac71a",
            "/paper/Planar-object-detection-from-3D-point-clouds-based-Hu-Bai/80c700444196f7f05e3431f3075ab720da373648",
            "/paper/Vers-un-syst%C3%A8me-de-vision-artificielle-opportuniste-Mai/f4eb1ee5c8634e7079ad9841b75fcd119e6f7725",
            "/paper/Homography-based-ground-plane-detection-for-mobile-Conrad-DeSouza/c9867b36f37387cedafcdcab91899bbe2eae1b39",
            "/paper/Vision-based-real-time-obstacle-detection-and-for-Yang-Yu/1ec16512e56183c65e61049fc81d45fe86af8bc1",
            "/paper/Visual-Detection-of-Obstacles-Assuming-a-Locally-Lourakis-Orphanoudakis/a329fc626225e8b191195d1a4c4ed6290cc4a58c",
            "/paper/Layered-ground-floor-detection-for-vision-based-Kim-Kim/ceea410c95792740cb31f9e9ae4c7fa37453ac9f",
            "/paper/Dominant-plane-detection-from-optical-flow-for-Ohnishi-Imiya/49a59250716c524e7e24d36a7e2db9b08dac92b3",
            "/paper/Uncalibrated-obstacle-detection-using-normal-flow-Santos-Victor-Sandini/d0394f4cf681b001f10ee7e70acfcd7fb160499b",
            "/paper/Efficient-plane-detection-from-a-single-moving-Bouchafa-Patri/c7dd76e2d014a7aee28de00d5bcdca92450baa84",
            "/paper/Motion-Estimation-for-a-Mobile-Robot-Based-on-Wang-Gao/4a21381aa655f69c8608fd802e6cff80c6feedab",
            "/paper/An-Efficient-Approach-to-Onboard-Stereo-Vision-Pose-Sappa-Dornaika/d8d1fb804d1f4760393c6fd70c9072fa1b39f02c",
            "/paper/Planar-direct-method%3A-a-new-framework-for-stereo-Vincent-Tjahjadi/90c706594fd6f49519e48cd08229b3060de56c75"
        ]
    },
    {
        "id": "79112ae62a5f8ce03829d3c77e218269d4c0d35c",
        "title": "Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation",
        "abstract": "This paper proposes to automate the design of metric-specific loss functions by searching differentiable surrogate losses for each metric by substitute the non-differentiable operations in the metrics with parameterized functions, and conduct parameter search to optimize the shape of loss surfaces. Designing proper loss functions is essential in training deep networks. Especially in the field of semantic segmentation, various evaluation metrics have been proposed for diverse scenarios. Despite the success of the widely adopted cross-entropy loss and its variants, the mis-alignment between the loss functions and evaluation metrics degrades the network performance. Meanwhile, manually designing loss functions for each specific metric requires expertise and significant manpower. In this paper, we propose to automate the design of metric-specific loss functions by searching differentiable surrogate losses for each metric. We substitute the non-differentiable operations in the metrics with parameterized functions, and conduct parameter search to optimize the shape of loss surfaces. Two constraints are introduced to regularize the search space and make the search efficient. Extensive experiments on PASCAL VOC and Cityscapes demonstrate that the searched surrogate losses outperform the manually designed loss functions consistently. The searched losses can generalize well to other datasets and networks. Code shall be released.",
        "publication_year": "2020",
        "authors": [
            "Hao Li",
            "Chenxin Tao",
            "Xizhou Zhu",
            "Xiaogang Wang",
            "Gao Huang",
            "Jifeng Dai"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "18",
        "reference_count": "36",
        "references": [
            "/paper/AutoLoss-Zero%3A-Searching-Loss-Functions-from-for-Li-Fu/755cbcbbbc0eda95254390a9df7aded9fea84c25",
            "/paper/Automatic-Loss-Function-Search-for-Problems-with-Wang-Yi/d42b866ad0b57225916a96e1a50b44c00adaf47b",
            "/paper/Learning-Generalized-Intersection-Over-Union-for-Yu-Xu/1d7329bc40c4911a24af3e34b62d53d600824cb7",
            "/paper/Jaccard-Metric-Losses%3A-Optimizing-the-Jaccard-Index-Wang-Blaschko/c13cc96c717333e2854d4a2d0cee84794282d5be",
            "/paper/Neural-Architecture-Search-Survey%3A-A-Computer-Kang-Kang/82e597010ee1bede0b50f7ff8c6a249eba3a67a8",
            "/paper/AutoLoss-GMS%3A-Searching-Generalized-Margin-based-Gu-Li/62165de1bd768408ae19c7998f975e7736afe473",
            "/paper/Heterogeneous-data-fusion-and-loss-function-design-Liu-Tian/a219c192b21577e33033904d825aee30da1e3251",
            "/paper/AutoLossGen%3A-Automatic-Loss-Function-Generation-for-Li-Ji/54f1d97e5973d8dae0c57074c33510cee117bd05",
            "/paper/PolyLoss%3A-A-Polynomial-Expansion-Perspective-of-Leng-Tan/30c6126b7bb567d06c5c62ad811175fd400a38f8",
            "/paper/Automated-Progressive-Learning-for-Efficient-of-Li-Zhuang/159be298e25b7210ae577d7962cceb5e73aee687",
            "/paper/Neuro-IoU%3A-Learning-a-Surrogate-Loss-for-Semantic-Nagendar-Singh/e4450b61f1ccbe5bbec1e777baad5dd69fd6edbe",
            "/paper/The-Lovasz-Softmax-Loss%3A-A-Tractable-Surrogate-for-Berman-Triki/c7afd747b5c6b77dc22eaa87a8b22888243842b6",
            "/paper/AM-LFS%3A-AutoML-for-Loss-Function-Search-Li-Lin/f4a44b94d4b184e4638cb1019526170590f99ac4",
            "/paper/Learning-Surrogates-via-Deep-Embedding-Patel-Hodan/2efc828b7d4faaa98fc8d9390d64024036230f40",
            "/paper/Distance-Map-Loss-Penalty-Term-for-Semantic-Caliv%C3%A1-Iriondo/8d1ae1fb2fdaddd58e57f089a1df8c78786510cf",
            "/paper/BASNet%3A-Boundary-Aware-Salient-Object-Detection-Qin-Zhang/cebd4ab4ab52be88b26d976aa7d4fb35cc19c2a2",
            "/paper/Optimizing-Intersection-Over-Union-in-Deep-Neural-Rahman-Wang/64dc671107cfdf98e02e76cbc993732d47210549",
            "/paper/DeepLab%3A-Semantic-Image-Segmentation-with-Deep-and-Chen-Papandreou/cab372bc3824780cce20d9dd1c22d4df39ed081a",
            "/paper/Segmentation-Loss-Odyssey-Ma/3738ec445ccc6fc6460632b3e742597ddf147aea",
            "/paper/Proximal-Policy-Optimization-Algorithms-Schulman-Wolski/dce6f9d4017b1785979e7520fd0834ef8cf02f4b"
        ]
    },
    {
        "id": "9c042383bd8eb39eac442da96812e6b2273036bf",
        "title": "Octree Guided CNN With Spherical Kernels for 3D Point Clouds",
        "abstract": "The effectiveness of the proposed octree guided neural network architecture and spherical convolutional kernel for machine learning from arbitrary 3D point clouds is established on the benchmark tasks of 3D object classification and segmentation, achieving competitive performance on ShapeNet and RueMonge2014 datasets. We propose an octree guided neural network architecture and spherical convolutional kernel for machine learning from arbitrary 3D point clouds. The network architecture capitalizes on the sparse nature of irregular point clouds,and hierarchically coarsens the data representation with space partitioning. At the same time, the proposed spherical kernels systematically quantize point neighborhoods to identify local geometric structures in the data, while maintaining the properties of translation-invariance and asymmetry. We specify spherical kernels with the help of network neurons that in turn are associated with spatial locations.We exploit this association to avert dynamic kernel generation during network training that enables efficient learning with high resolution point clouds. The effectiveness of the proposed technique is established on the benchmark tasks of 3D object classification and segmentation, achieving competitive performance on ShapeNet and RueMonge2014 datasets.",
        "publication_year": "2019",
        "authors": [
            "Huan Lei",
            "Naveed Akhtar",
            "A. Mian"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "112",
        "reference_count": "46",
        "references": [
            "/paper/Spherical-Kernel-for-Efficient-Graph-Convolution-on-Lei-Akhtar/6265ed23e558a80c9af599477b847c4f7e0027fc",
            "/paper/PointSpherical%3A-Deep-Shape-Context-for-Point-Cloud-Lin-Fan/cc6dac718102aed2ccf881326a08e378e4ec00a8",
            "/paper/Point-Attention-Network-for-Semantic-Segmentation-Feng-Zhang/cc8b0f6c874cf018e4d2527870f517a2101d389b",
            "/paper/PAI-Conv%3A-Permutable-Anisotropic-Convolutional-for-Gao-Zhai/4d1241ca70fd81d8d001fa89fa0a69c782be4e2a",
            "/paper/Learning-to-Segment-3D-Point-Clouds-in-2D-Image-Lyu-Huang/99d0c46c0cf846c5849a4280c05489f817e735dc",
            "/paper/Permutation-Matters%3A-Anisotropic-Convolutional-for-Gao-Zhai/c3564076c2335624e526c98f6968800a541e184b",
            "/paper/3DDACNN%3A-3D-dense-attention-convolutional-neural-Han-Huang/1a311c467ec8dd3bb44a5a5a419b9c92a86f201d",
            "/paper/SegGCN%3A-Efficient-3D-Point-Cloud-Segmentation-With-Lei-Akhtar/13079de18092149aa9fb9ee6c64726d7c2df0348",
            "/paper/Semantic-Feature-Mining-for-3D-Object-and-Lu-Zhao/057230eb736125456b8abb51669f3ce5c86da263",
            "/paper/DV-ConvNet%3A-Fully-Convolutional-Deep-Learning-on-3D-Su-Tan/334c979ac6c0243136e5ceffb6feafb2a3baba53",
            "/paper/PointNet%3A-Deep-Learning-on-Point-Sets-for-3D-and-Qi-Su/d997beefc0922d97202789d2ac307c55c2c52fba",
            "/paper/Escape-from-Cells%3A-Deep-Kd-Networks-for-the-of-3D-Klokov-Lempitsky/f48d322244c906b45792b28206df7cfb23495004",
            "/paper/Vote3Deep%3A-Fast-object-detection-in-3D-point-clouds-Engelcke-Rao/da09eaa1412aaf0a62622f01f50e2e801a3b4c52",
            "/paper/OctNet%3A-Learning-Deep-3D-Representations-at-High-Riegler-Ulusoy/511ff60d4aa5ab2660552f02ee85692425912b46",
            "/paper/3D-Semantic-Segmentation-with-Submanifold-Sparse-Graham-Engelcke/1d6a5d0299ed8458191e4e0407d4d513e6a7dd7e",
            "/paper/FPNN%3A-Field-Probing-Neural-Networks-for-3D-Data-Li-Pirk/15ca7adccf5cd4dc309cdcaa6328f4c429ead337",
            "/paper/PointCNN-Li-Bu/10c6eddfff30567d8b06475422a60920f3c84857",
            "/paper/Spherical-CNNs-Cohen-Geiger/ce2845cadc5233ff0a647aa22ae3bbe646258890",
            "/paper/VoxNet%3A-A-3D-Convolutional-Neural-Network-for-Maturana-Scherer/3f1c6749edfaf4f89bac38b2d15a0493bd9aa253",
            "/paper/SyncSpecCNN%3A-Synchronized-Spectral-CNN-for-3D-Shape-Yi-Su/ade1b1bfe2abefbcfe324499aa284db2b8fc50a6"
        ]
    },
    {
        "id": "1039e8bfbcacfeb9f8941f09e827efa8badb58eb",
        "title": "Local Optimal-Oriented Pattern and Exponential Weighed-Jaya Optimization-Based Deep Convolutional Networks for Video Summarization",
        "abstract": "The ultimate goal of the research is to concentrate on an effective video summarization methodology that represents the development of short summary from the entire video stream in an effective manner. Video summarization is used to generate a short summary video for providing the users a very useful visual and synthetic abstract of the video content. There are various methods are developed for video summarization in existing, still an effective method is required due to some drawbacks, like cost and time. The ultimate goal of the research is to concentrate on an effective video summarization methodology that represents the development of short summary from the entire video stream in an effective manner. At first, the input cricket video consisting of number of frames is given to the keyframe generation phase, which is performed based on Discrete Cosine Transform (DCT) and Euclidean distance for obtaining the keyframes. Then, the residual keyframe generation is carried out based on Deep Convolutional Neural Network (DCNN), which is trained optimally using the proposed Exponential weighed moving average-Jaya (EWMA-Jaya) optimization.",
        "publication_year": "2022",
        "authors": [
            "L. Jimson.",
            "J. P. Ananth"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "28",
        "references": [
            "/paper/Video-Summarization-With-Attention-Based-Networks-Ji-Xiong/88a8baa1be5292e62622f1cb8e627fbf759bf741",
            "/paper/Video-Summarization-by-Learning-Deep-Side-Semantic-Yuan-Mei/493293a9f4b53146ef358269459205281fde96f9",
            "/paper/Deep-Attentive-Video-Summarization-With-Consistency-Ji-Zhao/c1f1a999d84dce8e84c28b7d33edd6aa706b85c2",
            "/paper/Motion-State-Adaptive-Video-Summarization-via-Zhang-Tao/7f93bff0d796f866f41bec712a73c5bbe2e112d8",
            "/paper/MSKVS%3A-Adaptive-mean-shift-based-keyframe-for-video-Hannane-Elboushaki/60d24a4ea84930968db44b17184c2dfc0b80d77e",
            "/paper/Rate-distortion-optimal-video-summary-generation-Li-Schuster/a35c960f11db579ac4f9a7116d8fcf92c3bd4bab",
            "/paper/Context-Driven-Optimized-Perceptual-Video-and-Thomas-Gupta/9b4335a255d9ea4b5dfa19f5c134f56119046769",
            "/paper/An-efficient-method-for-video-shot-boundary-and-Hannane-Elboushaki/c58958a1d50e3d31d2811ccf8da06254d6694d50",
            "/paper/Unsupervised-Video-Summarization-via-Relation-Aware-Gao-Yang/4a7de98a1e0a28a76236feb3599235253cd96f15",
            "/paper/Diversity-Aware-Multi-Video-Summarization-Panda-Mithun/bc7c8974c3990c9c7b6d20cfd748e3315c44bd26"
        ]
    },
    {
        "id": "32db2d409384575aeae453acc45220b51fe96301",
        "title": "Unsupervised Domain Adaptation for Nighttime Aerial Tracking",
        "abstract": "A unique object discovery approach is provided to generate training patches from raw nighttime tracking videos to tackle the domain discrepancy and demonstrate the robustness and domain adaptability of the proposed framework in nighttime aerial tracking. Previous advances in object tracking mostly reported on favorable illumination circumstances while neglecting performance at nighttime, which significantly impeded the development of related aerial robot applications. This work instead develops a novel unsupervised domain adaptation framework for nighttime aerial tracking (named UDAT). Specifically, a unique object discovery approach is provided to generate training patches from raw nighttime tracking videos. To tackle the domain discrepancy, we employ a Transformer-based bridging layer post to the feature extractor to align image features from both domains. With a Transformer day/night feature discriminator, the day-time tracking model is adversarially trained to track at night. Moreover, we construct a pioneering benchmark namely NAT2021 for unsupervised domain adaptive night-time tracking, which comprises a test set of 180 manually annotated tracking sequences and a train set of over 276k unlabelled nighttime tracking frames. Exhaustive experiments demonstrate the robustness and domain adaptability of the proposed framework in nighttime aerial tracking. The code and benchmark are available at https://github.com/vision4robotics/UDAT.",
        "publication_year": "2022",
        "authors": [
            "Junjie Ye",
            "Changhong Fu",
            "Guang-Zheng Zheng",
            "D. Paudel",
            "Guang Chen"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "15",
        "reference_count": "56",
        "references": [
            "/paper/Scale-Aware-Domain-Adaptation-for-Robust-UAV-Fu-Li/9dfd3330017c41f510d6e7274d2af46958a1ccf2",
            "/paper/Cascaded-Denoising-Transformer-for-UAV-Nighttime-Lu-Fu/6a86342bc85a21f7169e03b7a5425c3ad95cb166",
            "/paper/2PCNet%3A-Two-Phase-Consistency-Training-for-Domain-Kennerley-Wang/23c8a415b79d2a469d6eeed25056e60316f08009",
            "/paper/HighlightNet%3A-Highlighting-Low-Light-Potential-for-Fu-Dong/7d0a977179bfe592a29d6aac84d309d3e532e2a9",
            "/paper/A-robust-spatial-temporal-correlation-filter-for-Chen-Liu/1e318864509718478dbf363fee229594a7fda5d3",
            "/paper/Siamese-Object-Tracking-for-Vision-Based-UAM-with-Zheng-Fu/e4aca36034926c355b57411bec6cba467b4ed6af",
            "/paper/Siamese-Object-Tracking-for-Unmanned-Aerial-A-and-Fu-Lu/1171234cb2f3e1589592e3d04eb10c132fc6a5c8",
            "/paper/PVT%2B%2B%3A-A-Simple-End-to-End-Latency-Aware-Visual-Li-Huang/8fa3221af7f6d3008e00cc0c459ecbde6f1014da",
            "/paper/GeoMultiTaskNet%3A-remote-sensing-unsupervised-domain-Marsocci-Gonthier/a7981aea995453e0fcaddccf9056d5e79ab30f65",
            "/paper/Object-Tracking-Based-on-a-Time-Varying-Regularized-Wang-Jia/dc1485141a67953c51af5a644b1b5107efff2399",
            "/paper/Tracker-Meets-Night%3A-A-Transformer-Enhancer-for-UAV-Ye-Fu/9e5d4094cbe46de2879ce588748083e1b52aa3be",
            "/paper/DarkLighter%3A-Light-Up-the-Darkness-for-UAV-Tracking-Ye-Fu/a0e544f6db3b6659a5f77c419908238f91b188bc",
            "/paper/DANNet%3A-A-One-Stage-Domain-Adaptation-Network-for-Wu-Wu/70116719b7e4d8f5dfe9729b37bebb3e731f0dd2",
            "/paper/ADTrack%3A-Target-Aware-Dual-Filter-Learning-for-UAV-Li-Fu/689b230b228c7ff5e2bb5d500c5349f54bcc6d3c",
            "/paper/HiFT%3A-Hierarchical-Feature-Transformer-for-Aerial-Cao-Fu/9916ed982600be133ed2d185b70fe721809a3096",
            "/paper/Unsupervised-Deep-Representation-Learning-for-Wang-Zhou/b29eef7a4b8597ab4fc90579e1326980d6d16b51",
            "/paper/Siamese-Anchor-Proposal-Network-for-High-Speed-Fu-Cao/827ee2cdc56ea65ef644bd8ca085f4274b106f03",
            "/paper/Transformer-Meets-Tracker%3A-Exploiting-Temporal-for-Wang-Zhou/75284d5e4dfe1cd8a9ce69085210319e14fcfa3d",
            "/paper/AutoTrack%3A-Towards-High-Performance-Visual-Tracking-Li-Fu/0619650ae0f698bcc38244a6858cc270df9dfaad",
            "/paper/Multi-Regularized-Correlation-Filter-for-UAV-and-Ye-Fu/4d57a810cd768b5ad4de6d1e21673f3d3b89f00d"
        ]
    },
    {
        "id": "26f36554b6895af2dd7c4a74cf9021972d7afd52",
        "title": "Cross-Media and Multilingual Image Understanding Method Based on Attention Mechanism",
        "abstract": "Semantic Scholar extracted view of \"Cross-Media and Multilingual Image Understanding Method Based on Attention Mechanism\" by Lingchao Gao et al.",
        "publication_year": "2021",
        "authors": [
            "Lingchao Gao",
            "Zhenyu Chen",
            "Lutao Wang",
            "Bo Li",
            "Ling Nie",
            "Fei Zheng"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "27",
        "references": [
            "/paper/Mining-Spatial-Temporal-Similarity-for-Visual-Zhang-Gao/07d6f5d6f76becdd3d30dbc308c63dcf3fbae2b3",
            "/paper/Learning-passive%E2%80%93aggressive-correlation-filter-for-Zhang-Gao/012b6d96840abb0f11b27353a3d033317678ae0a",
            "/paper/Describing-Video-With-Attention-Based-Bidirectional-Bin-Yang/af6d6271f317a1a5a30908fdeac0fc054cd0493b",
            "/paper/CAM-RNN%3A-Co-Attention-Model-Based-RNN-for-Video-Zhao-Li/acc2cfe35343195a4f3d0df5d7841d47708208fb",
            "/paper/Spatiotemporal-Recurrent-Convolutional-Networks-for-Xia-Hong/257f768056aae1c4d976334cde8ef2d2aec410d9",
            "/paper/E2E-MLT-an-Unconstrained-End-to-End-Method-for-Text-Patel-Busta/2bd299a61f925ab71c5e0702ca298103493448a5",
            "/paper/A-study-of-evaluation-metrics-and-datasets-for-Park-Song/2e8d9299af393da5e7534f0a8cce5a270c0b7775",
            "/paper/Deep-TextSpotter%3A-An-End-to-End-Trainable-Scene-and-Busta-Neumann/64ff7f81f066a26a40f52e41931a97c166db094d",
            "/paper/Towards-End-to-End-Text-Spotting-with-Convolutional-Li-Wang/3470684522ba013135a61fd6644a102e2f14cc7c",
            "/paper/Dense-Captioning-Events-in-Videos-Krishna-Hata/96dd1fc39a368d23291816d57763bc6eb4f7b8d6"
        ]
    },
    {
        "id": "8c4ced538ca85ce29d45c31f856056e318a764e4",
        "title": "Multiple Object Tracking With Appearance Feature Prediction and Similarity Fusion",
        "abstract": "Experimental results demonstrate that the tracker proposed in this paper outperforms other trackers across five publicly available datasets, indicating its effectiveness and potential for further development. Object tracking is a crucial research area within the field of intelligent transportation, providing a vital foundation for anomalous behavior analysis and traffic statistics. Although pedestrian detectors have shown impressive results, leading to the advancement of detection-based tracking methods, target association in complex scenarios remains a difficult and less efficient task due to the lack of feature robustness in the presence of partial occlusions. In the proposed tracking method, we extract convolutional features on each entire object and its local blocks, segmented by the superpixel algorithm. Aiming to emphasize the global and local information respectively, the global features for each entire object are extracted from the last layer of the backbone network, while local features are derived from a specific intermediate layer of the backbone network. The association between tracked targets and detected pedestrian candidates relies on fused similarity degrees. Furthermore, we use the transformer\u2019s self-attention mechanism to predict features for the current frame based on the information within past frames, aiming to eliminate the effects of target appearance variations. Additionally, we remove redundant background pixels in the detected rectangles of pedestrian candidates by using a background modeling algorithm. Experimental results demonstrate that the tracker proposed in this paper outperforms other trackers across five publicly available datasets, indicating its effectiveness and potential for further development.",
        "publication_year": "2023",
        "authors": [
            "ZHiyuan Li",
            "Jing Chen",
            "Jieran Bi"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "31",
        "references": [
            "/paper/Superpixel-tracking-Wang-Lu/b762ecb0624005831f2f3d8eb626d53e8eca4b6c",
            "/paper/Effective-multiple-pedestrian-tracking-system-in-Wang-Li/8845dffedaf7080d22df02b5ffcd5a286c1d8568",
            "/paper/Object-Saliency-Aware-Dual-Regularized-Correlation-Fu-Xu/bb1e5031d0b5f905fa88ed248c44aa7165f10714",
            "/paper/Online-Recommendation-based-Convolutional-Features-Duan-Fu/157173682fbc5eefce7588dfd871520fbdd4ae75",
            "/paper/Object-Detection-in-Videos-with-Tubelet-Proposal-Kang-Li/317e004f82a2411e5c98aea695526a3f6b173dfe",
            "/paper/MOTChallenge-2015%3A-Towards-a-Benchmark-for-Tracking-Leal-Taix%C3%A9-Milan/5bae9822d703c585a61575dced83fa2f4dea1c6d",
            "/paper/Multiple-Pedestrian-Tracking-From-Monocular-Videos-Jiang-Huynh/f2b627f11a36c359de9aab6c44796e1d30d30b64",
            "/paper/Leveraging-Local-and-Global-Cues-for-Visual-via-Zheng-Zhong/089a68d5921c0d41a33429543a54822f6a2ee8e1",
            "/paper/Learning-Rotation-Invariant-and-Fisher-Neural-for-Cheng-Han/5c5700614c0fc3db3a6f4d96d956027015c61690",
            "/paper/HiFT%3A-Hierarchical-Feature-Transformer-for-Aerial-Cao-Fu/9916ed982600be133ed2d185b70fe721809a3096"
        ]
    },
    {
        "id": "35e940ace815548f620709d9c1803da34c581e86",
        "title": "Animatable Neural Radiance Fields for Modeling Dynamic Human Bodies",
        "abstract": "This paper addresses the challenge of reconstructing an animatable human model from a multi-view video by introducing neural blend weight fields to produce the deformation fields and shows that this approach significantly outperforms recent human synthesis methods. This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a non-rigidly deforming scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce neural blend weight fields to produce the deformation fields. Based on the skeleton-driven deformation, blend weight fields are used with 3D human skeletons to generate observation-to-canonical and canonical-to-observation correspondences. Since 3D human skeletons are more observable, they can regularize the learning of deformation fields. Moreover, the learned blend weight fields can be combined with input skeletal motions to generate new deformation fields to animate the human model. Experiments show that our approach significantly outperforms recent human synthesis methods. The code and supplementary materials are available at https://zju3dv.github.io/animatable_nerf/.",
        "publication_year": "2021",
        "authors": [
            "Sida Peng",
            "Junting Dong",
            "Qianqian Wang",
            "Shang-Wei Zhang",
            "Qing Shuai",
            "Xiaowei Zhou",
            "H. Bao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "140",
        "reference_count": "68",
        "references": [
            "/paper/Animatable-Implicit-Neural-Representations-for-from-Peng-Xu/4fee2057164c787748353dbdded04d3b0aed15f5",
            "/paper/NDF%3A-Neural-Deformable-Fields-for-Dynamic-Human-Zhang-Chen/4c5bf1de1e4e2c3c65a2cc941124a3f14cbf1f06",
            "/paper/Structured-Local-Radiance-Fields-for-Human-Avatar-Zheng-Huang/59d25f507cfe13dd215ef243609dafea9eefae19",
            "/paper/MonoHuman%3A-Animatable-Human-Neural-Field-from-Video-Yu-Cheng/b5fb909d436856ba7c4d5e15bfdb83a847e7ff8a",
            "/paper/Animatable-Neural-Implicit-Surfaces-for-Creating-Peng-Zhang/f04d8a5f62099635d30284d1b0167c4ff2d82348",
            "/paper/MoDA%3A-Modeling-Deformable-3D-Objects-from-Casual-Song-Chen/64c8ac15fc2fc7a6c830d09b5c474a10f99dae3d",
            "/paper/Animatable-Neural-Radiance-Fields-from-Monocular-Wang-Sarafianos/0ccaf470f15f8ac57fccf15a0d2749089912e9ff",
            "/paper/Generalizable-Neural-Voxels-for-Fast-Human-Radiance-Yi-Fang/66758bc40271bf901744862c799567812ba85e86",
            "/paper/HandNeRF%3A-Neural-Radiance-Fields-for-Animatable-Guo-Zhou/912056e7bc70a673fe8822626b501304c8125c25",
            "/paper/BANMo%3A-Building-Animatable-3D-Neural-Models-from-Yang-Vo/e230950f1a784982df3bff6b4f753e50f9fbfb69",
            "/paper/Deformable-Neural-Radiance-Fields-Park-Sinha/923a605e9b8cf29ab593781ef122f4d9f24035a3",
            "/paper/S3%3A-Neural-Shape%2C-Skeleton%2C-and-Skinning-Fields-for-Yang-Wang/9bdc662c280119422e5303d874ace4bc81c3488b",
            "/paper/SMPLpix%3A-Neural-Avatars-from-3D-Human-Models-Prokudin-Black/5a87be01267cd3593e63314869ec96e6adc1f549",
            "/paper/ARCH%3A-Animatable-Reconstruction-of-Clothed-Humans-Huang-Xu/0ff2c939d136df8988f845ae5cdfb725939a82ab",
            "/paper/Combining-Implicit-Function-Learning-and-Parametric-Bhatnagar-Sminchisescu/03e5d09cc7682df9f3bdac63fcec87eb298fcff8",
            "/paper/Neural-Body%3A-Implicit-Neural-Representations-with-Peng-Zhang/af8faec7c0b8f4b2a28d42a86e0e7d499016c560",
            "/paper/D-NeRF%3A-Neural-Radiance-Fields-for-Dynamic-Scenes-Pumarola-Corona/694bdf6e5906992dad2987a3cc8d1a176de691c9",
            "/paper/Learning-to-Estimate-3D-Human-Pose-and-Shape-from-a-Pavlakos-Zhu/13ad6164dba75845f0f397e9314ad596e74eb946",
            "/paper/Convolutional-Mesh-Regression-for-Single-Image-Kolotouros-Pavlakos/9ce6d7373d307eb14f7ee76357e442f920be631a",
            "/paper/Pose-Guided-Human-Animation-from-a-Single-Image-in-Yoon-Liu/26bb9d36557977192404aa16ad3833e47d8daf48"
        ]
    },
    {
        "id": "07b87af5fb9cbb6624305b1d414078c62bba5869",
        "title": "Adaptive temporal feature modeling for visual tracking via cross-channel learning",
        "abstract": "Semantic Scholar extracted view of \"Adaptive temporal feature modeling for visual tracking via cross-channel learning\" by Yuanyun Wang et al.",
        "publication_year": "2023",
        "authors": [
            "Yuanyun Wang",
            "Wenshuang Zhang",
            "Changwang Lai",
            "Jun Wang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "19",
        "references": [
            "/paper/Learning-Channel-Aware-Correlation-Filters-for-Nai-Li/a1e9c22100e3fe1862188093e4d054ed08dde99b",
            "/paper/Learning-attention-modules-for-visual-tracking-Wang-Meng/f0e93ad223e82f16f2d6fa22a713744614ed8ee3",
            "/paper/RGBT-Tracking-by-Trident-Fusion-Network-Zhu-Li/c2f2ed50ec721ca53bd7463cb2c825f277e9d369",
            "/paper/Light-weight-UAV-object-tracking-network-based-on-Hua-Wang/54dfeb5e4460b53da4a5fbf1a3b295cd9fda2e53",
            "/paper/Robust-visual-tracking-via-spatio-temporal-adaptive-Liang-Liu/5dc3b582ebd95f029b1daa8622cf56bb192ef018",
            "/paper/Ocean%3A-Object-aware-Anchor-free-Tracking-Zhang-Peng/27d52bf3265bea0f9929980f6ffb4c2009eecfee",
            "/paper/Quality-Aware-Feature-Aggregation-Network-for-RGBT-Zhu-Li/281388c93fcdd7eaf6cb98015b28da09ee2cc071",
            "/paper/SiamFC%2B%2B%3A-Towards-Robust-and-Accurate-Visual-with-Xu-Wang/be412c7c7128cf91455233b652d6c94a6001a7c8",
            "/paper/Learning-Reinforced-Attentional-Representation-for-Gao-Zhang/2d9048a88a7ead488581ecdeccf09e8ac8256c75",
            "/paper/Siamese-Attentional-Keypoint-Network-for-High-Gao-Ma/fce3655dc22a783b1f82f09190410f070c7bf42c"
        ]
    },
    {
        "id": "da959782ef230a5653967585090ea31e67390ce1",
        "title": "Segmentation of Lungs in Chest X-Ray Image Using Generative Adversarial Networks",
        "abstract": "The use of generative adversarial networks (GAN) to perform the task of lung segmentation on a given CXR using four different discriminators referred to as D1, D2, D3, and D4, respectively is presented. Chest X-ray (CXR) is a low-cost medical imaging technique. It is a common procedure for the identification of many respiratory diseases compared to MRI, CT, and PET scans. This paper presents the use of generative adversarial networks (GAN) to perform the task of lung segmentation on a given CXR. GANs are popular to generate realistic data by learning the mapping from one domain to another. In our work, the generator of the GAN is trained to generate a segmented mask of a given input CXR. The discriminator distinguishes between a ground truth and the generated mask, and updates the generator through the adversarial loss measure. The objective is to generate masks for the input CXR, which are as realistic as possible compared to the ground truth masks. The model is trained and evaluated using four different discriminators referred to as D1, D2, D3, and D4, respectively. Experimental results on three different CXR datasets reveal that the proposed model is able to achieve a dice-score of 0.9740, and IOU score of 0.943, which are better than other reported state-of-the art results.",
        "publication_year": "2020",
        "authors": [
            "Faizan Munawar",
            "Shoaib Azmat",
            "Talha Iqbal",
            "C. Gr\u00f6nlund",
            "Hazrat Ali"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "32",
        "reference_count": "54",
        "references": [
            "/paper/Segmentation-and-classification-on-chest-a-survey-Agrawal-Choudhary/bb81652f87b117c3458a470c9874b33968ee2a4c",
            "/paper/Lung-cancer-diagnosis-using-Hessian-adaptive-in-Thirumagal-Saruladha/769291ce3ea1824a804d7d67957781bb37732c82",
            "/paper/Combating-COVID-19-Using-Generative-Adversarial-and-Ali-Shah/5d54a58aa726af2223824234c8d01532058d0f13",
            "/paper/Deep-LF-Net%3A-Semantic-Lung-Segmentation-from-Indian-Singh-Lall/a486e07e6121b5dab2cd42ef5f03b45a5c0d2c8b",
            "/paper/ResNet-50-vs-VGG-19-vs-Training-from-Scratch%3A-A-of-VictorIkechukwu-Murali/ef209cb43ee42805082bd65dafe68c3b3a67a0c3",
            "/paper/CX-Net%3A-an-efficient-ensemble-semantic-deep-neural-Ikechukwu-M./f9901ece592bee7377161f23cb793404bc1bfe36",
            "/paper/The-Role-of-Generative-Adversarial-Network-in-Image-Alamir-Alghamdi/70bd1cea2fa48ae0cd1dc60b03f3a04587068fb6",
            "/paper/Debiasing-pipeline-improves-deep-learning-model-for-Horry-Chakraborty/ba016c9c5fe994992de585056a277eb2d0895fa1",
            "/paper/Mammography-Data-Augmentation-Using-ACGAN-Fan-Jiao/06c99ffa76e01e43b497fb191c87aff46e648b8e",
            "/paper/A-Hybrid-Pipeline-for-Covid-19-Screening-Lungs-and-Abdulah-Huber/07e3a7aa332b2fac5b584aabbf300705a603f55c",
            "/paper/SCAN%3A-Structure-Correcting-Adversarial-Network-for-Dai-Dong/1ea332630dc12822f04642019097bde59168a2c5",
            "/paper/Automatic-Liver-Segmentation-Using-an-Adversarial-Yang-Xu/8b6b0c1139a3ab405e6df58aaa79305867f453ef",
            "/paper/Generative-Adversarial-Network-for-Medical-Images-Iqbal-Ali/5443e8aca3deb9dae313e7796435023727102130",
            "/paper/CT-Realistic-Lung-Nodule-Simulation-from-3D-for-Jin-Xu/eccbc278986057b5f1ea1bce9bdcc833712336d6",
            "/paper/Medical-Image-Synthesis-with-Context-Aware-Networks-Nie-Trullo/ef6abdcbd0871cb356e8669b6cdb31ed8b013cc2",
            "/paper/Generative-Adversarial-Network-based-Synthesis-for-Neff-Payer/6574fb5b30961ed8eb4068cf606204dd419850ba",
            "/paper/GAN-Augmentation%3A-Augmenting-Training-Data-using-Bowles-Chen/003ec88cbec156131058e53409115cd056057644",
            "/paper/Medical-Image-Synthesis-with-Deep-Convolutional-Nie-Trullo/2b37777583372237e63405f258a1c40a3eda8bc9",
            "/paper/Lung-Segmentation-in-Chest-Radiographs-Using-With-Candemir-Jaeger/86bf80d54ef110149398c5830606547c0537641f",
            "/paper/Lung-Image-Segmentation-Using-Deep-Learning-Methods-Kalinovsky-Kovalev/b71d7bccc3e4f5f545a52358d44eb0f799647bcf"
        ]
    },
    {
        "id": "4c95603a9611faa53c3ca851f324ac0ca6c95a17",
        "title": "Magnetic resonance imaging radiomics predicts preoperative axillary lymph node metastasis to support surgical decisions and is associated with tumor microenvironment in invasive breast cancer: A machine learning, multicenter study",
        "abstract": "Semantic Scholar extracted view of \"Magnetic resonance imaging radiomics predicts preoperative axillary lymph node metastasis to support surgical decisions and is associated with tumor microenvironment in invasive breast cancer: A machine learning, multicenter study\" by Yunfang Yu et al.",
        "publication_year": "2021",
        "authors": [
            "Yunfang Yu",
            "Zi-Li He",
            "J. Ouyang",
            "Y. Tan",
            "Yongjian Chen",
            "Y. Gu",
            "L. Mao",
            "W. Ren",
            "Jue Wang",
            "Lili Lin",
            "Zhuo Wu",
            "Jingwen Liu",
            "Q. Ou",
            "Qiu-Gen Hu",
            "Anlin Li",
            "Kai Chen",
            "Chenchen Li",
            "N. Lu",
            "Xiaohong Li",
            "F. Su",
            "Qiang Liu",
            "C. Xie",
            "H. Yao"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "46",
        "reference_count": "36",
        "references": [
            "/paper/Radiomics-nomogram-for-predicting-axillary-lymph-in-Zhang-Zhang/e4fd861a5ff7daf756f5681955da20197ccf5e6e",
            "/paper/Development-and-validation-of-radiomics-machine-on-Zhang-Cao/a4f8dc311d7d5416e126368f86b40e0c70485480",
            "/paper/Editorial-for-%E2%80%9CPreoperative-Prediction-of-Axillary-Narongrit-Rispoli/47c4ce16fb2f89949416bd31b548045b825e98bc",
            "/paper/MRI-Based-Radiomics-Nomogram%3A-Prediction-of-Lymph-Qiu-Zhang/9ce3625ccaeb3aedf1b784e6dd4da7be63c17556",
            "/paper/Prediction-of-Residual-Axillary-Nodal-Metastasis-on-Lee-Nguyen/749a05c31c5bfbfbed4aa10c01d3ddd0965889e0",
            "/paper/CT-Radiomics-for-Predicting-Pathological-Complete-A-Li-Wang/4075bb23c525c5a9a9ec6a33620dc1a111a27186",
            "/paper/A-nomogram-based-on-radiomics-signature-and-for-of-Wang-Hu/d820203793a7fa3fc51fceb45790e35f54a3d5ea",
            "/paper/A-delta-radiomic-lymph-node-model-using-dynamic-MRI-Liu-Du/e6acf7ff9307055ad164d3f39dedbf8999785ad3",
            "/paper/A-Clinical%E2%80%93Radiomics-Model-for-Predicting-Axillary-Gan-Ma/b59b2a1c9af5b06dd93a8ce18d42274403c1a168",
            "/paper/Preoperative-Prediction-of-Lymph-Node-Metastasis-of-Shi-Wang/63dbe898b9c0228d31ef4fb25e0b62e6f11ecd4a",
            "/paper/Development-and-Validation-of-a-Preoperative-to-and-Yu-Tan/211783a923ac1e38941247f65c256ec21a393045",
            "/paper/Radiomics-Analysis-of-Dynamic-Contrast-Enhanced-for-Liu-Sun/0948e800b153bcb32a92983d6f351e66abb4ab47",
            "/paper/Deep-learning-radiomics-can-predict-axillary-lymph-Zheng-Yao/a0de9e92ebb90a93b3519656f395b7afa6a4ad8e",
            "/paper/Radiomic-nomogram-for-prediction-of-axillary-lymph-Han-Zhu/1fe8ce3b7818426d9a826ea3135acaa6887957e1",
            "/paper/Radiomics-Signature-on-Magnetic-Resonance-Imaging%3A-Park-Lim/9ed2726646155ac381820f48440c331d149852ed",
            "/paper/Radiomics-of-Multiparametric-MRI-for-Pretreatment-A-Liu-Li/52fbaa733d540b64f1dbe7fd39f820a6874417cd",
            "/paper/Quantifying-the-Impact-of-Axillary-Surgery-and-on-a-Naoum-Roberts/eaf8a2f72aca7a579a58f9365bdef90e4e4959ea",
            "/paper/Diagnostic-Performance-of-Noninvasive-Imaging-for-A-Samiei-Mooij/a25a35895b8377508bc67cd4d9b568e672b6a2d5",
            "/paper/Sentinel-Lymph-Node-Biopsy-for-Patients-With-Breast-Lyman-Somerfield/7651dc7f8e73be39aec686542bdc418de69a8b31",
            "/paper/Association-of-Peritumoral-Radiomics-With-Tumor-and-Braman-Prasanna/9b744674a80c39103472b09fcc8a2c9a1001fc89"
        ]
    },
    {
        "id": "0d754abea13b6d99938d3aeb8d72073c89e69021",
        "title": "Multiclass skin lesion classification in dermoscopic images using swin transformer model",
        "abstract": "Automatic skin lesion classification in dermoscopic images is a very challenging task due to the huge intraclass variation, the high degree of interclass visual similarity, low contrast between skin lesion and surrounding normal skin, and the existence of extraneous and intrinsic artifacts. However, existing algorithms for skin lesion classification are developed by leveraging convolutional neural networks (CNNs), and the effectiveness of these algorithms is mostly validated for binary classification of skin lesions. In addition, the relatively low diagnostic sensitivity achieved by these studies demonstrates the uncertainty involved in skin lesion classification. In order to overcome these difficulties, a swin transformer model for multiclass skin lesion classification is proposed by taking advantage of both transformer and CNNs that are based on end-to-end mapping and do not require prior knowledge. Furthermore, the problem of class imbalance is addressed through a weighted cross entropy loss. Moreover, key components of the proposed approach are explored in detail in order to ensure efficient and effective learning process with multiclass data in the skin lesion classification. The proposed method is extensively evaluated on International Skin Imaging Collaboration (ISIC) 2019 Skin Lesion Analysis Towards Melanoma Detection Challenge dataset and achieves a sensitivity, specificity, accuracy, and balanced accuracy value of 82.3%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$82.3\\%$$\\end{document}, 97.9%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$97.9\\%$$\\end{document}, 97.2%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$97.2\\%$$\\end{document}, and 82.3%\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$82.3\\%$$\\end{document}, respectively. Experimental results demonstrate that the proposed method has the highest balanced accuracy value and outperforms most of the other state-of-the-art methods in multiclass skin lesion classification.",
        "publication_year": "2022",
        "authors": [
            "Selen Ayas"
        ],
        "related_topics": [
            "Materials Science"
        ],
        "citation_count": "2",
        "reference_count": "32",
        "references": [
            "/paper/A-novel-framework-of-multiclass-skin-lesion-from-AI-Ahmad-Shah/bed89a835cf8b203f3ca57fc62f01e60c0fa0b82",
            "/paper/A-survey%2C-review%2C-and-future-trends-of-skin-lesion-Hasan-Ahamad/5185cdcc385a11e87f53466e8d96ca3d0bdca1b8",
            "/paper/Dermoscopy-Image-Classification-Based-on-StyleGAN-Zhao-Shuai/e36c6f275ba1e3e82ca9084e7d086c648cb95143",
            "/paper/Automated-multi-class-classification-of-skin-deep-Iqbal-Younus/c4429a877680bdedd46f708af2761facf239f8e7",
            "/paper/Automated-Melanoma-Recognition-in-Dermoscopy-Images-Yu-Chen/e69b1314cd65a115c98082a5863b92daa4dcf9f0",
            "/paper/Skin-Lesions-Classification-Into-Eight-Classes-for-Kassem-Hosny/d93c65722d2e43ea9bc4e0ccd70d9357985fc145",
            "/paper/Skin-lesion-classification-using-ensembles-of-with-Gessert-Nielsen/2b73f023765f784a5e8bc943875777d2a26e06fa",
            "/paper/Improving-Dermoscopic-Image-Segmentation-With-Yuan-Lo/1259b42106742b3b387ad2d6a17fb9303e70c41b",
            "/paper/The-HAM10000-dataset%2C-a-large-collection-of-images-Tschandl-Rosendahl/f95f5b715eb0441ca4ee1b0fac6b4bcaaba65556",
            "/paper/An-Attention-Based-Mechanism-to-Combine-Images-and-Pacheco-Krohling/7e947dded26631755c5a9efe44f8fc438771bf94",
            "/paper/Classification-of-Dermoscopy-Skin-Lesion-Using-Molina-Molina-Solorza-Calder%C3%B3n/93d5b7ae135bac29d69edf6426a6db5cee024511",
            "/paper/Comparison-of-Segmentation-Methods-for-Melanoma-in-Silveira-Nascimento/8471c8f8a5f16896b7494c8cb8b053d5298b71e9"
        ]
    },
    {
        "id": "8863394d788287653f9b206dc5b00f277f235f5d",
        "title": "Deep convolutional neural network algorithm for the automatic segmentation of oral potentially malignant disorders and oral cancers",
        "abstract": "This study aimed to develop an algorithm to automatically segment the oral potentially malignant diseases (OPMDs) and oral cancers (OCs) of all oral subsites with various deep convolutional neural network applications. A total of 510 intraoral images of OPMDs and OCs were collected over 3 years (2006\u20142009). All images were confirmed both with patient records and histopathological reports. Following the labeling of the lesions the dataset was arbitrarily split, using random sampling in Python as the study dataset, validation dataset, and test dataset. Pixels were classified as the OPMDs and OCs with the OPMD/OC label and the rest as the background. U-Net architecture was used and the model with the best validation loss was chosen for the testing among the trained 500 epochs. Dice similarity coefficient (DSC) score was noted. The intra-observer ICC was found to be 0.994 while the inter-observer reliability was 0.989. The calculated DSC and validation accuracy across all clinical images were 0.697 and 0.805, respectively. Our algorithm did not maintain an excellent DSC due to multiple reasons for the detection of both OC and OPMDs in oral cavity sites. A better standardization for both 2D and 3D imaging (such as patient positioning) and a bigger dataset are required to improve the quality of such studies. This is the first study which aimed to segment OPMDs and OCs in all subsites of oral cavity which is crucial not only for the early diagnosis but also for higher survival rates.",
        "publication_year": "2023",
        "authors": [
            "G. \u00dcnsal",
            "A. Chaurasia",
            "N. Akkaya",
            "Nadler Chen",
            "R. Abdalla-Aslan",
            "Revan Birke Koca",
            "K. Orhan",
            "J. Roganovi\u0107",
            "P. Reddy",
            "D. Wahjuningrum"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "45",
        "references": [
            "/paper/A-Novel-Lightweight-Deep-Convolutional-Neural-for-Jubair-Al-karadsheh/a6ee87dc926e1ada80673d702c7de278e53682c9",
            "/paper/Automated-Detection-and-Classification-of-Oral-Deep-Tanriver-Tekke%C5%9Fin/bad52faef74960f458f3dfab35f7714335d334c0",
            "/paper/Dermatologist-level-classification-of-skin-cancer-Esteva-Kuprel/e1ec11a1cb3d9745fb18d3bf74247f95a6663d08",
            "/paper/Brain-metastasis-detection-using-machine-learning%3A-Cho-Sunwoo/a5873a77f57908751246b087f0e4aa0c020a26fc",
            "/paper/Raman-Spectroscopy-Analysis-for-Optical-Diagnosis-Jeng-Sharma/b5bc762418bf26c87d84f7176ebbad302e105ef0",
            "/paper/Development-of-Machine-Learning-and-Medical-Enabled-Anand-Rane/86183b8a7b86cdbbb1d290b9a1a257bc6ff75c8a",
            "/paper/Oral-premalignant-lesions-Amagasa/8d9524180873be9ca7ce3a17856d86f39e5724a6",
            "/paper/Nomenclature-and-classification-of-potentially-of-Warnakulasuriya-Johnson/f3f73ad79c243e52bc51e870a72694a8f15d1bb3",
            "/paper/Oral-potentially-malignant-disorders%3A-nomenclature-Warnakulasuriya-Kujan/553876d13b54db2ec45bc7687101317965e393bc",
            "/paper/Advanced-diagnostic-aids-for-oral-cancer.-Madhura-Rao/5c14c6bdddfbbe7f4457977d45ea627ddf406067"
        ]
    },
    {
        "id": "a53e428eec311e7b1880f2d009792b7798c975d2",
        "title": "CIRCA: comprehensible online system in support of chest X-rays-based COVID-19 diagnosis",
        "abstract": "Seventy-six percent of COVID-19 patients wrongly classified as normal cases were annotated by radiologists as with no signs of disease, and the online service to provide access to fast diagnosis support tools was developed. Due to the large accumulation of patients requiring hospitalization, the COVID-19 pandemic disease caused a high overload of health systems, even in developed countries. Deep learning techniques based on medical imaging data can help in the faster detection of COVID-19 cases and monitoring of disease progression. Regardless of the numerous proposed solutions for lung X-rays, none of them is a product that can be used in the clinic. Five different datasets (POLCOVID, AIforCOVID, COVIDx, NIH, and artificially generated data) were used to construct a representative dataset of 23 799 CXRs for model training;1 050 images were used as a hold-out test set, and 44 247 as independent test set (BIMCV database). A U-Net-based model was developed to identify a clinically relevant region of the CXR. Each image class (normal, pneumonia, and COVID-19) was divided into 3 subtypes using a 2D Gaussian mixture model. A decision tree was used to aggregate predictions from the InceptionV3 network based on processed CXRs and a dense neural network on radiomic features. The lung segmentation model gave the Sorensen-Dice coefficient of 94.86% in the validation dataset, and 93.36% in the testing dataset. In 5-fold cross-validation, the accuracy for all classes ranged from 91% to 93%, keeping slightly higher specificity than sensitivity and NPV than PPV. In the hold-out test set, the balanced accuracy ranged between 68% and 100%. The highest performance was obtained for the subtypes N1, P1, and C1. A similar performance was obtained on the independent dataset for normal and COVID-19 class subtypes. Seventy-six percent of COVID-19 patients wrongly classified as normal cases were annotated by radiologists as with no signs of disease. Finally, we developed the online service (https://circa.aei.polsl.pl) to provide access to fast diagnosis support tools.",
        "publication_year": "2022",
        "authors": [
            "Wojciech Prazuch",
            "A. Suwalska",
            "M. Socha",
            "J. Tobiasz",
            "P. Foszner",
            "J. Jaroszewicz",
            "K. Gruszczy\u0144ska",
            "M. Sliwinska",
            "J. Walecki",
            "T. Popiela",
            "G. Przybylski",
            "A. Cieszanowski",
            "Mateusz Nowak",
            "M. Paw\u0142owska",
            "R. Flisiak",
            "K. Simon",
            "Gabriela Zapolska",
            "Barbara Gizycka",
            "E. Szurowska",
            "M. Marczyk",
            "J. Pola\u0144ska"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "28",
        "references": [
            "/paper/POLCOVID%3A-a-multicenter-multiclass-chest-X-ray-Suwalska-Tobiasz/6c814dcca13812e254990aa26ad77a64cf3fe714",
            "/paper/Deep-COVID%3A-Predicting-COVID-19-from-chest-X-ray-Minaee-Kafieh/703fb8cc953fd8a9e221b605c75502d62e841abd",
            "/paper/Extracting-Possibly-Representative-COVID-19-from-to-Apostolopoulos-Aznaouridis/642f8a2b908e954e00ce74f5c44ebe4e1ce46505",
            "/paper/AIforCOVID%3A-Predicting-the-clinical-outcomes-in-AI-Soda-D'Amico/1f3c0029b2d66a8686de07e96795340afa777787",
            "/paper/The-Applications-of-Artificial-Intelligence-in-of-A-Laino-Ammirabile/57763cc6a9b52d369726216db37d9eb598c4bee8",
            "/paper/COVID-Net%3A-a-tailored-deep-convolutional-neural-for-Wang-Lin/235b9812305cff3df849394ac7b70a2c04a4685c",
            "/paper/AI-for-radiographic-COVID-19-detection-selects-over-DeGrave-Janizek/6e62a903dac8e643160bd1337d6cb8b09bf2f062",
            "/paper/PadChest%3A-A-large-chest-x-ray-image-dataset-with-Bustos-Pertusa/f1a17b7c4cae4513731f6d81b433e338cf4114eb",
            "/paper/Discovery-of-a-Generalization-Gap-of-Convolutional-Ahmed-Goldgof/a7fbbf17b5a456337b562e59f9046ff3275b04bf",
            "/paper/Can-AI-Help-in-Screening-Viral-and-COVID-19-Chowdhury-Rahman/29bf7bd9c64e18a7b57b5da80e03fab1d9f1de2e",
            "/paper/Common-pitfalls-and-recommendations-for-using-to-CT-Roberts-Driggs/69d49a06f09cf934310ccbf3bb2a360fa719272d"
        ]
    },
    {
        "id": "867ddab0cf818baaa8c88d6bd983b2569755ce46",
        "title": "Segmentation of Satellite Imagery using U-Net Models for Land Cover Classification",
        "abstract": "The segmentation models offer a solution for generating automatic land cover mappings based on Sentinel-2 satellite images and show a high IoU score for land cover classes such as forests, inland waters and arable land. The focus of this paper is using a convolutional machine learning model with a modified U-Net structure for creating land cover classification mapping based on satellite imagery. The aim of the research is to train and test convolutional models for automatic land cover mapping and to assess their usability in increasing land cover mapping accuracy and change detection. To solve these tasks, authors prepared a dataset and trained machine learning models for land cover classification and semantic segmentation from satellite images. The results were analysed on three different land classification levels. BigEarthNet satellite image archive was selected for the research as one of two main datasets. This novel and recent dataset was published in 2019 and includes Sentinel-2 satellite photos from 10 European countries made in 2017 and 2018. As a second dataset the authors composed an original set containing a Sentinel-2 image and a CORINE land cover map of Estonia. The developed classification model shows a high overall F\\textsubscript{1} score of 0.749 on multiclass land cover classification with 43 possible image labels. The model also highlights noisy data in the BigEarthNet dataset, where images seem to have incorrect labels. The segmentation models offer a solution for generating automatic land cover mappings based on Sentinel-2 satellite images and show a high IoU score for land cover classes such as forests, inland waters and arable land. The models show a capability of increasing the accuracy of existing land classification maps and in land cover change detection.",
        "publication_year": "2020",
        "authors": [
            "Priit Ulmas",
            "I. Liiv"
        ],
        "related_topics": [
            "Environmental Science",
            "Mathematics"
        ],
        "citation_count": "40",
        "reference_count": "28",
        "references": [
            "/paper/Land-Cover-Semantic-Segmentation-Using-ResUNet-Pollatos-Kouvaras/4871d441e775b3b0e6c8b6a594e9b6d54774f08c",
            "/paper/Improved-Agricultural-Field-Segmentation-in-Imagery-Safarov-Temurbek/e0c143e86a9f444a5d83d51c38e123e6e30c9f8a",
            "/paper/Using-Deep-Learning-Approach-for-Land-Use-and-based-Agarwal-Goel/44607c13992295ad450b69ab940a38b634929b48",
            "/paper/Semantic-Segmentation-of-Satellite-Images-using-Patil-Patil/deda2cb1c04b3260368911d4a19938345270f8c6",
            "/paper/Land-Use-Land-Cover-Classification-with-U-Net%3A-of-Sol%C3%B3rzano-Mas/70e7f80e0dc3d6424a07699cda599ad117f69006",
            "/paper/Using-machine-learning-to-produce-a-very-high-map-Walsh-Bessardon/f50200dba8fe99efef3092215fb7043df625c65f",
            "/paper/INVESTIGATIONS-ON-FEATURE-SIMILARITY-AND-THE-IMPACT-Voelsen-Torres/757ae695de656ed56913cdfa7f1a488f6624277a",
            "/paper/Useable-machine-learning-for-Sentinel-2-satellite-Langevin-Bethune/15675da252aa6d828495aa44308b0bf01c4c5685",
            "/paper/Analysing-Satellite-Images-using-Segmentation-with-Sinha-Senapati/93b11cefbf9417274d9424e62bb8a973fea68df7",
            "/paper/Classification-Of-Hyperspectral-Images-Using-Deep-M.-Jennifer/4a0dd1e844e628751a5876e02c404e1b44e26e5f",
            "/paper/Deep-Learning-Neural-Networks-for-Land-Use-Land-Storie-Henry/d25e7b3a4276969dd413a9484572b79c815db500",
            "/paper/Deep-Learning-Classification-of-Land-Cover-and-Crop-Kussul-Lavreniuk/7a9e471e31ac156cf22a5e2a5c1463697df866ab",
            "/paper/Weakly-Supervised-Deep-Learning-for-Segmentation-of-Wang-Chen/6eb53e1763f2eab95093eb1e6f67d34d2618fa9b",
            "/paper/Vegetation-Land-Use-Land-Cover-Extraction-From-on-Zhan-Zhang/3f9fae77d1607ca7829cc9537c932b592b9d12f1",
            "/paper/Fusion-of-Deep-Convolutional-Neural-Networks-for-of-Scott-Marcum/a333fe85efd2a78b34290ba03d0b92fe89ac52e0",
            "/paper/BigEarthNet-Dataset-with-A-New-Class-Nomenclature-Sumbul-Kang/b60e6bde955533c7226104e9e93f7820deb7a920",
            "/paper/Land-cover-intelligent-mapping-using-transfer-and-Benbahria-Smiej/871e6d3ddda55739458c3b168aab94ee4ec427f5",
            "/paper/Object-based-multi-temporal-and-multi-source-land-Gbodjo-Ienco/dd9235bcb44243c47e7d9f7227ca5d2e1493afc7",
            "/paper/Training-Deep-Convolutional-Neural-Networks-for-of-Scott-England/56acd10dd2c88b3d1ea7474a5ee97bd5700f6b46",
            "/paper/Bigearthnet%3A-A-Large-Scale-Benchmark-Archive-for-Sumbul-Charfuelan/0c3f83b98107c5d84fb00967f5a991dcceb3f04d"
        ]
    },
    {
        "id": "3f9f38f6929504d4430f88288d8eea8cd23c7bc6",
        "title": "Free vibration analysis of rotating piezoelectric/flexoelectric microbeams",
        "abstract": "In recent years, flexoelectric and piezoelectric effects have been investigated in many studies, but these effects have not been studied simultaneously in a rotating microbeam, taking into account the size effects. The simultaneous investigation of these effects is necessary to better understand the vibrational behavior of flexoelectric microstructures. It will be possible to identify the flexoelectric and size-dependent effects and their influence on the vibration frequency as a result of this analysis. Furthermore, it can lead to improved design of microstructures. Under the influence of the flexoelectric effect, the current research analyzes the free vibrations of a rotating piezoelectric microbeam. Also, all rotating microbeams are affected by the flexoelectric effect, and therefore the study of this phenomenon is of great importance. Due to the small strain and moderate rotation of the beam, Euler\u2013Bernoulli beam theory and von Karman strain\u2013displacement relations are employed in this article. The Gibbs energy function presented in this research includes the effects of couple stress, piezoelectric and flexoelectric. Hamilton's principle is used to obtain the equations of motion and boundary conditions of the rotating microbeam. In the final step, it has been examined in this study how parameters such as slenderness ratios, changes in rotation speed, and coefficients related to flexoelectric and piezoelectric properties affect the vibration of a rotating microbeam. The results show that the natural frequency increases as the rotation speed increases. Further, the use of piezoelectric and flexoelectric effects has increased the structure's stiffness and, as a result, increased its natural frequency.",
        "publication_year": "2023",
        "authors": [
            "S. Hosseini",
            "Y. Beni"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": 0,
        "reference_count": "52",
        "references": [
            "/paper/Flexoelectric-Effect-on-Vibration-of-Piezoelectric-Li-Luo/e5405dbb2bbdfa53cf863d3ba558e04a0ac5a62b",
            "/paper/A-micro-scale-Timoshenko-beam-model-for-with-and-Yue-Xu/9ae25d22fba4e8381e047484814b05242463aef4",
            "/paper/Geometrically-nonlinear-analysis-of-Timoshenko-with-Zarepour-Hosseini/e346685a4dbc2f2fd5a664bbbad00ebdf8f8f239",
            "/paper/On-the-size-dependent-nonlinear-dynamics-of-Bagheri-Beni/182d9c5ba979650f1344925c48a3c57df271c4e6",
            "/paper/Nonlinear-thermo-electromechanical-vibration-of-Ghobadi-Beni/294678eef24679893e90576fbd308c8bc8431920",
            "/paper/Analysis-of-size-dependent-smart-flexoelectric-Omidian-Beni/201128df5c58c94bfa8d89edeeec404ba536d639",
            "/paper/Vibration-behavior-of-a-rotating-non-uniform-FG-on-Shafiei-Mousavi/da97fdfe39ff727ac5a134b14c247078d565575c",
            "/paper/Effects-of-porosity-and-flexoelectricity-on-static-Zhao-Zheng/5526c6c1723a8dc7702f3bb694d3bd5a35ac6bb8",
            "/paper/Influence-of-flexoelectric%2C-small-scale%2C-surface-on-Arefi-Pourjamshidian/72ec2b5972c05300faacfc87667bb12ff0d080b9",
            "/paper/Free-vibration-and-buckling-analyses-of-FGM-based-%C5%BBur-Arefi/1efcac956f323a5c024f8da0184272ca3a6cd0fa"
        ]
    },
    {
        "id": "1195108ab46bd6c00ea714ff1549a5e10d8cf9eb",
        "title": "Efficient Processing of Reverse Top-k Dominating Queries",
        "abstract": "This work formalizes the RkDQ query and proposes an efficient algorithm for computing RKDQ with some smart pruning heuristics, which returns all data points so that the results of their top-k dominating queries contain the query point q. The top-k dominating queries (kDQ) are very useful to the users who hope to select their favorite products. It combines the characteristics of top-k query and skyline query. Although kDQ has been well-studied in the literature, there is, to the best of our knowledge, no prior work on one of the most intuitive and practical types of kDQ, namely, reverse top-k dominating queries (RkDQ), which returns all data points so that the results of their top-k dominating queries contain the query point q. We formalize the RkDQ query and then propose an efficient algorithm for computing RkDQ with some smart pruning heuristics. Extensive experimental results over real and synthetic datasets show that our proposed algorithms are effective and efficient.",
        "publication_year": "2018",
        "authors": [
            "Tao Jiang",
            "Bin Zhang",
            "Jun Yang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "12",
        "references": [
            "/paper/Efficient-Processing-of-Top-k-Dominating-Queries-on-Yiu-Mamoulis/e1e5936749e383c096e219b9bf85cf756585882b",
            "/paper/Continuous-Top-k-Dominating-Queries-Kontaki-Papadopoulos/caf62f1c21a671e7226078e8287995012a92c14e",
            "/paper/Progressive-processing-of-subspace-dominating-Tiakas-Papadopoulos/02fd54faaf73ef0943ffd47e84aefdb5a5dec1ba",
            "/paper/Threshold-based-probabilistic-top-k-dominating-Zhang-Lin/39bdad5c661ec01e665dd9a54c32e8616237651e",
            "/paper/Multi-dimensional-top-k-dominating-queries-Yiu-Mamoulis/5118b57f47915d49e9a78e41496383fd9649525e",
            "/paper/Processing-Top-k-Dominating-Queries-in-Metric-Tiakas-Valkanas/d3a11264d6c5ac0125a4cbc730ab81186671557f",
            "/paper/Close-Dominance-Graph%3A-An-Efficient-Framework-for-Santoso-Chiu/7ec1909b3c7b29c4a7b09f56f6449c0ccb7d08e6",
            "/paper/Metric-Based-Top-k-Dominating-Queries-Tiakas-Valkanas/abffc6d4c51fb851725f11050060c3193226c457",
            "/paper/Probabilistic-top-k-dominating-queries-in-uncertain-Lian-Chen/67a066602f934019460069ac7e10696acd10198f",
            "/paper/Progressive-skyline-computation-in-database-systems-Papadias-Tao/28e702e1a352854cf0748b9a6a9ad6679b1d4e83"
        ]
    },
    {
        "id": "d2b89d018b8423473f7478261508c70022c279d6",
        "title": "Seamless Texture Optimization for RGB-D Reconstruction",
        "abstract": "A novel tri-directional similarity texture synthesis method to eliminate the texture inconsistency in RGB-D 3D reconstruction and generate visually realistic texture mapping results and introduces a global color harmonization method to address the color inconsistency between texture images captured from different viewpoints. Restoring high-fidelity textures for 3D reconstructed models are an increasing demand in AR/VR, cultural heritage protection, entertainment, and other relevant fields. Due to geometric errors and camera pose drifting, existing texture mapping algorithms are either plagued by blurring and ghosting or suffer from undesirable visual seams. In this paper, we propose a novel tri-directional similarity texture synthesis method to eliminate the texture inconsistency in RGB-D 3D reconstruction and generate visually realistic texture mapping results. In addition to RGB color information, we incorporate a novel color image texture detail layer serving as an additional context to improve the effectiveness and robustness of the proposed method. First, we select an optimal texture image for each triangle face of the reconstructed model to avoid texture blurring and ghosting. During the selection procedure, the texture details are weighted to avoid generating texture chart partitions across high-frequency areas. Then, we optimize the camera pose of each texture image to align with the reconstructed 3D shape. Next, we propose a tri-directional similarity function to resynthesize the image context within the boundary stripe of texture charts, which can significantly diminish the occurrence of texture seams. Finally, we introduce a global color harmonization method to address the color inconsistency between texture images captured from different viewpoints. The experimental results demonstrate that the proposed method outperforms state-of-the-art texture mapping methods and effectively overcomes texture tearing, blurring, and ghosting artifacts.",
        "publication_year": "2021",
        "authors": [
            "Yanping Fu",
            "Qingan Yan",
            "Jie Liao",
            "Huajian Zhou",
            "Jin Tang",
            "Chunxia Xiao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "5",
        "reference_count": "51",
        "references": [
            "/paper/A-Robust-3D-Based-Color-Correction-Approach-for-Coelho-Dal%E2%80%99Col/7aea5ee6a11ca4666aeedea9001490e710c9a0fb",
            "/paper/A-Sequential-Color-Correction-Approach-for-Texture-Dal%E2%80%99Col-Coelho/49c2f3de221ed0a688731b6812540cc11d5a0300",
            "/paper/Self-supervised-coarse-to-fine-monocular-depth-a-Li-Luo/ebdd884a8e36fe830920868a0b993690e827bc95",
            "/paper/DGECN%3A-A-Depth-Guided-Edge-Convolutional-Network-6D-Cao-Luo/17a325cfe0997103a4a24fd0e7619a7919654ece",
            "/paper/Multi-scale-Convolutional-Feature-Fusion-For-6D-Ren-Liu/a82bf415a8e7b02c73fe1ab65d6825768baf80f1",
            "/paper/Texture-Mapping-for-3D-Reconstruction-with-RGB-D-Fu-Yan/fd8f3bc244999e003c799aba9e2bf642f570faa7",
            "/paper/Joint-Texture-and-Geometry-Optimization-for-RGB-D-Fu-Yan/1be4516222673337e2b4975ac1a3dcaf597b28d4",
            "/paper/3D-Reconstruction-and-Texture-Optimization-Using-a-Li-Xiao/ad6975a6c448eecc9209ffa218e1e057dd5c6ec6",
            "/paper/Plane-Based-Optimization-of-Geometry-and-Texture-of-Wang-Guo/bd6738efc5e87c9e1be73b411a0bbf83ca1c3ced",
            "/paper/A-Multi-resolution-Approach-for-Color-Correction-of-Rouhani-Fradet/591da5c81fd7a3d49979242ec9c09f6b9ee1d642",
            "/paper/On-multi-view-texture-mapping-of-indoor-using-depth-Do-Ma/d6e442eec15913d46ac6e92c6fc1e48c82fc3118",
            "/paper/Seamless-image-based-texture-atlases-using-blending-All%C3%A8ne-Pons/9f7be8afe4662fd01e850ebcc70576c520538edc",
            "/paper/Texture-map-generation-for-3D-reconstructed-scenes-Jeon-Jung/016ebe36d80c72ba50b3b8117e612bffe71bf478",
            "/paper/Patch-based-optimization-for-image-based-texture-Bi-Kalantari/414cc069420972c4186dd46ed4e7649f1af86443",
            "/paper/TextureFusion%3A-High-Quality-Texture-Acquisition-for-Lee-Ha/853fab7c02fccf79b00ff78d77085746846a0730"
        ]
    },
    {
        "id": "63a243afcb133569a962c41e9db956c076c5c4f3",
        "title": "Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression",
        "abstract": "A Distance-IoU (DIoU) loss is proposed by incorporating the normalized distance between the predicted box and the target box, which converges much faster in training than IoU and GIoU losses, thereby leading to faster convergence and better performance. Bounding box regression is the crucial step in object detection. In existing methods, while \u2113n-norm loss is widely adopted for bounding box regression, it is not tailored to the evaluation metric, i.e., Intersection over Union (IoU). Recently, IoU loss and generalized IoU (GIoU) loss have been proposed to benefit the IoU metric, but still suffer from the problems of slow convergence and inaccurate regression. In this paper, we propose a Distance-IoU (DIoU) loss by incorporating the normalized distance between the predicted box and the target box, which converges much faster in training than IoU and GIoU losses. Furthermore, this paper summarizes three geometric factors in bounding box regression, i.e., overlap area, central point distance and aspect ratio, based on which a Complete IoU (CIoU) loss is proposed, thereby leading to faster convergence and better performance. By incorporating DIoU and CIoU losses into state-of-the-art object detection algorithms, e.g., YOLO v3, SSD and Faster R-CNN, we achieve notable performance gains in terms of not only IoU metric but also GIoU metric. Moreover, DIoU can be easily adopted into non-maximum suppression (NMS) to act as the criterion, further boosting performance improvement. The source code and trained models are available at https://github.com/Zzh-tju/DIoU.",
        "publication_year": "2019",
        "authors": [
            "Zhaohui Zheng",
            "Ping Wang",
            "Wei Liu",
            "Jinze Li",
            "Rongguang Ye",
            "Dongwei Ren"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "1,308",
        "reference_count": "31",
        "references": [
            "/paper/Mixed-geometric-loss-for-bounding-box-regression-in-Ren-Luo/427ba0af0ccc9c6845b296029e3c4eeb6ee5f600",
            "/paper/Focal-and-Efficient-IOU-Loss-for-Accurate-Bounding-Zhang-Ren/eb20c617dc1b6e5b979f830fc38af37e7f1c9d3c",
            "/paper/SCALoss%3A-Side-and-Corner-Aligned-Loss-for-Bounding-Zheng-Zhao/04614b2a9c50902ffa2523b1c67aed8c54a65a8d",
            "/paper/Corner-Point-and-Foreground-Area-IoU-Loss%3A-Better-Cai-Zhang/ed30dfec0d0bab3921ead5906a1aaefa96ef5a37",
            "/paper/Improved-Design-Based-on-IoU-Loss-Functions-for-Box-Liu-Cheng/1461c59aa7de796cd1fceee5e6807a52de54809b",
            "/paper/SIoU-Loss%3A-More-Powerful-Learning-for-Bounding-Box-Gevorgyan/406b457cf2dc9a62417b05b755ef5434df0a4d33",
            "/paper/Accurate-Bounding-box-Regression-with-Distance-IoU-Yuan-Chang/435094f4d1ab7b9f32363fb7136255920bc1bd45",
            "/paper/Wise-IoU%3A-Bounding-Box-Regression-Loss-with-Dynamic-Tong-Chen/f2a42960b362bc673638643257d65c309b7324dd",
            "/paper/CRIoU%3A-A-Complete-and-Relevant-Bounding-Box-Method-Zhenjiu-Haoze/e3a25c1b3d96c7d77c699ea06b334678cfc006dc",
            "/paper/Designing-effective-power-law-based-loss-function-Aswal-Shukla/393ed2af7b99fb53da9944d6968bc3bad920e16b",
            "/paper/Generalized-Intersection-Over-Union%3A-A-Metric-and-a-Rezatofighi-Tsoi/889c81b4d7b7ed43a3f69f880ea60b0572e02e27",
            "/paper/Bounding-Box-Regression-With-Uncertainty-for-Object-He-Zhu/de9949331c81cc8697d48dfd1b9f54d604a7d85a",
            "/paper/UnitBox%3A-An-Advanced-Object-Detection-Network-Yu-Jiang/22264e60f1dfbc7d0b52549d1de560993dd96e46",
            "/paper/Acquisition-of-Localization-Confidence-for-Accurate-Jiang-Luo/7f75d4b462bd883f290461cdd8984f8cee6013ea",
            "/paper/Focal-Loss-for-Dense-Object-Detection-Lin-Goyal/72564a69bf339ff1d16a639c86a764db2321caab",
            "/paper/Cascade-R-CNN%3A-Delving-Into-High-Quality-Object-Cai-Vasconcelos/04957e40d47ca89d38653e97f728883c0ad26e5d",
            "/paper/Libra-R-CNN%3A-Towards-Balanced-Learning-for-Object-Pang-Chen/32a69681c103807704f71b838454c7924ceec5ce",
            "/paper/Multi-scale-Location-Aware-Kernel-Representation-Wang-Wang/c4fe488c0cff49e1ed6de1425ede27900005fd87",
            "/paper/You-Only-Look-Once%3A-Unified%2C-Real-Time-Object-Redmon-Divvala/f8e79ac0ea341056ef20f2616628b3e964764cfd",
            "/paper/FCOS%3A-Fully-Convolutional-One-Stage-Object-Tian-Shen/e2751a898867ce6687e08a5cc7bdb562e999b841"
        ]
    },
    {
        "id": "1c07e9a6395d352819f4800d7ae13c5e0fcb3a4c",
        "title": "Rich Features and Precise Localization with Region Proposal Network for Object Detection",
        "abstract": "This paper designs a new strategy for generating region proposals and proposes a new localization method for object detection that achieves the best recall and object detection accuracy. Deep Network greatly accelerates the development of object detection. Recent advances in object detection are mainly attributed to the combination of deep network and region proposal methods [1, 2, 3]. However, the accuracy of object detection on the complicated datasets is still not satisfied, especially on small object detection. This is mainly because of the coarseness of the convolution feature maps. In this paper, we design a new strategy for generating region proposals and propose a new localization method for object detection. Compared with previous baseline detectors such as Fast R-CNN [4] and Faster R-CNN [5], Our method makes use of the adjacent-level feature maps at all scales to generate region proposals and also adopts the cascaded region proposal network (RPN) to fine-tune the location of the bounding box. Compared with other state-of-the-art methods, our method achieves the best recall and object detection accuracy.",
        "publication_year": "2017",
        "authors": [
            "Mengdie Chu",
            "Shuai Wu",
            "Yifan Gu",
            "Yong Xu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "5",
        "reference_count": "30",
        "references": [
            "/paper/YADA%3A-you-always-dream-again-for-better-object-Nguyen-Nguyen/4e8148d59a57cff9e231961abcabc1e4fd66b7ff",
            "/paper/Image-denoising-using-deep-CNN-with-batch-Tian-Xu/b876422091c91ef739c958737f3f600a62ec9723",
            "/paper/Image-denoising-using-deep-CNNwith-batch-Tian-Xu/6fac0db975b31253ff44a530ceed36427687b737",
            "/paper/Multi-task-Network-Learning-Representation-Features-Wang-Lyu/cab6c4898fc6c8f9bec493b7fd60fc9d19a165a2",
            "/paper/Gait-feature-extraction-and-gait-classification-CNN-Wang-Zhang/1983cb431bfb2b4cbc311c5b6c08de5cbbc996a5",
            "/paper/Faster-R-CNN%3A-Towards-Real-Time-Object-Detection-Ren-He/424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "/paper/HyperNet%3A-Towards-Accurate-Region-Proposal-and-Kong-Yao/89c03327f45064f1fbfcd152f7fc0148d08d5acb",
            "/paper/R-FCN%3A-Object-Detection-via-Region-based-Fully-Dai-Li/b724c3f7ff395235b62537203ddeb710f0eb27bb",
            "/paper/Object-Detection-via-a-Multi-region-and-Semantic-Gidaris-Komodakis/e3ea1c7a3c6cc15590ab5e62a171536fe01b9b1c",
            "/paper/Feature-Pyramid-Networks-for-Object-Detection-Lin-Doll%C3%A1r/b9b4e05faa194e5022edd9eb9dd07e3d675c2b36",
            "/paper/A-Unified-Multi-scale-Deep-Convolutional-Neural-for-Cai-Fan/d6473533e89e5f946a6ff3ad07c6a74ee9b47672",
            "/paper/Scalable%2C-High-Quality-Object-Detection-Szegedy-Reed/4328ec9d98eff5d7eb70997f76d81b27849f3220",
            "/paper/DeepProposal%3A-Hunting-Objects-by-Cascading-Deep-Ghodrati-Diba/2725a68be6bc677bd435c19664569ecd45c52d7a",
            "/paper/Fast-R-CNN-Girshick/7ffdbc358b63378f07311e883dddacc9faeeaf4b",
            "/paper/What-Makes-for-Effective-Detection-Proposals-Hosang-Benenson/6c016579af5becc230fb9efc1f885f2afa65a46e"
        ]
    },
    {
        "id": "e6c033666c1e803921405b92af522ed87126d5a1",
        "title": "Planar flow-based planar region extraction for visual navigation using a single on-board camera",
        "abstract": "The conception of planar flow which describes the image motion pattern of the points on a plane is introduced, and the distance error function in image space between a point and the plane is defined, which can be used for planar point detection. Dominant plane detection is an essential task for autonomous navigation of mobile robots equipped with a vision system, as we assume that the mobile robot moves on the dominant plane. In this paper, the image point motion on a space plane is modeled, and the conception of planar flow which describes the image motion pattern of the points on a plane is introduced. Planar flow provides another geometric constraint on optical flow when there is planar region in image. Based on planar flow, the distance error function in image space between a point and the plane is defined, which can be used for planar point detection. To improve the robustness of planar region detection, some other prior knowledge, such as the pixel ratio or region size, can be combined with the basic process. Experiments are conducted to validate the proposed method. The experiments show that the proposed method can detect the planar region with low texture image.",
        "publication_year": "2011",
        "authors": [
            "Z. Wang",
            "F. Yi",
            "B. Cai",
            "Y. Liu"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": 0,
        "reference_count": "25",
        "references": [
            "/paper/Planar-region-alignment-based-visual-regulation-for-Wang-Liu/adfd953eb2eb52a37944ec95352e8284201138bd",
            "/paper/Reviews-on-Planar-Region-Detection-for-Visual-of-Wang-Cai/f903e1e285effd67fb141a55c6eb2b48bbbc4f86",
            "/paper/Dominant-plane-detection-from-optical-flow-for-Ohnishi-Imiya/49a59250716c524e7e24d36a7e2db9b08dac92b3",
            "/paper/Uncalibrated-obstacle-detection-using-normal-flow-Santos-Victor-Sandini/d0394f4cf681b001f10ee7e70acfcd7fb160499b",
            "/paper/Homography-based-ground-plane-detection-for-mobile-Conrad-DeSouza/c9867b36f37387cedafcdcab91899bbe2eae1b39",
            "/paper/Efficient-plane-detection-from-a-single-moving-Bouchafa-Patri/c7dd76e2d014a7aee28de00d5bcdca92450baa84",
            "/paper/Motion-Estimation-for-a-Mobile-Robot-Based-on-Wang-Gao/4a21381aa655f69c8608fd802e6cff80c6feedab",
            "/paper/An-Efficient-Approach-to-Onboard-Stereo-Vision-Pose-Sappa-Dornaika/d8d1fb804d1f4760393c6fd70c9072fa1b39f02c",
            "/paper/Planar-direct-method%3A-a-new-framework-for-stereo-Vincent-Tjahjadi/90c706594fd6f49519e48cd08229b3060de56c75",
            "/paper/Multiple-Plane-Segmentation-Using-Optical-Flow-Zucchelli-Santos-Victor/026a9628ddb113ac488b58a1197f42c5e9c20a17",
            "/paper/Homography-based-ground-plane-detection-using-a-Arr%C3%B3spide-Salgado/8c583939c241393a7c84862919b3e0ea0dc38d4a"
        ]
    },
    {
        "id": "755cbcbbbc0eda95254390a9df7aded9fea84c25",
        "title": "AutoLoss-Zero: Searching Loss Functions from Scratch for Generic Tasks",
        "abstract": "This paper proposes AutoLoss-Zero, which is a general framework for searching loss functions from scratch for generic tasks, and designs an elementary search space composed only of primitive mathematical operators to accommodate the heterogeneous tasks and evaluation metrics. Significant progress has been achieved in automating the design of various components in deep networks. However, the automatic design of loss functions for generic tasks with various evaluation metrics remains under-investigated. Previous works on handcrafting loss functions heavily rely on human expertise, which limits their extensibility. Meanwhile, searching for loss functions is nontrivial due to the vast search space. Existing efforts mainly tackle the issue by employing task-specific heuristics on specific tasks and particular metrics. Such work cannot be extended to other tasks without arduous human effort. In this paper, we propose AutoLoss-Zero, which is a general framework for searching loss functions from scratch for generic tasks. Specifically, we design an elementary search space composed only of primitive mathematical operators to accommodate the heterogeneous tasks and evaluation metrics. A variant of the evolutionary algorithm is employed to discover loss functions in the elementary search space. A loss-rejection protocol and a gradient-equivalence-check strategy are developed so as to improve the search efficiency, which are applicable to generic tasks. Extensive experiments on various computer vision tasks demonstrate that our searched loss functions are on par with or superior to existing loss functions, which generalize well to different datasets and networks. Code shall be released.",
        "publication_year": "2021",
        "authors": [
            "Hao Li",
            "Tianwen Fu",
            "Jifeng Dai",
            "Hongsheng Li",
            "Gao Huang",
            "Xizhou Zhu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "11",
        "reference_count": "67",
        "references": [
            "/paper/Reinforcement-Learning-with-Automated-Auxiliary-He-Zhang/551633338103610cb8a60e6df004ef6fd3841d17",
            "/paper/AutoLossGen%3A-Automatic-Loss-Function-Generation-for-Li-Ji/54f1d97e5973d8dae0c57074c33510cee117bd05",
            "/paper/AutoLoss-GMS%3A-Searching-Generalized-Margin-based-Gu-Li/62165de1bd768408ae19c7998f975e7736afe473",
            "/paper/Searching-Parameterized-AP-Loss-for-Object-Tao-Li/12cc7266789d161fe046ef6ea6f87a02cf48e3cd",
            "/paper/Online-Loss-Function-Learning-Raymond-Chen/6dc9b7502012d979654be7e60c5d9e2f51d00e7b",
            "/paper/Learning-Symbolic-Model-Agnostic-Loss-Functions-via-Raymond-Chen/7be09e6d87685288ad7ed478eb9edaa91e1d06ad",
            "/paper/Hierarchical-Bilevel-Learning-with-Architecture-and-Zhu-Ma/c1890fda11e05f3eaea539e069b3c6cf678e5004",
            "/paper/Tightening-the-Approximation-Error-of-Adversarial-Xia-Li/6b0eaf826ea8b8e23bb779a0766a0564d2b50ac7",
            "/paper/Alpha-IoU%3A-A-Family-of-Power-Intersection-over-for-He-Erfani/b9644d9db7c6448a7bec26c4b5f9fd05fe3b794a",
            "/paper/Jaccard-Metric-Losses%3A-Optimizing-the-Jaccard-Index-Wang-Blaschko/c13cc96c717333e2854d4a2d0cee84794282d5be",
            "/paper/AM-LFS%3A-AutoML-for-Loss-Function-Search-Li-Lin/f4a44b94d4b184e4638cb1019526170590f99ac4",
            "/paper/Auto-Seg-Loss%3A-Searching-Metric-Surrogates-for-Li-Tao/79112ae62a5f8ce03829d3c77e218269d4c0d35c",
            "/paper/AutoML%3A-A-Survey-of-the-State-of-the-Art-He-Zhao/3a7fa673ff8ec4ec2f322473de005f3cd09ea820",
            "/paper/Loss-Function-Discovery-for-Object-Detection-via-Liu-Zhang/3c49b14137703eb2fd777bdc4896918c0db38d4f",
            "/paper/A-Comprehensive-Survey-of-Neural-Architecture-and-Ren-Xiao/0d5cb85a6825ce7cd48c9b2ce49f3c1dc0daf8e1",
            "/paper/FBNetV3%3A-Joint-Architecture-Recipe-Search-using-Dai-Wan/ac8af4202f8ac179e727013929a95f6ecc4936e8",
            "/paper/Efficient-Neural-Architecture-Search-via-Parameter-Pham-Guan/fe9b8aac9fa3bfd9724db5a881a578e471e612d7",
            "/paper/Efficient-Optimization-for-Rank-Based-Loss-Mohapatra-Rolinek/cd06e0964181d1a537c7ca07f5327aa85804a610",
            "/paper/Evolving-Normalization-Activation-Layers-Liu-Brock/4987a09316e186efaf776cc521a3d84e21d20b34",
            "/paper/Loss-Function-Search-for-Face-Recognition-Wang-Wang/850f51cbfaef254769004750eac5c4f293df0773"
        ]
    },
    {
        "id": "6265ed23e558a80c9af599477b847c4f7e0027fc",
        "title": "Spherical Kernel for Efficient Graph Convolution on 3D Point Clouds",
        "abstract": "The proposed spherical kernel for efficient graph convolution of 3D point clouds maintains translation-invariance and asymmetry properties, where the former guarantees weight sharing among similar local structures in the data and the latter facilitates fine geometric learning. We propose a spherical kernel for efficient graph convolution of 3D point clouds. Our metric-based kernels systematically quantize the local 3D space to identify distinctive geometric relationships in the data. Similar to the regular grid CNN kernels, the spherical kernel maintains translation-invariance and asymmetry properties, where the former guarantees weight sharing among similar local structures in the data and the latter facilitates fine geometric learning. The proposed kernel is applied to graph neural networks without edge-dependent filter generation, making it computationally attractive for large point clouds. In our graph networks, each vertex is associated with a single point location and edges connect the neighborhood points within a defined range. The graph gets coarsened in the network with farthest point sampling. Analogous to the standard CNNs, we define pooling and unpooling operations for our network. We demonstrate the effectiveness of the proposed spherical kernel with graph neural networks for point cloud classification and semantic segmentation using ModelNet, ShapeNet, RueMonge2014, ScanNet and S3DIS datasets. The source code and the trained models can be downloaded from https://github.com/hlei-ziyan/SPH3D-GCN.",
        "publication_year": "2019",
        "authors": [
            "Huan Lei",
            "Naveed Akhtar",
            "A. Mian"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "102",
        "reference_count": "93",
        "references": [
            "/paper/Exploiting-Local-Geometry-for-Feature-and-Graph-for-Srivastava-Sharma/58817ced8646e06d36175b2d34d7139aadc54317",
            "/paper/SegGCN%3A-Efficient-3D-Point-Cloud-Segmentation-With-Lei-Akhtar/13079de18092149aa9fb9ee6c64726d7c2df0348",
            "/paper/Adaptive-Graph-Convolution-for-Point-Cloud-Analysis-Zhou-Feng/c64c827a6094b894985f28bb980a1aa452b33870",
            "/paper/Learning-of-3D-Graph-Convolution-Networks-for-Point-Lin-Huang/942cd93249dd7435797ee2e21670d61cba4298cd",
            "/paper/AGConv%3A-Adaptive-Graph-Convolution-on-3D-Point-Wei-Wei/4cadf361575ad311334a55129af8617a9359ffd9",
            "/paper/SPNet%3A-Multi-Shell-Kernel-Convolution-for-Point-Li-Fan/2d3230ac7af5bd6f7af93aae3bdf3c23261f7eab",
            "/paper/Spherical-Interpolated-Convolutional-Network-With-Wang-Yang/2279d989b4bc089fe1f40d49966475e903655721",
            "/paper/Point-Cloud-Convolution-Network-Based-on-Spatial-Xv-Deng/31a5abd43f7979ea9d25feba3619d345dd623e37",
            "/paper/Graph-Convolutional-Neural-Networks-based-3D-Hand-Castro-Vargas-Martinez-Gonzalez/506329dda335c9229e8e9947275ad545a042d3d3",
            "/paper/DANet%3A-Density-Adaptive-Convolutional-Network-with-He-Yu/f59ec0727131eb29b2ccb53b7ea3d8962f385537",
            "/paper/Octree-Guided-CNN-With-Spherical-Kernels-for-3D-Lei-Akhtar/9c042383bd8eb39eac442da96812e6b2273036bf",
            "/paper/Dynamic-Graph-CNN-for-Learning-on-Point-Clouds-Wang-Sun/e1799aaf23c12af6932dc0ef3dfb1638f01413d1",
            "/paper/Mining-Point-Cloud-Local-Structures-by-Kernel-and-Shen-Feng/5eb4a5ac7faa2fc75e1739b45c511ba3fab5bb56",
            "/paper/PointConv%3A-Deep-Convolutional-Networks-on-3D-Point-Wu-Qi/5ee147684b06ffc4db0f6326e0cba017d12ceff3",
            "/paper/Graph-Attention-Convolution-for-Point-Cloud-Wang-Huang/292f120b2664a37335e557bcb2a860a7a8b507a6",
            "/paper/KPConv%3A-Flexible-and-Deformable-Convolution-for-Thomas-Qi/9218b3e7e447048a070e565b9add26e41d861964",
            "/paper/SyncSpecCNN%3A-Synchronized-Spectral-CNN-for-3D-Shape-Yi-Su/ade1b1bfe2abefbcfe324499aa284db2b8fc50a6",
            "/paper/PointNet%3A-Deep-Learning-on-Point-Sets-for-3D-and-Qi-Su/d997beefc0922d97202789d2ac307c55c2c52fba",
            "/paper/PointCNN%3A-Convolution-On-X-Transformed-Points-Li-Bu/6400c36efdb8a66b401b6aef26c057227266fddd",
            "/paper/Spherical-Fractal-Convolutional-Neural-Networks-for-Rao-Lu/759e41c0c0d9b7f63303f03a064754dd30916441"
        ]
    },
    {
        "id": "88a8baa1be5292e62622f1cb8e627fbf759bf741",
        "title": "Video Summarization With Attention-Based Encoder\u2013Decoder Networks",
        "abstract": "This paper proposes a novel video summarization framework named attentive encoder\u2013decoder networks forVideo summarization (AVS), in which the encoder uses a bidirectional long short-term memory (BiLSTM) to encode the contextual information among the input video frames. This paper addresses the problem of supervised video summarization by formulating it as a sequence-to-sequence learning problem, where the input is a sequence of original video frames, and the output is a keyshot sequence. Our key idea is to learn a deep summarization network with attention mechanism to mimic the way of selecting the keyshots of human. To this end, we propose a novel video summarization framework named attentive encoder\u2013decoder networks for video summarization (AVS), in which the encoder uses a bidirectional long short-term memory (BiLSTM) to encode the contextual information among the input video frames. As for the decoder, two attention-based LSTM networks are explored by using additive and multiplicative objective functions, respectively. Extensive experiments are conducted on two video summarization benchmark datasets, i.e., SumMe and TVSum. The results demonstrate the superiority of the proposed AVS-based approaches against the state-of-the-art approaches, with remarkable improvements on both datasets.",
        "publication_year": "2017",
        "authors": [
            "Zhong Ji",
            "Kailin Xiong",
            "Yanwei Pang",
            "Xuelong Li"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "206",
        "reference_count": "50",
        "references": [
            "/paper/Deep-Attentive-Video-Summarization-With-Consistency-Ji-Zhao/c1f1a999d84dce8e84c28b7d33edd6aa706b85c2",
            "/paper/Unsupervised-Video-Summarization-via-Deep-Learning-Yuan-Zhang/47f88c2c6cd82b4ec24b79c292e095cfff14a020",
            "/paper/Attentive-and-Adversarial-Learning-for-Video-Fu-Tai/b0e6921c84263c5c961cd286e3145ca5eda352e0",
            "/paper/Deep-Semantic-and-Attentive-Network-for-Video-Zhong-Lin/17399e79529b7ba5a189158a3ab2336343bfc645",
            "/paper/Learning-Hierarchical-Self-Attention-for-Video-Liu-Li/1d5ed33c72ccb5a7cb1e36fe24a9235d1ecd6edb",
            "/paper/Video-summarization-with-a-graph-convolutional-Li-Tang/059551ea629c70bd2a1939c5bfde74c7f8a39833",
            "/paper/Video-Summarization-Using-Deep-Neural-Networks%3A-A-Apostolidis-Adamantidou/40ffdf58932db8284a33a56b6ce8bded2f5a829b",
            "/paper/Video-summarization-with-u-shaped-transformer-Chen-Guo/38a09c45b8abbfa8d47a06271aae25d184b017f2",
            "/paper/Property-Constrained-Dual-Learning-for-Video-Zhao-Li/b83b6069fc6ad861c6d7cb574825488e0fca8521",
            "/paper/Unsupervised-Video-Summarization-with-a-Attentive-Liang-Lv/4881c8c48881f28b7d29226f1a54f38b71481048",
            "/paper/Retrospective-Encoders-for-Video-Summarization-Zhang-Grauman/3a965d76c1a7414edcf7258a011b4237cfdf0ac7",
            "/paper/Unsupervised-Video-Summarization-with-Adversarial-Mahasseni-Lam/620fe6c786d15efca7f553ad70f295e2b693b391",
            "/paper/Video-Summarization-Using-Fully-Convolutional-Rochan-Ye/8c413c2ee66664909d8c194f3f3e08c5f109c3c1",
            "/paper/Video-Summarization-with-Long-Short-Term-Memory-Zhang-Chao/1dbc12e54ceb70f2022f956aa0a46e2706e99962",
            "/paper/A-General-Framework-for-Edited-Video-and-Raw-Video-Li-Zhao/18a6a7edfbce1cc11fb7447a133c0ac150b2785c",
            "/paper/HSA-RNN%3A-Hierarchical-Structure-Adaptive-RNN-for-Zhao-Li/1ea6c0f1bde1f800e3c9c23573325e0d5283b12c",
            "/paper/A-Memory-Network-Approach-for-Story-Based-Temporal-Lee-Sung/e68c133947bbf14834f5353126ae85cc048642db",
            "/paper/Improving-Sequential-Determinantal-Point-Processes-Sharghi-Borji/45785f61445527acef55b9cf269c1562fce938fa",
            "/paper/Query-Aware-Sparse-Coding-for-Multi-Video-Ji-Ma/6172c935175fafade8a2acd85284dd0237bc041c",
            "/paper/Sequence-to-Sequence-Video-to-Text-Venugopalan-Rohrbach/e58a110fa1e4ddf247d5c614d117d64bfbe135c4"
        ]
    },
    {
        "id": "9dfd3330017c41f510d6e7274d2af46958a1ccf2",
        "title": "Scale-Aware Domain Adaptation for Robust UAV Tracking",
        "abstract": "A contrastive learning-inspired network is proposed to guide the training phase of Transformer-based feature alignment for small objects in a novel scale-aware domain adaptation framework, i.e., ScaleAwareDA. Siamese object tracking has facilitated diversified applications for autonomous unmanned aerial vehicles (UAVs). However, they are typically trained on general images with relatively large objects instead of small objects observed from UAV. The gap on object scale between the training and inference phases is prone to suboptimal tracking performance or even failure. To solve the gap issue and tailor general Siamese trackers for UAV tracking, this work proposes a novel scale-aware domain adaptation framework, i.e., ScaleAwareDA. Specifically, a contrastive learning-inspired network is proposed to guide the training phase of Transformer-based feature alignment for small objects. In this network, feature projection module is designed to avoid information loss of small objects. Feature prediction module is developed to drive the aforementioned training phase in a self-supervised way. In addition, to construct the target domain, training datasets with UAV-specific attributes are obtained by downsampling general training datasets. Consequently, this novel training approach can assist a tracker to represent objects in UAV scenarios more powerfully and thus maintain its robustness. Extensive experiments on three authoritative challenging UAV tracking benchmarks have demonstrated the superior tracking performance of ScaleAwareDA. In addition, quantitative real-world tests further attest to its practicality.",
        "publication_year": "2023",
        "authors": [
            "Changhong Fu",
            "Teng Li",
            "Junjie Ye",
            "Guang-Zheng Zheng",
            "Sihang Li",
            "Peng Lu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "34",
        "references": [
            "/paper/Siamese-Anchor-Proposal-Network-for-High-Speed-Fu-Cao/827ee2cdc56ea65ef644bd8ca085f4274b106f03",
            "/paper/The-Unmanned-Aerial-Vehicle-Benchmark%3A-Object-and-Du-Qi/a213bbf9740854a276fdf71dad8f30cfbe3ea4d4",
            "/paper/Multi-Regularized-Correlation-Filter-for-UAV-and-Ye-Fu/4d57a810cd768b5ad4de6d1e21673f3d3b89f00d",
            "/paper/AutoTrack%3A-Towards-High-Performance-Visual-Tracking-Li-Fu/0619650ae0f698bcc38244a6858cc270df9dfaad",
            "/paper/Unsupervised-Domain-Adaptation-for-Nighttime-Aerial-Ye-Fu/32db2d409384575aeae453acc45220b51fe96301",
            "/paper/Onboard-Real-Time-Aerial-Tracking-With-Efficient-Fu-Cao/069cece5dc7c52914f6a9dfcb14dd10834bc98a3",
            "/paper/Ocean%3A-Object-aware-Anchor-free-Tracking-Zhang-Peng/27d52bf3265bea0f9929980f6ffb4c2009eecfee",
            "/paper/A-Benchmark-and-Simulator-for-UAV-Tracking-Mueller-Smith/27850781e39df9f750e05409b8072261124068e8",
            "/paper/TrackingNet%3A-A-Large-Scale-Dataset-and-Benchmark-in-M%C3%BCller-Bibi/8c11e517c2c028d63bc70c7d90c6b3d3ab805b1b",
            "/paper/Siamese-Box-Adaptive-Network-for-Visual-Tracking-Chen-Zhong/cce1fecc800d2782da638f3060d5b2e887739f74"
        ]
    },
    {
        "id": "07d6f5d6f76becdd3d30dbc308c63dcf3fbae2b3",
        "title": "Mining Spatial-Temporal Similarity for Visual Tracking",
        "abstract": "This paper proposes a prediction based CF tracking framework by learning the spatial-temporal similarity of consecutive frames for sample managing, template regularization, and training response pre-weighting, and provides effective optimization algorithms to solve the learning task. Correlation filter (CF) is a critical technique to improve accuracy and speed in the field of visual object tracking. Despite being studied extensively, most existing CF methods suffer from failing to make the most of the inherent spatial-temporal prior of videos. To address this limitation, as consecutive frames are eminently resemble in most videos, we investigate a novel scheme to predict targets\u2019 future state by exploiting previous observations. Specifically, in this paper, we propose a prediction based CF tracking framework by learning the spatial-temporal similarity of consecutive frames for sample managing, template regularization, and training response pre-weighting. We model the learning problem theoretically as a novel objective and provide effective optimization algorithms to solve the learning task. In addition, we implement two CF trackers with different features. Extensive experiments are conducted on three popular benchmarks to validate our scheme. The encouraging results demonstrate that the proposed scheme can significantly boost the accuracy of CF tracking, and the two trackers achieve competitive performances against state-of-the-art trackers. We finally present a comprehensive analysis on the efficacy of our proposed method and the efficiency of our trackers to facilitate real-world visual tracking applications.",
        "publication_year": "2020",
        "authors": [
            "Yu Zhang",
            "Xingyu Gao",
            "Zhenyu Chen",
            "Huicai Zhong",
            "Hongtao Xie",
            "C. Yan"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "8",
        "reference_count": "71",
        "references": [
            "/paper/A-background-aware-correlation-filter-with-adaptive-Zhang-Yuan/72c43972036b430ca5caafd9e674ee22f589e9c4",
            "/paper/Explicitly-Modeling-the-Discriminability-for-Visual-Wang-Yang/8eaa77cd1622a1b14c4988ba16dd10b92dfc57f3",
            "/paper/Learning-Future-Aware-Correlation-Filters-for-UAV-Zhang-Ma/1b0be91db095b41a50cbf1d72e0f28c88162da4c",
            "/paper/SIHRNet%3A-a-fully-convolutional-network-for-single-a-Wang-Tao/4deeb0b3078d0fb2145bd146869df0a018f9ac11",
            "/paper/Intelligent-Quality-Evaluation-System-for-Vertical-Ma-Chen/11c34d6d44d31c642e42d83a04c411d97a3dc22a",
            "/paper/The-Fast-Simulation-Architecture-Construction-for-Li-Chen/759f68e4549abd5636106ab4f1ac691f2445b539",
            "/paper/An-Electromagnetic-Field-Detection-Module-of-Wang-Gong/6ce50d00ff8bfa51dc89521f6317704b37bb3800",
            "/paper/Cross-Media-and-Multilingual-Image-Understanding-on-Gao-Chen/26f36554b6895af2dd7c4a74cf9021972d7afd52",
            "/paper/Visual-Tracking-via-Adaptive-Spatially-Regularized-Dai-Wang/e5820233ea8abd27c81c0a83b1b782a9d4ca8db2",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Learning-Spatial-Temporal-Regularized-Correlation-Li-Tian/9f45b55af027503fab557f55f70e81e43c6c1db7",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Reliable-Patch-Trackers%3A-Robust-visual-tracking-by-Li-Zhu/a9448de3265ed9ecdc11c79273aee92daa31f79a",
            "/paper/A-novel-adaptive-kernel-correlation-filter-tracker-Liu-Lian/e81d82bedf7728ce9a76cbdbbcd7d39e913b24da",
            "/paper/Real-Time-Long-Term-Tracking-With-Liang-Wu/894e4376750b83b63649cc518b121f345ca0df83",
            "/paper/Context-Aware-Correlation-Filter-Tracking-Mueller-Smith/8c2ea6e7063199bd2d7403abd70418d5cffad6a4",
            "/paper/Long-term-correlation-tracking-Ma-Yang/754504cf01ef3846259783e748b1d3ea52fa2c81"
        ]
    },
    {
        "id": "b762ecb0624005831f2f3d8eb626d53e8eca4b6c",
        "title": "Superpixel tracking",
        "abstract": "This paper presents a discriminative appearance model based on superpixels, thereby facilitating a tracker to distinguish the target and the background with mid-level cues and is shown to perform favorably against existing methods for object tracking. While numerous algorithms have been proposed for object tracking with demonstrated success, it remains a challenging problem for a tracker to handle large change in scale, motion, shape deformation with occlusion. One of the main reasons is the lack of effective image representation to account for appearance variation. Most trackers use high-level appearance structure or low-level cues for representing and matching target objects. In this paper, we propose a tracking method from the perspective of mid-level vision with structural information captured in superpixels. We present a discriminative appearance model based on superpixels, thereby facilitating a tracker to distinguish the target and the background with mid-level cues. The tracking task is then formulated by computing a target-background confidence map, and obtaining the best candidate by maximum a posterior estimate. Experimental results demonstrate that our tracker is able to handle heavy occlusion and recover from drifts. In conjunction with online update, the proposed algorithm is shown to perform favorably against existing methods for object tracking.",
        "publication_year": "2011",
        "authors": [
            "Shu Wang",
            "Huchuan Lu",
            "Fan Yang",
            "Ming-Hsuan Yang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "568",
        "reference_count": "23",
        "references": [
            "/paper/Robust-Superpixel-Tracking-Yang-Lu/739d084e486702dbdad01d668f77b431228bae9d",
            "/paper/SPiKeS%3A-Superpixel-Keypoints-structure-for-robust-Derue-Bilodeau/266e7a0f78181ca973d2fa3e4f7a2b9965aa13dc",
            "/paper/Pixel-based-object-tracking-Phalke-Hegadi/0e25a38f0bfba8d792c4ea03209e6382d7708550",
            "/paper/Visual-Tracking-Using-Superpixel-Based-Appearance-Shahed-Rushdi/04223032a83ba3e597a4045e6bbfd22795b5b130",
            "/paper/On-large-appearance-change-in-visual-tracking-Liang-Wang/bc39eece102a6752b104c987f59740531ef7a389",
            "/paper/Robust-Superpixel-Tracking-via-Depth-Fusion-Yuan-Fang/694a324f50153c8f8a8a3b11378d90c9bfe24a9f",
            "/paper/Robust-Object-Tracking-Using-Constellation-Model-Wang-Nevatia/3da22a90b766f7535292be3b5e27f3cc90c33c4b",
            "/paper/Compressive-Tracking-based-on-Superpixel-Chen-Sahli/25554df51451fd7abefc9040f8e694cf59d00bf2",
            "/paper/Robust-Superpixel-Tracking-with-Weighted-Learning-Cheng-Li/22d7c6661c2bbeb22edd92f9652b08750a84045c",
            "/paper/Efficient-Occlusion-Handling-Object-Tracking-System-Sathish-Roger/b25ea8c4e19809b8f40bb6a574db70dc7ca3c7e5",
            "/paper/Visual-tracking-decomposition-Kwon-Lee/29e1e20323f7cb6c15c6acf5cc6573a2f84e6478",
            "/paper/Incremental-Learning-for-Visual-Tracking-Lim-Ross/14ee8cf76e31fdf4381d29c1fb61017beb52f672",
            "/paper/Visual-tracking-with-online-Multiple-Instance-Babenko-Yang/421bf4eeba623f722bf98340d71e3d229881e92d",
            "/paper/Robust-online-appearance-models-for-visual-tracking-Jepson-Fleet/4411f262853bf7f1eb8e2efe03eb0402f5e9ad2c",
            "/paper/A-Nonparametric-Treatment-for-Location-Segmentation-Lu-Hager/e20c24e03ae025e563d6917a1ebf032820e2a519",
            "/paper/Tracking-as-Repeated-Figure-Ground-Segmentation-Ren-Malik/6288fa41fee6d00eb6a98471c7433849527a51da",
            "/paper/PROST%3A-Parallel-robust-online-simple-tracking-Santner-Leistner/16e36a4b59e214786737aa4ebc3ba86075b61e49",
            "/paper/An-adaptive-color-based-particle-filter-Nummiaro-Koller-Meier/e11dab9ddf9ec9b2a0fa35e4b91656ee2ad63aa0",
            "/paper/Robust-Fragments-based-Tracking-using-the-Integral-Adam-Rivlin/15cd7d675e499d6e53014916d7cf4a1714341f6a",
            "/paper/On-line-selection-of-discriminative-tracking-Collins-Liu/f51cb4be6e22f7e2b7126c9f2e250071a2351e08"
        ]
    },
    {
        "id": "4fee2057164c787748353dbdded04d3b0aed15f5",
        "title": "Animatable Implicit Neural Representations for Creating Realistic Avatars from Videos",
        "abstract": "This paper introduces a pose-driven deformation field based on the linear blend skinning algorithm, which combines the blend weight field and the 3D human skeleton to produce observation-to-canonical correspondences and significantly outperforms recent human modeling methods. This paper addresses the challenge of reconstructing an animatable human model from a multi-view video. Some recent works have proposed to decompose a non-rigidly deforming scene into a canonical neural radiance field and a set of deformation fields that map observation-space points to the canonical space, thereby enabling them to learn the dynamic scene from images. However, they represent the deformation field as translational vector field or SE(3) field, which makes the optimization highly under-constrained. Moreover, these representations cannot be explicitly controlled by input motions. Instead, we introduce a pose-driven deformation field based on the linear blend skinning algorithm, which combines the blend weight field and the 3D human skeleton to produce observation-to-canonical correspondences. Since 3D human skeletons are more observable, they can regularize the learning of the deformation field. Moreover, the pose-driven deformation field can be controlled by input skeletal motions to generate new deformation fields to animate the canonical human model. Experiments show that our approach significantly outperforms recent human modeling methods. The code is available at https://zju3dv.github.io/animatable_nerf/.",
        "publication_year": "2022",
        "authors": [
            "Sida Peng",
            "Zhenqi Xu",
            "Junting Dong",
            "Qianqian Wang",
            "Shang-Wei Zhang",
            "Qing Shuai",
            "H. Bao",
            "Xiaowei Zhou"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "93",
        "references": [
            "/paper/FIANCEE%3A-Faster-Inference-of-Adversarial-Networks-Karpikova-Ekaterina/03093ee286d7f8c575577df7e7451f5a70c8516f",
            "/paper/Volume-Rendering-of-Neural-Implicit-Surfaces-Yariv-Gu/eded1f3acaba853499fd5a6b3de63fa9d5e0cef2",
            "/paper/NeuS%3A-Learning-Neural-Implicit-Surfaces-by-Volume-Wang-Liu/cf5647cb2613f5f697729eab567383006dcd4913",
            "/paper/Animatable-Neural-Implicit-Surfaces-for-Creating-Peng-Zhang/f04d8a5f62099635d30284d1b0167c4ff2d82348",
            "/paper/A-NeRF%3A-Articulated-Neural-Radiance-Fields-for-and-Su-Yu/74bb19d1ce2ec9fb9605545a750e68281a067e63",
            "/paper/Neural-Body%3A-Implicit-Neural-Representations-with-Peng-Zhang/af8faec7c0b8f4b2a28d42a86e0e7d499016c560",
            "/paper/Multi-View-Neural-Human-Rendering-Wu-Wang/61f0a31390a1453607e4c5050692bc15cffc60c9",
            "/paper/DeepCap%3A-Monocular-Human-Performance-Capture-Using-Habermann-Xu/57cf4fa6ab46f705e42671b23eace2495214b53c",
            "/paper/Pose-space-deformation%3A-a-unified-approach-to-shape-Lewis-Cordner/94d88e179a187aee3a5418c7e288e1a317c0b109",
            "/paper/SelfRecon%3A-Self-Reconstruction-Your-Digital-Avatar-Jiang-Hong/17a4c0e0e859b8e36a0591f4b5ff26b62e83ea60",
            "/paper/Neural-Actor%3A-Neural-Free-view-Synthesis-of-Human-Liu-Habermann/29da3be81905d577dac9144c1c3cb5c6678f7027"
        ]
    },
    {
        "id": "a1e9c22100e3fe1862188093e4d054ed08dde99b",
        "title": "Learning Channel-Aware Correlation Filters for Robust Object Tracking",
        "abstract": "A new and effective channel-aware correlation filters (CACF) method for boosting the tracking performance by dynamically select representative and discriminative feature channels from high-dimensional CNN features to reduce the model complexity and better distinguish the target object from the background. Correlation filters with Convolutional Neural Networks (CNNs) features have obtained tremendous attention and success in visual tracking. However, redundant and noisy feature channels existed in CNN features may cause severe over-fitting and greatly limit the discriminative power of the tracking model. To tackle the issue, in this paper, we develop a new and effective channel-aware correlation filters (CACF) method for boosting the tracking performance. Our CACF method aims to dynamically select representative and discriminative feature channels from high-dimensional CNN features to reduce the model complexity and better distinguish the target object from the background. Moreover, the CACF model is solved by the alternating direction method of multipliers (ADMM) to learn correlation filters. By retaining reliable feature channels, our CACF tracking method can reach better generalization ability and discriminative ability to accurately localize the target object. Comprehensive experiments are conducted on challenging tracking datasets, and the experiment results prove that our CACF method obtains favorable tracking accuracy compared to several popular tracking methods.",
        "publication_year": "2022",
        "authors": [
            "Ke Nai",
            "Zhiyong Li",
            "Haidong Wang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3",
        "reference_count": "74",
        "references": [
            "/paper/Adaptive-temporal-feature-modeling-for-visual-via-Wang-Zhang/07b87af5fb9cbb6624305b1d414078c62bba5869",
            "/paper/Template-guided-frequency-attention-and-adaptive-Xue-Jin/3a779c4da3b1ed8e163fffd8d4c60b720bce0351",
            "/paper/Fast-visual-tracking-with-lightweight-Siamese-and-Zhang-Liu/bc40e3608975b0bd070cb9b0edca333ec8d9fd70",
            "/paper/Adaptive-Channel-Selection-for-Robust-Visual-Object-Xu-Feng/08dda937ef87aa6933fd40d6dde18b909ef78d85",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Joint-Group-Feature-Selection-and-Discriminative-Xu-Feng/7a96b5e56cd1a3964f22d0cb3a82eeddf331a5a5",
            "/paper/Complementary-Discriminative-Correlation-Filters-on-Zhu-Wu/c824e01622546f8d56e5af4f319293c7b46bbc36",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Channel-Graph-Regularized-Correlation-Filters-for-Jain-Tyagi/2c139fcf76e13b2636545249e5f948b62077391b",
            "/paper/Multi-Task-Convolution-Operators-With-Object-for-Zheng-Liu/436ab95cc2caf24c96ebf50a7a680bf710f33f38",
            "/paper/Discriminative-Correlation-Filter-with-Channel-and-Luke%C5%BEi%C4%8D-Voj%C3%ADr/b16a583ee173f222c690242aaff7925838893fe8",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "/paper/Learning-Adaptive-Discriminative-Correlation-via-Xu-Feng/bd5ecaea14eeb27c776b2c153b3e4716448bad4e"
        ]
    },
    {
        "id": "bb81652f87b117c3458a470c9874b33968ee2a4c",
        "title": "Segmentation and classification on chest radiography: a systematic survey",
        "abstract": "The research conducted using chest X-rays for the lung segmentation and detection/classification of pulmonary disorders on publicly available datasets using the Generative Adversarial Network (GAN) models is focused on. Chest radiography (X-ray) is the most common diagnostic method for pulmonary disorders. A trained radiologist is required for interpreting the radiographs. But sometimes, even experienced radiologists can misinterpret the findings. This leads to the need for computer-aided detection diagnosis. For decades, researchers were automatically detecting pulmonary disorders using the traditional computer vision (CV) methods. Now the availability of large annotated datasets and computing hardware has made it possible for deep learning to dominate the area. It is now the modus operandi for feature extraction, segmentation, detection, and classification tasks in medical imaging analysis. This paper focuses on the research conducted using chest X-rays for the lung segmentation and detection/classification of pulmonary disorders on publicly available datasets. The studies performed using the Generative Adversarial Network (GAN) models for segmentation and classification on chest X-rays are also included in this study. GAN has gained the interest of the CV community as it can help with medical data scarcity. In this study, we have also included the research conducted before the popularity of deep learning models to have a clear picture of the field. Many surveys have been published, but none of them is dedicated to chest X-rays. This study will help the readers to know about the existing techniques, approaches, and their significance.",
        "publication_year": "2022",
        "authors": [
            "Taruna Agrawal",
            "P. Choudhary"
        ],
        "related_topics": [
            "Medicine",
            "Computer Science"
        ],
        "citation_count": "12",
        "reference_count": "193",
        "references": [
            "/paper/CXLSeg-Dataset%3A-Chest-X-ray-with-Lung-Segmentation-Nimalsiri-Hennayake/fd88f6b78781a7ab8f89cf7b8debc69e1bcf25a0",
            "/paper/Radiopaths%3A-Deep-Multimodal-Analysis-on-Chest-Kohankhaki-Ayad/f8ec289d7f4e12440c5b535a6fdd7bb8f0e42280",
            "/paper/A-Patient-Specific-Algorithm-for-Lung-Segmentation-Silva-Narayanan/456ff37b4750cc214fd125b26429d68deba5a96e",
            "/paper/A-Survey-on-Tools-and-Techniques-for-Localizing-in-Aasem-Iqbal/08cc23a285242e5f51d4bf949be13cf7517448d7",
            "/paper/X-Ray-Lung-Image-Classification-Using-a-Canny-Edge-Jadwaa/fa0d3f97cb6c6fcf32c93ebd810b1db3a781268b",
            "/paper/Computer-assisted-diagnosis-for-an-early-of-lung-in-Juan-Monso/3f981fc749c2856290fc71d70ee6025650f85e4a",
            "/paper/OWAE-Net%3A-COVID-19-detection-from-ECG-images-using-Prashant-Choudhary/3d0e6838bff8e8f14e424f63cb5b6f6d49db5867",
            "/paper/Data-privacy-protection-domain-adaptation-by-and-Yuan-Erdt/b63e46625525031bea5e6a4f9e6146223f1bd66c",
            "/paper/APPLICATION-OF-COMPUTER-VISION-FOR-DIAGNOSTICS-OF-%D0%A2%D0%B5%D0%BF%D0%BB%D1%8F%D0%BA%D0%BE%D0%B2%D0%B0-%D0%A1%D1%82%D0%B0%D1%80%D0%BA%D0%BE%D0%B2/f4454821d64a6862a62a006b4f23bc94163cd5f5",
            "/paper/Evaluation-of-geometric-differences-between-right-Tulo-Govindarajan/71553bb57a514d611eea7ef2d03dca9c6146fd46",
            "/paper/Classification-and-Predictions-of-Lung-Diseases-V2-Souid-Sakli/9bbd1701eb416bdac1ff0e93736f8ca5519bfbc8",
            "/paper/Deep-Learning-for-Detection-and-Localization-of-Rakshit-Saha/fb8dd39e76e4c725d87c3c9dfe860e16453a9326",
            "/paper/An-automatic-method-for-lung-segmentation-and-in-Souza-Diniz/e79932b0be807431a8c5ce016dc4a1312c8e3ec5",
            "/paper/Multi-resolution-convolutional-networks-for-chest-Li-Shen/721dda10300436dc990300f7badd459de6d4ece0",
            "/paper/Deep-Learning-at-Chest-Radiography%3A-Automated-of-by-Lakhani-Sundaram/28bab81994b60eadc04033885d1023a9116f8e95",
            "/paper/Weakly-Supervised-Deep-Learning-for-Thoracic-and-on-Yan-Yao/2f5b0acdb7e9bbaebbee6e33e618fda3ca7e3bac",
            "/paper/Convolution-neural-network-based-detection-of-lung-Hasegawa-Lo/554e90bbe835ba26452899f730cca3c06aaf2499",
            "/paper/Identifying-disease-free-chest-x-ray-images-with-Wong-Moradi/e42ef6754e8eb6b5a92e6b9cc93f31ee13807715",
            "/paper/Attention-U-Net-Based-Adversarial-Architectures-for-Ga'al-Maga/7c9af320dc827a1805f627f7da197955e5de1643",
            "/paper/Segmentation-of-Lungs-in-Chest-X-Ray-Image-Using-Munawar-Azmat/da959782ef230a5653967585090ea31e67390ce1"
        ]
    },
    {
        "id": "a4f8dc311d7d5416e126368f86b40e0c70485480",
        "title": "Development and validation of radiomics machine learning model based on contrast-enhanced computed tomography to predict axillary lymph node metastasis in breast cancer",
        "abstract": "Overall, the nomogram based on radiomics features and clinical factors can help radiologists to predict axillary lymph node metastasis preoperatively and provide valuable information for individual treatment. Preoperative identification of axillary lymph node metastasis can play an important role in treatment selection strategy and prognosis evaluation. This study aimed to establish a clinical nomogram based on lymph node images to predict lymph node metastasis in breast cancer patients. A total of 193 patients with non-specific invasive breast cancer were divided into training (n \u2550 135) and validation set (n \u2550 58). Radiomics features were extracted from lymph node images instead of tumor region, and the least absolute shrinkage and selection operator logistic algorithm was used to select the extracted features and generate radiomics score. Then, the important clinical factors and radiomics score were integrated into a nomogram. A receiver operating characteristic curve was used to evaluate the nomogram, and the clinical benefit of using the nomogram was evaluated by decision curve analysis. We found that clinical N stage and radiomics score were independent clinical predictors. Besides, the nomogram accurately predicted axillary lymph node metastasis, yielding an area under the receiver operating characteristic curve of 0.95 (95% confidence interval 0.93\u20130.98) in the validation set, indicating satisfactory calibration. Decision curve analysis confirmed that the nomogram had higher clinical utility than clinical N stage or radiomics score alone. Overall, the nomogram based on radiomics features and clinical factors can help radiologists to predict axillary lymph node metastasis preoperatively and provide valuable information for individual treatment.",
        "publication_year": "2022",
        "authors": [
            "Jie Zhang",
            "Gaofei Cao",
            "H. Pang",
            "Jin Li",
            "Xiaopeng Yao"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "2",
        "reference_count": "26",
        "references": [
            "/paper/Radiomics-model-of-diffusion-weighted-whole-body-in-Haraguchi-Kobayashi/2fb524759f430cdde1cef7d694716fb695c54385",
            "/paper/Machine-learning-to-improve-prognosis-prediction-of-Yang-Ma/ed65771898fb246a3b490a21f5c9c68b1d3979e6",
            "/paper/Prediction-of-Metastasis-in-the-Axillary-Lymph-of-A-Yang-Dong/3a8f6d7248b322debefe2fe5e03168a3ba52b5a4",
            "/paper/Magnetic-resonance-imaging-radiomics-predicts-lymph-Yu-He/4c95603a9611faa53c3ca851f324ac0ca6c95a17",
            "/paper/Value-of-the-Application-of-CE-MRI-Radiomics-and-in-Zhu-Yang/26bf8a7aea976e86f10852efcfe9a419053331aa",
            "/paper/Development-of-High-Resolution-Dedicated-PET-Based-Cheng-Ren/e7cb0c19c6126b00b4983caf93fc25086b5c88d6",
            "/paper/Development-and-Validation-of-a-Preoperative-to-and-Yu-Tan/211783a923ac1e38941247f65c256ec21a393045",
            "/paper/Deep-learning-radiomics-can-predict-axillary-lymph-Zheng-Yao/a0de9e92ebb90a93b3519656f395b7afa6a4ad8e",
            "/paper/Predictive-Value-of-Preoperative-Multidetector-Row-Chen-Zhang/7a629b3187caccf36b5fd78ac9baed1d01705926",
            "/paper/Lymph-node-shape-in-computed-tomography-imaging-as-Kutomi-Ohmura/f817431699169084f1a588d9a1d7964cdd949fb9",
            "/paper/Comparison-of-models-to-predict-nonsentinel-lymph-a-Coutant-Olivier/748af7c4309217eaedeb47a70721cbd79727aa80",
            "/paper/CT-pathologic-correlation-of-axillary-lymph-nodes-March-Wechsler/e765cd5a328f7a5170481f16facdf2a88322f1db"
        ]
    },
    {
        "id": "5185cdcc385a11e87f53466e8d96ca3d0bdca1b8",
        "title": "A survey, review, and future trends of skin lesion segmentation and classification",
        "abstract": "Semantic Scholar extracted view of \"A survey, review, and future trends of skin lesion segmentation and classification\" by M. Hasan et al.",
        "publication_year": "2022",
        "authors": [
            "M. Hasan",
            "Md. Asif Ahamad",
            "C. Yap",
            "Guang Yang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "546",
        "references": [
            "/paper/DermSynth3D%3A-Synthesis-of-in-the-wild-Annotated-Sinha-Kawahara/41fc37bdbb53df4b0f99fff4871d30b71f299c73",
            "/paper/Study-on-different-Skin-lesion-Segmentation-and-Gangwar/60f2c7cdc06e19644df21d52deb4f53081d32564",
            "/paper/COMPARISON-OF-SKIN-LESION-IMAGE-BETWEEN-ALGORITHMS-Salih-Al-Raheym/4cd612c4efe57d08de126856af1e7c4460322a2c",
            "/paper/Skin-lesion-boundary-segmentation-with-fully-deep-Goyal-Ng/b6dacb10fb81a39d563695c28dcc35959b587e86",
            "/paper/Towards-Improved-Skin-Lesion-Classification-using-Pundhir-Dadhich/8fc1e3c6ebe14c3b1f162718ab8c2b5cbc91e1cd",
            "/paper/Automatic-Lesion-Detection-System-(ALDS)-for-Skin-Farooq-Azhar/1cf15f62f743eb69dddfc76271da69b2518cdf1a",
            "/paper/Skin-Lesion-Segmentation-with-Improved-Neural-%C3%96zt%C3%BCrk-%C3%96zkaya/2deb8d7db30d818e2538beb124d22c0d84e94c30",
            "/paper/The-Effects-of-Skin-Lesion-Segmentation-on-the-of-Mahbod-Tschandl/17919120c9f4020d32e6a6b73cad1333c1506fad",
            "/paper/Accurate-Segmentation-and-Registration-of-Skin-to-Navarro-Escudero-Vi%C3%B1olo/26c1b3a66fb8e81101c54c59f6ef931e234a972c",
            "/paper/An-improved-strategy-for-skin-lesion-detection-and-Nasir-Khan/cc1cdb6c24dc4824e719aca7991cef2ae2a01510",
            "/paper/Automated-skin-lesion-segmentation-of-dermoscopic-Jaisakthi-Mirunalini/39d5cb431355823f1b457c1491811b75dd2df78f"
        ]
    },
    {
        "id": "a6ee87dc926e1ada80673d702c7de278e53682c9",
        "title": "A Novel Lightweight Deep Convolutional Neural Network for early detection of oral cancer.",
        "abstract": "Deep CNNs can be an effective method to build low-budget embedded vision devices with limited computation power and memory capacity for diagnosis of oral cancer and Artificial intelligence (AI) can improve the quality and reach of Oral cancer screening and early detection. OBJECTIVES\nTo develop a lightweight deep convolutional neural network (CNN) for binary classification of oral lesions into benign and malignant or potentially malignant using standard real-time clinical images.\n\n\nMETHODS\nA small deep CNN, that uses a pre-trained EfficientNet-B0 as a lightweight transfer learning model, was proposed. A dataset of 716 clinical images was used to train and test the proposed model. Accuracy, specificity, sensitivity, receiver operating characteristics (ROC), and area under curve (AUC) were used to evaluate performance. Bootstrapping with 120 repetitions was used to calculate arithmetic means and 95% confidence intervals (CIs).\n\n\nRESULTS\nThe proposed CNN model achieved an accuracy of 85.0% (95% CI: 81.0% - 90.0%), a specificity of 84.5% (95% CI: 78.9% - 91.5%), a sensitivity of 86.7% (95% CI: 80.4% - 93.3%), and an AUC of 0.928 (95% CI: 0.88-0.96).\n\n\nCONCLUSIONS\nDeep CNNs can be an effective method to build low-budget embedded vision devices with limited computation power and memory capacity for diagnosis of oral cancer. Artificial intelligence (AI) can improve the quality and reach of oral cancer screening and early detection.",
        "publication_year": "2021",
        "authors": [
            "Fahed Jubair",
            "Omar Al-karadsheh",
            "D. Malamos",
            "Samara Al Mahdi",
            "Yusser Saad",
            "Y. Hassona"
        ],
        "related_topics": [
            "Medicine",
            "Computer Science"
        ],
        "citation_count": "26",
        "reference_count": "50",
        "references": [
            "/paper/Deep-convolutional-neural-network-algorithm-for-the-%C3%9Cnsal-Chaurasia/8863394d788287653f9b206dc5b00f277f235f5d",
            "/paper/An-Optimized-Predictive-Model-Based-on-Deep-Neural-Sharma-Chadha/7da6a61e08a85b4c7b249352975806cd762fbc2e",
            "/paper/Automated-Oral-Cancer-Detection-using-Deep-Muqeet-Mohammad/bfa71958d8b449e86cd14768d6396fe6d8056b79",
            "/paper/Machine-learning-concepts-applied-to-oral-pathology-Ara%C3%BAjo-Silva/18e6f09e8433844f7ddea2511e583882c37e92b3",
            "/paper/Deep-transfer-learning-techniques-with-hybrid-in-of-Bansal-Bathla/1fafff85108fc64f9fd3956ae2606992d81169de",
            "/paper/Utilizing-Deep-Machine-Learning-for-Prognostication-Alabi-Bello/3201c9dea99875eb656337e763392acad6bc34e6",
            "/paper/Deep-Machine-Learning-for-Oral-Cancer%3A-From-Precise-Alabi-Almangush/9ff15f25f4c323822e1a5c961418cdf741ae4b36",
            "/paper/Diagnosis-of-Oral-Squamous-Cell-Carcinoma-Using-and-Deif-Attar/18496aa760fac675cb75e5197a85f5d270bde472",
            "/paper/Efficacy-of-Artificial-Intelligence-Assisted-of-on-Kim-Kim/4d645cab3ab7db16a129b83d947e8fee7b3c5c6c",
            "/paper/Application-and-Performance-of-Artificial-(AI)-in-A-Khanagar-Alkadi/4e1a7210dc8dedbed521c96691e5c71483904095",
            "/paper/Active-deep-learning%3A-Improved-training-efficiency-Folmsbee-Liu/4645e7882f3196213eb86187cbf88504521a051c",
            "/paper/Computer-assisted-medical-image-classification-for-Jeyaraj-Nadar/11ded8e630c3d52bf3563c30b72cc06f6e66af7f",
            "/paper/Automated-Detection-and-Classification-of-Oral-Deep-Welikala-Remagnino/230b8255925d56b72444df3f358f5851b3158b59",
            "/paper/Automated-Lung-Cancer-Detection-Using-Artificial-A-Sathyakumar-Munoz/882df9c6bf9895d9149cee58e7a6d2ca2b61cd65",
            "/paper/Automatic-classification-of-dual-modalilty%2C-oral-Song-Sunny/eeebfa2343ef9180d647d7bbec008e249d250d88",
            "/paper/Robustness-of-convolutional-neural-networks-in-of-Maron-Haggenm%C3%BCller/719d72c8d67aaac90ef9a68def6324ae1255c564",
            "/paper/Dermatologist-level-classification-of-skin-cancer-Esteva-Kuprel/e1ec11a1cb3d9745fb18d3bf74247f95a6663d08",
            "/paper/Cancer-Diagnosis-Using-Deep-Learning%3A-A-Review-Munir-Elahi/bfb706604835b03b19ee71120a88115f49a01d3f",
            "/paper/Brain-metastasis-detection-using-machine-learning%3A-Cho-Sunwoo/a5873a77f57908751246b087f0e4aa0c020a26fc",
            "/paper/Development-of-a-light%E2%80%90weight-deep-learning-model-Huang-Hsu/0468414d14a73129cf47803ff8b3ca2cbb1539a4"
        ]
    },
    {
        "id": "6c814dcca13812e254990aa26ad77a64cf3fe714",
        "title": "POLCOVID: a multicenter multiclass chest X-ray database (Poland, 2020\u20132021)",
        "abstract": "POLCOVID dataset is introduced, containing chest X-ray (CXR) images of patients with COVID-19 or other-type pneumonia, and healthy individuals gathered from 15 Polish hospitals, and the set of matched images and lung masks may serve for the development of lung segmentation solutions. The outbreak of the SARS-CoV-2 pandemic has put healthcare systems worldwide to their limits, resulting in increased waiting time for diagnosis and required medical assistance. With chest radiographs (CXR) being one of the most common COVID-19 diagnosis methods, many artificial intelligence tools for image-based COVID-19 detection have been developed, often trained on a small number of images from COVID-19-positive patients. Thus, the need for high-quality and well-annotated CXR image databases increased. This paper introduces POLCOVID dataset, containing chest X-ray (CXR) images of patients with COVID-19 or other-type pneumonia, and healthy individuals gathered from 15 Polish hospitals. The original radiographs are accompanied by the preprocessed images limited to the lung area and the corresponding lung masks obtained with the segmentation model. Moreover, the manually created lung masks are provided for a part of POLCOVID dataset and the other four publicly available CXR image collections. POLCOVID dataset can help in pneumonia or COVID-19 diagnosis, while the set of matched images and lung masks may serve for the development of lung segmentation solutions.",
        "publication_year": "2022",
        "authors": [
            "A. Suwalska",
            "J. Tobiasz",
            "Wojciech Prazuch",
            "M. Socha",
            "P. Foszner",
            "J. Jaroszewicz",
            "K. Gruszczy\u0144ska",
            "M. Sliwinska",
            "J. Walecki",
            "T. Popiela",
            "Grzegorz Przybylski",
            "Mateusz Nowak",
            "P. Fiedor",
            "M. Paw\u0142owska",
            "R. Flisiak",
            "K. Simon",
            "Gabriela Zapolska",
            "Barbara Gizycka",
            "E. Szurowska",
            "M. Marczyk",
            "A. Cieszanowski",
            "Joanna Pola\u0144ska"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "26",
        "references": [
            "/paper/Pathological-changes-or-technical-artefacts-The-of-Socha-Prazuch/ea33320a3299bb0811d0072df691e4d0a731dba6",
            "/paper/CIRCA%3A-comprehensible-online-system-in-support-of-Prazuch-Suwalska/a53e428eec311e7b1880f2d009792b7798c975d2",
            "/paper/Deep-COVID%3A-Predicting-COVID-19-from-chest-X-ray-Minaee-Kafieh/703fb8cc953fd8a9e221b605c75502d62e841abd",
            "/paper/AIforCOVID%3A-Predicting-the-clinical-outcomes-in-AI-Soda-D'Amico/1f3c0029b2d66a8686de07e96795340afa777787",
            "/paper/ChestX-Ray8%3A-Hospital-Scale-Chest-X-Ray-Database-on-Wang-Peng/05e882679d61f4c64a68ebe21826251a39f87e98",
            "/paper/Frequency-and-Distribution-of-Chest-Radiographic-in-Wong-Lam/b8ff34b2f34182d346b4b2656febb4ab34891afa",
            "/paper/COVID-Net%3A-a-tailored-deep-convolutional-neural-for-Wang-Lin/235b9812305cff3df849394ac7b70a2c04a4685c",
            "/paper/Two-public-chest-X-ray-datasets-for-computer-aided-Jaeger-Candemir/0fd259546e31fa7ff4df38d21d18c149177faeb8",
            "/paper/Deep-learning-approaches-for-COVID-19-detection-on-Ismael-%C5%9Eeng%C3%BCr/03f0a38ae87eb40f5f3ad9bc06f3212c78eebe25",
            "/paper/Public-Covid-19-X-ray-datasets-and-their-impact-on-Cruz-Bossa/aeedccf2a65a217cd3bb5a055a6b21e5f28d6a2a",
            "/paper/Can-AI-Help-in-Screening-Viral-and-COVID-19-Chowdhury-Rahman/29bf7bd9c64e18a7b57b5da80e03fab1d9f1de2e"
        ]
    },
    {
        "id": "4871d441e775b3b0e6c8b6a594e9b6d54774f08c",
        "title": "Land Cover Semantic Segmentation Using ResUNet",
        "abstract": "This work presents an automated system for land cover classification that takes a multiband satellite image of an area as input and outputs the land cover map of the area at the same resolution as the input. In this paper we present our work on developing an automated system for land cover classification. This system takes a multiband satellite image of an area as input and outputs the land cover map of the area at the same resolution as the input. For this purpose convolutional machine learning models were trained in the task of predicting the land cover semantic segmentation of satellite images. This is a case of supervised learning. The land cover label data were taken from the CORINE Land Cover inventory and the satellite images were taken from the Copernicus hub. As for the model, U-Net architecture variations were applied. Our area of interest are the Ionian islands (Greece). We created a dataset from scratch covering this particular area. In addition, transfer learning from the BigEarthNet dataset [1] was performed. In [1] simple classification of satellite images into the classes of CLC is performed but not segmentation as we do. However, their models have been trained into a dataset much bigger than ours, so we applied transfer learning using their pretrained models as the first part of out network, utilizing the ability these networks have developed to extract useful features from the satellite images (we transferred a pretrained ResNet50 into a U-Res-Net). Apart from transfer learning other techniques were applied in order to overcome the limitations set by the small size of our area of interest. We used data augmentation (cutting images into overlapping patches, applying random transformations such as rotations and flips) and cross validation. The results are tested on the 3 CLC class hierarchy levels and a comparative study is made on the results of different approaches.",
        "publication_year": "2020",
        "authors": [
            "Vasilis Pollatos",
            "Loukas Kouvaras",
            "E. Charou"
        ],
        "related_topics": [
            "Computer Science",
            "Environmental Science"
        ],
        "citation_count": 0,
        "reference_count": "23",
        "references": [
            "/paper/Detection-of-Detached-Ice-fragments-at-Martian-a-Su-Fanara/fcb21ecd2a79ede56a9afe6bba6fe76ac94fbca8",
            "/paper/Segmentation-of-Satellite-Imagery-using-U-Net-for-Ulmas-Liiv/867ddab0cf818baaa8c88d6bd983b2569755ce46",
            "/paper/Weakly-Supervised-Semantic-Segmentation-of-Images-Nivaggioli-Randrianarivo/64cfc2822ebeeb16664eb35b2fa7dca9f05514cf",
            "/paper/Deep-Learning-for-Multilabel-Land-Cover-Scene-Using-Stivaktakis-Tsagkatakis/dd5a2767e55acd323cd10655f0c791bc5b8e5488",
            "/paper/ResUNet-a%3A-a-deep-learning-framework-for-semantic-Diakogiannis-Waldner/9c6eca31f311eae935e84efa6966c0165bc4e14a",
            "/paper/Updating-Land-Cover-Maps-by-Classification-of-Image-Demir-Bovolo/482ca824b96d18e061f4187a6ef9d1729a323a4c",
            "/paper/Using-long-short-term-memory-recurrent-neural-in-on-Sun-Di/980c1b7c06498645ce945b91ba1e5850e7f5d752",
            "/paper/Deep-Learning-Classification-of-Land-Cover-and-Crop-Kussul-Lavreniuk/7a9e471e31ac156cf22a5e2a5c1463697df866ab",
            "/paper/Weakly-Supervised-Deep-Learning-for-Segmentation-of-Wang-Chen/6eb53e1763f2eab95093eb1e6f67d34d2618fa9b",
            "/paper/Deep-Learning-for-Agricultural-Land-Detection-in-Charou-Felekis/8dca3ee9c13e28b2ecb02e4491f6c4bd8542636f",
            "/paper/Bigearthnet%3A-A-Large-Scale-Benchmark-Archive-for-Sumbul-Charfuelan/0c3f83b98107c5d84fb00967f5a991dcceb3f04d"
        ]
    },
    {
        "id": "e5405dbb2bbdfa53cf863d3ba558e04a0ac5a62b",
        "title": "Flexoelectric Effect on Vibration of Piezoelectric Microbeams Based on a Modified Couple Stress Theory",
        "abstract": "A novel electric Gibbs function was proposed for the piezoelectric microbeams (PMBs) by employing a modified couple stress theory. Based on the new Gibbs function and the Euler-Bernoulli beam theory, the governing equations which incorporate the effects of couple stress, flexoelectricity, and piezoelectricity were derived for the mechanics of PMBs. The analysis of the effective bending rigidity shows the effects of size and flexoelectricity can greaten the stiffness of PMBs so that the natural frequency increases significantly compared with the Euler-Bernoulli beam, and then the mechanical and electrical properties of PMBs are enhanced compared to the classical beam. This study can guide the design of microscale piezoelectric/flexoelectric structures which may find potential applications in the microelectromechanical systems (MEMS).",
        "publication_year": "2017",
        "authors": [
            "Xingjia Li",
            "Ying Luo"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": "20",
        "reference_count": "29",
        "references": [
            "/paper/Analyses-of-natural-frequency-and-electromechanical-Zeng-Wang/d593243302cce8414cbd9d6db53c57bdc5fabb5c",
            "/paper/Free-vibration-analysis-of-rotating-piezoelectric-Hosseini-Beni/3f9f38f6929504d4430f88288d8eea8cd23c7bc6",
            "/paper/Vibration-Characteristics-of-Piezoelectric-Nanobeam-Wu-Zhang/01d9156c9f4d21005e6299611e16fc1d595dcb0f",
            "/paper/Nonlinear-electromechanical-analysis-of-micro-based-Yademellat-Ansari/03457abbe726d65df4ce3807baa11ff2bf905450",
            "/paper/Flexoelectric-effect-on-vibration-responses-of-in-Zhang-Lei/8070fa876e41b8efb3f37554ddf0a329eec5fb4d",
            "/paper/Flexoelectric-effect-on-vibration-responses-of-in-Zhang-Lei/95429e89441f022cb032c55286e977e71dee800e",
            "/paper/Modelling-of-the-flexoelectric-effect-on-rotating-Tho-Thanh/62d8928e67488dc8f5a041da1023e0b83d0e87ab",
            "/paper/Vibration-and-thermal-buckling-analyses-of-based-on-Abbaspour-Arvin/c6c9ed940595615471f795a69c5c311692daa855",
            "/paper/Vibration-Analysis-of-a-Unimorph-Nanobeam-with-a-of-Naderi-Quoc-Thai/7282ad2bb686b65c62322367b5d05059e0afbebc",
            "/paper/Nonlinear-vibration-of-piezoelectric-sandwich-with-Zeng-Wang/bceb77a11537767cf26fbd8dc4571a61a8088989",
            "/paper/Vibration-Characteristics-of-Piezoelectric-Based-on-Ansari-Ashrafi/b5a5b1d5c7c762cbb7f5b7b1f01a62b0e850dfda",
            "/paper/Size-dependent-buckling-and-vibration-behaviors-of-Liang-Hu/9d29066ec3845da74f1009f2c644affd488533db",
            "/paper/A-theory-of-flexoelectricity-with-surface-effect-Shen-Hu/95494bc33cecf7b96c5fe430f4dc88f602b1023d",
            "/paper/Variational-principles-and-governing-equations-in-Hu-Shen/9d6b6cad54acb2f9757dd1d0e796cc8d73aea187",
            "/paper/On-the-possibility-of-piezoelectric-nanocomposites-Sharma-Maranganti/05023333f397b4e25d39aa1e1d20e158a40ffbf5",
            "/paper/Flexoelectric-effect-in-ceramic-lead-zirconate-Ma-Cross/932888f000963343d34115316aaf70132e07eaeb",
            "/paper/A-sensor-for-the-direct-measurement-of-curvature-on-Yan-Huang/83135ded882a59641423bdfa085732770739e54c",
            "/paper/Flexoelectric-effects%3A-Charge-separation-in-solids-Cross/91bda599121b3f019b0fbd57cd2a713e22f360d2",
            "/paper/Bernoulli%E2%80%93Euler-beam-model-based-on-a-modified-Park-Gao/c5371387fdd39123d660a93c2df10e2cd2934cc9",
            "/paper/Flexoelectricity-in-barium-strontium-titanate-thin-Kwon-Huang/91181f1cc1108e5e568bb6982599e83faeb24c36"
        ]
    },
    {
        "id": "e1e5936749e383c096e219b9bf85cf756585882b",
        "title": "Efficient Processing of Top-k Dominating Queries on Multi-Dimensional Data",
        "abstract": "The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages: (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. In this paper, we design specialized algorithms that apply on indexed multi-dimensional data and fully exploit the characteristics of the problem. Experiments on synthetic datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach, while our results on real datasets show the meaningfulness of top-k dominating queries.",
        "publication_year": "2007",
        "authors": [
            "Man Lung Yiu",
            "N. Mamoulis"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "203",
        "reference_count": "28",
        "references": [
            "/paper/Multi-dimensional-top-k-dominating-queries-Yiu-Mamoulis/5118b57f47915d49e9a78e41496383fd9649525e",
            "/paper/Enhancing-Query-Efficiency-Using-Pruning-Techniques-Babu-Swapna/d2540dee0cd20eed5938e60c66af8eba543eeae2",
            "/paper/Parallel-Processing-of-Top-k-Dominating-Queries-on-Ding-Yan/93dc678ad52cdb1f21655bfaa3fbcc2baed6f69c",
            "/paper/Top-k-Dominating-Queries-%3A-a-Survey-Tiakas-Papadopoulos/ce661789ac6793e884e162f8476753236e27616c",
            "/paper/Dynamic-Processing-of-Dominating-Queries-with-Kosmatopoulos-Papadopoulos/0c9887b6cb4baafaa379c2af336746775e05b516",
            "/paper/Continuous-Top-k-Dominating-Queries-Kontaki-Papadopoulos/caf62f1c21a671e7226078e8287995012a92c14e",
            "/paper/Top-k-Dominating-Queries-on-Incomplete-Data-Miao-Gao/a747dbfe0f35784b119fd5e9e257b17b0232cc12",
            "/paper/Efficient-Processing-of-Reverse-Top-k-Dominating-Jiang-Zhang/1195108ab46bd6c00ea714ff1549a5e10d8cf9eb",
            "/paper/Processing-Top-k-Dominating-Queries-in-Metric-Tiakas-Valkanas/d3a11264d6c5ac0125a4cbc730ab81186671557f",
            "/paper/Continuous-Top-k-Dominating-Queries-in-Subspaces-Kontaki-Papadopoulos/0693c6b9323a59d1890dec9fad454ef66994f79d",
            "/paper/SUBSKY%3A-Efficient-Computation-of-Skylines-in-Tao-Xiao/85d542ce705eb3fcb923ca93d781d93c4a6ee061",
            "/paper/On-High-Dimensional-Skylines-Chan-Jagadish/242552a6eb324f2a427acde5ba79ae2d4d5c0772",
            "/paper/Maximal-Vector-Computation-in-Large-Data-Sets-Godfrey-Shipley/a7414f68a7c63109402ab3f4ed495dde2df596d0",
            "/paper/Efficient-Computation-of-the-Skyline-Cube-Yuan-Lin/c255e88ccaa9b1dbce891834802a5952ce01ca3c",
            "/paper/PREFER%3A-a-system-for-the-efficient-execution-of-Hristidis-Koudas/b47283c7d69d0116e656beb6ddcc64e97dbaef57",
            "/paper/A-model-for-the-prediction-of-R-tree-performance-Theodoridis-Sellis/f8065fe8f36436740253389a383c05a05629ac8f",
            "/paper/Progressive-skyline-computation-in-database-systems-Papadias-Tao/28e702e1a352854cf0748b9a6a9ad6679b1d4e83",
            "/paper/Finding-k-dominant-skylines-in-high-dimensional-Chan-Jagadish/dcb4c7d69cbf07968dd89ac5e248144c9b2e3411",
            "/paper/Supporting-ad-hoc-ranking-aggregates-Li-Chang/f3935338f4385ced5d5ce31e2b1da4cece9cd44d",
            "/paper/Efficient-Distributed-Skylining-for-Web-Information-Balke-G%C3%BCntzer/4e10d2ac250eae426b8dd8db10ac9dfa59dddf02"
        ]
    },
    {
        "id": "ebdd884a8e36fe830920868a0b993690e827bc95",
        "title": "Self-supervised coarse-to-fine monocular depth estimation using a lightweight attention module",
        "abstract": "A coarse-to-fine method with a normalized convolutional block attention module (NCBAM) to overcome the texture-copy and depth drift problems and can produce results competitive with state-of-the-art methods. Self-supervised monocular depth estimation has been widely investigated and applied in previous works. However, existing methods suffer from texture-copy, depth drift, and incomplete structure. It is difficult for normal CNN networks to completely understand the relationship between the object and its surrounding environment. Moreover, it is hard to design the depth smoothness loss to balance depth smoothness and sharpness. To address these issues, we propose a coarse-to-fine method with a normalized convolutional block attention module (NCBAM). In the coarse estimation stage, we incorporate the NCBAM into depth and pose networks to overcome the texture-copy and depth drift problems. Then, we use a new network to refine the coarse depth guided by the color image and produce a structure-preserving depth result in the refinement stage. Our method can produce results competitive with state-of-the-art methods. Comprehensive experiments prove the effectiveness of our two-stage method using the NCBAM.",
        "publication_year": "2022",
        "authors": [
            "Yuanzhen Li",
            "Fei Luo",
            "Chunxia Xiao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "64",
        "references": [
            "/paper/MonoNeuralFusion%3A-Online-Monocular-Neural-3D-with-Zou-Huang/e605271aefa33cdf4c77e0ea801cd8e2410fceb0",
            "/paper/Neural-3D-reconstruction-from-sparse-views-using-Mu-Chen/606cf82256303029acb348fff680d81504b672ff",
            "/paper/Self-Supervised-Monocular-Trained-Depth-Estimation-Johnston-Carneiro/d177361ddf2765d509882b414a697e94d57dc59e",
            "/paper/Unsupervised-Learning-of-Geometry-From-Videos-With-Yang-Wang/a804db3860817f901fff18ba3f334d9e56dcb8f4",
            "/paper/Deeper-Depth-Prediction-with-Fully-Convolutional-Laina-Rupprecht/2f7ae3361b2aafa24fc289252a9cad8f1135edae",
            "/paper/Digging-Into-Self-Supervised-Monocular-Depth-Godard-Aodha/589cfcb2f995c94b0a98c902cc1f5e0f27cbd927",
            "/paper/Unsupervised-Monocular-Depth-Estimation-with-Godard-Aodha/4463dc4a32b948f0230f3b782cbfecaf1c9e5b1d",
            "/paper/3D-Packing-for-Self-Supervised-Monocular-Depth-Guizilini-Ambrus/44a462b3240a1987756cf6071427b29653446af1",
            "/paper/Learning-Monocular-Depth-Estimation-with-Trinocular-Poggi-Tosi/d8c04365ed0627a5043996cdd26c1a56b5a630b8",
            "/paper/Learning-Monocular-Depth-by-Distilling-Cross-domain-Guo-Li/4f19d33e808a6675f11fb624499d303368deafa1",
            "/paper/Unsupervised-Adversarial-Depth-Estimation-Using-Pilzer-Xu/b9ca3db76702111955ff583527fa40e0a157b796",
            "/paper/Depth-Map-Prediction-from-a-Single-Image-using-a-Eigen-Puhrsch/06feba1ffd596b41884cea6e8ef0da89b6dd2233"
        ]
    },
    {
        "id": "427ba0af0ccc9c6845b296029e3c4eeb6ee5f600",
        "title": "Mixed geometric loss for bounding box regression in object detection",
        "abstract": "A mixed geometric (MG) regression loss is designed to increase the similarity and the overlapping area of two bounding boxes to achieve competitive convergence speed and regression accuracy in object detection. Abstract. Predicting bounding box with higher intersection over union (IoU) is one of the most important issues in many computer vision tasks. The \u2113n-norm loss and IoU-based loss are two conventional approaches to guide a training process in recent methods. However, the optimization direction of \u2113n-norm loss is not exactly the same as maximizing the metric. In addition, IoU-based loss suffers from some inevitable disadvantages due to the direct addition of IoU. According to the shape, size, and position properties, we design a mixed geometric (MG) regression loss to increase the similarity and the overlapping area of two bounding boxes. The shape is described by the cosine similarity of diagonal vectors, the size is measured by the length or width of the boxes, and the location is calculated by the center positions of the boxes. Simulation experiments verify that the proposed MG loss can achieve competitive convergence speed and regression accuracy. By introducing the state-of-the-art models in object detection, experiments are carried out on a well-known benchmark dataset, and the results demonstrate the effectiveness of our method in object detection.",
        "publication_year": "2020",
        "authors": [
            "Xudie Ren",
            "Fucai Luo",
            "Shenghong Li"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "36",
        "references": [
            "/paper/MAM%3A-Multiple-Attention-Mechanism-Neural-Networks-Ren-Wang/340e5710e0cf9ff79f57b9d4d71035f903667ebe",
            "/paper/Distance-IoU-Loss%3A-Faster-and-Better-Learning-for-Zheng-Wang/63a243afcb133569a962c41e9db956c076c5c4f3",
            "/paper/Generalized-Intersection-Over-Union%3A-A-Metric-and-a-Rezatofighi-Tsoi/889c81b4d7b7ed43a3f69f880ea60b0572e02e27",
            "/paper/UnitBox%3A-An-Advanced-Object-Detection-Network-Yu-Jiang/22264e60f1dfbc7d0b52549d1de560993dd96e46",
            "/paper/Objects-as-Points-Zhou-Wang/6a2e2fd1b5bb11224daef98b3fb6d029f68a73f2",
            "/paper/CenterNet%3A-Keypoint-Triplets-for-Object-Detection-Duan-Bai/52e7190540745960333ab483623099edf1641257",
            "/paper/Focal-Loss-for-Dense-Object-Detection-Lin-Goyal/72564a69bf339ff1d16a639c86a764db2321caab",
            "/paper/DeNet%3A-Scalable-Real-Time-Object-Detection-with-Tychsen-Smith-Petersson/9ff87609cfa2bcfddc2ad01d91a1dee4b39653ce",
            "/paper/SSD%3A-Single-Shot-MultiBox-Detector-Liu-Anguelov/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
            "/paper/Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue/2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "/paper/You-Only-Look-Once%3A-Unified%2C-Real-Time-Object-Redmon-Divvala/f8e79ac0ea341056ef20f2616628b3e964764cfd"
        ]
    },
    {
        "id": "4e8148d59a57cff9e231961abcabc1e4fd66b7ff",
        "title": "YADA: you always dream again for better object detection",
        "abstract": "This paper proposes a novel framework addressing the problem of generating relevant data and how to use them effectively, and applies lucid data synthesizing which generates data by mining hard examples and embedding them to the same context locations. Object detection has been attracting a lot of attention from the computer vision community. It has a wide range of practical applications ranging from the traditional use such as image annotation to modern uses such as self-driving vehicles, robotics, surveillance systems, and augmented reality. Recently, deep learning has significantly improved the state-of-the-art performance of the object detection task. Many works explore various deep network structures to improve the performance. However, the impact of training data is still not well investigated. Although some works focus on data augmentation and data synthesis, there is no guarantee that they are effective for the training process. In this paper, we propose a novel framework addressing the problem of generating relevant data and how to use them effectively. We apply lucid data synthesizing which generates data by mining hard examples and embedding them to the same context locations. Further, we utilize a dual-level deep network leveraged with these generated data to effectively detect hard objects in images. Extensive experiments on two benchmarks, PASCAL VOC and KITTI, demonstrate the superiority of our approach over the state-of-the-art methods.",
        "publication_year": "2019",
        "authors": [
            "Khanh-Duy Nguyen",
            "Khang Nguyen",
            "Duy-Dinh Le",
            "D. Duong",
            "Tam V. Nguyen"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "5",
        "reference_count": "53",
        "references": [
            "/paper/Data-Augmentation-Analysis-in-Vehicle-Detection-Chung-Le/2a662c841bfd02864de876db19cfa523d493041c",
            "/paper/Guided-Instance-Segmentation-Framework-for-Video-Tran-Le/b15b0b851bd770cad713b5f36ef0fc1231d3ba25",
            "/paper/Contextual-Guided-Segmentation-Framework-for-Video-Le-Nguyen/736aa914c092b7dadce7ec478cd99c5a2d93570b",
            "/paper/Multiple-Sclerosis-Detection-via-6-layer-Stochastic-Wang-Lima/a3b6ce44bd025487843d0cac335bb7e2077b7f9f",
            "/paper/Improved-VIDAR-and-machine-learning-based-road-Wang-Zhu/d2d241bb9388e8216d812e3dc0d9ea2d9c5768a1",
            "/paper/You-always-look-again%3A-Learning-to-detect-the-Nguyen-Nguyen/263cd05600a8a5f934f140b35a90d8d5ad342da7",
            "/paper/Advanced-Deep-Learning-Techniques-for-Salient-and-A-Han-Zhang/943c372336ced4b28e15e02fe8db1f4b23bf6835",
            "/paper/Driving-in-the-Matrix%3A-Can-virtual-worlds-replace-Johnson-Roberson-Barto/6cd7a47bbba11a994cd8e68ee5eae2fcb0033054",
            "/paper/Training-Region-Based-Object-Detectors-with-Online-Shrivastava-Gupta/63333669bcf694aba2e1928f6060ab1d6a5161fe",
            "/paper/You-Only-Look-Once%3A-Unified%2C-Real-Time-Object-Redmon-Divvala/f8e79ac0ea341056ef20f2616628b3e964764cfd",
            "/paper/Cut%2C-Paste-and-Learn%3A-Surprisingly-Easy-Synthesis-Dwibedi-Misra/12a91c9d4a55fc93f15f4acef078c8908af3c9b9",
            "/paper/Training-Deep-Networks-with-Synthetic-Data%3A-the-Gap-Tremblay-Prakash/9b6e4cbf1f8d6fbf09017769ae65ff90234e0aa0",
            "/paper/Learning-Deep-Object-Detectors-from-3D-Models-Peng-Sun/dfff09a657b77989ef83b6d61cac05059426ad0a",
            "/paper/Focal-Loss-for-Dense-Object-Detection-Lin-Goyal/72564a69bf339ff1d16a639c86a764db2321caab",
            "/paper/Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue/2f4df08d9072fc2ac181b7fced6a245315ce05c8"
        ]
    },
    {
        "id": "adfd953eb2eb52a37944ec95352e8284201138bd",
        "title": "Planar region alignment based visual regulation for mobile robot",
        "abstract": "It has been proved that, based on planar region alignment, camera ego-motion motion can be robustly estimated with a multistage approach in [24]. Based on this observation, a visual regulation approach based on planar region alignment is proposed. This paper is composed of two parts. In the first part, the image motion model of planar point is reviewed first, which will be used for camera motion estimation. Then a simple least square method for computing homography matrix H is introduced. The basic procedure of multistage motion estimation based on planar region alignment is summarized. In the second part, the kinematic model of monocular camera based mobile robot system is deduced with a constrained camera-robot configuration, the visual regulation control law is derived and the stability of the close loop system is analyzed in the sense of Lyapunov stability theory. The motion estimation experiment results are presented with real images, and simulation results show the convergence of the proposed visual regulation.",
        "publication_year": "2012",
        "authors": [
            "Z. Wang",
            "Y. Liu",
            "B. Cai",
            "J. Zhao"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": 0,
        "reference_count": "24",
        "references": [
            "/paper/Planar-region-alignment-based-ego-motion-estimation-Wang-Cai/2941e4d1980c99c11bd9bba99289946e259e1e2b",
            "/paper/Planar-flow-based-planar-region-extraction-for-a-Wang-Yi/e6c033666c1e803921405b92af522ed87126d5a1",
            "/paper/Homography-based-visual-servo-regulation-of-mobile-Fang-Dixon/ded02c9a7b56ba1e37d417f52753dd9fdb8f6d8c",
            "/paper/Homography-based-visual-servo-tracking-control-of-a-Chen-Dixon/72492468c63cd6c380564b410d332c2519b82f8b",
            "/paper/Image-Based-Visual-Servoing-for-Nonholonomic-Mobile-Mariottini-Oriolo/35efe7652cb421bf074ef6a09573af84edc80195",
            "/paper/Survey-of-vision-based-robot-control-Malis/be74fc01360c30eac3037914d954061cd286840f",
            "/paper/Recovery-of-Ego-Motion-Using-Region-Alignment-Irani-Rousso/7c3e5196511dc9c8b7f7a0be4e9eeb48f604f1ae",
            "/paper/Determining-Three-Dimensional-Motion-and-Structure-Adiv/3733124d9fedd380e7e0c6b82eef9f2db3344fc3",
            "/paper/Robust-structure-from-motion-using-motion-parallax-Cipolla-Okamoto/15c4cca0d98f2ec7537d15d9f722041995ad7db7",
            "/paper/Direct-methods-for-recovering-motion-Horn-Weldon/e88791964c01c293f9b647b449b07b8d97b29bd9"
        ]
    },
    {
        "id": "551633338103610cb8a60e6df004ef6fd3841d17",
        "title": "Reinforcement Learning with Automated Auxiliary Loss Search",
        "abstract": "A principled and universal method for learning better representations with auxiliary loss functions, named Automated Auxiliary Loss Search (A2LS), which automatically searches for top-performing Auxiliary loss functions for RL with promising generalization ability to different settings and even different benchmark domains. A good state representation is crucial to solving complicated reinforcement learning (RL) challenges. Many recent works focus on designing auxiliary losses for learning informative representations. Unfortunately, these handcrafted objectives rely heavily on expert knowledge and may be sub-optimal. In this paper, we propose a principled and universal method for learning better representations with auxiliary loss functions, named Automated Auxiliary Loss Search (A2LS), which automatically searches for top-performing auxiliary loss functions for RL. Specifically, based on the collected trajectory data, we define a general auxiliary loss space of size $7.5 \\times 10^{20}$ and explore the space with an efficient evolutionary search strategy. Empirical results show that the discovered auxiliary loss (namely, A2-winner) significantly improves the performance on both high-dimensional (image) and low-dimensional (vector) unseen tasks with much higher efficiency, showing promising generalization ability to different settings and even different benchmark domains. We conduct a statistical analysis to reveal the relations between patterns of auxiliary losses and RL performance.",
        "publication_year": "2022",
        "authors": [
            "Tairan He",
            "Yuge Zhang",
            "Kan Ren",
            "Minghuan Liu",
            "Che Wang",
            "Weinan Zhang",
            "Yuqing Yang",
            "Dongsheng Li"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "55",
        "references": [
            "/paper/A-Comprehensive-Survey-of-Data-Augmentation-in-Ma-Wang/336eea1e32e2b93c63f97931710d3de54d05a336",
            "/paper/State-wise-Safe-Reinforcement-Learning%3A-A-Survey-Zhao-He/a979a0b243b3b9c5d305bc6785df2fe8bb8f3c31",
            "/paper/Evolving-Pareto-Optimal-Actor-Critic-Algorithms-for-Garau-Luis-Miao/3b1520c1303f6a1969f587b1ff0b3e0339809560",
            "/paper/State-wise-Constrained-Policy-Optimization-Zhao-Chen/890f6af60c514d37e5fbaf3ff6ad5a24be68ccb0",
            "/paper/Improving-Sample-Efficiency-in-Model-Free-Learning-Yarats-Zhang/88dd6594c9ddd4c4bb7f9b407b162e283907f4f3",
            "/paper/Incentivizing-Exploration-In-Reinforcement-Learning-Stadie-Levine/2470fcf0f89082de874ac9133ccb3a8667dd89a8",
            "/paper/Data-Efficient-Reinforcement-Learning-with-Schwarzer-Anand/7c4356ec0dca6e6df0af7a882e2cd1571c8bf3dc",
            "/paper/Sample-Efficient-Automated-Deep-Reinforcement-Franke-Koehler/283f975e221f56974f30a3b12c7fb015c0c77377",
            "/paper/Reinforcement-Learning-with-Unsupervised-Auxiliary-Jaderberg-Mnih/d7bd6e3addd8bc8e2e154048300eea15f030ed33",
            "/paper/Can-Increasing-Input-Dimensionality-Improve-Deep-Ota-Oiki/7647a8ca91463a627f381ac54077a87147df2f8d",
            "/paper/IMPALA%3A-Scalable-Distributed-Deep-RL-with-Weighted-Espeholt-Soyer/80196cdfcd0c6ce2953bf65a7f019971e2026386",
            "/paper/Integrating-State-Representation-Learning-Into-Deep-Bruin-Kober/84bb62e3f40434a1e367d24783bd81432a5396d6",
            "/paper/Decoupling-Representation-Learning-from-Learning-Stooke-Lee/17985b57240bfaea02a6098a7a34e71e780180eb",
            "/paper/Improving-Performance-in-Reinforcement-Learning-by-Ghiassian-Rafiee/7f98ef854578a5a8f25a5faad6d5d971a838d1d2"
        ]
    },
    {
        "id": "58817ced8646e06d36175b2d34d7139aadc54317",
        "title": "Exploiting Local Geometry for Feature and Graph Construction for Better 3D Point Cloud Processing with Graph Neural Networks",
        "abstract": "It is demonstrated with multiple challenging benchmarks, with relatively clean CAD models, as well as with real world noisy scans, that the proposed method achieves state of the art results on benchmarks for 3D classification, part segmentation and semantic segmentation. We propose simple yet effective improvements in point representations and local neighborhood graph construction within the general framework of graph neural networks (GNNs) for 3D point cloud processing. As a first contribution, we propose to augment the vertex representations with important local geometric information of the points, followed by nonlinear projection using a MLP. As a second contribution, we propose to improve the graph construction for GNNs for 3D point clouds. The existing methods work with a k-NN based approach for constructing the local neighborhood graph. We argue that it might lead to reduction in coverage in case of dense sampling by sensors in some regions of the scene. The proposed methods aims to counter such problems and improve coverage in such cases. As the traditional GNNs were designed to work with general graphs, where vertices may have no geometric interpretations, we see both our proposals as augmenting the general graphs to incorporate the geometric nature of 3D point clouds. While being simple, we demonstrate with multiple challenging benchmarks, with relatively clean CAD models, as well as with real world noisy scans, that the proposed method achieves state of the art results on benchmarks for 3D classification (ModelNet40) , part segmentation (ShapeNet) and semantic segmentation (Stanford 3D Indoor Scenes Dataset). We also show that the proposed network achieves faster training convergence, i.e. \u223c 40% less epochs for classification. The project details are available at https://siddharthsrivastava.github.io/publication/geomgcnn/",
        "publication_year": "2021",
        "authors": [
            "Siddharth Srivastava",
            "Gaurav Sharma"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3",
        "reference_count": "47",
        "references": [
            "/paper/Towards-Efficient-Point-Cloud-Graph-Neural-Networks-Tailor-Jong/e25dea5f5c2275d96021e59dc25d9a19de875603",
            "/paper/Adaptive-neighbourhood-recovery-method-for-machine-Xue-Men/119f5916575dd7e45773eea3280b5ac956a70027",
            "/paper/Beyond-Image-to-Depth%3A-Improving-Depth-Prediction-Parida-Srivastava/04d70c5e2f16b7825c1247938d339e325725c270",
            "/paper/Spherical-Kernel-for-Efficient-Graph-Convolution-on-Lei-Akhtar/6265ed23e558a80c9af599477b847c4f7e0027fc",
            "/paper/Geometry-Sharing-Network-for-3D-Point-Cloud-and-Xu-Zhou/d022b850a55c5102b1c44e24949c6bf801782a25",
            "/paper/Dynamic-Graph-CNN-for-Learning-on-Point-Clouds-Wang-Sun/e1799aaf23c12af6932dc0ef3dfb1638f01413d1",
            "/paper/Hierarchical-Depthwise-Graph-Convolutional-Neural-Liang-Yang/f15243b299c223bdf4211236bb71e6807bf342fa",
            "/paper/Learning-to-Segment-3D-Point-Clouds-in-2D-Image-Lyu-Huang/99d0c46c0cf846c5849a4280c05489f817e735dc",
            "/paper/Relation-Shape-Convolutional-Neural-Network-for-Liu-Fan/a58715797b61588cbd020b9a98292a98f8483420",
            "/paper/PointNet%3A-Deep-Learning-on-Point-Sets-for-3D-and-Qi-Su/d997beefc0922d97202789d2ac307c55c2c52fba",
            "/paper/SegGCN%3A-Efficient-3D-Point-Cloud-Segmentation-With-Lei-Akhtar/13079de18092149aa9fb9ee6c64726d7c2df0348",
            "/paper/Linked-Dynamic-Graph-CNN%3A-Learning-on-Point-Cloud-Zhang-Hao/db63dc894d36167776e8c1e4d2def9f6516fb92a",
            "/paper/Deep-FusionNet-for-Point-Cloud-Semantic-Zhang-Fang/b658628f0118c745da15c09ee01d8ede4d5fc5b1"
        ]
    },
    {
        "id": "c1f1a999d84dce8e84c28b7d33edd6aa706b85c2",
        "title": "Deep Attentive Video Summarization With Distribution Consistency Learning",
        "abstract": "This article incorporates a self-attention mechanism in the encoder to highlight the important keyframes in a short-term context and proposes a distribution consistency learning method by employing a simple yet effective regularization loss term, which seeks a consistent distribution for the two sequences. This article studies supervised video summarization by formulating it into a sequence-to-sequence learning framework, in which the input and output are sequences of original video frames and their predicted importance scores, respectively. Two critical issues are addressed in this article: short-term contextual attention insufficiency and distribution inconsistency. The former lies in the insufficiency of capturing the short-term contextual attention information within the video sequence itself since the existing approaches focus a lot on the long-term encoder\u2013decoder attention. The latter refers to the distributions of predicted importance score sequence and the ground-truth sequence is inconsistent, which may lead to a suboptimal solution. To better mitigate the first issue, we incorporate a self-attention mechanism in the encoder to highlight the important keyframes in a short-term context. The proposed approach alongside the encoder\u2013decoder attention constitutes our deep attentive models for video summarization. For the second one, we propose a distribution consistency learning method by employing a simple yet effective regularization loss term, which seeks a consistent distribution for the two sequences. Our final approach is dubbed as Attentive and Distribution consistent video Summarization (ADSum). Extensive experiments on benchmark data sets demonstrate the superiority of the proposed ADSum approach against state-of-the-art approaches.",
        "publication_year": "2020",
        "authors": [
            "Zhong Ji",
            "Yuxiao Zhao",
            "Yanwei Pang",
            "Xi Li",
            "Jungong Han"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "39",
        "reference_count": "41",
        "references": [
            "/paper/Video-Summarization-With-Spatiotemporal-Vision-Hsu-Liao/cf49f64cbdc1ba3959843f8fc5889762a384ab99",
            "/paper/Spatiotemporal-two-stream-LSTM-network-for-video-Hu-Hu/f27c8877f5b493af0be24fe131dd83b1f5667344",
            "/paper/Reconstructive-Sequence-Graph-Network-for-Video-Zhao-Li/ba039aa0ce03682901b8e24f7c3eb36786635dc6",
            "/paper/Hierarchical-Multimodal-Transformer-to-Summarize-Zhao-Gong/52f8f58267fd2ab50cf6832be51d539d6606dd65",
            "/paper/Summarization-of-User-Generated-Videos-Fusing-and-Psallidas-Spyrou/c8800ace9537266a7763d0488dbbaec342999ae0",
            "/paper/DSNet%3A-A-Flexible-Detect-to-Summarize-Network-for-Zhu-Lu/49a547e6255bc323f65f0934f378ef0930863f9c",
            "/paper/Estimation-of-concise-video-summaries-from-long-via-Kaur-Mishra/ad21272d355b6e0bd74a88a86269bf0462d480c5",
            "/paper/Video-summarization-with-local-and-global-attention-Wang/7d4969f41112360836aab8e1fdd114930e9dadb7",
            "/paper/AudioVisual-Video-Summarization-Zhao-Gong/9cc01e0c22ab7ba54c6e52e44717a29078fc2f6b",
            "/paper/A-Hierarchical-Spatial%E2%80%93Temporal-Cross-Attention-for-Teng-Gui/5aab75fc39c1720b14f13d3e688281d7e6d1181a",
            "/paper/Video-Summarization-With-Attention-Based-Networks-Ji-Xiong/88a8baa1be5292e62622f1cb8e627fbf759bf741",
            "/paper/Retrospective-Encoders-for-Video-Summarization-Zhang-Grauman/3a965d76c1a7414edcf7258a011b4237cfdf0ac7",
            "/paper/Discriminative-Feature-Learning-for-Unsupervised-Jung-Cho/69b3b29c6fabaea88d382922346d9157395a3226",
            "/paper/Video-Summarization-with-Long-Short-Term-Memory-Zhang-Chao/1dbc12e54ceb70f2022f956aa0a46e2706e99962",
            "/paper/Attentive-and-Adversarial-Learning-for-Video-Fu-Tai/b0e6921c84263c5c961cd286e3145ca5eda352e0",
            "/paper/Video-Summarization-via-Semantic-Attended-Networks-Wei-Ni/6ec09bad57cc81a71ef7596f57e94ee13b380ae3",
            "/paper/Deep-Reinforcement-Learning-for-Unsupervised-Video-Zhou-Qiao/9e9e5f0c36548cfe2855aae46b519b146aa8c9ae",
            "/paper/Hierarchical-Recurrent-Neural-Network-for-Video-Zhao-Li/454e65c2a9b019a00790a1d6029dc5539edad35d",
            "/paper/Unsupervised-Video-Summarization-with-Adversarial-Mahasseni-Lam/620fe6c786d15efca7f553ad70f295e2b693b391",
            "/paper/A-General-Framework-for-Edited-Video-and-Raw-Video-Li-Zhao/18a6a7edfbce1cc11fb7447a133c0ac150b2785c"
        ]
    },
    {
        "id": "827ee2cdc56ea65ef644bd8ca085f4274b106f03",
        "title": "Siamese Anchor Proposal Network for High-Speed Aerial Tracking",
        "abstract": "A novel two-stage Siamese network-based method for aerial tracking, i.e., stage-1 for high-quality anchor proposal generation, stage-2 for refining the anchor proposal, which can increase the robustness and generalization to different objects with various sizes and make calculation feasible due to the substantial decrease of anchor numbers. In the domain of visual tracking, most deep learning-based trackers highlight the accuracy but casting aside efficiency. Therefore, their real-world deployment on mobile platforms like the unmanned aerial vehicle (UAV) is impeded. In this work, a novel two-stage Siamese network-based method is proposed for aerial tracking, i.e., stage-1 for high-quality anchor proposal generation, stage-2 for refining the anchor proposal. Different from anchor-based methods with numerous pre-defined fixed-sized anchors, our no-prior method can 1) increase the robustness and generalization to different objects with various sizes, especially to small, occluded, and fast-moving objects, under complex scenarios in light of the adaptive anchor generation, 2) make calculation feasible due to the substantial decrease of anchor numbers. In addition, compared to anchor-free methods, our framework has better performance owing to refinement at stage-2. Comprehensive experiments on three benchmarks have proven the superior performance of our approach, with a speed of \u223c200 frames/s.",
        "publication_year": "2020",
        "authors": [
            "Changhong Fu",
            "Ziang Cao",
            "Yiming Li",
            "Junjie Ye",
            "Chen Feng"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "24",
        "reference_count": "34",
        "references": [
            "/paper/Siamese-Object-Tracking-for-Unmanned-Aerial-A-and-Fu-Lu/1171234cb2f3e1589592e3d04eb10c132fc6a5c8",
            "/paper/DarkLighter%3A-Light-Up-the-Darkness-for-UAV-Tracking-Ye-Fu/a0e544f6db3b6659a5f77c419908238f91b188bc",
            "/paper/Adversarial-Blur-Deblur-Network-for-Robust-UAV-Zuo-Fu/f06a22165f0bcc92eac865e3eeddec3c7e2cf44c",
            "/paper/Siamese-Multi-Scale-Aggregation-Network-for-UAV-Yao-Wu/40611f52c8de95cab129001a465c1d2161524d73",
            "/paper/PVT%2B%2B%3A-A-Simple-End-to-End-Latency-Aware-Visual-Li-Huang/8fa3221af7f6d3008e00cc0c459ecbde6f1014da",
            "/paper/Siamese-Adaptive-Transformer-Network-for-Real-Time-Xing-Tsoukalas/70ff1775dae2a299b8078f614cf1be02e1337059",
            "/paper/AMST2%3A-aggregated-multi-level-spatial-and-temporal-Park-Lee/5c0b964fabb701523680c394303b5caa2bf1d1f7",
            "/paper/Cascaded-Denoising-Transformer-for-UAV-Nighttime-Lu-Fu/6a86342bc85a21f7169e03b7a5425c3ad95cb166",
            "/paper/Scale-Aware-Domain-Adaptation-for-Robust-UAV-Fu-Li/9dfd3330017c41f510d6e7274d2af46958a1ccf2",
            "/paper/Template-guided-frequency-attention-and-adaptive-Xue-Jin/3a779c4da3b1ed8e163fffd8d4c60b720bce0351",
            "/paper/Training-Set-Distillation-for-Real-Time-UAV-Object-Li-Fu/118b86a39eaf99a735130a12e2048105dfe7c3a6",
            "/paper/Keyfilter-Aware-Real-Time-UAV-Object-Tracking-Li-Fu/9269b52994d8af23dc17dfbc225cd25c0902686c",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "/paper/AutoTrack%3A-Towards-High-Performance-Visual-Tracking-Li-Fu/0619650ae0f698bcc38244a6858cc270df9dfaad",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Boundary-Effect-Aware-Visual-Tracking-for-UAV-with-Fu-Huang/770a74e86e7a39eec441d8af00e37329e247d2c8",
            "/paper/SiamCAR%3A-Siamese-Fully-Convolutional-Classification-Guo-Wang/738165f33c50b059e87b14d8b4a129230e14eacd",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/SiamFC%2B%2B%3A-Towards-Robust-and-Accurate-Visual-with-Xu-Wang/be412c7c7128cf91455233b652d6c94a6001a7c8"
        ]
    },
    {
        "id": "72c43972036b430ca5caafd9e674ee22f589e9c4",
        "title": "A background-aware correlation filter with adaptive saliency-aware regularization for visual tracking",
        "abstract": "A background-aware correlation filter model with saliency-aware regularization is established and an adaptive updating mechanism based on the variation of target appearance and the reliability of tracking results, which can update the model online by adjusting the spatial weight distribution for precisely tracking in the spatio-temporal domain is proposed. Recently, the discriminative correlation filters (DCF)-based methods have performed excellent precision and speed in object tracking. Due to the continuous change and expansion of the search region, the problem of insufficient training samples is solved by the periodicity hypothesis, which inevitably introduces boundary effects that can lead to severe failures in the detection stage. In this paper, we firstly add a background penalty factor into the correlation filter and propose a novel spatial regularization term by using the saliency detection method. Based on the above two points, a background-aware correlation filter model with saliency-aware regularization is established. Secondly, in order to solve the model better and faster, we introduce an energy function for the solution of the spatial weight and apply the alternating direction method of multipliers (ADMM) method and deduce the closed-form solution of each subproblem of the objective function efficiently. Thirdly, we propose an adaptive updating mechanism based on the variation of target appearance and the reliability of tracking results, which can update the model online by adjusting the spatial weight distribution for precisely tracking in the spatio-temporal domain. Finally, we apply two BAASR models to estimate the position and the scale of the target, respectively. One model adopts hand-crafted features at multiple scales to select the optimal scale, while the other model predicts the optimal position by fusing hand-crafted features with deep features extracted from the trained network models. Extensive experiments are carried out on the following five datasets, OTB-2013, OTB-2015, UAV123, UAV20L, and TC128. Experimental results demonstrate that our tracker has superior robustness and performance.",
        "publication_year": "2022",
        "authors": [
            "Jianming Zhang",
            "Ting Yuan",
            "Yaoqi He",
            "Jin Wang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "11",
        "reference_count": "51",
        "references": [
            "/paper/Learning-background-aware-and-spatial-temporal-for-Zhang-He/d365a37c54ad4641dbe0073290ff41d8e7658886",
            "/paper/Visual-attention-learning-and-antiocclusion-based-Huang-Chen/01fcf00ebd163b2bfa185c982e5f803aa084a2b5",
            "/paper/SiamOA%3A-siamese-offset-aware-object-tracking-Zhang-Xie/53201f6a34e5bedd793f0132e3b4c59a098c2274",
            "/paper/PACR%3A-Pixel-Attention-in-Classification-and-for-Li-Chai/bc22787a65d5b55ba9ea1b088ec4a347c03acac8",
            "/paper/Learning-Adaptive-Sparse-Spatially-Regularized-for-Zhang-He/805344ee2b3be98fc6fd01d817dbcd88658340e3",
            "/paper/A-practical-evaluation-of-correlation-filter-based-Mohamed-Elhenawy/0d9281a58c437b53bb3fd18e8783859b13b90ecb",
            "/paper/An-adaptive-spatiotemporal-correlation-filtering-Liu-Yan/f3b92cacb8fe1ca675b7b441071447e2287e7f56",
            "/paper/Siamese-visual-tracking-based-on-criss-cross-and-Zhang-Huang/1e5f9d81b83a89ab7f4c8f42c379db6d6257ac1b",
            "/paper/Real-time-traffic-sign-detection-based-on-attention-Zhang-Ye/9d1270b9d9191476a95293e417d844dab73f6360",
            "/paper/Separable-programming-based-probabilistic-iteration-Cao-Wu/b0c8592e1d8574d27261815ff4269e5253965c4c",
            "/paper/Robust-visual-tracker-combining-temporal-consistent-Zhang-Liu/9493bfe9e4fd277f998fac166f8729cd198e9a10",
            "/paper/Distractor-aware-visual-tracking-using-hierarchical-Zhang-Liu/9af31cba16dc857d96c28c5161c4e73af848a8aa",
            "/paper/Adaptive-multi-branch-correlation-filters-for-Li-Huang/c02b127dc3cb96aa051632e5b47288000ae004ca",
            "/paper/Learning-Spatial-Temporal-Regularized-Correlation-Li-Tian/9f45b55af027503fab557f55f70e81e43c6c1db7",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Visual-object-tracking-based-on-residual-network-Zhang-Sun/cad39e1bd27156d07c384eff97517c1896496bcf",
            "/paper/Spatially-Attentive-Visual-Tracking-Using-Adaptive-Zhang-Wu/5e41f97c4611277401e989cc154093a60f36f95b",
            "/paper/Learning-Local%E2%80%93Global-Multiple-Correlation-Filters-Zhang-Liu/c821eb03f769df480e164a616194ffc73a826fec",
            "/paper/Large-Margin-Object-Tracking-with-Circulant-Feature-Wang-Liu/ece7625a346edbc5f6fab541c0c246ec06939121",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c"
        ]
    },
    {
        "id": "739d084e486702dbdad01d668f77b431228bae9d",
        "title": "Robust Superpixel Tracking",
        "abstract": "This paper presents a discriminative appearance model based on superpixels, thereby facilitating a tracker to distinguish the target and the background with midlevel cues and facilitates foreground and background segmentation during tracking. While numerous algorithms have been proposed for object tracking with demonstrated success, it remains a challenging problem for a tracker to handle large appearance change due to factors such as scale, motion, shape deformation, and occlusion. One of the main reasons is the lack of effective image representation schemes to account for appearance variation. Most of the trackers use high-level appearance structure or low-level cues for representing and matching target objects. In this paper, we propose a tracking method from the perspective of midlevel vision with structural information captured in superpixels. We present a discriminative appearance model based on superpixels, thereby facilitating a tracker to distinguish the target and the background with midlevel cues. The tracking task is then formulated by computing a target-background confidence map, and obtaining the best candidate by maximum a posterior estimate. Experimental results demonstrate that our tracker is able to handle heavy occlusion and recover from drifts. In conjunction with online update, the proposed algorithm is shown to perform favorably against existing methods for object tracking. Furthermore, the proposed algorithm facilitates foreground and background segmentation during tracking.",
        "publication_year": "2014",
        "authors": [
            "F. Yang",
            "Huchuan Lu",
            "Ming-Hsuan Yang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "294",
        "reference_count": "41",
        "references": [
            "/paper/Visual-tracking-based-on-object-appearance-and-Wang-Duan/cd5a981783faf9f65617f77a67e7bc5e143d7fe5",
            "/paper/Tracking-Using-Superpixel-Features-Jingjing-Ying/b3a5dacc26fbf552156e1d8cdd2c3722751055fc",
            "/paper/Spatial-Temporal-Segmentation-based-Tracking-Han-Xiao/a2593c4f4b3e8acc2069c5916f22e904bebe185c",
            "/paper/Compressive-Tracking-based-on-Superpixel-Chen-Sahli/25554df51451fd7abefc9040f8e694cf59d00bf2",
            "/paper/Robust-visual-tracking-using-a-contextual-boosting-Jiang-Wang/bdd2250cc06d8deab30ccfa4b2357ca47371e26b",
            "/paper/Robust-Object-Tracking-via-Structure-Learning-and-Li-Zhou/08206c92067b16bfdebf424af15ea32d8d18c7a6",
            "/paper/Structural-superpixel-descriptor-for-visual-Huang-Hu/58443e9368ee8e15b5ba1ebec5db155cf3c5b032",
            "/paper/Salient-Superpixel-Visual-Tracking-with-Graph-Model-Zhan-Zhao/e7dab7fc9fef81f00321d0d8a2b0df731157fb92",
            "/paper/A-Robust-Appearance-Model-for-Object-Tracking-Li-Lu/3981997df8613efd955a8f48a0eb97249bfced41",
            "/paper/Fast-Pixelwise-Adaptive-Visual-Tracking-of-Objects-Duffner-Garcia/0667bd18e0537359bc44a8f2ad94074b2e3a346e",
            "/paper/Superpixel-tracking-Wang-Lu/b762ecb0624005831f2f3d8eb626d53e8eca4b6c",
            "/paper/Online-Object-Tracking-With-Sparse-Prototypes-Wang-Lu/a65c76169bdb8479353806556f61bf94fdec7e10",
            "/paper/Visual-tracking-decomposition-Kwon-Lee/29e1e20323f7cb6c15c6acf5cc6573a2f84e6478",
            "/paper/Robust-Real-Time-Visual-Tracking-Using-Pixel-Wise-Bibby-Reid/3d98b9822e6efc2eec421e0bfd7546fd4ba30407",
            "/paper/Robust-online-appearance-models-for-visual-tracking-Jepson-Fleet/4411f262853bf7f1eb8e2efe03eb0402f5e9ad2c",
            "/paper/Incremental-Learning-for-Visual-Tracking-Lim-Ross/14ee8cf76e31fdf4381d29c1fb61017beb52f672",
            "/paper/Visual-tracking-with-online-Multiple-Instance-Babenko-Yang/421bf4eeba623f722bf98340d71e3d229881e92d",
            "/paper/Robust-visual-tracking-using-%26%23x2113%3B1-minimization-Mei-Ling/b5e17a0ed14349d6c4066d2408409751f9595e04",
            "/paper/Struck%3A-Structured-Output-Tracking-with-Kernels-Hare-Golodetz/61394599ed0aabe04b724c7ca3a778825c7e776f",
            "/paper/A-Nonparametric-Treatment-for-Location-Segmentation-Lu-Hager/e20c24e03ae025e563d6917a1ebf032820e2a519"
        ]
    },
    {
        "id": "03093ee286d7f8c575577df7e7451f5a70c8516f",
        "title": "FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits",
        "abstract": "This work proposes a method for diminishing computations by adding so-called early exit branches to the original architecture, and dynamically switching the computational path depending on how difficult it will be to render the output, for real-time applications such as synthesis of faces. Generative DNNs are a powerful tool for image synthesis, but they are limited by their computational load. On the other hand, given a trained model and a task, e.g. faces generation within a range of characteristics, the output image quality will be unevenly distributed among images with different characteristics. It follows, that we might restrain the models complexity on some instances, maintaining a high quality. We propose a method for diminishing computations by adding so-called early exit branches to the original architecture, and dynamically switching the computational path depending on how difficult it will be to render the output. We apply our method on two different SOTA models performing generative tasks: generation from a semantic map, and cross-reenactment of face expressions; showing it is able to output images with custom lower-quality thresholds. For a threshold of LPIPS<=0.1, we diminish their computations by up to a half. This is especially relevant for real-time applications such as synthesis of faces, when quality loss needs to be contained, but most of the inputs need fewer computations than the complex instances.",
        "publication_year": "2023",
        "authors": [
            "Polina Karpikova",
            "Radionova Ekaterina",
            "Anastasia Yaschenko",
            "Andrei A. Spiridonov",
            "Leonid Kostyushko",
            "Riccardo Fabbricatore",
            "Aleksei Ivakhnenko Samsung AI Center",
            "Higher School of Economics",
            "Lomonosov Moscow State University"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "88",
        "references": [
            "/paper/Drop-the-GAN%3A-In-Defense-of-Patches-Nearest-as-Granot-Shocher/f6571aed926ba5be4d1d307c29e54a2909d8eed0",
            "/paper/Generative-Adversarial-Nets-Goodfellow-Pouget-Abadie/54e325aee6b2d476bbbb88615ac15e251c6e8214",
            "/paper/Adversarial-Text-to-Image-Synthesis%3A-A-Review-Frolov-Hinz/abac50aa18e037f8149b2b212f1691d738f7b056",
            "/paper/Image-Synthesis-with-Adversarial-Networks%3A-a-Survey-Shamsolmoali-Zareapoor/d27aa2eb68e4761adc1e8045a39c29e3d8f8cb0b",
            "/paper/You-Only-Need-Adversarial-Supervision-for-Semantic-Sushko-Sch%C3%B6nfeld/8f7d4d61292886c111b1fe11f524ecaa8101de27",
            "/paper/High-Resolution-Image-Synthesis-and-Semantic-with-Wang-Liu/f0a0c0f0d6a7ff53abea40a8c0c678ed570bf851",
            "/paper/GANs-Trained-by-a-Two-Time-Scale-Update-Rule-to-a-Heusel-Ramsauer/231af7dc01a166cac3b5b01ca05778238f796e41",
            "/paper/Image-to-Image-Translation-with-Conditional-Isola-Zhu/8acbe90d5b852dadea7810345451a99608ee54c7",
            "/paper/The-Unreasonable-Effectiveness-of-Deep-Features-as-Zhang-Isola/c468bbde6a22d961829e1970e6ad5795e05418d1",
            "/paper/Why-Should-We-Add-Early-Exits-to-Neural-Networks-Scardapane-Scarpiniti/2238dda06c345a5f1ed87681a10d246c0fac0587"
        ]
    },
    {
        "id": "3a779c4da3b1ed8e163fffd8d4c60b720bce0351",
        "title": "Template-guided frequency attention and adaptive cross-entropy loss for UAV visual tracking",
        "abstract": "Semantic Scholar extracted view of \"Template-guided frequency attention and adaptive cross-entropy loss for UAV visual tracking\" by Yuanliang Xue et al.",
        "publication_year": "2023",
        "authors": [
            "Yuanliang Xue",
            "Guodong Jin",
            "Tao Shen",
            "Lining Tan",
            "Lianfeng Wang"
        ],
        "related_topics": "",
        "citation_count": 0,
        "reference_count": "40",
        "references": [
            "/paper/Learning-Channel-Aware-Correlation-Filters-for-Nai-Li/a1e9c22100e3fe1862188093e4d054ed08dde99b",
            "/paper/MobileTrack%3A-Siamese-efficient-mobile-network-for-Xue-Jin/ac3955255581453617149a02a69bc37ab4f9badc",
            "/paper/High-performance-UAVs-visual-tracking-using-deep-Yang-Xu/9abcd961f4a360bcba888f66eeb1266f9efc3574",
            "/paper/Siamese-tracking-combing-frequency-channel-with-Pang-Xie/c61ecd4885ad9a5ee204fc366e763c343213d0cc",
            "/paper/Learning-spatial-self-attention-information-for-Li-Zhang/1716fc21bd66e22410fc528b64d884d900cdd9db",
            "/paper/High-performance-UAVs-visual-tracking-based-on-Yang-Chen/7b386067a9f1d8aa6f78c58f6574fb0756ad87de",
            "/paper/SA-Net%3A-Shuffle-Attention-for-Deep-Convolutional-Zhang-Yang/a6afad3cc55ce0876dba39663f20edcac6257d18",
            "/paper/Paralleled-attention-modules-and-adaptive-focal-for-Zhao-Jiang/a958bc254d930771d398b14ca43e313b71c7b6d6",
            "/paper/FcaNet%3A-Frequency-Channel-Attention-Networks-Qin-Zhang/1834a1ff37052895c42906ceb163d9306badc00b",
            "/paper/Siamese-Anchor-Proposal-Network-for-High-Speed-Fu-Cao/827ee2cdc56ea65ef644bd8ca085f4274b106f03"
        ]
    },
    {
        "id": "fd88f6b78781a7ab8f89cf7b8debc69e1bcf25a0",
        "title": "CXLSeg Dataset: Chest X-ray with Lung Segmentation",
        "abstract": "This study has shown that the visual feature extraction process is optimized using segmented CXR radiographs of CXLSeg instead of original MIMIC-CXR images by a significant margin. With the advancement of robust deep learning techniques, a significant number of applications pertaining to biomedical research and clinical practice can be noticed within the computer vision domain. Radiologists use Chest X-Ray (CXR) images, prominent among medical imaging, to diagnose and treat diseases. Proper anatomical segmentation of CXR images is increasingly valuable as a critical image pre-processing step as well as in interpreting the deep learning models since it isolates the necessary area by extracting the region of interest. This paper proposes a CXR-segmented dataset based on the MIMIC-CXR dataset. A total of 243,324 frontal views of CXR images with segmented masks for each image are available in the dataset. A U-Net model with spatial attention (SA-UNet) architecture is utilized for segmenting the CXR images after a comparative analysis of different U-Net variants. The SA-UNet model achieved a 96.80% in dice similarity coefficient and 91.97% in IoU for lung segmentation. This study has shown that the visual feature extraction process is optimized using segmented CXR radiographs of CXLSeg instead of original MIMIC-CXR images by a significant margin.",
        "publication_year": "2023",
        "authors": [
            "Wimukthi Nimalsiri",
            "Mahela Hennayake",
            "Kasun Rathnayake",
            "Thanuja D. Ambegoda",
            "D. Meedeniya"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "14",
        "references": [
            "/paper/Segmentation-and-classification-on-chest-a-survey-Agrawal-Choudhary/bb81652f87b117c3458a470c9874b33968ee2a4c",
            "/paper/U-Net-Based-Chest-X-ray-Segmentation-with-Ensemble-Kumarasinghe-Kolonne/39f2bac74d99693706263b43d5d16b2cdebacf0f",
            "/paper/Chest-X-ray-analysis-empowered-with-deep-learning%3A-Meedeniya-Kumarasinghe/85e8a49a5d656e230a89f24d2e61065d2b57f965",
            "/paper/MIMIC-CXR%2C-a-de-identified-publicly-available-of-Johnson-Pollard/d1f407b16fb8d99487baee37ed0805676c58e7ac",
            "/paper/U-Net%3A-Convolutional-Networks-for-Biomedical-Image-Ronneberger-Fischer/6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "/paper/Attention-U-Net%3A-Learning-Where-to-Look-for-the-Oktay-Schlemper/ae1c89817a3a239e5344293138bdd80293983460",
            "/paper/Two-public-chest-X-ray-datasets-for-computer-aided-Jaeger-Candemir/0fd259546e31fa7ff4df38d21d18c149177faeb8",
            "/paper/UNet%2B%2B%3A-Redesigning-Skip-Connections-to-Exploit-in-Zhou-Siddiquee/42b0a8f757e45462e627e57f9af7e9849dcdacdf",
            "/paper/Chest-X-Ray-Caption-Generation-with-CheXNet-Wijerathna-Raveen/307da0f197fb49d51480f5482c326fa96db92fd1",
            "/paper/Sample-Efficient-Deep-Learning-for-COVID-19-Based-He-Yang/bc1edd72df527404508d48292ecfa6e0137ae333"
        ]
    },
    {
        "id": "2fb524759f430cdde1cef7d694716fb695c54385",
        "title": "Radiomics model of diffusion-weighted whole-body imaging with background signal suppression (DWIBS) for predicting axillary lymph node status in breast cancer.",
        "abstract": "Use of machine learning models incorporating with the quantitative radiomic features derived from the DWIBS and STIR sequences can potentially predict ALN status. BACKGROUND\nIn breast cancer diagnosis and treatment, non-invasive prediction of axillary lymph node (ALN) metastasis can help avoid complications related to sentinel lymph node biopsy.\n\n\nOBJECTIVE\nThis study aims to develop and evaluate machine learning models using radiomics features extracted from diffusion-weighted whole-body imaging with background signal suppression (DWIBS) examination for predicting the ALN status.\n\n\nMETHODS\nA total of 100 patients with histologically proven, invasive, clinically N0 breast cancer who underwent DWIBS examination consisting of short tau inversion recovery (STIR) and DWIBS sequences before surgery were enrolled. Radiomics features were calculated using segmented primary lesions in DWIBS and STIR sequences and were divided into former (n\u200a=\u200a75) and latter (n\u200a=\u200a25) datasets based on the examination date. Using the former dataset, optimal feature selection was performed using the least absolute shrinkage and selection operator algorithm, and the logistic regression model and support vector machine (SVM) classifier model were constructed with DWIBS, STIR, or a combination of DWIBS and STIR sequences to predict ALN status. Receiver operating characteristic curves were used to assess the prediction performance of radiomics models.\n\n\nRESULTS\nFor the latter dataset, the logistic regression model using DWIBS, STIR, and a combination of both sequences yielded an area under the curve (AUC) of 0.765 (95% confidence interval: 0.548-0.982), 0.801 (0.597-1.000), and 0.779 (0.567-0.992), respectively, whereas the SVM classifier model using DWIBS, STIR, and a combination of both sequences yielded an AUC of 0.765 (0.548-0.982), 0.757 (0.538-0.977), and 0.779 (0.567-0.992), respectively.\n\n\nCONCLUSIONS\nUse of machine learning models incorporating with the quantitative radiomic features derived from the DWIBS and STIR sequences can potentially predict ALN status.",
        "publication_year": "2023",
        "authors": [
            "Takafumi Haraguchi",
            "Yasuyuki Kobayashi",
            "Daisuke Hirahara",
            "Tatsuaki Kobayashi",
            "E. Takaya",
            "M. Nagai",
            "Hayato Tomita",
            "Jun Okamoto",
            "Y. Kanemaki",
            "K. Tsugawa"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "44",
        "references": [
            "/paper/Preoperative-prediction-of-sentinel-lymph-node-in-Dong-Feng/f95f11ea57d4ace576d033f0dec86293c7d411ea",
            "/paper/Role-of-diffusion-weighted-MRI%3A-predicting-axillary-Chung-Youk/329967290aec09a79ba2dde5bf5b8c17de39cf8d",
            "/paper/Development-and-validation-of-radiomics-machine-on-Zhang-Cao/a4f8dc311d7d5416e126368f86b40e0c70485480",
            "/paper/Preoperative-Prediction-of-Axillary-Lymph-Node-in-Tan-Gan/7813f2e40454307bf9f572b50ba7b47439c7f289",
            "/paper/A-machine-learning-based-radiomics-model-for-the-of-Song/140fac7ac2deeeb89256b9a273849ef84ddf78e9",
            "/paper/Diagnostic-Usefulness-of-Diffusion-Weighted-MRI-for-Cho-Park/d81c678b7752aa5522f18b175417016e4976aa40",
            "/paper/The-Diagnostic-Performance-of-Machine-Radiomics-of-Zhang-Li/f22857746a4c1faec90d5ebdc81dbfb82063569d",
            "/paper/Whole-body-MRI-for-detecting-metastatic-bone-tumor%3A-Nakanishi-Kobayashi/ff22342bde3dac417070fd3cb47aaf12ce2911cb",
            "/paper/Comparison-of-diffusion-weighted-whole-body-MRI-and-Gutzeit-Doert/fb7535e85b0c96ebe4844e52ee8306ad3f54d88d",
            "/paper/Predictive-Value-of-18F-FDG-PET-CT-for-Axillary-in-Song-Kim/32944a4e6c95fa4f30bcceff6847f0f960535973"
        ]
    },
    {
        "id": "41fc37bdbb53df4b0f99fff4871d30b71f299c73",
        "title": "DermSynth3D: Synthesis of in-the-wild Annotated Dermatology Images",
        "abstract": "The DermSynth3D framework generates photo-realistic 2D dermoscopy images and the corresponding dense annotations for semantic segmentation of the skin, skin conditions, body parts, bounding boxes around lesions, depth maps, and other 3D scene parameters, such as camera position and lighting conditions. In recent years, deep learning (DL) has shown great potential in the field of dermatological image analysis. However, existing datasets in this domain have significant limitations, including a small number of image samples, limited disease conditions, insufficient annotations, and non-standardized image acquisitions. To address these shortcomings, we propose a novel framework called DermSynth3D. DermSynth3D blends skin disease patterns onto 3D textured meshes of human subjects using a differentiable renderer and generates 2D images from various camera viewpoints under chosen lighting conditions in diverse background scenes. Our method adheres to top-down rules that constrain the blending and rendering process to create 2D images with skin conditions that mimic in-the-wild acquisitions, ensuring more meaningful results. The framework generates photo-realistic 2D dermoscopy images and the corresponding dense annotations for semantic segmentation of the skin, skin conditions, body parts, bounding boxes around lesions, depth maps, and other 3D scene parameters, such as camera position and lighting conditions. DermSynth3D allows for the creation of custom datasets for various dermatology tasks. We demonstrate the effectiveness of data generated using DermSynth3D by training DL models on synthetic data and evaluating them on various dermatology tasks using real 2D dermatological images. We make our code publicly available at https://github.com/sfu-mial/DermSynth3D.",
        "publication_year": "2023",
        "authors": [
            "Ashish Sinha",
            "J. Kawahara",
            "Arezou Pakzad",
            "Kumar Abhishek",
            "Matthieu Ruthven",
            "Enjie Ghorbel",
            "Anis Kacem",
            "Djamila Aouada",
            "G. Hamarneh"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "84",
        "references": [
            "/paper/Illumination-based-Transformations-Improve-Skin-in-Abhishek-Hamarneh/8625e1c396deced35f7b0a8b7ecf2ebc2b5d830f",
            "/paper/Generating-Highly-Realistic-Images-of-Skin-Lesions-Baur-Albarqouni/a24f99959b8d7273bdfb12a75f080be976b80fc0",
            "/paper/Mask2Lesion%3A-Mask-Constrained-Adversarial-Skin-Abhishek-Hamarneh/502dd3f72a41bc81d42ae76b1ffce404c9ec864d",
            "/paper/A-Survey-on-Deep-Learning-for-Skin-Lesion-Mirikharaji-Barata/9cea08824884fdd289a0ed6714fc032933f2b0c0",
            "/paper/Dermoscopy-Image-Analysis%3A-Overview-and-Future-Celebi-Codella/3d06115ef839b18012d2a87c808c2e7aabcf78ac",
            "/paper/Sketch-guided-and-progressive-growing-GAN-for-and-Liang-Yang/edc212bc3700e0a00c1e441a58e64fc57a4e6433",
            "/paper/GANs-for-Medical-Image-Synthesis%3A-An-Empirical-Skandarani-Jodoin/6b59cd34fe244592b8e5a364d1bb89a8a3b4c535",
            "/paper/Estimating-Skin-Tone-and-Effects-on-Classification-Kinyanjui-Odonga/088977ac55f66421fa87393830409b44dbe207d3",
            "/paper/Skin3D%3A-Detection-and-longitudinal-tracking-of-skin-Zhao-Kawahara/d06fd7a747908cdc2521e1675ba03d38ebb45a81",
            "/paper/GANs-for-Medical-Image-Analysis-Kazeminia-Baur/04507020e4e003042150dcc8602505d1bec71c49"
        ]
    },
    {
        "id": "7da6a61e08a85b4c7b249352975806cd762fbc2e",
        "title": "An Optimized Predictive Model Based on Deep Neural Network for Detection of Skin Cancer and Oral Cancer",
        "abstract": "Convolutional neural networks and VGG19 are combined in the proposed model to identify and classify various forms of skin and oral cancer, demonstrating that these methodologies\u2019 hybridization provides greater accuracy than conventional techniques. Cancer is becoming a serious illness having a high mortality range. Cancer is a disease that causes death even before the age of 60, according to recent research conducted in 90 nations. More than 9 million individuals will pass away in 2020, with cancer accounting for 1 in 6 of those fatalities. Skin and oral malignancies are considered the greatest prevalent cancer types. Predicting the cancerous type based on pictures is crucial. This research work introduces revolutionary deep-learning methodologies for skin and oral cancer diagnosis at the start of the disease. The main goal of this research work is to assist physicians in early oral and skin cancer diagnosis. Convolutional neural networks and VGG19 are combined in the proposed model to identify and classify various forms of skin and oral cancer. The author pares the dataset with other competing classifiers to validate the algorithm. The findings demonstrate that these methodologies\u2019 hybridization provides greater accuracy than conventional techniques. The proposed model has a 97.17% accuracy, a 92.19% precision, a 92.18% recall, and a 93.10% F1-score value. In the future, this technique can be improved with additional features for treating other tumors.",
        "publication_year": "2023",
        "authors": [
            "Geetika Sharma",
            "Raman Chadha"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "44",
        "references": [
            "/paper/Early-detection-of-skin-cancer-using-deep-learning-Demir-Yilmaz/aaca1d1b8708e7a1374d7effe456c96f964c5433",
            "/paper/Melanoma-Skin-Cancer-Detection-using-Deep-Learning-Santos/16b344577f33fd5857ab848b8b9eba18cb226d69",
            "/paper/Melanoma-skin-cancer-detection-using-deep-learning-Daghrir-Tlig/1a788c2ab0db8db6f591a57a371f4302def3003e",
            "/paper/Automated-Detection-and-Classification-of-Oral-Deep-Welikala-Remagnino/230b8255925d56b72444df3f358f5851b3158b59",
            "/paper/Classification-of-Skin-cancer-using-deep-learning%2C-Manne-Kantheti/a446f512678e8b36f9c1e0bb92d023b2a3a317ef",
            "/paper/AI-Recognition-in-Skin-Pathologies-Detection-Gavrilov-Lazarenko/1203039a72b40ed5055be8978d46acc4df8f3bdf",
            "/paper/Convolutional-Neural-Network-Strategy-for-Skin-and-Nour-Boufama/74c319339d3fa1a16283f63fd2e36d0277b0c031",
            "/paper/Application-of-artificial-intelligence-and-machine-Alhazmi-Alhazmi/d3e3fb44123cf4c8beff3be49dcd3caeec50f591",
            "/paper/From-Deep-Learning-Towards-Finding-Skin-Lesion-Li-Wu/114770e88875f4fa208c8e731a610804710fbaa5",
            "/paper/A-Novel-Lightweight-Deep-Convolutional-Neural-for-Jubair-Al-karadsheh/a6ee87dc926e1ada80673d702c7de278e53682c9"
        ]
    },
    {
        "id": "ea33320a3299bb0811d0072df691e4d0a731dba6",
        "title": "Pathological changes or technical artefacts? The problem of the heterogenous databases in COVID-19 CXR image analysis",
        "abstract": "Semantic Scholar extracted view of \"Pathological changes or technical artefacts? The problem of the heterogenous databases in COVID-19 CXR image analysis\" by M. Socha et al.",
        "publication_year": "2023",
        "authors": [
            "M. Socha",
            "Wojciech Prazuch",
            "A. Suwalska",
            "P. Foszner",
            "J. Tobiasz",
            "J. Jaroszewicz",
            "K. Gruszczy\u0144ska",
            "M. Sliwinska",
            "Mateusz Nowak",
            "Barbara Gizycka",
            "Gabriela Zapolska",
            "T. Popiela",
            "Grzegorz Przybylski",
            "P. Fiedor",
            "M. Paw\u0142owska",
            "R. Flisiak",
            "K. Simon",
            "J. Walecki",
            "A. Cieszanowski",
            "E. Szurowska",
            "M. Marczyk",
            "Joanna Pola\u0144ska",
            "Joanna Michal Wojciech Aleksandra Marek Pawel Joanna Mate Polanska Marczyk Prazuch Suwalska Socha Foszner To",
            "Joanna Pola\u0144ska",
            "M. Marczyk",
            "Wojciech Prazuch",
            "A. Suwalska",
            "M. Socha",
            "P. Foszner",
            "J. Tobiasz",
            "Mateusz Nowak",
            "P. Fiedor",
            "A. Cieszanowski",
            "Agnieszka Oronowicz-Ja\u015bkowiak",
            "Bogumi\u0142 Go\u0142\u0119biewski",
            "K. Simon",
            "M. Sliwinska",
            "Mateusz Rataj",
            "Przemyslaw Chmielarz",
            "Adrianna Tur",
            "Grzegorz Drabik",
            "T. Popiela",
            "J. Kozub",
            "Grzegorz Przybylski",
            "Anna Kozanecka",
            "E. Szurowska",
            "Sebastian Hildebrandt",
            "Katarzyna Krutul-Walenciej",
            "J. Baron",
            "K. Gruszczy\u0144ska",
            "J. Jaroszewicz",
            "Damian Piotrowski",
            "J. Walecki",
            "Piotr Wasilewski",
            "S. Mazur",
            "R. Flisiak",
            "Gabriela Zapolska",
            "Krzysztof Klaude",
            "K. Rataj",
            "Bogumi\u0142 Go\u0142\u0119biewski",
            "Ma\u0142gorzata Paw\u0142owska",
            "Piotr Rabiko",
            "P. Rajewski",
            "Barbara Gizycka",
            "Piotr Blewaska",
            "K. Sznajder",
            "R. Ple\u015bniak"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "55",
        "references": [
            "/paper/Current-limitations-to-identify-COVID-19-using-with-L%C3%B3pez-Cabrera-Orozco-Morales/5edf0185ea2552bae0a085ef286d7262b2260546",
            "/paper/Xception%3A-Deep-Learning-with-Depthwise-Separable-Chollet/5b6ec746d309b165f9f9def873a2375b6fb40f3d",
            "/paper/Identity-Mappings-in-Deep-Residual-Networks-He-Zhang/77f0a39b8e02686fd85b01971f8feb7f60971f80",
            "/paper/Common-pitfalls-and-recommendations-for-using-to-CT-Roberts-Driggs/69d49a06f09cf934310ccbf3bb2a360fa719272d",
            "/paper/MobileNets%3A-Efficient-Convolutional-Neural-Networks-Howard-Zhu/3647d6d0f151dc05626449ee09cc7bce55be497e",
            "/paper/Densely-Connected-Convolutional-Networks-Huang-Liu/5694e46284460a648fe29117cbc55f6c9be3fa3c",
            "/paper/Rethinking-the-Inception-Architecture-for-Computer-Szegedy-Vanhoucke/23ffaa0fe06eae05817f527a47ac3291077f9e58",
            "/paper/U-Net%3A-Convolutional-Networks-for-Biomedical-Image-Ronneberger-Fischer/6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "/paper/POLCOVID%3A-a-multicenter-multiclass-chest-X-ray-Suwalska-Tobiasz/6c814dcca13812e254990aa26ad77a64cf3fe714",
            "/paper/COVID-19-Diagnosis-from-Chest-X-ray-Images-Using-a-Monday-Li/432da8cda913f82fbeb2012b7d46a9acd86ab7a8"
        ]
    },
    {
        "id": "fcb21ecd2a79ede56a9afe6bba6fe76ac94fbca8",
        "title": "Detection of Detached Ice-fragments at Martian Polar Scarps Using a Convolutional Neural Network",
        "abstract": "Good performance and quick processing speed of the developed model allow us to efficiently study large-scale areas, which is an important step in estimating the ongoing mass wasting and studying the evolution of the martian polar scarps. Repeated high-resolution imaging has revealed current mass wasting in the form of ice block falls at steep scarps of Mars. However, both the accuracy and efficiency of ice-fragments\u2019 detection are limited when using conventional computer vision methods. Existing deep learning methods suffer from the problem of shadow interference and indistinguishability between classes. To address these issues, we proposed a deep learning-driven change detection model that focuses on regions of interest. A convolutional neural network simultaneously analyzed bitemporal images, i.e., pre- and postdetach images. An augmented attention module was integrated in order to suppress irrelevant regions such as shadows while highlighting the detached ice-fragments. A combination of dice loss and focal loss was introduced to deal with the issue of imbalanced classes and hard, misclassified samples. Our method showed a true positive rate of 84.2% and a false discovery rate of 16.9%. Regarding the shape of the detections, the pixel-based evaluation showed a balanced accuracy of 85% and an F1 score of 73.2% for the detached ice-fragments. This last score reflected the difficulty in delineating the exact boundaries of some events both by a human and the machine. Compared with five state-of-the-art change detection methods, our method can achieve a higher F1 score and surpass other methods in excluding the interference of the changed shadows. Assessing the detections of the detached ice-fragments with the help of previously detected corresponding shadow changes demonstrated the capability and robustness of our proposed model. Furthermore, the good performance and quick processing speed of our developed model allow us to efficiently study large-scale areas, which is an important step in estimating the ongoing mass wasting and studying the evolution of the martian polar scarps.",
        "publication_year": "2023",
        "authors": [
            "S. Su",
            "L. Fanara",
            "Haifeng Xiao",
            "E. Hauber",
            "J. Oberst"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "65",
        "references": [
            "/paper/Deep-Learning-Driven-Detection-and-Mapping-of-on-Bickel-Conway/e2a589d8ba15bec05e170b1181afdf6b9da811bd",
            "/paper/Automated-Detection-of-Lunar-Rockfalls-Using-a-Bickel-Lanaras/a702d3413b2b9823a26fcd471361a44af8910373",
            "/paper/DETECTING-THE-SOURCES-OF-ICE-BLOCK-FALLS-AT-THE-BY-Su-Fanara/08f2a52b38ea12d5175e8df25d55d2bf96fb135c",
            "/paper/An-automatic-approach-for-ice-block-falls-detection-Song-Liu/8600955215dde4bb5108358b4b52d4cc12b0663d",
            "/paper/Mapping-Landslides-on-EO-Data%3A-Performance-of-Deep-Prakash-Manconi/e72f29e88350ba95376e7131da98804a8a68993a",
            "/paper/Automated-detection-of-block-falls-in-the-north-of-Fanara-Gwinner/615b7cbef55f824c12f665e5a124cbc3d333211b",
            "/paper/Automatic-Mapping-of-Landslides-by-the-ResU-Net-Qi-Wei/4e63aaebd77de3cd7ecc603d428c077d6b3d60d7",
            "/paper/A-Combined-Loss-Based-Multiscale-Fully-Network-for-Li-He/bc612fbe733d428b9086ab95ac95f460b5c4a26f",
            "/paper/Landslide-Recognition-by-Deep-Convolutional-Neural-Shi-Zhang/42da5e60aa98abf2aa6e32acf69ee0c3b6c3b2a6",
            "/paper/Focal-Loss-for-Dense-Object-Detection-Lin-Goyal/72564a69bf339ff1d16a639c86a764db2321caab"
        ]
    },
    {
        "id": "d593243302cce8414cbd9d6db53c57bdc5fabb5c",
        "title": "Analyses of natural frequency and electromechanical behavior of flexoelectric cylindrical nanoshells under modified couple stress theory",
        "abstract": "Different from piezoelectric effect, the flexoelectric effect is size dependent and becomes more significant at nanoscale. A nonlinear dynamic model of nanoscale flexoelectric shells is developed based on modified couples stress theory. The governing equations for nonlinear vibration of a flexoelectric cylindrical nanoshell are obtained. The natural frequency and generated voltage of the flexoelectric cylindrical nanoshell is achieved. Effects of geometric dimension, character material length, and vibration amplitude on the natural frequency and the generated voltage are discussed in detail. Results demonstrate that modified couple stress theory and large deformation theory are coupled together and interact with each other in the analyses of natural frequency and electromechanical behavior. Simultaneously taking nonlinearity and couple stress theory into account is necessary.",
        "publication_year": "2018",
        "authors": [
            "S. Zeng",
            "B. Wang",
            "K.F. Wang"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": "19",
        "reference_count": "78",
        "references": [
            "/paper/Porous-flexoelectric-cylindrical-nanoshell-based-on-Dehkordi-Goojani/75cba98893c0d686d90e87572d0b4df2e5ef76f4",
            "/paper/Thermo-electro-mechanical-vibration-and-buckling-of-Lyu-Liu/d235ad88ccd3ba757992af3f559b7fcc0999422d",
            "/paper/On-the-size-dependent-nonlinear-dynamics-of-Bagheri-Beni/182d9c5ba979650f1344925c48a3c57df271c4e6",
            "/paper/Vibration-and-thermal-buckling-analyses-of-based-on-Abbaspour-Arvin/c6c9ed940595615471f795a69c5c311692daa855",
            "/paper/A-flexoelectric-theory-with-rotation-gradient-and-Li-Zhou/5b0bb7a9d2c820571a0f563381f3586a9321bddc",
            "/paper/Dynamic-Analysis-of-Cylindrical-Shells-with-Han-Yan/bf8ee5a8981711c79af36c7b2bd26f5217cd0f41",
            "/paper/How-does-flexoelectricity-affect-static-bending-and-Ajri-Rastgoo/a47dc6a82ef94e1d35d940c24a88da3d037be2ab",
            "/paper/Vibration-behavior-of-a-micro-cylindrical-sandwich-Anvari-Mohammadimehr/7452f48b5c775b861651560f89bc036a43c7b487",
            "/paper/A-coupled-thermomechanics-approach-for-frequency-of-Al-Furjan-Habibi/671e812459bf61a00a971467bfe1198501a6c93a",
            "/paper/Acoustic-wave-transmission-of-double-walled-graded-Heydari-Mokhtarian/4cd199b5c45bd828fef7a249a33bc9ceae639bdb",
            "/paper/Flexoelectric-Effect-on-Vibration-of-Piezoelectric-Li-Luo/e5405dbb2bbdfa53cf863d3ba558e04a0ac5a62b",
            "/paper/Surface-effects-on-the-vibration-behavior-of-based-Ebrahimi-Barati/5854f52eb58bb04674701bb1c28c8fecac1ec765",
            "/paper/On-the-coupling-effects-of-piezoelectricity-and-in-He-Lou/d31a26b31b0ab5d23e7901ac4fd07d283942aa54",
            "/paper/Thermo-electro-mechanical-vibration-of-cylindrical-Ke-Wang/83723c948811e4b7970ab942183bdfdfe06300a7",
            "/paper/Size-dependent-electro-mechanical-vibration-of-thin-Kheibari-Beni/f89d75b2c431eeeed57db1a5820203767193f860",
            "/paper/Size-dependent-analysis-of-a-three-layer-microbeam-Li-Zhou/fc65b031037da9101e069df148beae108fa395a4",
            "/paper/Electro-mechanical-free-vibration-of-single-walled-Dehkordi-Beni/58b218cf4942e71cca3e08dd9786102f4cbc6ae1",
            "/paper/Size-dependent-vibration-and-instability-of-graded-Ansari-Gholami/c01d52fe7f89d9821ebbc47f3c1d459fae19e835",
            "/paper/Analytical-treatment-of-the-nonlinear-free-of-based-Rouhi-Ansari/5f21676ea8f1e08c31720e341a3f1e8338a37c82",
            "/paper/Modeling-of-a-nanoscale-flexoelectric-energy-with-Yan/d133452ef2114f80e87de9dc50826ec8d8adc03e"
        ]
    },
    {
        "id": "5118b57f47915d49e9a78e41496383fd9649525e",
        "title": "Multi-dimensional top-k dominating queries",
        "abstract": "An extensive study on the evaluation of top-k dominating queries, which proposes a set of algorithms that apply on indexed multi-dimensional data and investigates query evaluation on data that are not indexed. The top-k dominating query returns k data objects which dominate the highest number of objects in a dataset. This query is an important tool for decision support since it provides data analysts an intuitive way for finding significant objects. In addition, it combines the advantages of top-k and skyline queries without sharing their disadvantages: (i) the output size can be controlled, (ii) no ranking functions need to be specified by users, and (iii) the result is independent of the scales at different dimensions. Despite their importance, top-k dominating queries have not received adequate attention from the research community. This paper is an extensive study on the evaluation of top-k dominating queries. First, we propose a set of algorithms that apply on indexed multi-dimensional data. Second, we investigate query evaluation on data that are not indexed. Finally, we study a relaxed variant of the query which considers dominance in dimensional subspaces. Experiments using synthetic and real datasets demonstrate that our algorithms significantly outperform a previous skyline-based approach. We also illustrate the applicability of this multi-dimensional analysis query by studying the meaningfulness of its results on real data.",
        "publication_year": "2009",
        "authors": [
            "Man Lung Yiu",
            "N. Mamoulis"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "122",
        "reference_count": "32",
        "references": [
            "/paper/Enhancing-Query-Efficiency-Using-Pruning-Techniques-Babu-Swapna/d2540dee0cd20eed5938e60c66af8eba543eeae2",
            "/paper/Top-k-Dominating-Queries-%3A-a-Survey-Tiakas-Papadopoulos/ce661789ac6793e884e162f8476753236e27616c",
            "/paper/Dynamic-Processing-of-Dominating-Queries-with-Kosmatopoulos-Papadopoulos/0c9887b6cb4baafaa379c2af336746775e05b516",
            "/paper/Continuous-Top-k-Dominating-Queries-Kontaki-Papadopoulos/caf62f1c21a671e7226078e8287995012a92c14e",
            "/paper/Processing-Top-k-Dominating-Queries-in-Metric-Tiakas-Valkanas/d3a11264d6c5ac0125a4cbc730ab81186671557f",
            "/paper/Top-k-Dominating-Queries-on-Incomplete-Data-Miao-Gao/a747dbfe0f35784b119fd5e9e257b17b0232cc12",
            "/paper/Preference-Based-Top-k-Representative-Skyline-on-Nguyen-Cao/c427f106da57d9f971e73de0abc777c78ae4e3d7",
            "/paper/Close-Dominance-Graph%3A-An-Efficient-Framework-for-Santoso-Chiu/7ec1909b3c7b29c4a7b09f56f6449c0ccb7d08e6",
            "/paper/on-Top-K-Dominating-Queries-on-Incomplete-Data-AnoopSreelekshmi/255dfac08c2174e1c8950a1d4e2de060e9fb9f16",
            "/paper/Efficient-Processing-of-Reverse-Top-k-Dominating-Jiang-Zhang/1195108ab46bd6c00ea714ff1549a5e10d8cf9eb",
            "/paper/Efficient-Processing-of-Top-k-Dominating-Queries-on-Yiu-Mamoulis/e1e5936749e383c096e219b9bf85cf756585882b",
            "/paper/SUBSKY%3A-Efficient-Computation-of-Skylines-in-Tao-Xiao/85d542ce705eb3fcb923ca93d781d93c4a6ee061",
            "/paper/Maximal-Vector-Computation-in-Large-Data-Sets-Godfrey-Shipley/a7414f68a7c63109402ab3f4ed495dde2df596d0",
            "/paper/On-High-Dimensional-Skylines-Chan-Jagadish/242552a6eb324f2a427acde5ba79ae2d4d5c0772",
            "/paper/PREFER%3A-a-system-for-the-efficient-execution-of-Hristidis-Koudas/b47283c7d69d0116e656beb6ddcc64e97dbaef57",
            "/paper/Finding-k-dominant-skylines-in-high-dimensional-Chan-Jagadish/dcb4c7d69cbf07968dd89ac5e248144c9b2e3411",
            "/paper/DADA%3A-a-data-cube-for-dominant-relationship-Li-Ooi/10e4cd0d4f1dbf0accdedc5261555499a2e0c34d",
            "/paper/Stratified-computation-of-skylines-with-domains-Chan-Eng/6344f634db0084d38bd3c0bbc4ea95fa56675471",
            "/paper/A-model-for-the-prediction-of-R-tree-performance-Theodoridis-Sellis/f8065fe8f36436740253389a383c05a05629ac8f",
            "/paper/Efficient-Computation-of-the-Skyline-Cube-Yuan-Lin/c255e88ccaa9b1dbce891834802a5952ce01ca3c"
        ]
    },
    {
        "id": "e605271aefa33cdf4c77e0ea801cd8e2410fceb0",
        "title": "MonoNeuralFusion: Online Monocular Neural 3D Reconstruction with Geometric Priors",
        "abstract": "This paper presents MonoNeuralFusion to perform the online neural 3D reconstruction from monocular videos, by which the 3D scene geometry is efficiently generated and optimized during the on-the-fly 3D monocular scanning. High-fidelity 3D scene reconstruction from monocular videos continues to be challenging, especially for complete and fine-grained geometry reconstruction. The previous 3D reconstruction approaches with neural implicit representations have shown a promising ability for complete scene reconstruction, while their results are often over-smooth and lack enough geometric details. This paper introduces a novel neural implicit scene representation with volume rendering for high-fidelity online 3D scene reconstruction from monocular videos. For fine-grained reconstruction, our key insight is to incorporate geometric priors into both the neural implicit scene representation and neural volume rendering, thus leading to an effective geometry learning mechanism based on volume rendering optimization. Benefiting from this, we present MonoNeuralFusion to perform the online neural 3D reconstruction from monocular videos, by which the 3D scene geometry is efficiently generated and optimized during the on-the-fly 3D monocular scanning. The extensive comparisons with state-of-the-art approaches show that our MonoNeuralFusion consistently generates much better complete and fine-grained reconstruction results, both quantitatively and qualitatively.",
        "publication_year": "2022",
        "authors": [
            "Zi-Xin Zou",
            "Shi-Sheng Huang",
            "Yan-Pei Cao",
            "Tai-Jiang Mu",
            "Ying Shan",
            "Hongbo Fu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "5",
        "reference_count": "67",
        "references": [
            "/paper/FineRecon%3A-Depth-aware-Feed-forward-Network-for-3D-Stier-Ranjan/e00fe237ff1d0daa918a0a203c94b70e9d14b94b",
            "/paper/PlaNeRF%3A-SVD-Unsupervised-3D-Plane-Regularization-Wang-Louys/05ab73c0bd2c70728e61c2dd74e5fe5d69363ec0",
            "/paper/SurfelNeRF%3A-Neural-Surfel-Radiance-Fields-for-of-Gao-Cao/dde71563f9c4281aaf6a0071c4733f98b557e7ee",
            "/paper/ESLAM%3A-Efficient-Dense-SLAM-System-Based-on-Hybrid-Johari-Carta/4029ade0264485431f7ed98e76b2f789208e3028",
            "/paper/Point-SLAM%3A-Dense-Neural-Point-Cloud-based-SLAM-Sandstr%C3%B6m-Li/607aecc0f0ec965d550aed112be791f2a6ec7620",
            "/paper/MonoSDF%3A-Exploring-Monocular-Geometric-Cues-for-Yu-Peng/c6c518ccd441bae6fdc788a58b6d39db1f33f1a4",
            "/paper/SimpleRecon%3A-3D-Reconstruction-Without-3D-Sayed-Gibson/caf96058774d96dd08df799319ef0d5430121518",
            "/paper/NeuralRecon%3A-Real-Time-Coherent-3D-Reconstruction-Sun-Xie/8db0bf0fa406254d4cd57eb413443a0b96bd12b8",
            "/paper/Neural-3D-Scene-Reconstruction-with-the-Assumption-Guo-Peng/283a4e3e45f65d825c5640d582d30aa4d6dddf7e",
            "/paper/DI-Fusion%3A-Online-Implicit-3D-Reconstruction-with-Huang-Huang/7fe9575df3ced24994121dea3a0171fd29445b6c",
            "/paper/NeuS%3A-Learning-Neural-Implicit-Surfaces-by-Volume-Wang-Liu/cf5647cb2613f5f697729eab567383006dcd4913",
            "/paper/VolumeFusion%3A-Deep-Depth-Fusion-for-3D-Scene-Choe-Im/0ba9140dad37b1b3d8cfcdb6d54389edd701c58d",
            "/paper/NeuRIS%3A-Neural-Reconstruction-of-Indoor-Scenes-Wang-Wang/0b52e3d7631e1f2b4bd5e5922f71eac8ad273f7a",
            "/paper/Occupancy-Networks%3A-Learning-3D-Reconstruction-in-Mescheder-Oechsle/2e689bdce24cf3644432505ce2783f03a1445ed2",
            "/paper/VoRTX%3A-Volumetric-3D-Reconstruction-With-for-View-Stier-Rich/a358c152d177ad4ff4161f8822187d4a3e22bc24"
        ]
    },
    {
        "id": "340e5710e0cf9ff79f57b9d4d71035f903667ebe",
        "title": "MAM: Multiple Attention Mechanism Neural Networks for Cross-Age Face Recognition",
        "abstract": "The proposed Multiple Attention Mechanism Network (MAM-CNN) framework can focus on essential face regions to highlight identity features and diminish the distractions caused by aging features. Cross-age face recognition problem is of great challenge in practical applications because face features of the same person at different ages contain variant aging features in addition to the invariant identity features. To better extract the age-invariant identity features hiding beneath the age-variant aging features, a deep learning-based approach with multiple attention mechanisms is proposed in this paper. First, we propose the stepped local pooling strategy to improve the SE module. Then by incorporating the residual-attention mechanism, the self-attention mechanism, and the improved channel-attention mechanism to the backbone network, we proposed the Multiple Attention Mechanism Network (MAM-CNN) framework for the cross-age face recognition problem. The proposed framework can focus on essential face regions to highlight identity features and diminish the distractions caused by aging features. Experiments are carried out on two well-known public domain face aging datasets (MORPH and CACD-VS). The results yielded prove that the introduced multiple mechanisms jointly enhance the model performance by 0.96% and 0.52%, respectively, over the state-of-the-art algorithms.",
        "publication_year": "2022",
        "authors": [
            "Xudie Ren",
            "Jialve Wang",
            "Shenghong Li"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "33",
        "references": [
            "/paper/Latent-Factor-Guided-Convolutional-Neural-Networks-Wen-Li/b99a389eea084b718a6f7888bc84806e66df46be",
            "/paper/Age-Estimation-Guided-Convolutional-Neural-Network-Zheng-Deng/c2bd9322fa2d0a00fc62075cc0f1996fc75d42a8",
            "/paper/Age-Related-Factor-Guided-Joint-Task-Modeling-for-Li-Hu/2d7c2c015053fff5300515a7addcd74b523f3f66",
            "/paper/Hidden-Factor-Analysis-for-Age-Invariant-Face-Gong-Li/217a21d60bb777d15cd9328970cab563d70b5d23",
            "/paper/Cross-Age-Reference-Coding-for-Age-Invariant-Face-Chen-Chen/c44c84540db1c38ace232ef34b03bda1c81ba039",
            "/paper/Residual-Attention-Network-for-Image-Classification-Wang-Jiang/77d30cf9a34fb6b50979c6a68863099da9a060ad",
            "/paper/A-Discriminative-Model-for-Age-Invariant-Face-Li-Park/96350ffbd6e201cf21f509401148ea7674c6e82d",
            "/paper/FaceNet%3A-A-unified-embedding-for-face-recognition-Schroff-Kalenichenko/5aa26299435bdf7db874ef1640a6c3b5a4a2c394",
            "/paper/Age-Invariant-Face-Recognition-Park-Tong/f2264d05eaf42572bfdd779f72cfadcafcc0c5f3",
            "/paper/Attention-to-Scale%3A-Scale-Aware-Semantic-Image-Chen-Yang/9f48616039cb21903132528c0be5348b3019db50"
        ]
    },
    {
        "id": "2a662c841bfd02864de876db19cfa523d493041c",
        "title": "Data Augmentation Analysis in Vehicle Detection from Aerial Videos",
        "abstract": "By adding similar data, randomly rotating and cropping images with respect to the model's input size, this paper has remarkably increased the accuracy of YOLOv3, one of the state-of-the-art and real-time object detection methods. Recent growth in deep learning and computer vision has opened up many opportunities for advanced intelligent systems. While the dataset quality plays a crucial role in the training phase and also affects the performance of a model, creating a reliable and diverse dataset appears to be challenging. Therefore, data augmentation can be used as a preprocessing step with a view to tackling the problem. In this paper, we conduct a comprehensive analysis on different augmentation strategies to investigate their impact on vehicle detection in top-down videos captured by drones. Our experiments show that by adding similar data, randomly rotating and cropping images with respect to the model's input size, we have remarkably increased the accuracy of YOLOv3, one of the state-of-the-art and real-time object detection methods.",
        "publication_year": "2020",
        "authors": [
            "Quynh M. Chung",
            "Tien-Dung Le",
            "Thin Dang",
            "Nguyen D. Vo",
            "Tam V. Nguyen",
            "Khang Nguyen"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "7",
        "reference_count": "14",
        "references": [
            "/paper/Vehicle-Detection-at-Night-Time-Ho-Pham/b8e63209b36a62162d94ebf0ecdad26f18169fa4",
            "/paper/Rethinking-Classification-of-Oriented-Object-in-Nguyen-Truong/560f6027c93f011c7b2e5c296ebbe35c6f30c8e5",
            "/paper/An-emperical-study-of-Double-Head-for-vehicle-in-Truc-Quy%C3%AAn/72e5f1307c6d865ce5dbbfe6731da7d6561272b9",
            "/paper/Olympic-Games-Event-Recognition-via-Transfer-with-Mohamad-Baraheem/bff94259da13d8b4afdcf719932728279c141323",
            "/paper/An-object-detection-method-for-aerial-hazy-images-Tran-Tran/9c9ce1a6303881e8055e3110c860e90d0b38f6c0",
            "/paper/MC-OCR-Challenge-2021%3A-Deep-Learning-Approach-for-Bui-Truong/1e82e4f54c7804ba3131d38c182603265351e164",
            "/paper/KHAI-PHA%CC%81-HA%CC%80M-CHI-PHI%CC%81-CHO-PHA%CC%81T-HIE%CC%A3%CC%82N-Doanh-Nguy%C3%AAn/3a12db3df2918eff7749bafc066907e68e6185a0",
            "/paper/YADA%3A-you-always-dream-again-for-better-object-Nguyen-Nguyen/4e8148d59a57cff9e231961abcabc1e4fd66b7ff",
            "/paper/Vision-Meets-Drones%3A-A-Challenge-Zhu-Wen/98bf42055160845e6f8f3c022298e3b8e4e55f80",
            "/paper/You-always-look-again%3A-Learning-to-detect-the-Nguyen-Nguyen/263cd05600a8a5f934f140b35a90d8d5ad342da7",
            "/paper/Faster-R-CNN%3A-Towards-Real-Time-Object-Detection-Ren-He/424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "/paper/SSD%3A-Single-Shot-MultiBox-Detector-Liu-Anguelov/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
            "/paper/You-Only-Look-Once%3A-Unified%2C-Real-Time-Object-Redmon-Divvala/f8e79ac0ea341056ef20f2616628b3e964764cfd",
            "/paper/Microsoft-COCO%3A-Common-Objects-in-Context-Lin-Maire/71b7178df5d2b112d07e45038cb5637208659ff7",
            "/paper/YOLO9000%3A-Better%2C-Faster%2C-Stronger-Redmon-Farhadi/7d39d69b23424446f0400ef603b2e3e22d0309d6"
        ]
    },
    {
        "id": "2941e4d1980c99c11bd9bba99289946e259e1e2b",
        "title": "Planar region alignment based ego-motion estimation for mobile robot",
        "abstract": "This paper assumes the mobile robot with visual observer is moved on flat ground and a constrained camera-robot configuration is used to simplify the camera motion model, and the multistage process for ego-motion estimation is introduced. The ego-motion estimation is a basic task in many operations of a mobile robot. An accurate and robust result of motion estimation can greatly improve the operation. In this paper, we assume the mobile robot with visual observer is moved on flat ground and a constrained camera-robot configuration is used to simplify the camera motion model. Based on the assumption and simplified camera-robot configuration, the 3D camera motion can be separately estimated by planar region alignment between two frames. Firstly, a detected 2D motion between two frames is used to align corresponding planar regions, such an alignment or registration removes the effects of camera rotation, and the resulting residual parallax displacement field between the two region-aligned images is an epipolar field centered at the FOE (Focus-of-Expansion). Then the camera translation can be recovered from the epipolar field. The camera rotation is recovered from the computed 3D translation and the detected 2D motion. The camera motion model is derived and expressed by two parts, the multistage process for ego-motion estimation is introduced. Some experiments with real images demonstrate the robustness of the motion estimation. And the factors which may affect the results are discussed.",
        "publication_year": "2012",
        "authors": [
            "Z. Wang",
            "B. Cai",
            "F. Yi",
            "Y. Liu",
            "J. Zhao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "14",
        "references": [
            "/paper/Planar-region-alignment-based-visual-regulation-for-Wang-Liu/adfd953eb2eb52a37944ec95352e8284201138bd",
            "/paper/An-inverse-projective-mapping-based-approach-for-Wang-Wu/b71c307c73e89667f1cfce8be2ca28e8fd6de622",
            "/paper/Recovery-of-Ego-Motion-Using-Region-Alignment-Irani-Rousso/7c3e5196511dc9c8b7f7a0be4e9eeb48f604f1ae",
            "/paper/Direct-multi-resolution-estimation-of-ego-motion-Hanna/396ffa315a2944832c7c2f8a58e909b77afb841d",
            "/paper/Robust-structure-from-motion-using-motion-parallax-Cipolla-Okamoto/15c4cca0d98f2ec7537d15d9f722041995ad7db7",
            "/paper/The-coupling-of-rotation-and-translation-in-motion-Daniilidis-Nagel/f4575c234d71f736eb3103ee4465c35c0a5ce157",
            "/paper/Determining-Three-Dimensional-Motion-and-Structure-Adiv/3733124d9fedd380e7e0c6b82eef9f2db3344fc3",
            "/paper/Direct-methods-for-recovering-motion-Horn-Weldon/e88791964c01c293f9b647b449b07b8d97b29bd9",
            "/paper/Simple-method-for-computing-3D-motion-and-depth-Heeger-Jepson/f15696ac207320066698bb2ca361fb68d584f023",
            "/paper/The-Use-of-Difference-Fields-in-Processing-Sensor-Lawton-Rieger/4da45ccc3a01701dafdf977f54836ee3ae732270",
            "/paper/A-robust-method-for-computing-vehicle-ego-motion-Stein-Mano/fd0e3bde64ea33afc8d39ab19351e2ac3748430c",
            "/paper/Motion-recovery-from-image-sequences-using-optical-Negahdaripour-Lee/f78a88a86244b71ccb47325a2e51c80c8f29a061"
        ]
    },
    {
        "id": "336eea1e32e2b93c63f97931710d3de54d05a336",
        "title": "A Comprehensive Survey of Data Augmentation in Visual Reinforcement Learning",
        "abstract": "A unified framework for analyzing visual RL and understanding the role of DA is proposed and a principled taxonomy of the existing augmentation techniques used in visual RL is presented to offer valuable guidance to this emerging field. Visual reinforcement learning (RL), which makes decisions directly from high-dimensional visual inputs, has demonstrated significant potential in various domains. However, deploying visual RL techniques in the real world remains challenging due to their low sample efficiency and large generalization gaps. To tackle these obstacles, data augmentation (DA) has become a widely used technique in visual RL for acquiring sample-efficient and generalizable policies by diversifying the training data. This survey aims to provide a timely and essential review of DA techniques in visual RL in recognition of the thriving development in this field. In particular, we propose a unified framework for analyzing visual RL and understanding the role of DA in it. We then present a principled taxonomy of the existing augmentation techniques used in visual RL and conduct an in-depth discussion on how to better leverage augmented data in different scenarios. Moreover, we report a systematic empirical evaluation of DA-based techniques in visual RL and conclude by highlighting the directions for future research. As the first comprehensive survey of DA in visual RL, this work is expected to offer valuable guidance to this emerging field.",
        "publication_year": "2022",
        "authors": [
            "Guozheng Ma",
            "Zhen Wang",
            "Zhecheng Yuan",
            "Xueqian Wang",
            "Bo Yuan",
            "Dacheng Tao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "6",
        "reference_count": "160",
        "references": [
            "/paper/Learning-Better-with-Less%3A-Effective-Augmentation-Ma-Zhang/d2796dec7c1c94f3a8fc22bea03b82d9a0196c1a",
            "/paper/Normalization-Enhances-Generalization-in-Visual-Li-Lyu/e9258b5024e636627ecb50aa56e22c8c03d42985",
            "/paper/C2RL%3A-Convolutional-Contrastive-Learning-for-Based-Park-Kim/9f53e55d869f08f080f31b88e6f1aac0dc588c7e",
            "/paper/On-Pre-Training-for-Visuo-Motor-Control%3A-Revisiting-Hansen-Yuan/7e65d56f71b6a831b8a0385798d14b8c558df6b7",
            "/paper/MA2CL%3A-Masked-Attentive-Contrastive-Learning-for-Song-Feng/f83d3a91b28a9b7dfc77f82eff6402a7e06670a6",
            "/paper/Diffusion-Model-is-an-Effective-Planner-and-Data-He-Bai/702e3b669912cbe9df70c7fb731842bb9801f25f",
            "/paper/Pre-Trained-Image-Encoder-for-Generalizable-Visual-Yuan-Xue/0672e63cb2fb7a754e6549598440d3b0c66a002d",
            "/paper/Stabilizing-Deep-Q-Learning-with-ConvNets-and-under-Hansen-Su/390e248117f8bafad8b47b8e799352980c2f3f70",
            "/paper/Don't-Touch-What-Matters%3A-Task-Aware-Lipschitz-Data-Yuan-Ma/670472a67a5ab6af73f8a4d452f8eab8ce47160f",
            "/paper/Reinforcement-Learning-with-Augmented-Data-Laskin-Lee/744139d65c3bf6da6a6acd384a32d94a06f44f62",
            "/paper/VRL3%3A-A-Data-Driven-Framework-for-Visual-Deep-Wang-Luo/7ed1566a286068f39effa67ae5c7489dbea06414",
            "/paper/Automatic-Data-Augmentation-for-Generalization-in-Raileanu-Goldstein/dff2e7721d11826f841ca99f9fedc664c5266d67",
            "/paper/Generalizing-Reinforcement-Learning-through-Fusing-Wu-Wu/775c6ff63220e9aad98d5aee32788f1d45358075",
            "/paper/Automatic-Data-Augmentation-by-Upper-Confidence-for-Gil-Baek/ce5f699b18082dda94ba7c25ca6f5b15ad3dbaeb",
            "/paper/Generalization-in-Reinforcement-Learning-by-Soft-Hansen-Wang/3bf4ff1ffd283af79c0fec7e5b7ea76e6c8c6b75",
            "/paper/SECANT%3A-Self-Expert-Cloning-for-Zero-Shot-of-Visual-Fan-Wang/7ad1b82507b61c7113c4bde17fa3d89bb256cff3"
        ]
    },
    {
        "id": "e25dea5f5c2275d96021e59dc25d9a19de875603",
        "title": "Towards Efficient Point Cloud Graph Neural Networks Through Architectural Simplification",
        "abstract": "It is found that it is possible to radically simplify graph neural network models so long as the feature extraction layer is retained with minimal degradation to model performance; further, it is discovered that it was possible to improve performance overall on ModelNet40 and S3DIS by improving the design of the feature extractor. In recent years graph neural network (GNN)-based approaches have become a popular strategy for processing point cloud data, regularly achieving state-of-the-art performance on a variety of tasks. To date, the research community has primarily focused on improving model expressiveness, with secondary thought given to how to design models that can run efficiently on resource constrained mobile devices including smartphones or mixed reality headsets. In this work we make a step towards improving the efficiency of these models by making the observation that these GNN models are heavily limited by the representational power of their first, feature extracting, layer. We find that it is possible to radically simplify these models so long as the feature extraction layer is retained with minimal degradation to model performance; further, we discover that it is possible to improve performance overall on ModelNet40 and S3DIS by improving the design of the feature extractor. Our approach reduces memory consumption by 20\u00d7 and latency by up to 9.9\u00d7 for graph layers in models such as DGCNN; overall, we achieve speed-ups of up to 4.5\u00d7 and peak memory reductions of 72.5%.",
        "publication_year": "2021",
        "authors": [
            "Shyam A. Tailor",
            "R. D. Jong",
            "Tiago Azevedo",
            "Matthew Mattina",
            "Partha P. Maji"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "37",
        "references": [
            "/paper/Hardware-Aware-Graph-Neural-Network-Automated-for-Zhou-Yang/0124aeae9d107027ace7a31a82fd577fe75d04f5",
            "/paper/Parallel-and-Distributed-Graph-Neural-Networks%3A-An-Besta-Hoefler/bf423874ed96158a1ab1126ae2862af07a88025f",
            "/paper/Availability-Attacks-on-Graph-Neural-Networks-Tailor-Tairum-Cruz/97d98a0cf1a3d229148d8d71cb256c63c084720f",
            "/paper/Mind-the-Label-Shift-of-Augmentation-based-Graph-Yu-Liang/93a3240983894e150717d6ac132b3f4d77bb2653",
            "/paper/Towards-Efficient-Graph-Convolutional-Networks-for-Li-Chen/2ac749deb74db6dffdd7e15fb2ce5b2014231f87",
            "/paper/Adaptive-Filters-and-Aggregator-Fusion-for-Graph-Tailor-Opolka/7f4dd7927ad7ecf5b79ec1412311a4e8db5ebeb4",
            "/paper/BiPointNet%3A-Binary-Neural-Network-for-Point-Clouds-Qin-Cai/404bf88bd0194e2ad1de79b4730c70bff5c841d4",
            "/paper/Dynamic-Graph-CNN-for-Learning-on-Point-Clouds-Wang-Sun/e1799aaf23c12af6932dc0ef3dfb1638f01413d1",
            "/paper/A-Closer-Look-at-Local-Aggregation-Operators-in-Liu-Hu/d11cedaf557af1883b906b4cc152254f9b8615f9",
            "/paper/Exploiting-Local-Geometry-for-Feature-and-Graph-for-Srivastava-Sharma/58817ced8646e06d36175b2d34d7139aadc54317",
            "/paper/Degree-Quant%3A-Quantization-Aware-Training-for-Graph-Tailor-Fern%C3%A1ndez-Marqu%C3%A9s/9c0e855382de7e708c8eea7b4d5cf792bcd4a326",
            "/paper/Revisiting-Point-Cloud-Shape-Classification-with-a-Goyal-Law/4e5dff8d9db8ee21b636e9bf93a2fc96999079aa",
            "/paper/A-Comprehensive-Survey-on-Graph-Neural-Networks-Wu-Pan/81a4fd3004df0eb05d6c1cef96ad33d5407820df",
            "/paper/How-Powerful-are-Graph-Neural-Networks-Xu-Hu/62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9"
        ]
    },
    {
        "id": "cf49f64cbdc1ba3959843f8fc5889762a384ab99",
        "title": "Video Summarization With Spatiotemporal Vision Transformer",
        "abstract": "A novel transformer-based method named spatiotemporal vision transformer (STVT) for video summarization that outperforms state-of-the-art methods in both of the SumMe and TVSum datasets. Video summarization aims to generate a compact summary of the original video for efficient video browsing. To provide video summaries which are consistent with the human perception and contain important content, supervised learning-based video summarization methods are proposed. These methods aim to learn important content based on continuous frame information of human-created summaries. However, simultaneously considering both of inter-frame correlations among non-adjacent frames and intra-frame attention which attracts the humans for frame importance representations are rarely discussed in recent methods. To address these issues, we propose a novel transformer-based method named spatiotemporal vision transformer (STVT) for video summarization. The STVT is composed of three dominant components including the embedded sequence module, temporal inter-frame attention (TIA) encoder, and spatial intra-frame attention (SIA) encoder. The embedded sequence module generates the embedded sequence by fusing the frame embedding, index embedding and segment class embedding to represent the frames. The temporal inter-frame correlations among non-adjacent frames are learned by the TIA encoder with the multi-head self-attention scheme. Then, the spatial intra-frame attention of each frame is learned by the SIA encoder. Finally, a multi-frame loss is computed to drive the learning of the network in an end-to-end trainable manner. By simultaneously using both inter-frame and intra-frame information, our method outperforms state-of-the-art methods in both of the SumMe and TVSum datasets. The source code of the spatiotemporal vision transformer will be available at https://github.com/nchucvml/STVT.",
        "publication_year": "2023",
        "authors": [
            "Tzu-Chun Hsu",
            "Yiping Liao",
            "Chun-Rong Huang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "75",
        "references": [
            "/paper/ViTEraser%3A-Harnessing-the-Power-of-Vision-for-Scene-Peng-Liu/fee6fb37ce240cc147bf88c37b6d9d7cfaf1edf4",
            "/paper/Learning-Hierarchical-Self-Attention-for-Video-Liu-Li/1d5ed33c72ccb5a7cb1e36fe24a9235d1ecd6edb",
            "/paper/Hierarchical-Multimodal-Transformer-to-Summarize-Zhao-Gong/52f8f58267fd2ab50cf6832be51d539d6606dd65",
            "/paper/Video-Summarization-With-Attention-Based-Networks-Ji-Xiong/88a8baa1be5292e62622f1cb8e627fbf759bf741",
            "/paper/Combining-Global-and-Local-Attention-with-Encoding-Apostolidis-Balaouras/6814ef7df12da2a8e56b673268be509503079985",
            "/paper/Video-Summarization-via-Semantic-Attended-Networks-Wei-Ni/6ec09bad57cc81a71ef7596f57e94ee13b380ae3",
            "/paper/Deep-Attentive-Video-Summarization-With-Consistency-Ji-Zhao/c1f1a999d84dce8e84c28b7d33edd6aa706b85c2",
            "/paper/Exploring-global-diverse-attention-via-pairwise-for-Li-Ye/e3b914637e63c88eec0721e41eca195921476e94",
            "/paper/Reconstructive-Sequence-Graph-Network-for-Video-Zhao-Li/ba039aa0ce03682901b8e24f7c3eb36786635dc6",
            "/paper/Video-Summarization-With-Frame-Index-Vision-Hsu-Liao/9ac3f7a8da37d5c4b696926edb9702ecad28fd33",
            "/paper/Video-Summarization-Using-Fully-Convolutional-Rochan-Ye/8c413c2ee66664909d8c194f3f3e08c5f109c3c1"
        ]
    },
    {
        "id": "f06a22165f0bcc92eac865e3eeddec3c7e2cf44c",
        "title": "Adversarial Blur-Deblur Network for Robust UAV Tracking",
        "abstract": "A tracking-oriented adversarial blur-deblur network (ABDNet), composed of a novel deblurrer to recover the visual appearance of the tracked object, and a brand-new blur generator to produce realistic blurry images for adversarial training. Unmanned aerial vehicle (UAV) tracking has been widely applied in real-world applications such as surveillance and monitoring. However, the inherent high maneuverability and agility of UAV often lead to motion blur, which can impair the visual appearance of the target object and easily degrade the existing trackers. To overcome this challenge, this work proposes a tracking-oriented adversarial blur-deblur network (ABDNet), composed of a novel deblurrer to recover the visual appearance of the tracked object, and a brand-new blur generator to produce realistic blurry images for adversarial training. More specifically, the deblurrer progressively refines the features through pixel-wise, spatial-wise, and channel-wise stages to achieve excellent deblurring performance. The blur generator adaptively fuses an image sequence with a learnable kernel to create realistic blurry images. During training, ABDNet is plugged into the state-of-the-art real-time trackers and trained with blurring-deblurring loss as well as tracking loss. During inference, the blur generator is removed, while the deblurrer and the tracker can work together for UAV tracking. Extensive experiments in both public datasets and real-world testing have validated the effectiveness of ABDNet.",
        "publication_year": "2023",
        "authors": [
            "Haobo Zuo",
            "Changhong Fu",
            "Sihang Li",
            "Kunhan Lu",
            "Yiming Li",
            "Chen Feng"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "38",
        "references": [
            "/paper/Cascaded-Denoising-Transformer-for-UAV-Nighttime-Lu-Fu/6a86342bc85a21f7169e03b7a5425c3ad95cb166",
            "/paper/Among-Us%3A-Adversarially-Robust-Collaborative-by-Li-Fang/eb3b8edf6c771850e4c667071fe37ec8b2c745c4",
            "/paper/Learning-to-Adversarially-Blur-Visual-Object-Guo-Cheng/235a720e00c1d770afcc199b7badbc994d22b505",
            "/paper/Watch-out!-Motion-is-Blurring-the-Vision-of-Your-Guo-Juefei-Xu/0f142d13437d01e11ccfb2ec735f37da5ffb298b",
            "/paper/Robust-Correlation-Tracking-in-Unmanned-Aerial-via-Zhang-Wang/14d3b8af761ee4a7b780ad1f4a059c89a17db296",
            "/paper/Siamese-Anchor-Proposal-Network-for-High-Speed-Fu-Cao/827ee2cdc56ea65ef644bd8ca085f4274b106f03",
            "/paper/Siamese-Object-Tracking-for-Unmanned-Aerial-A-and-Fu-Lu/1171234cb2f3e1589592e3d04eb10c132fc6a5c8",
            "/paper/Learning-Aberrance-Repressed-Correlation-Filters-Huang-Fu/1b3a7e6ebd89a1fb03c992110952d7bddae3345a",
            "/paper/AutoTrack%3A-Towards-High-Performance-Visual-Tracking-Li-Fu/0619650ae0f698bcc38244a6858cc270df9dfaad",
            "/paper/Multi-Regularized-Correlation-Filter-for-UAV-and-Ye-Fu/4d57a810cd768b5ad4de6d1e21673f3d3b89f00d",
            "/paper/Siamese-Box-Adaptive-Network-for-Visual-Tracking-Chen-Zhong/cce1fecc800d2782da638f3060d5b2e887739f74",
            "/paper/Visual-Object-Tracking-for-Unmanned-Aerial-A-and-Li-Yeung/6ebc40a061433c24a3ea1f305bb6533b8f3dd5f4"
        ]
    },
    {
        "id": "d365a37c54ad4641dbe0073290ff41d8e7658886",
        "title": "Learning background-aware and spatial-temporal regularized correlation filters for visual tracking",
        "abstract": "This work builds an intelligent tracking system, proposes a new background-aware and spatial-temporal regularized correlation filters model (BSTCF), and transforms the objective function of BSTCF into an unconstrained Augmented Lagrange multiplier formular to promote convergence to the global optimum solution. In visual tracking, correlation Filters (CFs) have attracted increasing research attention and achieved superior performance. However, owing to the larger search area, more background information is introduced to the shifted samples, meaning that tracking errors are prone to appear in the detection stage. Accordingly, in this work, firstly, hand-crafted features and deep features extracted from pre-trained convolutional networks are combined to improve the representation ability of object appearance. For deep features, we use two different VGG networks for extraction. Secondly, in an attempt to solve the problem of the object background of the traditional CF model not being modeled over time, and owing to the lack of spatial-temporal information of the image, we propose a new background-aware and spatial-temporal regularized correlation filters model (BSTCF) that introduces the background constraint and spatial-temporal regularization. The proposed BSTCF can effectively model not only the background but also variations in the background over time. Finally, we transform the objective function of BSTCF into an unconstrained Augmented Lagrange multiplier formular to promote convergence to the global optimum solution. Moreover, we adopt the alternating direction multiplier method (ADMM) to produce three sub-problems with closed-form solution, then propose a corresponding algorithm. Based on the above, we construct an intelligent tracking system and carry out extensive experiments to test its performance on OTB-2013, OTB-2015, TC128, UAV123, and VOT2016 public datasets. The experimental results demonstrate that the tracking algorithm achieves superior performance.",
        "publication_year": "2022",
        "authors": [
            "Jianming Zhang",
            "Yaoqi He",
            "Wenjun Feng",
            "Jin Wang",
            "Neal N. Xiong"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "40",
        "references": [
            "/paper/Visual-attention-learning-and-antiocclusion-based-Huang-Chen/01fcf00ebd163b2bfa185c982e5f803aa084a2b5",
            "/paper/An-adaptive-spatiotemporal-correlation-filtering-Liu-Yan/f3b92cacb8fe1ca675b7b441071447e2287e7f56",
            "/paper/Learning-spatial-variance-key-surrounding-aware-via-Elayaperumal-Joo/c2f7ea4af9f4391ce459f320c0e89d590393684b",
            "/paper/Adaptive-spatial-temporal-surrounding-aware-filter-Moorthy-Joo/32539ca3a692ed0782a2e5b43b80c7d5df18555f",
            "/paper/A-background-aware-correlation-filter-with-adaptive-Zhang-Yuan/72c43972036b430ca5caafd9e674ee22f589e9c4",
            "/paper/Learning-Spatial-Temporal-Regularized-Correlation-Li-Tian/9f45b55af027503fab557f55f70e81e43c6c1db7",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Visual-object-tracking-based-on-residual-network-Zhang-Sun/cad39e1bd27156d07c384eff97517c1896496bcf",
            "/paper/Spatial-and-semantic-convolutional-features-for-Zhang-Jin/726d072a772a04c6e650e10e9a4f07650108c260",
            "/paper/GradNet%3A-Gradient-Guided-Network-for-Visual-Object-Li-Chen/47a58f8bec1d34004a7d7cf837e27a26de64f0f7",
            "/paper/Hierarchical-Convolutional-Features-for-Visual-Ma-Huang/5c8a6874011640981e4103d120957802fa28f004"
        ]
    },
    {
        "id": "cd5a981783faf9f65617f77a67e7bc5e143d7fe5",
        "title": "Visual tracking based on object appearance and structure preserved local patches matching",
        "abstract": "A new tracking approach that resolves this problem by three multi-level collaborative components: a high-level global appearance tracker provides a basic prediction, upon which the structure preserved low-level local patches matching helps to guarantee precise tracking with minimized drift. Drift is the most difficult issue in object visual tracking based on framework of \u201ctracking-by-detection\u201d. Due to the self-taught learning, the mis-aligned samples are potentially to be incorporated in learning and degrade the discrimination of the tracker. This paper proposes a new tracking approach that resolves this problem by three multi-level collaborative components: a high-level global appearance tracker provides a basic prediction, upon which the structure preserved low-level local patches matching helps to guarantee precise tracking with minimized drift. Those local patches are deliberately deployed on the foreground object via foreground/background segmentation, which is realized by a simple and efficient classifier trained by super-pixel segments. Experimental results show that the three closely collaborated components enable our tracker runs in real time and performs favourably against state-of-the-art approaches on challenging benchmark sequences.",
        "publication_year": "2016",
        "authors": [
            "Wei Wang",
            "Kun Duan",
            "T. Tian",
            "Ting Yu",
            "Ser-Nam Lim",
            "H. Qi"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "32",
        "references": [
            "/paper/Robust-Superpixel-Tracking-Yang-Lu/739d084e486702dbdad01d668f77b431228bae9d",
            "/paper/Structure-Preserving-Object-Tracking-Zhang-Maaten/0d0b880e2b531c45ee8227166a489bf35a528cb9",
            "/paper/Robust-object-tracking-via-sparsity-based-model-Zhong-Lu/fe2aaad872a2cf08c09dd52ca972f323666306db",
            "/paper/Robust-Object-Tracking-with-Online-Multiple-Babenko-Yang/201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9",
            "/paper/Part-Based-Visual-Tracking-with-Online-Latent-Yao-Shi/236d4de0b1c73217238f370e7d30243c7ee9707a",
            "/paper/Incremental-Learning-for-Robust-Visual-Tracking-Ross-Lim/505f48d8236eb25f871da272c2ac2fe4b41ea289",
            "/paper/Learning-Features-for-Tracking-Grabner-Grabner/10d2a56557a3760f1d14fe6869e0cad76f0a24f4",
            "/paper/Real-Time-Compressive-Tracking-Zhang-Zhang/9d57723b4908397654fb1846d37db403d8b2b56a",
            "/paper/Exemplar-based-linear-discriminant-analysis-for-Gao-Chen/684d51b78409da1bbc95e320524f5bc96092c1e6",
            "/paper/Visual-tracking-decomposition-Kwon-Lee/29e1e20323f7cb6c15c6acf5cc6573a2f84e6478"
        ]
    },
    {
        "id": "f6571aed926ba5be4d1d307c29e54a2909d8eed0",
        "title": "Drop the GAN: In Defense of Patches Nearest Neighbors as Single Image Generative Models",
        "abstract": "This paper revisits the classical patch-based methods, and shows that - unlike previously believed - classical methods can be adapted to tackle these novel \u201cGAN-only\u201d tasks better and faster than single-image GAN- based methods. Image manipulation dates back long before the deep learning era. The classical prevailing approaches were based on maximizing patch similarity between the input and generated output. Recently, single-image GANs were introduced as a superior and more sophisticated solution to image manipulation tasks. Moreover, they offered the opportunity not only to manipulate a given image, but also to generate a large and diverse set of different outputs from a single natural image. This gave rise to new tasks, which are considered \u201cGAN-only\u201d. However, despite their impressiveness, single-image GANs require long training time (usually hours) for each image and each task and often suffer from visual artifacts. In this paper we revisit the classical patch-based methods, and show that - unlike previously believed - classical methods can be adapted to tackle these novel \u201cGAN-only\u201d tasks. Moreover, they do so better and faster than single-image GAN-based methods. More specifically, we show that: (i) by introducing slight modifications, classical patch-based methods are able to unconditionally generate diverse images based on a single natural image; (ii) the generated output visual quality exceeds that of single-image GANs by a large margin (confirmed both quantitatively and qualitatively); (iii) they are orders of magnitude faster (runtime reduced from hours to seconds). 22This project received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 788535), and the Carolito Stiftung. Dr Bagon is a Robin Chemers Neustein AI Fellow.",
        "publication_year": "2021",
        "authors": [
            "Niv Granot",
            "Assaf Shocher",
            "Ben Feinstein",
            "Shai Bagon",
            "M. Irani"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "34",
        "reference_count": "41",
        "references": [
            "/paper/FewGAN%3A-Generating-from-the-Joint-Distribution-of-a-Ben-Moshe-Benaim/5edc266d424ca59260a4c23469773bb47c0d873e",
            "/paper/A-Patch-Based-Algorithm-for-Diverse-and-High-Single-Cherel-Almansa/3a2c513842fd16e3590867c2b81602924f7e6d01",
            "/paper/Generating-Novel-Scene-Compositions-from-Single-and-Sushko-Zhang/1bab85e9f74625d0e8dce26f1ac43d28eec2ed99",
            "/paper/PetsGAN%3A-Rethinking-Priors-for-Single-Image-Zhang-Liu/a1caeccf0902cf7cc5532f01ac3b331757ad9ca9",
            "/paper/Generating-natural-images-with-direct-Patch-Elnekave-Weiss/770fa3a9570f49f97d4ed2bcfdced7eb9a185a38",
            "/paper/Diverse-Generation-from-a-Single-Video-Made-Haim-Feinstein/9b1546bc2ecbc2a699d83f113c549806212a7398",
            "/paper/Diverse-Video-Generation-from-a-Single-Video-Haim-Feinstein/a0274068d613e389a4f14277b49eb879b81c54ef",
            "/paper/ExSinGAN%3A-Learning-an-Explainable-Generative-Model-Zhang-Han/745297a4d15cbddf4df785c85c8e2ae3b4a650be",
            "/paper/Self-Distilled-StyleGAN%3A-Towards-Generation-from-Mokady-Yarom/56fd6fdc46e76cae7441e3abc6171d2af3b58e0c",
            "/paper/TcGAN%3A-Semantic-Aware-and-Structure-Preserved-GANs-Jiang-Yan/0aff9b20826d8de78da18d7c0c66ad8e3d387fa3",
            "/paper/Improved-Techniques-for-Training-Single-Image-GANs-Hinz-Fisher/1bf6b1aec3403eee7044935345c581a1b6a9f1ed",
            "/paper/Deep-Image-Prior-Ulyanov-Vedaldi/faa98e73eeee551c40923c896817ab640925ce20",
            "/paper/Deep-Single-Image-Manipulation-Vinker-Horwitz/ae0fa176a1323829e500ad8b0c1ae0393597669d",
            "/paper/MOGAN%3A-Morphologic-structure-aware-Generative-from-Chen-Xu/0d1140f37ef3e4fe98da42839f69e92aba536556",
            "/paper/Zero-Shot-Restoration-of-Back-lit-Images-Using-Deep-Zhang-Zhang/29a5199b7357a7f34b730e19864f11ac84255d4f",
            "/paper/SinGAN%3A-Learning-a-Generative-Model-From-a-Single-Shaham-Dekel/ccaf15d4ad006171061508ca0a99c73814671501",
            "/paper/Texture-Synthesis-with-Spatial-Generative-Networks-Jetchev-Bergmann/60b3fb579734593ebedaa177569052e90a778009",
            "/paper/InGAN%3A-Capturing-and-Remapping-the-%22DNA%22-of-a-Image-Shocher-Bagon/e4c49561f85d70c9a214b6c3c5586cf5f721e7bf",
            "/paper/%22Zero-Shot%22-Super-Resolution-Using-Deep-Internal-Shocher-Cohen/e4092ce5b2c5a1997be2702335f7d33ba7a353ef",
            "/paper/Learning-Texture-Manifolds-with-the-Periodic-GAN-Bergmann-Jetchev/d532a73e7d1e3bde23f862a0b3105d6613f9fd89"
        ]
    },
    {
        "id": "ac3955255581453617149a02a69bc37ab4f9badc",
        "title": "MobileTrack: Siamese efficient mobile network for high-speed UAV tracking",
        "abstract": "Semantic Scholar extracted view of \"MobileTrack: Siamese efficient mobile network for high-speed UAV tracking\" by Yuanliang Xue et al.",
        "publication_year": "2022",
        "authors": [
            "Yuanliang Xue",
            "Guodong Jin",
            "Tao Shen",
            "Lining Tan",
            "Jing Yang",
            "Xiaohan Hou"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "10",
        "references": [
            "/paper/Template-guided-frequency-attention-and-adaptive-Xue-Jin/3a779c4da3b1ed8e163fffd8d4c60b720bce0351",
            "/paper/Prior-combined-dehazing-network-based-on-mutual-Qiao-Kong/83caee7c41f85b0c6b2cfa41a05911399544aab3",
            "/paper/High-performance-UAVs-visual-tracking-using-deep-Yang-Xu/9abcd961f4a360bcba888f66eeb1266f9efc3574",
            "/paper/High-performance-UAVs-visual-tracking-based-on-Yang-Chen/7b386067a9f1d8aa6f78c58f6574fb0756ad87de",
            "/paper/Adaptive-UAV-target-tracking-algorithm-based-on-Fang-Yanan/bfadc22df7b84f4aa63bfee4cd0bb691ebc41faa",
            "/paper/Correlation-Filter-for-UAV-Based-Aerial-Tracking%3A-A-Fu-Li/f6883f7ba0380a65323be7a10c09ae44e9f1b8fe",
            "/paper/Ocean%3A-Object-aware-Anchor-free-Tracking-Zhang-Peng/27d52bf3265bea0f9929980f6ffb4c2009eecfee",
            "/paper/GlobalTrack%3A-A-Simple-and-Strong-Baseline-for-Huang-Zhao/5664e24cacf3f6374c26b5597765099ee9537413",
            "/paper/Visual-Object-Tracking-for-Unmanned-Aerial-A-and-Li-Yeung/6ebc40a061433c24a3ea1f305bb6533b8f3dd5f4",
            "/paper/ImageNet-Large-Scale-Visual-Recognition-Challenge-Russakovsky-Deng/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd"
        ]
    },
    {
        "id": "39f2bac74d99693706263b43d5d16b2cdebacf0f",
        "title": "U-Net Based Chest X-ray Segmentation with Ensemble Classification for Covid-19 and Pneumonia",
        "abstract": "This paper focuses on using a modified version of the U-Net architecture to conduct segmentation on chest X-rays and then use segmented images for classification to assess the impact on the performance. Respiratory diseases have been known to be a main cause of death worldwide. Pneumonia and Covid-19 are two of the dominant diseases. Several deep learning based studies are available in the literature that classifies infection conditions in chest X-ray images. In addition, image segmentation has been also applied to obtain promising results in deep learning approaches. This paper focuses on using a modified version of the U-Net architecture to conduct segmentation on chest X-rays and then use segmented images for classification to assess the impact on the performance. We achieved an Intersection over Union of 93.53% with the proposed modified U-Net architecture and achieved 99.83% accuracy on segmentation aided ensemble classification.",
        "publication_year": "2022",
        "authors": [
            "Hashara Kumarasinghe",
            "Shammi Kolonne",
            "Chamodi Fernando",
            "D. Meedeniya"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "7",
        "reference_count": 0,
        "references": [
            "/paper/Lung-Chest-X-Ray-Image-Segmentation-for-Detection-Arjuna-Wahab/fc819ca42438d05bb6569e15daba086d161221e4",
            "/paper/CXLSeg-Dataset%3A-Chest-X-ray-with-Lung-Segmentation-Nimalsiri-Hennayake/fd88f6b78781a7ab8f89cf7b8debc69e1bcf25a0",
            "/paper/Chest-X-ray-analysis-empowered-with-deep-learning%3A-Meedeniya-Kumarasinghe/85e8a49a5d656e230a89f24d2e61065d2b57f965",
            "/paper/Computer-Aided-Diagnosis-of-COVID-19-from-Chest-and-Shaheed-Szczuko/be69460abcc58de950bbeb66a0e7bb6a33f81d51",
            "/paper/A-Review-of-Recent-Advances-in-Deep-Learning-Models-Nasser-Akhloufi/7ed2fbad9b2755fdeceaa339eaedd8f89412ad6a",
            "/paper/Automated-Radiology-Report-Generation-Using-Nimalsiri-Hennayake/ae7562a36d1ee7351f2e99e93db8ff708db77489",
            "/paper/Radiomics-Guided-Global-Local-Transformer-for-in-Han-Holste/1e9dcd6dea74ae513c2728fa0b284015f86a67cf"
        ]
    },
    {
        "id": "42b0a8f757e45462e627e57f9af7e9849dcdacdf",
        "title": "UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation",
        "abstract": "UNet++ is proposed, a new neural architecture for semantic and instance segmentation by alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths and redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme. The state-of-the-art models for medical image segmentation are variants of U-Net and fully convolutional networks (FCN). Despite their success, these models have two limitations: (1) their optimal depth is apriori unknown, requiring extensive architecture search or inefficient ensemble of models of varying depths; and (2) their skip connections impose an unnecessarily restrictive fusion scheme, forcing aggregation only at the same-scale feature maps of the encoder and decoder sub-networks. To overcome these two limitations, we propose UNet++, a new neural architecture for semantic and instance segmentation, by (1) alleviating the unknown network depth with an efficient ensemble of U-Nets of varying depths, which partially share an encoder and co-learn simultaneously using deep supervision; (2) redesigning skip connections to aggregate features of varying semantic scales at the decoder sub-networks, leading to a highly flexible feature fusion scheme; and (3) devising a pruning scheme to accelerate the inference speed of UNet++. We have evaluated UNet++ using six different medical image segmentation datasets, covering multiple imaging modalities such as computed tomography (CT), magnetic resonance imaging (MRI), and electron microscopy (EM), and demonstrating that (1) UNet++ consistently outperforms the baseline models for the task of semantic segmentation across different datasets and backbone architectures; (2) UNet++ enhances segmentation quality of varying-size objects\u2014an improvement over the fixed-depth U-Net; (3) Mask RCNN++ (Mask R-CNN with UNet++ design) outperforms the original Mask R-CNN for the task of instance segmentation; and (4) pruned UNet++ models achieve significant speedup while showing only modest performance degradation. Our implementation and pre-trained models are available at https://github.com/MrGiovanni/UNetPlusPlus.",
        "publication_year": "2019",
        "authors": [
            "Zongwei Zhou",
            "M. R. Siddiquee",
            "Nima Tajbakhsh",
            "Jianming Liang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "1,063",
        "reference_count": "65",
        "references": [
            "/paper/TMD-Unet%3A-Triple-Unet-with-Multi-Scale-Input-and-Tran-Cheng/7f575a0661bf6396cdbb92db763ffcd566142d87",
            "/paper/UNet%23%3A-A-UNet-like-Redesigning-Skip-Connections-for-Qian-Zhou/86a25de9047a686099cb8e68c3b0abc8527baea8",
            "/paper/R2U%2B%2B%3A-a-multiscale-recurrent-residual-U-Net-with-Mubashar-Ali/ea53c399e6cb48ca0c5bb5718276a10ab08fc1a6",
            "/paper/MDA-Unet%3A-A-Multi-Scale-Dilated-Attention-U-Net-for-Amer-Lambrou/057159cbdca99b6ad047cd63f8c53904eb07e8ca",
            "/paper/Enhancing-U-Net-with-Spatial-Channel-Attention-Gate-Khanh-Dao/2cffb1d85b1664b8bfc027bae0792f15f06ba73c",
            "/paper/Res2-UNeXt%3A-a-novel-deep-learning-framework-for-Chan-Huang/09b569429158c166f95ce8dca5cf2403a9b9176d",
            "/paper/KiU-Net%3A-Overcomplete-Convolutional-Architectures-Valanarasu-Sindagi/380f9376e00ae9e56c79c1bef7e4e3a10ae75365",
            "/paper/iU-Net%3A-a-hybrid-structured-network-with-a-novel-Jiang-Dong/bca61b46a57a93fa9d8bd9a81f2ab31ff139270c",
            "/paper/ADS_UNet%3A-A-Nested-UNet-for-Histopathology-Image-Yang-Dasmahapatra/3ef64f29f483fe000dae831b72c149d8b866e88a",
            "/paper/ASU-Net%3A-U-shape-adaptive-scale-network-for-mass-in-Sun-Xin/c2fc124b05d522aaa9a0d051659b6f8dae2e789e",
            "/paper/UNet%2B%2B%3A-A-Nested-U-Net-Architecture-for-Medical-Zhou-Siddiquee/a6876ea89e677a7cc42dd43f27165ff6fd414de5",
            "/paper/ErrorNet%3A-Learning-Error-Representations-from-Data-Tajbakhsh-Lai/0c1c17c0e752a41fbceef64b8fd447a80cb58313",
            "/paper/U-Net%3A-Convolutional-Networks-for-Biomedical-Image-Ronneberger-Fischer/6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "/paper/Embracing-Imperfect-Datasets%3A-A-Review-of-Deep-for-Tajbakhsh-Jeyaseelan/374b4b7db9bc7cfff74dd37c8dadccbd67e16f87",
            "/paper/3D-deeply-supervised-network-for-automated-of-Dou-Yu/b31192d40ef06eafc80fef13698ef765bf8e0e68",
            "/paper/3D-Deeply-Supervised-Network-for-Automatic-Liver-CT-Dou-Chen/9e2d1a10e5732c1d9efca5ec72193a3252869b02",
            "/paper/Generalised-Dice-overlap-as-a-deep-learning-loss-Sudre-Li/2d8edc4e38bf9907170238726ec902cb3739393b",
            "/paper/Multiscale-dense-convolutional-neural-network-for-Meng-Sun/be9eb3da822fe7f1369f8be928dfc34656fc5f13",
            "/paper/RefineNet%3A-Multi-path-Refinement-Networks-for-Lin-Milan/de4ee92cfad3734ca820d004bc9ee75fc9dcfbf4",
            "/paper/V-Net%3A-Fully-Convolutional-Neural-Networks-for-Milletar%C3%AC-Navab/50004c086ffd6a201a4b782281aaa930fbfe6ecf"
        ]
    },
    {
        "id": "ff22342bde3dac417070fd3cb47aaf12ce2911cb",
        "title": "Whole-body MRI for detecting metastatic bone tumor: diagnostic value of diffusion-weighted images.",
        "abstract": "WB-MRI that included DWI was useful for detecting bone metastasis and compared it with that of skeletal scintigraphy (SS), which was superior to those of session 1 (sensitivity: 88%; PPV: 95%) and those of SS (s sensitivity: 96%;PPV: 94%). PURPOSE\nWe assessed the diagnostic value of whole body magnetic resonance (MR) imaging (WB-MRI) using diffusion-weighted images (DWI) for detecting bone metastasis and compared it with that of skeletal scintigraphy (SS).\n\n\nMATERIALS AND METHODS\nThirty patients with malignancies (breast cancer, 17 patients; prostate cancer, 9; and one patient each, thyroid cancer, liposarcoma, leiomyosarcoma, and extraskeletal Ewing sarcoma) underwent both WB-MRI and SS to detect bone metastasis. All patients were followed more than 6 months by MR imaging, SS, or computed tomographic (CT) examination. For WB-MRI, patients were placed in feet-first supine position with table-top extender and quadrature body coil. We acquired DWI (axial plane from lower neck to proximal femur) (single shot short TI inversion-recovery [STIR]: repetition time [TR] 6243/echo time [TE] 59/inversion time [TI] 180 ms; b value: 600 s/mm(2); 5-mm slice thickness; 112 x 112 matrix), T(1)-weighted fast spin echo (T(1)WI), and STIR (sagittal plane of total spine images and coronal plane of whole body images) images. Four blinded readers independently and separately interpreted images of combined MR sequences of T(1)WI+STIR (session 1) and T(1)WI+STIR+DWI (session 2).\n\n\nRESULTS\nIn 10 of 30 patients, we detected a total of 52 metastatic bone lesions; in the other 20, follow-up examinations confirmed no metastatic bone lesions. For these 52 lesions, for session 2, the mean sensitivity was 96% and the positive predictive value (PPV) was 98%. Those values were superior to those of session 1 (sensitivity: 88%; PPV: 95%) and those of SS (sensitivity: 96%; PPV: 94%).\n\n\nCONCLUSION\nWB-MRI that included DWI was useful for detecting bone metastasis.",
        "publication_year": "2007",
        "authors": [
            "K. Nakanishi",
            "Midori Kobayashi",
            "K. Nakaguchi",
            "M. Kyakuno",
            "N. Hashimoto",
            "H. Onishi",
            "N. Maeda",
            "S. Nakata",
            "M. Kuwabara",
            "T. Murakami",
            "Hironobu Nakamura"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "181",
        "reference_count": "20",
        "references": [
            "/paper/Role-of-whole-body-diffusion-weighted-MRI-in-bone-Vescovo-Frauenfelder/f540eb8b9cc4467f8fda96f4a44684c7dc17043d",
            "/paper/COMPARISON-OF-WHOLE-BODY-MR-IMAGING-WITH-IMAGES-AND-Nakaguchi-Nakanishi/d0b34df68ff4b6bc956a50c6f14e0b62fb0894ee",
            "/paper/Feasibility-of-whole-body-diffusion-weighted-in-on-Xu-Ma/1d8654f2fd37ebdc9b716434f7efb42d3626e913",
            "/paper/Comparison-of-diffusion-weighted-whole-body-MRI-and-Gutzeit-Doert/fb7535e85b0c96ebe4844e52ee8306ad3f54d88d",
            "/paper/11-C-choline-PET-CT-and-whole-body-MRI-including-Wieder-Beer/bea5d6b54460c99a22c29627dc17412fcf484540",
            "/paper/Molecular-imaging-of-malignant-tumor-metabolism%3A-of-Reiner-Fischer/2b7ee3c0931198b45c58932178504cf58dec5a46",
            "/paper/Bone-metastases-from-prostate%2C-breast-and-multiple-Pearce-Philip/0488d2b772e99c4e2ca37d9516eb6119f16f69e5",
            "/paper/Prostate-carcinoma%3A-diffusion-weighted-imaging-as-Luboldt-K%C3%BCfer/c7efff4fb8202fd11dc224f2d71d9977cc0e64e5",
            "/paper/Detection-of-bone-metastases-using-diffusion-with-Goudarzi-Kishimoto/4912ede2c5a798b23aeb2810d8bb8967eb88ff9c",
            "/paper/Diagnostic-value-of-diffusion-weighted-magnetic-to-Heusner-Kuemmel/a5fd2d31880ce230ce425417681ca6d6a00b3a1b",
            "/paper/Whole-body-MRI-for-detecting-metastatic-bone-tumor%3A-Nakanishi-Kobayashi/5627acec9ecce4c474cea029a22c927050858a26",
            "/paper/%5BComparison-of-whole-body-MR-imaging-and-bone-in-of-Tamada-Nagai/de53c084c3703b876f8d3ec1baf802529191dc5c",
            "/paper/Whole-body-bone-marrow-MRI-in-patients-with-disease-Steinborn-Heuck/6841f9dd6d6d5a9f89b71b3972adcf28debc63b1",
            "/paper/Turbo-STIR-magnetic-resonance-imaging-as-a-tool-for-Walker-Kessar/6a7e00cd00842b325b7f4a9c3cf198a9e27b2a9d",
            "/paper/%5BWhole-body-MRI-in-comparison-to-skeletal-for-of-in-Ghanem-Kelly/1f09d60007f0fad26f602b6a6b7d3291bf3f0a41",
            "/paper/%5BFundamental-study-of-the-detection-of-metastatic-Iizuka-Nagai/5c65fb43854c71b9762a4f418ba6cfb41150c2fc",
            "/paper/Whole-body-MRI-using-a-rolling-table-platform-for-Lauenstein-Freudenberg/ee0ef20c66be05a5c704946d9451e29888e6a4e5",
            "/paper/The-value-of-bone-scintigraphy%2C-bone-marrow-and-in-Haubold-Reuter-Duewell/25d8ba2fbd7b00ca3d5245ad8527e09233e56bd8",
            "/paper/Comparison-of-whole-body-MRI-and-radioisotope-bone-Chan-Chan/a81d58b69244fc30a3972390050eac23d93efc4a",
            "/paper/Comparative-detectability-of-bone-metastases-and-on-Altehoefer-Ghanem/745833eb739e51423c5e76d9ee24b53b330ec2ac"
        ]
    },
    {
        "id": "088977ac55f66421fa87393830409b44dbe207d3",
        "title": "Estimating Skin Tone and Effects on Classification Performance in Dermatology Datasets",
        "abstract": "This paper uses individual typology angle (ITA) to approximate skin tone in dermatology datasets and finds no measurable correlation between performance of machine learning model and ITA values, though more comprehensive data is needed for further validation. Recent advances in computer vision and deep learning have led to breakthroughs in the development of automated skin image analysis. In particular, skin cancer classification models have achieved performance higher than trained expert dermatologists. However, no attempt has been made to evaluate the consistency in performance of machine learning models across populations with varying skin tones. In this paper, we present an approach to estimate skin tone in benchmark skin disease datasets, and investigate whether model performance is dependent on this measure. Specifically, we use individual typology angle (ITA) to approximate skin tone in dermatology datasets. We look at the distribution of ITA values to better understand skin color representation in two benchmark datasets: 1) the ISIC 2018 Challenge dataset, a collection of dermoscopic images of skin lesions for the detection of skin cancer, and 2) the SD-198 dataset, a collection of clinical images capturing a wide variety of skin diseases. To estimate ITA, we first develop segmentation models to isolate non-diseased areas of skin. We find that the majority of the data in the the two datasets have ITA values between 34.5\u00b0 and 48\u00b0, which are associated with lighter skin, and is consistent with under-representation of darker skinned populations in these datasets. We also find no measurable correlation between performance of machine learning model and ITA values, though more comprehensive data is needed for further validation.",
        "publication_year": "2019",
        "authors": [
            "N. M. Kinyanjui",
            "Timothy Odonga",
            "C. Cintas",
            "N. Codella",
            "R. Panda",
            "P. Sattigeri",
            "K. Varshney"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "31",
        "reference_count": "34",
        "references": [
            "/paper/Evaluating-Deep-Neural-Networks-Trained-on-Clinical-Groh-Harris/069b9321f85b6bc005fff3c254d34c791f6c0741",
            "/paper/Improving-Skin-Color-Diversity-in-Cancer-Detection%3A-Rezk-Eltorki/3c24c12ab17deab39b186a9adc5adf9d00efd3c3",
            "/paper/Detecting-Melanoma-Fairly%3A-Skin-Tone-Detection-and-Bevan-Atapour-Abarghouei/963bb07a3c03774c30b7617002b1287d52a57a84",
            "/paper/Enhanced-Dermatoscopic-Skin-Lesion-Classification-Varalakshmi-Devi/94340b56c6bbf049cfa35f95254e62aad6ed51a3",
            "/paper/Leveraging-Artificial-Intelligence-to-Improve-the-Rezk-Eltorki/c86516925bc43b082c016017a6f643b4327ac77e",
            "/paper/Towards-Transparency-in-Dermatology-Image-Datasets-Groh-Harris/2d47fe33b68f36a06e576dee9c13ee38fbd62220",
            "/paper/Out-of-Distribution-Detection-in-Dermatology-Using-Kim-Tadesse/8d3e6cc6e36afc9820cd242799e6563c0370c2fb",
            "/paper/Exploring-strategies-to-generate-Fitzpatrick-skin-Corbin-Marques/6983d87b8c6a668848cbc23ac428bdc9d4e02cb5",
            "/paper/DermSynth3D%3A-Synthesis-of-in-the-wild-Annotated-Sinha-Kawahara/41fc37bdbb53df4b0f99fff4871d30b71f299c73",
            "/paper/Determining-the-clinical-applicability-of-machine-A-Steele-Tan/3b4aef5c352d6301f183388a58e4522130fbb3a1",
            "/paper/Dermatologist-level-classification-of-skin-cancer-Esteva-Kuprel/e1ec11a1cb3d9745fb18d3bf74247f95a6663d08",
            "/paper/A-Benchmark-for-Automatic-Visual-Classification-of-Sun-Yang/cf6d04a970ad89de2f36c64af269e5e1e1f9c9ef",
            "/paper/Deep-learning-ensembles-for-melanoma-recognition-in-Codella-Nguyen/70ec156f7e6de0275c7e4e95e35f1bc1e92e29b3",
            "/paper/The-HAM10000-dataset%2C-a-large-collection-of-images-Tschandl-Rosendahl/f95f5b715eb0441ca4ee1b0fac6b4bcaaba65556",
            "/paper/Skin-Lesion-Analyser%3A-An-Efficient-Seven-Way-Skin-Chaturvedi-Gupta/dace7da474a19f997b598a15803815b0440c4e97",
            "/paper/Skin-Lesion-Analysis-Toward-Melanoma-Detection-A-by-Codella-Rotemberg/1f3ff6ca41574a7136f5fe4925b7bc54189837fb",
            "/paper/Dermoscopy-Image-Analysis%3A-Overview-and-Future-Celebi-Codella/3d06115ef839b18012d2a87c808c2e7aabcf78ac",
            "/paper/Gender-Shades%3A-Intersectional-Accuracy-Disparities-Buolamwini-Gebru/18858cc936947fc96b5c06bbe3c6c2faa5614540",
            "/paper/Man-against-machine%3A-diagnostic-performance-of-a-in-Haenssle-Fink/546785490ac417be1f83ced6a8272e934934f411",
            "/paper/Computerized-analysis-of-pigmented-skin-lesions%3A-A-Korotkov-Garc%C3%ADa/81d66934aeeb79fa701c2bbe804bea02ec931084"
        ]
    },
    {
        "id": "d3e3fb44123cf4c8beff3be49dcd3caeec50f591",
        "title": "Application of artificial intelligence and machine learning for prediction of oral cancer risk.",
        "abstract": "The results demonstrate that the artificial neural network could perform well in estimating the probability of malignancy and improve the positive predictive value that could help to predict the individuals' risk of developing OC based on knowledge of their risk factors, systemic medical conditions, and clinic-pathological data. BACKGROUND\nOral cancer requires early diagnosis and treatment to increase the chances of survival. This study aimed to develop an artificial neural network model that helps to predict the individuals' risk of developing oral cancer based on data on risk factors, systematic medical condition, and clinic-pathological features.\n\n\nMETHODS\nA popular data mining algorithm artificial neural network was used for developing the artificial intelligence-based prediction model. A total of 29 variables that were associated with the patients were used for developing the model. The dataset was randomly split into the training dataset 54 (75%) cases and testing dataset 19 (25%) cases. All records and observations were reviewed by Board-certified oral pathologist.\n\n\nRESULTS\nA total of 73 patients met the eligibility criteria. Twenty-two (30.13%) were benign cases, and 51 (69.86%) were malignant cases. Thirty-seven were female, and 36 were male, with a mean age of 63.09 years. Our analysis displayed that the average sensitivity and specificity of ANN for oral cancer prediction based on the 10-fold cross-validation analysis was 85.71% (95% confidence interval [CI], 57.19-98.22) and 60.00% (95% CI, 14.66-94.73), respectively. The accuracy of ANN for oral cancer prediction was 78.95 % (95% CI, 54.43 -931.95).\n\n\nCONCLUSION\nOur results suggest that this machine learning technique has the potential to help in oral cancer screening and diagnosis based on the data sets. The results demonstrate that the artificial neural network could perform well in estimating the probability of malignancy and improve the positive predictive value that could help to predict the individuals' risk of developing OC based on knowledge of their risk factors, systemic medical conditions, and clinic-pathological data.",
        "publication_year": "2020",
        "authors": [
            "A. Alhazmi",
            "Y. Alhazmi",
            "A. Makrami",
            "A. Masmali",
            "Nourah Salawi",
            "Khulud Masmali",
            "S. Patil"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "20",
        "reference_count": "4",
        "references": [
            "/paper/Application-and-Performance-of-Artificial-in-Oral-A-Khanagar-Naik/6067820b7198e9de211ade6df3ab5a88a3990ceb",
            "/paper/An-Optimized-Predictive-Model-Based-on-Deep-Neural-Sharma-Chadha/7da6a61e08a85b4c7b249352975806cd762fbc2e",
            "/paper/Artificial-intelligence-in-early-diagnosis-and-of-Hegde-Ajila/c75834e99d0715b5fa62ee50cacddbdc2078ef73",
            "/paper/The-Role-of-AI-in-%E2%80%9COral-Cancer%E2%80%9D-Detection%3A-A-Review-Ramya-Minu/9dde976c208d5db195a13d71cd456d3cf485de30",
            "/paper/Artificial-Intelligence-in-the-Diagnosis-of-Oral-Patil-Albogami/9d8297d6e6b7d2d1088604ceb28a5a3ca19d8e66",
            "/paper/Artificial-Intelligence-and-Its-Application-in-the-Nath-Raveendran/3fb01cf932b0becf63f0401b3ef6f3c24c1980fd",
            "/paper/Identification-of-diagnostic-and-prognostic-derived-Wu-Yao/67df642f29bf0c8283d09ec6e507c954d0e5babb",
            "/paper/A-Current-Review-of-Machine-Learning-and-Deep-in-Dixit-Kumar/fb5fcf878c1f390c0eb1b81e30a8672589459117",
            "/paper/A-Web-Based-Prediction-Model-for-Overall-Survival-A-Tang-Wang/f27307dc37ff25e3c4de123082b5b06788551975",
            "/paper/Data-centric-artificial-intelligence-in-oncology%3A-a-Adeoye-Hui/66e46478fc83b19353a5fd67044f2592ff511a9b",
            "/paper/The-Use-of-Artificial-Intelligence-to-Identify-at-%3A-Rosma/fbd2c7bff911f7060b27c3d31c32c5cb58482cea",
            "/paper/Analysis-of-SEER-Dataset-for-Breast-Cancer-using-Anand/0970a38a38aa3c644cc36f3cf9b6f7501cea5a7c",
            "/paper/%5BOral-cavity-cancer%3A-epidemiology-and-early-Ghantous-Yaffi/abe34cadec802d16b7f4f3b65dfcfc473f14430e",
            "/paper/Evaluation-of-screening-strategies-for-improving-a-Kujan-Glenny/0c1c75d9b6dfaa2d8aeaa957122594d4456f8f7c"
        ]
    },
    {
        "id": "6364fdaa0a0eccd823a779fcdd489173f938e91a",
        "title": "U-Net: Convolutional Networks for Biomedical Image Segmentation",
        "abstract": "It is shown that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",
        "publication_year": "2015",
        "authors": [
            "O. Ronneberger",
            "P. Fischer",
            "T. Brox"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "47,313",
        "reference_count": "16",
        "references": [
            "/paper/KiU-Net%3A-Overcomplete-Convolutional-Architectures-Valanarasu-Sindagi/380f9376e00ae9e56c79c1bef7e4e3a10ae75365",
            "/paper/A-Learning-Guided-Hierarchical-Approach-for-Image-Jiang-Sarma/d040f63c7c2b2597bc20fa3cba7afec5241c1cdb",
            "/paper/V-Net%3A-Fully-Convolutional-Neural-Networks-for-Milletar%C3%AC-Navab/50004c086ffd6a201a4b782281aaa930fbfe6ecf",
            "/paper/O-Net%3A-An-Overall-Convolutional-Network-for-Tasks-Maghsoudi-Gastounioti/17de5f657f2fbffe8aa6326fa9adf91839956c1f",
            "/paper/Cell-Segmentation-with-the-U-Net-Convolutional-Danner/d26ad81b3b9baf673c25cdab8a2c40dc7bcfb53f",
            "/paper/KUnet%3A-Microscopy-Image-Segmentation-With-Deep-Unet-Chang-Liao/3572b8194a4b05c444b7babd3f0389b3c4741c27",
            "/paper/DoubleU-Net%3A-A-Deep-Convolutional-Neural-Network-Jha-Riegler/61af9ff6ec3dbb5299455746a1c63588ccf62cf8",
            "/paper/Test-Time-Adaptable-Neural-Networks-for-Robust-Karani-Chaitanya/adc09569ee315835a77515958b2be7733e21bd26",
            "/paper/Dense-Dilated-Deep-Multiscale-Supervised-U-Network-Bose-Chowdhury/f366827f1ae67104c0ed4809cf0b934a8cc43880",
            "/paper/An-Auto-Encoder-Strategy-for-Adaptive-Image-Yu-Iglesias/1f377ecffa9c7b6665c02d36513e2216ea660310",
            "/paper/Fully-convolutional-networks-for-semantic-Shelhamer-Long/317aee7fc081f2b137a85c4f20129007fd8e717e",
            "/paper/Deep-Neural-Networks-Segment-Neuronal-Membranes-in-Ciresan-Giusti/09193e19b59fc8f05bee9d6efbfb1607ca5b6501",
            "/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108",
            "/paper/ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "/paper/Caffe%3A-Convolutional-Architecture-for-Fast-Feature-Jia-Shelhamer/6bdb186ec4726e00a8051119636d4df3b94043b5",
            "/paper/Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue/2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "/paper/Hypercolumns-for-object-segmentation-and-Hariharan-Arbel%C3%A1ez/428db42e86f6d51292e23fa57797e35cecd0e2ee",
            "/paper/Image-Segmentation-with-Cascaded-Hierarchical-and-Seyedhosseini-Sajjadi/4f03888450fde9e8234f616badaae499740e57a4",
            "/paper/Fully-Convolutional-Multi-Class-Multiple-Instance-Pathak-Shelhamer/f341bfc9284972bd7e470082caf3c805a94662b4",
            "/paper/A-benchmark-for-comparison-of-cell-tracking-Ma%C5%A1ka-Ulman/3bffa23a16c273ac2228a13e65dade6766ce7777"
        ]
    },
    {
        "id": "bc612fbe733d428b9086ab95ac95f460b5c4a26f",
        "title": "A Combined Loss-Based Multiscale Fully Convolutional Network for High-Resolution Remote Sensing Image Change Detection",
        "abstract": "A method based on a multiscales fully convolutional neural network (MFCN), which uses multiscale convolution kernels to extract the detailed features of the ground object features and a loss function combining weighted binary cross-entropy (WBCE) loss and dice coefficient loss is proposed, so that the model can be trained from unbalanced samples. In the task of change detection (CD), high-resolution remote sensing images (HRSIs) can provide rich ground object information. However, the interference from noise and complex background information can also bring some challenges to CD. In recent years, deep learning methods represented by convolutional neural networks (CNNs) have achieved good CD results. However, the existing methods have difficulty in detecting the detailed change information of the ground objects effectively. The imbalance of positive and negative samples can also seriously affect the CD results. In this letter, to solve the above problems, we propose a method based on a multiscale fully convolutional neural network (MFCN), which uses multiscale convolution kernels to extract the detailed features of the ground object features. A loss function combining weighted binary cross-entropy (WBCE) loss and dice coefficient loss is also proposed, so that the model can be trained from unbalanced samples. The proposed method was compared with six state-of-the-art CD methods on the DigitalGlobe dataset. The experiments showed that the proposed method can achieve a higher F1-score, and the detection effect of the detailed changes was better than that of the other methods.",
        "publication_year": "2021",
        "authors": [
            "Xinghua Li",
            "Meizhen He",
            "Huifang Li",
            "Huanfeng Shen"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "52",
        "reference_count": "15",
        "references": [
            "/paper/A-transformer-based-Siamese-network-and-an-open-for-Yuan-Zhao/ade59dfcea16d13aa44ca702f59bf93ad979760c",
            "/paper/Adjacent-level-Feature-Cross-Fusion-with-3D-CNN-for-Ye-Wang/eb0ed9c17cb8295eace97d245f9f118773689bf3",
            "/paper/A-Novel-Boundary-Loss-Function-in-Deep-Networks-to-Hosseinpour-Samadzadegan/915ee8a34590b943a09aa7b17de77434db5cf473",
            "/paper/MDESNet%3A-Multitask-Difference-Enhanced-Siamese-for-Zheng-Tian/04e9a06fbbbb7e83fc1f0f0d0f8c2d22e39aa04c",
            "/paper/A-Hyperspectral-Image-Change-Detection-Framework-Ou-Liu/3927bd7cea2a2281e475878835572434ee89ee02",
            "/paper/Spatial%E2%80%93Spectral-Attention-Network-Guided-With-for-Lv-Wang/f8aa6aad38064cb5369c0503cff83d6080768994",
            "/paper/Multiscale-Attention-Network-Guided-With-Change-for-Lv-Zhong/abf645ec0452835fd78fb39f132d619b0088f2ae",
            "/paper/SAUNet3%2BCD%3A-A-Siamese-Attentive-UNet3%2B-for-Change-Mo-Seong/e1183c1fb0241f9c199bf7aee323eca36e51d717",
            "/paper/Remote-Sensing-Image-Change-Detection-Network-Based-Wu-Ma/97614ec585ce6834fc452543bfc37cb6fc05c323",
            "/paper/Calibrated-Focal-Loss-for-Semantic-Labeling-of-Bai-Cheng/01fb399644820a97d94f4f5c907a8f1a82a39e19",
            "/paper/Street-view-change-detection-with-deconvolutional-Alcantarilla-Stent/b02aacf33655cb2d4d1d3a42500180c779d0a0c3",
            "/paper/Detecting-Changes-Between-Optical-Images-of-Spatial-Ferraris-Dobigeon/130fc8bd7688a317a5221a4ce43c2aa44b95b9d0",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Change-Daudt-Saux/1d385debcb66dc259119093e9a390b0870e10037",
            "/paper/Change-Detection-Based-on-IR-MAD-Model-for-GF-5-Xu-Li/b8b2975ba87ccc4de4e2817355e95c27c64967d7",
            "/paper/Slow-Feature-Analysis-for-Change-Detection-in-Wu-Du/549fcde12d071b55f2e0f2a494de491e7fb97140",
            "/paper/CHANGE-DETECTION-IN-REMOTE-SENSING-IMAGES-USING-Lebedev-Vizilter/ae15e5ccccaaff44ab542003386349ef1d3b7511",
            "/paper/Going-deeper-with-convolutions-Szegedy-Liu/e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "/paper/Detecting-Changes-in-Hyperspectral-Imagery-Using-a-Meola-Eismann/045870fede5f8571efdcc5817661691a5b842642",
            "/paper/Adaptive-Change-Detection-With-Significance-Test-Ke-Lin/32bfeb246a9e18665a1690256c9297056e6e17ea",
            "/paper/A-Novel-Approach-to-Unsupervised-Change-Detection-Yan-Xia/083452e95d7e61c97e8e58b2c81d7bc23d8dde4a"
        ]
    },
    {
        "id": "7452f48b5c775b861651560f89bc036a43c7b487",
        "title": "Vibration behavior of a micro cylindrical sandwich panel reinforced by graphene platelet",
        "abstract": "In this article, vibration behavior of a micro cylindrical sandwich panel with foam core and reinforced graphene platelet composite layers on the top and bottom resting on elastic foundation based on modified couple stress theory is investigated. Hamilton\u2019s principle is used to determine the governing equations of motion. These equations are solved by Navier\u2019s method to obtain the natural frequencies. The results are compared with the extracted results by the other literatures. The effects of different parameters such as temperature change, volume fraction of graphene platelet, length to radius ratio, and the elastic foundation on the natural frequencies have been carried out. Also, the effects of reinforced materials for layers is discussed and compared with unreinforced composites layers. Sandwich structures are wildly used in different applications such as spacecraft, aeronautical, pressurized gas tanks, boilers, aircraft fuselage, marines, and civil structures, and these cases need high strength and low weight. The present work is a theoretical background for more explorations and further experimental researchers in the field of cylindrical reinforced panels.",
        "publication_year": "2020",
        "authors": [
            "M. Anvari",
            "M. Mohammadimehr",
            "A. Amiri"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": "8",
        "reference_count": "44",
        "references": [
            "/paper/Vibration-analysis-of-a-micro-cylindrical-sandwich-Amiri-Mohammadimehr/98ad1bb36cb67c74c3b6143e6d9331876d349653",
            "/paper/Free-vibration-analysis-of-cylindrical-sandwich-and-Ghavidel-Alibeigloo/bc6250ebce4f36db1784a17b39e4e9db6642256b",
            "/paper/Geometrically-nonlinear-dynamic-analysis-of-graded-Khayat-Baghlani/0a1d08ae88fe3c6bd4544b18a8e8b6c0d700c625",
            "/paper/Free-vibration-analysis-of-rotating-thin-walled-on-Du-Sun/abe3f5d295a0d38b11ab2d8ca745e72ed4e3cadc",
            "/paper/Sandwich-Engineering-Structures-with-Cellular-Core%3A-Soleimani-Javid-Mourad/8f91f88401dc424c97785b6937eb191e8f33feb4",
            "/paper/The-effect-of-nanoparticles-on-enhancement-of-the-A-Arani-Farazin/da9910aa541b3f06833215c2f07b92be0aeeec94",
            "/paper/Free-vibration-analysis-of-cylindrical-honeycomb-Razgordanisharahi-Ghassabi/13d5ddd64bf3ef74116168ef659b64f9292592f1",
            "/paper/Analytical-solution-for-vibration-and-buckling-of-Keleshteri-Jelovica/754b8fdb6848d8392ac6f6588eec3a4538133eee",
            "/paper/Vibration-analysis-of-pressurized-sandwich-FG-CNTRC-Ansari-Hasrati/75fd81eec7cd3755087f4e66acac14a220cc4b38",
            "/paper/Nonlinear-vibration-of-metal-foam-cylindrical-with-Wang-Ye/73d5664b988d921d7fe61d8bdbf3e925a7dccde9",
            "/paper/Nonlinear-free-vibration-of-functionally-graded-Feng-Kitipornchai/1cc687dac78330ffaf8c166285e7720cfc8f52b3",
            "/paper/Parametric-instability-of-thick-doubly-curved-CNT-Sankar-El-Borgi/4c1f65bb39752a4eaa400875f84186d8b2da886c",
            "/paper/Free-vibration-analysis-of-cylindrical-sandwich-and-Mohammadimehr-Alavi/0310baa2f9ae929ca256640e1d5d39680d971754",
            "/paper/Buckling-and-free-vibration-analyses-of-graded-on-Yang-Chen/52cc2713b991dae071de3e2f4ac5d570c9e7dc15",
            "/paper/Analytical-prediction-of-the-impact-response-of-and-Dong-Zhu/e027bc759d3e909b5a3fe02e99419257809bc088",
            "/paper/Nonlinear-free-vibration-of-graded-graphene-shells%3A-Dong-Zhu/5ebcff19bf684e855ce28a7668c289dadc93543d",
            "/paper/Vibration-characteristics-of-functionally-graded-Dong-Li/bbe275b8bcd55bf861039f2582570db43053a83f",
            "/paper/Vibration-analysis-of-the-embedded-piezoelectric-in-Gharib-Karimi/13995ac62fc61babb8a80e482a0899ec5599619f"
        ]
    },
    {
        "id": "255dfac08c2174e1c8950a1d4e2de060e9fb9f16",
        "title": "on Top-K Dominating Queries on Incomplete Data",
        "abstract": "This work carries out a systematic study of TKD queries on incomplete data, which includes the data having missing dimensional value(s), and introduces an algorithm for answering TkD queries over incomplete data. Data mining is a powerful way to discover knowledge within the large amount of the data. Incomplete data is general, finding out and querying these type of data is important recently. The top-k dominating (TKD) queries return k objects that overrides maximum number of objects in a given dataset. It merges the advantages of skyline and top-k queries. This plays an important role in many decision support applications. Incomplete data holds in real datasets, due to device failure, privacy preservation, data loss. Here, we carry out a systematic study of TKD queries on incomplete data, which includes the data having missing dimensional value(s). We solve this problem, and introduce an algorithm for answering TKD queries over incomplete data. Our methods employ some methods, such as upper bound score pruning, bitmap pruning, and partial score pruning, to hike up the efficiency of queries. Extended experimental evaluation using both real and synthetic datasets shows the effectiveness of the developed pruning rules and confirms performance of algorithms.",
        "publication_year": "2017",
        "authors": [
            "Anoop Sreelekshmi B"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "7",
        "references": [
            "/paper/Top-K-Dominating-Queries-On-Incomplete-Data-%3A-A-Sajeev-Noorjahan/d188025ea680c5317dbd178321b19c8c6e1d404a",
            "/paper/International-Journal-of-Scientific-Research-in-and-Raja/b68e27950ef3767823a0667508e187a72c4695ac",
            "/paper/Efficient-Processing-of-Top-k-Dominating-Queries-on-Yiu-Mamoulis/e1e5936749e383c096e219b9bf85cf756585882b",
            "/paper/Multi-dimensional-top-k-dominating-queries-Yiu-Mamoulis/5118b57f47915d49e9a78e41496383fd9649525e",
            "/paper/Continuous-Top-k-Dominating-Queries-Kontaki-Papadopoulos/caf62f1c21a671e7226078e8287995012a92c14e",
            "/paper/Threshold-based-probabilistic-top-k-dominating-Zhang-Lin/39bdad5c661ec01e665dd9a54c32e8616237651e",
            "/paper/Skyline-Query-Processing-for-Incomplete-Data-Khalefa-Mokbel/54605dbecc088ff126d228b8ea85556806dd75d9",
            "/paper/Processing-k-skyband%2C-constrained-skyline%2C-and-on-Gao-Miao/f65a79f9257c4d2e345ce226dc316f3c69ebe686",
            "/paper/Progressive-skyline-computation-in-database-systems-Papadias-Tao/28e702e1a352854cf0748b9a6a9ad6679b1d4e83"
        ]
    },
    {
        "id": "caf96058774d96dd08df799319ef0d5430121518",
        "title": "SimpleRecon: 3D Reconstruction Without 3D Convolutions",
        "abstract": "This work proposes a simple state-of-the-art multi-view depth estimator with two main contributions: a carefully-designed 2D CNN which utilizes strong image priors alongside a plane-sweep feature volume and geometric losses, combined with the integration of keyframe and geometric metadata into the cost volume which allows informed depth plane scoring. Traditionally, 3D indoor scene reconstruction from posed images happens in two phases: per-image depth estimation, followed by depth merging and surface reconstruction. Recently, a family of methods have emerged that perform reconstruction directly in final 3D volumetric feature space. While these methods have shown impressive reconstruction results, they rely on expensive 3D convolutional layers, limiting their application in resource-constrained environments. In this work, we instead go back to the traditional route, and show how focusing on high quality multi-view depth prediction leads to highly accurate 3D reconstructions using simple off-the-shelf depth fusion. We propose a simple state-of-the-art multi-view depth estimator with two main contributions: 1) a carefully-designed 2D CNN which utilizes strong image priors alongside a plane-sweep feature volume and geometric losses, combined with 2) the integration of keyframe and geometric metadata into the cost volume which allows informed depth plane scoring. Our method achieves a significant lead over the current state-of-the-art for depth estimation and close or better for 3D reconstruction on ScanNet and 7-Scenes, yet still allows for online real-time low-memory reconstruction. Code, models and results are available at https://nianticlabs.github.io/simplerecon",
        "publication_year": "2022",
        "authors": [
            "Mohamed Sayed",
            "J. Gibson",
            "Jamie Watson",
            "V. Prisacariu",
            "Michael Firman",
            "Cl\u00e9ment Godard"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "21",
        "reference_count": "87",
        "references": [
            "/paper/FineRecon%3A-Depth-aware-Feed-forward-Network-for-3D-Stier-Ranjan/e00fe237ff1d0daa918a0a203c94b70e9d14b94b",
            "/paper/CVRecon%3A-Rethinking-3D-Geometric-Feature-Learning-Feng-Yang/c736a949dac2a67219788ab2f48b02068865872a",
            "/paper/MonoNeuralFusion%3A-Online-Monocular-Neural-3D-with-Zou-Huang/e605271aefa33cdf4c77e0ea801cd8e2410fceb0",
            "/paper/VisFusion%3A-Visibility-aware-Online-3D-Scene-from-Gao-Mao/a97cb7fbb308e9a8f020b38e1b091dcfe4763a43",
            "/paper/Heightfields-for-Efficient-Scene-Reconstruction-for-Watson-Vicente/fbd4f728870ce1da8d9b0fb7a18be762fef466b0",
            "/paper/Cross-Dimensional-Refined-Learning-for-Real-Time-3D-Hong-Yue/a3272f80526e0ea32f5985e640fb5a388fbc02e6",
            "/paper/Incremental-Dense-Reconstruction-From-Monocular-Zuo-Yang/fd69a99ef8ac13e7952757ffc92442aa7d5568d1",
            "/paper/DiffRoom%3A-Diffusion-based-High-Quality-3D-Room-and-Ju-Huang/5e1e7afb6d99f865bdfa2d7ce67c262362b9fc60",
            "/paper/LivePose%3A-Online-3D-Reconstruction-from-Monocular-Stier-Angles/1a8379d280cd643ad737c7fb6ba935c3ef39e436",
            "/paper/SurfelNeRF%3A-Neural-Surfel-Radiance-Fields-for-of-Gao-Cao/dde71563f9c4281aaf6a0071c4733f98b557e7ee",
            "/paper/VolumeFusion%3A-Deep-Depth-Fusion-for-3D-Scene-Choe-Im/0ba9140dad37b1b3d8cfcdb6d54389edd701c58d",
            "/paper/3DVNet%3A-Multi-View-Depth-Prediction-and-Volumetric-Rich-Stier/534ff47a5bd665ae38398053a90f7295a0e0b30b",
            "/paper/Atlas%3A-End-to-End-3D-Scene-Reconstruction-from-Murez-As/35e2056e29d5295b2669fe696ae0c27007c94625",
            "/paper/Learning-to-Recover-3D-Scene-Shape-from-a-Single-Yin-Zhang/4ac318309807b5a7a50a1a292ced43e6f5f0ffbb",
            "/paper/VoRTX%3A-Volumetric-3D-Reconstruction-With-for-View-Stier-Rich/a358c152d177ad4ff4161f8822187d4a3e22bc24",
            "/paper/TransformerFusion%3A-Monocular-RGB-Scene-using-Bovzivc-Palafox/64ac6c834bee275eb17f9a34d58ebf18e0f98e6a",
            "/paper/Enforcing-Geometric-Constraints-of-Virtual-Normal-Yin-Liu/b7a2d31bf89ae5d9204dd2ce75db9dfa23e8b1ac",
            "/paper/InfiniTAM-v3%3A-A-Framework-for-Large-Scale-3D-with-Prisacariu-K%C3%A4hler/59c84aed6d9d5e09753aebd913cb284d1e369ea4",
            "/paper/DELTAS%3A-Depth-Estimation-by-Learning-Triangulation-Sinha-Murez/036e93a559ab9cb8e79c77eb2dc0f87edcbf1fb4",
            "/paper/NeuralRecon%3A-Real-Time-Coherent-3D-Reconstruction-Sun-Xie/8db0bf0fa406254d4cd57eb413443a0b96bd12b8"
        ]
    },
    {
        "id": "96350ffbd6e201cf21f509401148ea7674c6e82d",
        "title": "A Discriminative Model for Age Invariant Face Recognition",
        "abstract": "This paper proposes a discriminative model to address face matching in the presence of age variation and shows that this approach outperforms a state-of-the-art commercial face recognition engine on two public domain face aging data sets: MORPH and FG-NET. Aging variation poses a serious problem to automatic face recognition systems. Most of the face recognition studies that have addressed the aging problem are focused on age estimation or aging simulation. Designing an appropriate feature representation and an effective matching framework for age invariant face recognition remains an open problem. In this paper, we propose a discriminative model to address face matching in the presence of age variation. In this framework, we first represent each face by designing a densely sampled local feature description scheme, in which scale invariant feature transform (SIFT) and multi-scale local binary patterns (MLBP) serve as the local descriptors. By densely sampling the two kinds of local descriptors from the entire facial image, sufficient discriminatory information, including the distribution of the edge direction in the face image (that is expected to be age invariant) can be extracted for further analysis. Since both SIFT-based local features and MLBP-based local features span a high-dimensional feature space, to avoid the overfitting problem, we develop an algorithm, called multi-feature discriminant analysis (MFDA) to process these two local feature spaces in a unified framework. The MFDA is an extension and improvement of the LDA using multiple features combined with two different random sampling methods in feature and sample space. By random sampling the training set as well as the feature space, multiple LDA-based classifiers are constructed and then combined to generate a robust decision via a fusion rule. Experimental results show that our approach outperforms a state-of-the-art commercial face recognition engine on two public domain face aging data sets: MORPH and FG-NET. We also compare the performance of the proposed discriminative model with a generative aging model. A fusion of discriminative and generative models further improves the face matching accuracy in the presence of aging.",
        "publication_year": "2011",
        "authors": [
            "Zhifeng Li",
            "U. Park",
            "Anil K. Jain"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "266",
        "reference_count": "57",
        "references": [
            "/paper/Survey-on-Age-Invariant-Facial-Recognition-Kumar-Srivastava/f05c892a9d86558c6b001379798142ac2afd0b28",
            "/paper/Multiview-discriminative-learning-for-age-invariant-Sungatullina-Lu/6bfb0f8dd1a2c0b44347f09006dc991b8a08559c",
            "/paper/SIFT-%26-MRLBP-Descriptors-Based-Age-Invariant-Face-Hina-Jondhale/6aca1bc86a1ddbefcafbfa9c12ce3e198d50b1b7",
            "/paper/Age-invariant-face-recognition-using-multiple-along-Kamarajugadda-Rao/cc6ab3182a5290ea543c751244888300c51c00f4",
            "/paper/Age-invariant-face-recognition-based-on-texture-Yang-Huang/978b32ff990d636f7e2050bb05b8df7dfcbb42a1",
            "/paper/Stable-Local-feature-based-Age-Invariant-Face-Nayak-Indiramma/41f0c18379e186fa0c26b975ff8e9e1b6a975307",
            "/paper/Novel-local-feature-extraction-for-age-invariant-Tripathi-Jalal/6308b2611ce7131398054ff4299286ba65c14de8",
            "/paper/Robust-feature-encoding-for-age-invariant-face-Hou-Ding/59fe66eeb06d1a7e1496a85f7ffc7b37512cd7e5",
            "/paper/Deep-feature-encoding-based-discriminative-model-Shakeel-Lam/3873e7f15fa0b89f36606ec98b36298653b0abae",
            "/paper/Feature-aging-for-age-invariant-face-recognition-Zhou-Wong/8c995bb3483feb3ef11a236f80f8a493387b5829",
            "/paper/Age-Invariant-Face-Recognition-Park-Tong/f2264d05eaf42572bfdd779f72cfadcafcc0c5f3",
            "/paper/Face-recognition-with-learning-based-descriptor-Cao-Yin/6043006467fb3fd1e9783928d8040ee1f1db1f3a",
            "/paper/Automatic-Age-Estimation-Based-on-Facial-Aging-Geng-Zhou/c7584dee6cc077d74b7126e030b68e266fbe11b1",
            "/paper/A-Study-of-Face-Recognition-as-People-Age-Ling-Soatto/d5f1886482893adf2f61e3b344d5483c86792ab5",
            "/paper/Face-Verification-Across-Age-Progression-Using-Ling-Soatto/6c49b1cc03bf415ede597b14eea7a9f9f55fc1d5",
            "/paper/Random-sampling-LDA-for-face-recognition-Wang-Tang/acb9bb0e2046d63bee5289cbfac254dd1d7aa6f9",
            "/paper/Face-recognition-using-multiple-interest-point-and-Fern%C3%A1ndez-Vicente/701d1a2501eac11054225b79fc5cf13fa4745785",
            "/paper/Cross-Age-Face-Recognition-on-a-Very-Large-The-Age-Guo-Mu/4b90d50763546f516dad6db9413dc42b554281ce",
            "/paper/Face-Recognition-with-Local-Binary-Patterns-Julsing/15885415819f2a9db8eee46ea6b6224c759801fa",
            "/paper/Face-recognition-using-sift-features-Geng-Jiang/a9c6a407d1cf119e368e4bc2f1962fc33c04e282"
        ]
    },
    {
        "id": "3a12db3df2918eff7749bafc066907e68e6185a0",
        "title": "KHAI PHA\u0301 HA\u0300M CHI PHI\u0301 CHO PHA\u0301T HIE\u0323\u0302N",
        "abstract": "This study conducts a survey on the efficiency of L1 and IoU loss functions on the same method DETR on the XDUAV dataset to observe the model improvement. Detecting aerial vehicles is a practical problem that solves many traffic problems in the city. However, the existing datasets are collected with multiple angles, the object at a high rotation angle is often very small, which becomes one of the challenges of the problem. In this study, we conduct a survey on the efficiency of L1 and IoU loss functions on the same method DETR on the XDUAV dataset to observe the model improvement. Experimentally, we obtained the highest result of mAP50 = 94.9% when using the Balanced L1 loss function.",
        "publication_year": "2021",
        "authors": [
            "B\u00f9i Cao Doanh",
            "V\u00f5 Duy Nguy\u00ean"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "15",
        "references": [
            "/paper/Data-Augmentation-Analysis-in-Vehicle-Detection-Chung-Le/2a662c841bfd02864de876db19cfa523d493041c",
            "/paper/VisDrone-DET2019%3A-The-Vision-Meets-Drone-Object-in-Zhu-Wen/291a0754ea1e073ef2706dcb4384e5deb034da96",
            "/paper/MOR-UAV%3A-A-Benchmark-Dataset-and-Baselines-for-in-Mandal-Kumar/9bb4cfeaad4c79baa47a1b51d7f4b50103984955",
            "/paper/Detecting-Objects-from-Space%3A-An-Evaluation-of-Nguyen-Huynh/1c59e62eda0fb5ddea6dfb80f5ab3ee3913487e4",
            "/paper/IoU-Loss-for-2D-3D-Object-Detection-Zhou-Fang/a7df9d524b89fb0e2133ef8ad9dab6f165d74c3a",
            "/paper/Distance-IoU-Loss%3A-Faster-and-Better-Learning-for-Zheng-Wang/63a243afcb133569a962c41e9db956c076c5c4f3",
            "/paper/IoU-balanced-Loss-Functions-for-Single-stage-Object-Wu-Li/a075b4f1e78902d4651be154e395b4694e49bb9a",
            "/paper/Enhancing-Geometric-Factors-in-Model-Learning-and-Zheng-Wang/63a06bd5f65a8d822a84905c370260e223cc78b2",
            "/paper/End-to-End-Object-Detection-with-Transformers-Carion-Massa/962dc29fdc3fbdc5930a10aba114050b82fe5a3e",
            "/paper/Generalized-Intersection-Over-Union%3A-A-Metric-and-a-Rezatofighi-Tsoi/889c81b4d7b7ed43a3f69f880ea60b0572e02e27"
        ]
    },
    {
        "id": "3733124d9fedd380e7e0c6b82eef9f2db3344fc3",
        "title": "Determining Three-Dimensional Motion and Structure from Optical Flow Generated by Several Moving Objects",
        "abstract": "A new approach for the interpretation of optical flow fields is presented, where the flow field is partitioned into connected segments of flow vectors, where each segment is consistent with a rigid motion of a roughly planar surface. A new approach for the interpretation of optical flow fields is presented. The flow field, which can be produced by a sensor moving through an environment with several independently moving, rigid objects, is allowed to be sparse, noisy, and partially incorrect. The approach is based on two main stages. In the first stage, the flow field is partitioned into connected segments of flow vectors, where each segment is consistent with a rigid motion of a roughly planar surface. In the second stage, segments are grouped under the hypothesis that they are induced by a single, rigidly moving object. Each hypothesis is tested by searching for three-dimensional (3-D) motion parameters which are compatible with all the segments in the corresponding group. Once the motion parameters are recovered, the relative environmental depth can be estimated as well. Experiments based on real and simulated data are presented.",
        "publication_year": "1985",
        "authors": [
            "Gilad Adiv"
        ],
        "related_topics": [
            "Physics"
        ],
        "citation_count": "914",
        "reference_count": "34",
        "references": [
            "/paper/Three-dimensional-motion-analysis-of-scene-multiple-Yamamoto/8c8031d26553b7394f64686dadf8df289d173b31",
            "/paper/Motion-based-grouping-of-optical-flow-fields%3A-the-Pei-Liou/8fb6241642e7dd3455652ebead057238a04126d5",
            "/paper/Inherent-Ambiguities-in-Recovering-3-D-Motion-and-a-Adiv/7589a6ffd99de0420257914c9b17e641adaa4c04",
            "/paper/3-D-Motion-Estimation-from-Motion-Field-Gupta-Kanal/5c31b47366ea383333bd8bec6f6838b91d06d087",
            "/paper/Detection-of-optical-flow-from-a-noisy-image-Chiba-Ozawa/46be23ddfd827cce81928e040f4519ca4deb9143",
            "/paper/The-computation-of-optical-flow-Beauchemin-Barron/f85b4c5dae9bbb355ed0c349e0b90beb35444e0e",
            "/paper/Recovering-the-Three-Dimensional-Motion-and-of-from-Wang-Duncan/43d78720697711e7d35b3945da528703bd633da2",
            "/paper/Recovery-of-Three-Dimensional-Motion-Parameters-and-Mukai-Ohnishi/8ee7822cad4073762bb8b3884993e8e2ec6bd846",
            "/paper/Independent-3D-motion-detection-using-residual-flow-Lourakis-Argyros/c7051ee031757a811b64a876d2fadbc5a49ed846",
            "/paper/Motion-and-structure-from-prospectively-projected-Mukai-Ohnishi/07a5516a6aa3c537634490e1a6f89857f1c4fca0",
            "/paper/Determining-The-Instantaneous-Direction-Of-Motion-A-Prazdny/18d41a6590bb30fbeb3bafffdd8ab8c408fb0a7a",
            "/paper/Recovering-2-D-Motion-Parameters-in-Scenes-Multiple-Adiv/9039aeb9b935871838cfcf503634bb3f6db6226a",
            "/paper/Determining-the-instantaneous-axis-of-translation-Rieger-Lawton/49df9cb6ffedbfb259e250b634b5803e869a790a",
            "/paper/Passive-navigation-Bruss-Horn/856ea669a402b36e3431bf0aa1e0aa58ffcd30ab",
            "/paper/An-image-flow-paradigm-Waxman/9c9ff47980e233e6654856c7f98872649394216f",
            "/paper/Processing-dynamic-image-sequences-from-a-moving-Lawton/c6d523a69088b45be549f8fee1fac6a3580f52b9",
            "/paper/Uniqueness-and-Estimation-of-Three-Dimensional-of-Tsai-Huang/4935da965dfc4f8824cd61268e77a8dd4b1b5bd5",
            "/paper/Computer-Interpretation-of-a-Dynamic-Image-from-a-Williams/fe199fa24f98082b99e598daed9c855355134531",
            "/paper/Motion-Analysis-via-Local-Translational-Processing.-Lawton/37c0ba13b2311ff16e523b42eed32a765127c41c",
            "/paper/Velocity-determination-in-scenes-containing-several-Fennema-Thompson/67301c286439a7c24368300ea13e9785bd666aed"
        ]
    },
    {
        "id": "0672e63cb2fb7a754e6549598440d3b0c66a002d",
        "title": "Pre-Trained Image Encoder for Generalizable Visual Reinforcement Learning",
        "abstract": "Surprisingly, it is found that the early layers in an ImageNet pre-trained ResNet model could provide rather generalizable representations for visual RL, a simple yet effective framework that can generalize to the unseen visual scenarios in a zero-shot manner. Learning generalizable policies that can adapt to unseen environments remains challenging in visual Reinforcement Learning (RL). Existing approaches try to acquire a robust representation via diversifying the appearances of in-domain observations for better generalization. Limited by the specific observations of the environment, these methods ignore the possibility of exploring diverse real-world image datasets. In this paper, we investigate how a visual RL agent would benefit from the off-the-shelf visual representations. Surprisingly, we find that the early layers in an ImageNet pre-trained ResNet model could provide rather generalizable representations for visual RL. Hence, we propose Pre-trained Image Encoder for Generalizable visual reinforcement learning (PIE-G), a simple yet effective framework that can generalize to the unseen visual scenarios in a zero-shot manner. Extensive experiments are conducted on DMControl Generalization Benchmark, DMControl Manipulation Tasks, Drawer World, and CARLA to verify the effectiveness of PIE-G. Empirical evidence suggests PIE-G improves sample efficiency and significantly outperforms previous state-of-the-art methods in terms of generalization performance. In particular, PIE-G boasts a 55% generalization performance gain on average in the challenging video background setting. Project Page: https://sites.google.com/view/pie-g/home.",
        "publication_year": "2022",
        "authors": [
            "Zhecheng Yuan",
            "Zhengrong Xue",
            "Bo Yuan",
            "Xueqian Wang",
            "Yi Wu",
            "Yang Gao",
            "Huazhe Xu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "11",
        "reference_count": "83",
        "references": [
            "/paper/A-Comprehensive-Survey-of-Data-Augmentation-in-Ma-Wang/336eea1e32e2b93c63f97931710d3de54d05a336",
            "/paper/Normalization-Enhances-Generalization-in-Visual-Li-Lyu/e9258b5024e636627ecb50aa56e22c8c03d42985",
            "/paper/VRL3%3A-A-Data-Driven-Framework-for-Visual-Deep-Wang-Luo/7ed1566a286068f39effa67ae5c7489dbea06414",
            "/paper/Pre-training-Contextualized-World-Models-with-for-Wu-Ma/4d93f5956461d3311122ecdaeaf327618ddb50ba",
            "/paper/For-Pre-Trained-Vision-Models-in-Motor-Control%2C-Not-Hu-Wang/480877d41ef927b901d67b9b7e1ab861564c079f",
            "/paper/On-Pre-Training-for-Visuo-Motor-Control%3A-Revisiting-Hansen-Yuan/7e65d56f71b6a831b8a0385798d14b8c558df6b7",
            "/paper/Simple-Emergent-Action-Representations-from-Policy-Hua-Chen/a78248272e8b87e6f7650fdc29dbb77454c1a745",
            "/paper/Behavior-Contrastive-Learning-for-Unsupervised-Yang-Bai/ac01b6062fec7a726426f9ff8c5b0dfc3f321bd7",
            "/paper/Off-Policy-RL-Algorithms-Can-be-Sample-Efficient-Lyu-Wan/8a84ee6294bc7f88d0343c8615a12f1c763209bd",
            "/paper/Policy-Expansion-for-Bridging-Offline-to-Online-Zhang-Xu/7f270c9b098727675c9d8b893e362b561d61f27e",
            "/paper/SECANT%3A-Self-Expert-Cloning-for-Zero-Shot-of-Visual-Fan-Wang/7ad1b82507b61c7113c4bde17fa3d89bb256cff3",
            "/paper/The-Surprising-Effectiveness-of-Representation-for-Pari-Shafiullah/5e39c030eb7e3ad516cbe1bdcd7b8c4a9e51a6a9",
            "/paper/Curious-Representation-Learning-for-Embodied-Du-Gan/4e9f9e1a69e1924aa9345d77510d43285900f465",
            "/paper/Don't-Touch-What-Matters%3A-Task-Aware-Lipschitz-Data-Yuan-Ma/670472a67a5ab6af73f8a4d452f8eab8ce47160f",
            "/paper/Stabilizing-Deep-Q-Learning-with-ConvNets-and-under-Hansen-Su/390e248117f8bafad8b47b8e799352980c2f3f70",
            "/paper/Unsupervised-Visual-Attention-and-Invariance-for-Wang-Lian/42effe8b3f3c39e83c84f06d2b0e6efb18d8d44f",
            "/paper/The-Unsurprising-Effectiveness-of-Pre-Trained-for-Parisi-Rajeswaran/7b3d26bd1d65ed5937c76043b5cd058260d8469f",
            "/paper/R3M%3A-A-Universal-Visual-Representation-for-Robot-Nair-Rajeswaran/c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204",
            "/paper/Learning-Invariant-Representations-for-Learning-Zhang-McAllister/131007c8d7f79004f52599e9279936c333b3c083",
            "/paper/Learning-to-See-before-Learning-to-Act%3A-Visual-for-Lin-Zeng/995611e2d4f89f5319b77c72bad2726e9d4ec9be"
        ]
    },
    {
        "id": "404bf88bd0194e2ad1de79b4730c70bff5c841d4",
        "title": "BiPointNet: Binary Neural Network for Point Clouds",
        "abstract": "This work discovers that the immense performance drop of binarized models for point clouds is caused by two main challenges: aggregation-induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. To alleviate the resource constraint for real-time point clouds applications that run on edge devices, we present BiPointNet, the first model binarization approach for efficient deep learning on point clouds. In this work, we discover that the immense performance drop of binarized models for point clouds is caused by two main challenges: aggregation-induced feature homogenization that leads to a degradation of information entropy, and scale distortion that hinders optimization and invalidates scale-sensitive structures. With theoretical justifications and in-depth analysis, we propose Entropy-Maximizing Aggregation(EMA) to modulate the distribution before aggregation for the maximum information entropy, andLayer-wise Scale Recovery(LSR) to efficiently restore feature scales. Extensive experiments show that our BiPointNet outperforms existing binarization methods by convincing margins, at the level even comparable with the full precision counterpart. We highlight that our techniques are generic which show significant improvements on various fundamental tasks and mainstream backbones. BiPoint-Net gives an impressive 14.7 times speedup and 18.9 times storage saving on real-world resource-constrained devices.",
        "publication_year": "2020",
        "authors": [
            "Haotong Qin",
            "Zhongang Cai",
            "Mingyuan Zhang",
            "Yifu Ding",
            "Haiyu Zhao",
            "Shuai Yi",
            "Xianglong Liu",
            "Hao Su"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "31",
        "reference_count": "52",
        "references": [
            "/paper/SVNet%3A-Where-SO(3)-Equivariance-Meets-Binarization-Su-Welling/e55498cde3be426504a615a7177391596686f896",
            "/paper/POEM%3A-1-bit-Point-wise-Operations-based-on-for-Xu-Li/05e2ac9ec56f7dc45e44a8e3e93993b6d824685f",
            "/paper/BiRe-ID%3A-Binary-Neural-Network-for-Efficient-Person-Xu-Liu/809edeb47770334d41ee8f908ea30c648cbc8678",
            "/paper/BiBench%3A-Benchmarking-and-Analyzing-Network-Qin-Zhang/f39f01f39813fd8ed791f88ec7378a44dbf2a175",
            "/paper/Distribution-Sensitive-Information-Retention-for-Qin-Zhang/edf09f868f4f0cd23df86889dffb67dc3cee4c57",
            "/paper/Binarizing-Sparse-Convolutional-Networks-for-Point-Xu-Wang/d5b73eb753010a413880b7d3382968d77b2efbc6",
            "/paper/BPT%3A-Binary-Point-Cloud-Transformer-for-Place-Hou-Shang/f6d857f2bdec575e5ec90134537236d5c4f5f99c",
            "/paper/Adversarial-Attacks-on-Aerial-Imagery-%3A-The-and-Kazmi-Aafaq/2f8ee91997d9f5fc65c04c3517e51a1950c879ea",
            "/paper/Semantic-Guided-Fine-grained-Point-Cloud-Framework-Feng-Tang/af29e99ea31003b00a212fbba77dd8fe79190c13",
            "/paper/Model-and-Data-Agreement-for-Learning-with-Noisy-Zhang-Deng/8fd76d11bea60151e117d9ebbadc8b10d21e2e76",
            "/paper/Grid-GCN-for-Fast-and-Scalable-Point-Cloud-Learning-Xu-Sun/4e5185df2e5ef2ee9ea4a32a443b35a65335069a",
            "/paper/SqueezeSegV3%3A-Spatially-Adaptive-Convolution-for-Xu-Wu/07c100ec7b951a36300082fc470f23371953b9b4",
            "/paper/Binary-Neural-Networks%3A-A-Survey-Qin-Gong/bc3706d600729f1b9007c91052258c7c22864f69",
            "/paper/RandLA-Net%3A-Efficient-Semantic-Segmentation-of-Hu-Yang/587110558b1744b912108dd0b74970f131317e0d",
            "/paper/XNOR-Net%2B%2B%3A-Improved-binary-neural-networks-Bulat-Tzimiropoulos/e61b99889e6f811b5c2657a534ad0de468c545a9",
            "/paper/Regularizing-Activation-Distribution-for-Training-Ding-Chin/0cdef47d342cae516f67efd6e4991f4825043b70",
            "/paper/Dynamic-Curriculum-Learning-for-Imbalanced-Data-Wang-Gan/91ca3e7c507745e5d90552274fdbeb8797997979",
            "/paper/PointCNN%3A-Convolution-On-X-Transformed-Points-Li-Bu/6400c36efdb8a66b401b6aef26c057227266fddd",
            "/paper/Bi-Real-Net%3A-Enhancing-the-Performance-of-1-bit-and-Liu-Wu/4bd23d951846832bdf550df574cdec07bc08dec1",
            "/paper/Towards-Accurate-Binary-Convolutional-Neural-Lin-Zhao/8eecf4cf29de06c754f282cf0cb603aa369e6ba1"
        ]
    },
    {
        "id": "e3b914637e63c88eec0721e41eca195921476e94",
        "title": "Exploring global diverse attention via pairwise temporal relation for video summarization",
        "abstract": "Semantic Scholar extracted view of \"Exploring global diverse attention via pairwise temporal relation for video summarization\" by P. Li et al.",
        "publication_year": "2020",
        "authors": [
            "P. Li",
            "Qinghao Ye",
            "Luming Zhang",
            "L. Yuan",
            "Xianghua Xu",
            "Ling Shao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "39",
        "reference_count": "46",
        "references": [
            "/paper/Video-summarization-with-a-graph-convolutional-Li-Tang/059551ea629c70bd2a1939c5bfde74c7f8a39833",
            "/paper/Exploring-Global-Diversity-and-Local-Context-for-Pan-Huang/0e0960b146ed82f5f595ef1d0729f84300157195",
            "/paper/Hierarchical-Multimodal-Transformer-to-Summarize-Zhao-Gong/52f8f58267fd2ab50cf6832be51d539d6606dd65",
            "/paper/Video-Summarization-With-Spatiotemporal-Vision-Hsu-Liao/cf49f64cbdc1ba3959843f8fc5889762a384ab99",
            "/paper/MHSCNet%3A-A-Multimodal-Hierarchical-Shot-aware-for-Xu-Li/e479b716494896947920e9347750d56057557faa",
            "/paper/Summarizing-Videos-using-Concentrated-Attention-and-Apostolidis-Balaouras/544c338e5bb5f208c0360bed188a632ea45f5e0a",
            "/paper/Unsupervised-Video-Summarization-via-Deep-Learning-Yuan-Zhang/47f88c2c6cd82b4ec24b79c292e095cfff14a020",
            "/paper/Multi-Level-Spatiotemporal-Network-for-Video-Yao-Bai/94158b378b2c5f60ec7398c2863cfa9573110075",
            "/paper/A-comprehensive-study-of-automatic-video-techniques-Gupta-Sharma/abe4ea0e1f1d04288406818048bc3f973d930ec9",
            "/paper/Unsupervised-Video-Summarization-with-a-Attentive-Liang-Lv/4881c8c48881f28b7d29226f1a54f38b71481048",
            "/paper/Video-Summarization-With-Attention-Based-Networks-Ji-Xiong/88a8baa1be5292e62622f1cb8e627fbf759bf741",
            "/paper/Hierarchical-Recurrent-Neural-Network-for-Video-Zhao-Li/454e65c2a9b019a00790a1d6029dc5539edad35d",
            "/paper/Discriminative-Feature-Learning-for-Unsupervised-Jung-Cho/69b3b29c6fabaea88d382922346d9157395a3226",
            "/paper/Deep-Reinforcement-Learning-for-Unsupervised-Video-Zhou-Qiao/9e9e5f0c36548cfe2855aae46b519b146aa8c9ae",
            "/paper/Diverse-Sequential-Subset-Selection-for-Supervised-Gong-Chao/25da1b119ba1e0bb602be6ce8492d1e33dbac9ff",
            "/paper/HSA-RNN%3A-Hierarchical-Structure-Adaptive-RNN-for-Zhao-Li/1ea6c0f1bde1f800e3c9c23573325e0d5283b12c",
            "/paper/Video-Summarization-Using-Fully-Convolutional-Rochan-Ye/8c413c2ee66664909d8c194f3f3e08c5f109c3c1",
            "/paper/Summary-Transfer%3A-Exemplar-Based-Subset-Selection-Zhang-Chao/e3db6c8086082acd55f6c95070ad309ecb834517",
            "/paper/Summarizing-Videos-with-Attention-Fajtl-Sokeh/bb48b7b39d86cd5156cda58636088c702be10c50",
            "/paper/Video-Summarization-with-Long-Short-Term-Memory-Zhang-Chao/1dbc12e54ceb70f2022f956aa0a46e2706e99962"
        ]
    },
    {
        "id": "1b3a7e6ebd89a1fb03c992110952d7bddae3345a",
        "title": "Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking",
        "abstract": "By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects. Traditional framework of discriminative correlation filters (DCF) is often subject to undesired boundary effects. Several approaches to enlarge search regions have been already proposed in the past years to make up for this shortcoming. However, with excessive background information, more background noises are also introduced and the discriminative filter is prone to learn from the ambiance rather than the object. This situation, along with appearance changes of objects caused by full/partial occlusion, illumination variation, and other reasons has made it more likely to have aberrances in the detection process, which could substantially degrade the credibility of its result. Therefore, in this work, a novel approach to repress the aberrances happening during the detection process is proposed, i.e., aberrance repressed correlation filter (ARCF). By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects. Considerable experiments are conducted on different UAV datasets to perform object tracking from an aerial view, i.e., UAV123, UAVDT, and DTB70, with 243 challenging image sequences containing over 90K frames to verify the performance of the ARCF tracker and it has proven itself to have outperformed other 20 state-of-the-art trackers based on DCF and deep-based frameworks with sufficient speed for real-time applications.",
        "publication_year": "2019",
        "authors": [
            "Ziyuan Huang",
            "Changhong Fu",
            "Yiming Li",
            "Fuling Lin",
            "Peng Lu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "196",
        "reference_count": "31",
        "references": [
            "/paper/Object-Tracking-Based-on-a-Time-Varying-Regularized-Wang-Jia/dc1485141a67953c51af5a644b1b5107efff2399",
            "/paper/Learning-Aberrance-Repressed-and-Temporal-Filters-Wang-Fan/0b4114ebdfcc42b1b971bdf9030b6296bbae6e3d",
            "/paper/Learning-Augmented-Memory-Joint-Aberrance-Repressed-Ji-He/ecdee88e40c5a6d6a25ee880f1b3a4e733aa123c",
            "/paper/Sparse-Regularized-Correlation-Filter-for-UAV-with-Ji-Feng/215a5017a4f1e85013a5c1d95ed67144cd2c255a",
            "/paper/Learning-spatial-regularized-correlation-filters-Zhang/bffb233b89be225ce9fa6d09de10ae709afd196a",
            "/paper/Disruptor-Aware-Interval-Based-Response-for-Filters-Fu-Ye/5f5f11c405ce58fe378eaeff048cd11a7c540099",
            "/paper/Mutation-Sensitive-Correlation-Filter-for-Real-Time-Zheng-Fu/8f1b398b289388d4a6c726053887a12d34eb9fdb",
            "/paper/Learning-Spatial%E2%80%93Temporal-Background-Aware-Based-Gu-Liu/64863e04ce0a89e448a9bb7f0d8465f50273600b",
            "/paper/Learning-Temporary-Block-Based-Bidirectional-for-Lin-Fu/efd510a4c92606f8713673a9da593a0507b60405",
            "/paper/Spatio-temporal-joint-aberrance-suppressed-filter-Xu-Kim/862d3e899df1a25bf7da9e326586cf715cb515fd",
            "/paper/Boundary-Effect-Aware-Visual-Tracking-for-UAV-with-Fu-Huang/770a74e86e7a39eec441d8af00e37329e247d2c8",
            "/paper/Large-Margin-Object-Tracking-with-Circulant-Feature-Wang-Liu/ece7625a346edbc5f6fab541c0c246ec06939121",
            "/paper/Learning-Spatial-Temporal-Regularized-Correlation-Li-Tian/9f45b55af027503fab557f55f70e81e43c6c1db7",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "/paper/ECO%3A-Efficient-Convolution-Operators-for-Tracking-Danelljan-Bhat/a87cc499cf101b3697cacc65094b4b6590e0d061",
            "/paper/Hierarchical-convolutional-features-for-visual-via-Touil-Terki/e8bf310263240bf5c4fb75033986465346469029",
            "/paper/High-Speed-Tracking-with-Kernelized-Correlation-Henriques-Caseiro/65c9b4b1d49f46b3f8f64a5f617acfc14f85d031",
            "/paper/Multi-cue-Correlation-Filters-for-Robust-Visual-Wang-Zhou/a5278fc76eff08668bc1957b01b22eb627fa2c36",
            "/paper/Hierarchical-Convolutional-Features-for-Visual-Ma-Huang/5c8a6874011640981e4103d120957802fa28f004"
        ]
    },
    {
        "id": "311bc4e48838d8e5ef619df3ce0bc598aba788a1",
        "title": "Convolutional Features for Correlation Filter Based Visual Tracking",
        "abstract": "The results suggest that activations from the first layer provide superior tracking performance compared to the deeper layers, and show that the convolutional features provide improved results compared to standard hand-crafted features. Visual object tracking is a challenging computer vision problem with numerous real-world applications. This paper investigates the impact of convolutional features for the visual tracking problem. We propose to use activations from the convolutional layer of a CNN in discriminative correlation filter based tracking frameworks. These activations have several advantages compared to the standard deep features (fully connected layers). Firstly, they miti-gate the need of task specific fine-tuning. Secondly, they contain structural information crucial for the tracking problem. Lastly, these activations have low dimensionality. We perform comprehensive experiments on three benchmark datasets: OTB, ALOV300++ and the recently introduced VOT2015. Surprisingly, different to image classification, our results suggest that activations from the first layer provide superior tracking performance compared to the deeper layers. Our results further show that the convolutional features provide improved results compared to standard hand-crafted features. Finally, results comparable to state-of-the-art trackers are obtained on all three benchmark datasets.",
        "publication_year": "2015",
        "authors": [
            "Martin Danelljan",
            "Gustav H\u00e4ger",
            "F. Khan",
            "M. Felsberg"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "858",
        "reference_count": "42",
        "references": [
            "/paper/Adaptive-Weighted-CNN-Features-Integration-for-Li-Yang/66afdc8abaca12423dc6ee521f53e6bf856afd78",
            "/paper/Deep-Correlation-Filter-based-Real-Time-Tracker-Pu-Feng/53d7df04b1601b2d1f0ae1ad0651083ba8e57955",
            "/paper/Deep-GoogLeNet-Features-for-Visual-Object-Tracking-Aswathy-Siddhartha/f8ef9aa9e87522003d85bb529fe2de230614a48a",
            "/paper/Collaborative-Learning-based-on-Convolutional-and-Wibowo-Lee/93664b47db2e1042993bebaa711467bbd76fa3d8",
            "/paper/Collaborative-Learning-based-on-Convolutional-and-Wibowo-Lee/00e17efe147a8f850dea6f0a4b45f9933db4ff13",
            "/paper/High-Performance-Visual-Object-Tracking-with-Zhu-Zou/367d0aaebdb072f243b5093c49298501eb5c0bd4",
            "/paper/Deep-motion-features-for-visual-tracking-Gladh-Danelljan/2e7e3b4eb8bc0a7f29ca560b1cceb986a1dcd977",
            "/paper/Correlation-Filters-with-Weighted-Convolution-He-Fan/a102c524b2bdc3c1f9377a5ea079d47783d636c6",
            "/paper/Coarse-to-Fine-Object-Tracking-Using-Deep-Features-Zgaren-Bouachir/0e1962c41dec78b641f5564e565e8ef683e32b15",
            "/paper/Efficient-object-tracking-using-hierarchical-model-Abbass-Kwon/588d4467f4324f211c8d981eefe4797c03227442",
            "/paper/DeepTrack%3A-Learning-Discriminative-Feature-by-for-Li-Li/1b3a107739e7f7e05c50999a3d79b8225746f662",
            "/paper/Learning-a-Deep-Compact-Image-Representation-for-Wang-Yeung/b2180fc4f5cb46b5b5394487842399c501381d67",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Adaptive-Color-Attributes-for-Real-Time-Visual-Danelljan-Khan/f5dbe4550d24d5374d9e10fce44a35b105c7ee07",
            "/paper/Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue/2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "/paper/An-ensemble-of-deep-neural-networks-for-object-Zhou-Xie/e6824bc799efcf5cb854591474c92cdbb5716e32",
            "/paper/A-Scale-Adaptive-Kernel-Correlation-Filter-Tracker-Li-Zhu/0cae491292feccbc9ad1d864cf8b7144923ce6de",
            "/paper/Human-Tracking-Using-Convolutional-Neural-Networks-Fan-Xu/5b4b43f10c5779d67ccee15d8d0be10ed036971b",
            "/paper/CNN-Features-Off-the-Shelf%3A-An-Astounding-Baseline-Razavian-Azizpour/6270baedeba28001cd1b563a199335720d6e0fe0",
            "/paper/Transfer-Learning-Based-Visual-Tracking-with-Gao-Ling/9ea03a9cb11bdc68ac2f56e290c8486868511476"
        ]
    },
    {
        "id": "10d2a56557a3760f1d14fe6869e0cad76f0a24f4",
        "title": "Learning Features for Tracking",
        "abstract": "Contrary to existing approaches, this paper is able to start tracking of the object from scratch requiring no off-line training phase before tracking and can be used for real-time applications since on-line updating and evaluating classifiers can be done efficiently. We treat tracking as a matching problem of detected key-points between successive frames. The novelty of this paper is to learn classifier-based keypoint descriptions allowing to incorporate background information. Contrary to existing approaches, we are able to start tracking of the object from scratch requiring no off-line training phase before tracking. The tracker is initialized by a region of interest in the first frame. Afterwards an on-line boosting technique is used for learning descriptions of detected keypoints lying within the region of interest. New frames provide new samples for updating the classifiers which increases their stability. A simple mechanism incorporates temporal information for selecting stable features. In order to ensure correct updates a verification step based on estimating homographies using RANSAC is performed. The approach can be used for real-time applications since on-line updating and evaluating classifiers can be done efficiently.",
        "publication_year": "2007",
        "authors": [
            "Michael Grabner",
            "H. Grabner",
            "H. Bischof"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "105",
        "reference_count": "27",
        "references": [
            "/paper/A-new-framework-for-on-line-object-tracking-based-Miao-Wang/75a3f4ba22c36f6dccfdb469cc6b34d4790e090d",
            "/paper/TLM%3A-Tracking-learning-matching-of-keypoints-Nebehay-Pflugfelder/d94a10539b06e7f8cba2ba25ac74fe9f063722c4",
            "/paper/Semi-supervised-On-Line-Boosting-for-Robust-Grabner-Leistner/27d69a2d96600efb66fd907d8287ca3b6e734c59",
            "/paper/Visual-tracking-based-on-object-appearance-and-Wang-Duan/cd5a981783faf9f65617f77a67e7bc5e143d7fe5",
            "/paper/Online-Boosting-Based-Object-Tracking-Binh/d654c2f5b500ca3d561c356ac24121eedbcbc820",
            "/paper/Consensus-based-matching-and-tracking-of-keypoints-Nebehay-Pflugfelder/894767a3911ce9295844579380b4a727f7a2a0bf",
            "/paper/On-Line-Rigid-Object-Tracking-via-Discriminative-Miao-Shi/d870399b106983ecdb248a165c7117593ac2a42b",
            "/paper/Real-time-keypoint-based-object-tracking-via-online-Guo-Liu/390faba76c076e6bf8db4b0850366bd084be257a",
            "/paper/Scale-and-rotation-invariant-feature-based-object-Miao-Wang/cedd1a11148e99a569edaeea59429ee3b532fd58",
            "/paper/Kernel-Based-On-Line-Object-Tracking-Combining-both-Miao-Wang/ac920e31480cf3eedf36bfd8d651127f35efdc83",
            "/paper/Real-Time-Tracking-via-On-line-Boosting-Grabner-Grabner/460a435a492107e9c8ec23ae8e79b0420de619e9",
            "/paper/Feature-Harvesting-for-Tracking-by-Detection-%C3%96zuysal-Lepetit/59d710f21659460f6c368d2d50bcf47ea8b45690",
            "/paper/Tracking-Aspects-of-the-Foreground-against-the-Nguyen-Smeulders/0e8d058a657d3fa294865ccf5e13f043e278b3a0",
            "/paper/Incremental-Learning-for-Visual-Tracking-Lim-Ross/14ee8cf76e31fdf4381d29c1fb61017beb52f672",
            "/paper/Good-features-to-track-Shi-Tomasi/2ab46391005cea85fa5c204b6e77a9c870fdbaed",
            "/paper/Ensemble-Tracking-Avidan/82c0dba2a7a175113050a6c0dbd409c93cc48996",
            "/paper/Sparse-Bayesian-learning-for-efficient-visual-Williams-Blake/5029362f4e0966d21a2ebf788bfadb5ae55697aa",
            "/paper/Online-selection-of-discriminative-tracking-Collins-Liu/9c2f13c1fe9d8b894c62b4037491605cf8e45b89",
            "/paper/Point-matching-as-a-classification-problem-for-fast-Lepetit-Pilet/33a5206f36ed1b2433517fc5001edd96d9da32bd",
            "/paper/Robust-online-appearance-models-for-visual-tracking-Jepson-Fleet/4411f262853bf7f1eb8e2efe03eb0402f5e9ad2c"
        ]
    },
    {
        "id": "a0274068d613e389a4f14277b49eb879b81c54ef",
        "title": "Diverse Video Generation from a Single Video",
        "abstract": "GANs are able to perform generation and manipulation tasks, trained on a single video. However, these single video GANs require unreasonable amount of time to train on a single video, rendering them almost impractical. In this paper we question the necessity of a GAN for generation from a single video, and introduce a non-parametric baseline for a variety of generation and manipulation tasks. We revive classical space-time patches-nearest-neighbors approaches and adapt them to a scalable unconditional generative model, without any learning. This simple baseline surprisingly outperforms single-video GANs in visual quality and realism (confirmed by quantitative and qualitative evaluations), and is disproportionately faster (runtime reduced from several days to seconds). Our approach is easily scaled to Full-HD videos. We also use the same framework to demonstrate video analogies and spatio-temporal retargeting. These observations show that classical approaches significantly outperform heavy deep learning machinery for these tasks. This sets a new baseline for single-video generation and manipulation tasks, and no less important -- makes diverse generation from a single video practically possible for the first time.",
        "publication_year": "2022",
        "authors": [
            "Niv Haim",
            "Ben Feinstein",
            "Niv Granot",
            "Assaf Shocher",
            "Shai Bagon",
            "Tali Dekel",
            "M. Irani"
        ],
        "related_topics": [
            "Physics"
        ],
        "citation_count": 0,
        "reference_count": "51",
        "references": [
            "/paper/Hierarchical-Patch-VAE-GAN%3A-Generating-Diverse-from-Gur-Benaim/1a3284e3c7bc58a5f453e6573d9107bfb3686b9e",
            "/paper/ImaGINator%3A-Conditional-Spatio-Temporal-GAN-for-Wang-Bilinski/493e753062f82e36b9c24ec48a13dd4b424f65a3",
            "/paper/Few-shot-Video-to-Video-Synthesis-Wang-Liu/bea92a69b48287caa3a25ff3dfe727bed8888348",
            "/paper/Video-to-Video-Synthesis-Wang-Liu/c5b55f410365bb889c25a9f0354f2b86ec61c4f0",
            "/paper/Drop-the-GAN%3A-In-Defense-of-Patches-Nearest-as-Granot-Shocher/f6571aed926ba5be4d1d307c29e54a2909d8eed0",
            "/paper/Generating-Videos-with-Scene-Dynamics-Vondrick-Pirsiavash/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1",
            "/paper/MetaPix%3A-Few-Shot-Video-Retargeting-Lee-Ramanan/b23d8546b29cf67e61798ca139c4426ed54855e5",
            "/paper/World-Consistent-Video-to-Video-Synthesis-Mallya-Wang/633a7a0c92dd8c254b65a11b759b99c157e115c7",
            "/paper/MoCoGAN%3A-Decomposing-Motion-and-Content-for-Video-Tulyakov-Liu/e76edb86f270c3a77ed9f5a1e1b305461f36f96f",
            "/paper/TransMoMo%3A-Invariance-Driven-Unsupervised-Video-Yang-Zhu/caa0d7e51edd936b7b35d0ccbd6db1ee0ac5220b"
        ]
    },
    {
        "id": "27d52bf3265bea0f9929980f6ffb4c2009eecfee",
        "title": "Ocean: Object-aware Anchor-free Tracking",
        "abstract": "This paper introduces a feature alignment module to learn an object-aware feature from predicted bounding boxes that can further contribute to the classification of target objects and background and presents a novel tracking framework based on the anchor-free model. Anchor-based Siamese trackers have achieved remarkable advancements in accuracy, yet the further improvement is restricted by the lagged tracking robustness. We find the underlying reason is that the regression network in anchor-based methods is only trained on the positive anchor boxes (i.e., $IoU \\geq0.6$). This mechanism makes it difficult to refine the anchors whose overlap with the target objects are small. In this paper, we propose a novel object-aware anchor-free network to address this issue. First, instead of refining the reference anchor boxes, we directly predict the position and scale of target objects in an anchor-free fashion. Since each pixel in groundtruth boxes is well trained, the tracker is capable of rectifying inexact predictions of target objects during inference. Second, we introduce a feature alignment module to learn an object-aware feature from predicted bounding boxes. The object-aware feature can further contribute to the classification of target objects and background. Moreover, we present a novel tracking framework based on the anchor-free model. The experiments show that our anchor-free tracker achieves state-of-the-art performance on five benchmarks, including VOT-2018, VOT-2019, OTB-100, GOT-10k and LaSOT. The source code is available at this https URL.",
        "publication_year": "2020",
        "authors": [
            "Zhipeng Zhang",
            "Houwen Peng"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "286",
        "reference_count": "52",
        "references": [
            "/paper/SiamCorners%3A-Siamese-Corner-Networks-for-Visual-Yang-He/4b6a190bb897a626639849c09426d30d5a1462c8",
            "/paper/Towards-Accurate-Pixel-wise-Object-Tracking-by-Zhang-Li/171c292989d4d34163b59bd5e5cc1a383db9c0ab",
            "/paper/Target-Transformed-Regression-for-Accurate-Tracking-Cui-Jiang/0841aef61d9459d5809a2b9d2230a7a433979d80",
            "/paper/TDIOT%3A-Target-Driven-Inference-for-Deep-Video-Gurkan-Cerkezi/c140f878939f3ad2632372792f5aa8defd13a089",
            "/paper/Robust-Template-Adjustment-Siamese-Network-for-Tang-Qin/f01d53503cd27b64f512ec9ef57da4e42a770cd4",
            "/paper/Learning-Target-Candidate-Association-to-Keep-Track-Mayer-Danelljan/fb058786bbcb2cead98a3ef55b33d2b73b2119fc",
            "/paper/STMTrack%3A-Template-free-Visual-Tracking-with-Memory-Fu-Liu/811ffb185bc90ac5d02d6dbfbcdb6173756b52ef",
            "/paper/CRACT%3A-Cascaded-Regression-Align-Classification-for-Fan-Ling/6e0442456b3a475e1d836d7e345fdce98ef5ad26",
            "/paper/Learning-Spatio-Temporal-Transformer-for-Visual-Yan-Peng/72af9b2e03d3668e09edd0ec413b0b20cbce8f9c",
            "/paper/SiamCAN%3A-Real-Time-Visual-Tracking-Based-on-Siamese-Zhou-Wen/aad19f38536f29472d0a78be554a006199445dba",
            "/paper/ATOM%3A-Accurate-Tracking-by-Overlap-Maximization-Danelljan-Bhat/d74169a8fd2f90a06480d1d583d0ae5e980ea951",
            "/paper/Tracking-the-Known-and-the-Unknown-by-Leveraging-Tripathi-Danelljan/b70640b98c96b024c22de3c8b13c1df0d384b6de",
            "/paper/Meta-Tracker%3A-Fast-and-Robust-Online-Adaptation-for-Park-Berg/50c60583dc0ef09484358deab329f82ee22c2b66",
            "/paper/Siamese-Instance-Search-for-Tracking-Tao-Gavves/c316d5ec14e5768d7eda3d8916bddc1de142a1c2",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Fully-Convolutional-Siamese-Networks-for-Object-Bertinetto-Valmadre/29d1b9a6e6ff0a4216d10dd31376467d55e788a3",
            "/paper/GradNet%3A-Gradient-Guided-Network-for-Visual-Object-Li-Chen/47a58f8bec1d34004a7d7cf837e27a26de64f0f7",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/CenterNet%3A-Keypoint-Triplets-for-Object-Detection-Duan-Bai/52e7190540745960333ab483623099edf1641257",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8"
        ]
    },
    {
        "id": "1e9dcd6dea74ae513c2728fa0b284015f86a67cf",
        "title": "Radiomics-Guided Global-Local Transformer for Weakly Supervised Pathology Localization in Chest X-Rays",
        "abstract": "A Radiomics-Guided Transformer (RGT) that fuses global image information with local radiomics-guided auxiliary information to provide accurate cardiopulmonary pathology localization and classification without any bounding box annotations is proposed. Before the recent success of deep learning methods for automated medical image analysis, practitioners used handcrafted radiomic features to quantitatively describe local patches of medical images. However, extracting discriminative radiomic features relies on accurate pathology localization, which is difficult to acquire in real-world settings. Despite advances in disease classification and localization from chest X-rays, many approaches fail to incorporate clinically-informed domainspecific radiomic features. For these reasons, we propose a Radiomics-Guided Transformer (RGT) that fuses global image information with local radiomics-guided auxiliary information to provide accurate cardiopulmonary pathology localization and classification without any bounding box annotations. RGT consists of an image Transformer branch, a radiomics Transformer branch, and fusion layers that aggregate image and radiomics information. Using the learned self-attention of its image branch, RGT extracts a bounding box for which to compute radiomic features, which are further processed by the radiomics branch; learned image and radiomic features are then fused and mutually interact via cross-attention layers. Thus, RGT utilizes a novel end-to-end feedback loop that can bootstrap accurate pathology localization only using image-level disease labels. Experiments on the NIH ChestXRay dataset demonstrate that RGT outperforms prior works in weakly supervised disease localization (by an average margin of 3.6% over various intersection-over-union thresholds) and classification (by 1.1% in average area under the receiver operating characteristic curve). We publicly release our codes and pre-trained models at https://github.com/VITAGroup/chext.",
        "publication_year": "2022",
        "authors": [
            "Yan Han",
            "G. Holste",
            "Ying Ding",
            "Ahmed Tewfik",
            "Yifan Peng",
            "Zhangyang Wang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "72",
        "references": [
            "/paper/Artificial-Special-Visual-Geometry-Group-16(VGG)-of-Chary-Venkateswarlu/c4d60d547c5bd5d16c916dcdb40ab0d9503230dc",
            "/paper/Using-Radiomics-as-Prior-Knowledge-for-Thorax-and-Han-Chen/2eddea78a5fa414b2130cad29fb234fcc1b86a72",
            "/paper/Anatomy-Guided-Weakly-Supervised-Abnormality-in-Yu-Ghosh/8dd37abe52963c707d3076d920e74689315447f3",
            "/paper/Align%2C-Attend-and-Locate%3A-Chest-X-Ray-Diagnosis-via-Liu-Zhao/dcbeb0f0a7d3f56071cc23cc90bbc479a63928e5",
            "/paper/Pneumonia-Detection-On-Chest-X-Ray-Using-Radiomic-Han-Chen/cfd30c4cd51e338e1e1aa429592018e152787b5b",
            "/paper/Disease-Localization-and-Severity-Assessment-in-Chandra-Singh/7a29d754ea83690d9777d92882c97b457bb37033",
            "/paper/Knowledge-Distillation-with-Adaptive-Asymmetric-for-Wang-Zheng/4fef16d63c40048963088efc0d3de9315270280a",
            "/paper/Medical-Transformer%3A-Gated-Axial-Attention-for-Valanarasu-Oza/1e5e8106700c8dbdfa036a5a9be5e61e06c0ed02",
            "/paper/Localization-with-Limited-Annotation-for-Chest-Rozenberg-Freedman/3ace9c3b4f5155b0181c9f5b9b21aadc3480d7b7",
            "/paper/Boosted-Cascaded-Convnets-for-Multilabel-of-in-Kumar-Grewal/0fe2adeb08f1dbdeb2a968f031bcf7dc71d8cd9d",
            "/paper/TransUNet%3A-Transformers-Make-Strong-Encoders-for-Chen-Lu/24b8a0b02bcb7934967757fc59d273a71ba67e30"
        ]
    },
    {
        "id": "c2fc124b05d522aaa9a0d051659b6f8dae2e789e",
        "title": "ASU-Net: U-shape adaptive scale network for mass segmentation in mammograms",
        "abstract": "The proposed ASU-Net is a deep learning model for mammogram segmentation that contains two modules: adaptive scale module (ASM) and feature refinement module (FRM), which makes the network adaptively capture multi-scale features. U-Net is a commonly used deep learning model for mammogram segmentation. Despite outstanding overall performance in segmenting, U-Net still faces from two aspects of challenges: (1) the skip-connections in U-Net have limitations, which may not be able to effectively extract multi-scale features for breast masses with diverse shapes and sizes. (2) U-Net only merges low-level spatial information and high-level semantic information through concatenating, which neglects interdependencies between channels. To address these two problems, we propose the U-shape adaptive scale network (ASU-Net), which contains two modules: adaptive scale module (ASM) and feature refinement module (FRM). In each level of skip-connections, ASM is used to adaptively adjust the receptive fields according to the different scales of the mass, which makes the network adaptively capture multi-scale features. Besides, FRM is employed to allows the decoder to capture channel-wise dependencies, which make the network can selectively emphasize the feature representation of useful channels. Two commonly used mammogram databases including the DDSM-BCRP database and the INbreast database are used to evaluate the segmentation performance of ASU-Net. Finally, ASU-Net obtains the Dice Index (DI) of 91.41% and 93.55% in the DDSM-BCRP database and the INbreast database, respectively.",
        "publication_year": "2021",
        "authors": [
            "Kexin Sun",
            "Yuelan Xin",
            "Yide Ma",
            "Meng Lou",
            "Yunliang Qi",
            "Jie Zhu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "47",
        "references": [
            "/paper/UNet%2B%2B%3A-Redesigning-Skip-Connections-to-Exploit-in-Zhou-Siddiquee/42b0a8f757e45462e627e57f9af7e9849dcdacdf",
            "/paper/Adversarial-deep-structured-nets-for-mass-from-Zhu-Xiang/39fc5bbed62308da62a34c896b685e70cef31cd3",
            "/paper/Improved-Breast-Mass-Segmentation-in-Mammograms-Li-Chen/eb5f164a5f3b9238831f186219169f4da0b565d4",
            "/paper/A-Deep-Dual-path-Network-for-Improved-Mammogram-Li-Chen/13ca62cce7935411231d333a140cbb0894f0918d",
            "/paper/Deep-structured-learning-for-mass-segmentation-from-Dhungel-Carneiro/ab0ef1360df100dfb41953d836814a908590064a",
            "/paper/U-Net%3A-Convolutional-Networks-for-Biomedical-Image-Ronneberger-Fischer/6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "/paper/DeepLab%3A-Semantic-Image-Segmentation-with-Deep-and-Chen-Papandreou/cab372bc3824780cce20d9dd1c22d4df39ed081a",
            "/paper/Tree-RE-weighted-belief-propagation-using-deep-for-Dhungel-Carneiro/cbf80ca27355db47c6d5bcfdd9a6a92ee065dbf3",
            "/paper/Encoder-Decoder-with-Atrous-Separable-Convolution-Chen-Zhu/9217e28b2273eb3b26e4e9b7b498b4661e6e09f5",
            "/paper/SegNet%3A-A-Deep-Convolutional-Encoder-Decoder-for-Badrinarayanan-Kendall/b0c065cd43aa7280e766b5dcbcc7e26abce59330"
        ]
    },
    {
        "id": "a5fd2d31880ce230ce425417681ca6d6a00b3a1b",
        "title": "Diagnostic value of diffusion-weighted magnetic resonance imaging (DWI) compared to FDG PET/CT for whole-body breast cancer staging",
        "abstract": "DWI seems to be a sensitive but unspecific modality for the detection of locoregional or metastatic BC disease and DWI alone may not be recommended as a whole-body staging alternative to FDG PET(/CT). PurposeThe aim of the study was to prospectively compare the diagnostic value of whole-body diffusion-weighted imaging (DWI) and FDG PET/CT for breast cancer (BC) staging.MethodsTwenty BC patients underwent whole-body FDG PET/CT and 1.5-T DWI. Lesions with qualitatively elevated signal intensity on DW images (b\u2009=\u2009800\u00a0s/mm2) were rated as suspicious for tumour and mapped to individual lesions and different compartments (overall 552 lesions). The apparent diffusion coefficient (ADC) value was determined for quantitative evaluation. Histopathology, MRI findings, bone scan findings, concordant findings between FDG PET/CT and DWI, CT follow-up scans and plausibility served as the standards of reference defining malignancy.ResultsAccording to the standards of reference, breasts harboured malignancy in 11, regional lymph nodes in 4, M1 lymph nodes in 3, bone in 7, lung in 2, liver in 3 and other tissues in 3 patients. On a compartment basis, the sensitivity, specificity, accuracy, positive predictive value (PPV) and negative predictive value (NPV) for the detection of malignancies were 94, 99, 98, 97 and 98% for FDG PET/CT and 91, 72, 76, 50 and 96% for DWI, respectively. Of the lesions seen on DWI only, 348 (82%) turned out to be false-positive compared to 23 (11%) on FDG PET/CT. The average lesion ADC was 820\u2009\u00b1\u2009300 with true-positive lesions having 929\u2009\u00b1\u2009252 vs 713\u2009\u00b1\u2009305 in false-positive lesions (p\u2009<\u20090.0001).ConclusionBased on these initial data DWI seems to be a sensitive but unspecific modality for the detection of locoregional or metastatic BC disease. There was no possibility to quantitatively distinguish lesions using ADC. DWI alone may not be recommended as a whole-body staging alternative to FDG PET(/CT). Further studies are necessary addressing the question of whether full-body MRI including DWI may become an alternative to FDG PET/CT for whole-body breast cancer staging.",
        "publication_year": "2010",
        "authors": [
            "T. Heusner",
            "S. Kuemmel",
            "A. Koeninger",
            "M. Hamami",
            "S. Hahn",
            "A. Quinsten",
            "A. Bockisch",
            "M. Forsting",
            "T. Lauenstein",
            "G. Antoch",
            "A. Stahl"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "145",
        "reference_count": "54",
        "references": [
            "/paper/The-Comparison-of-Whole-Body-Diffusion-MRI-with-in-Akta%C5%9F-Akt%C3%BCrk/ed79b3f0d185e70aad52f7bd49edf1b5ac247770",
            "/paper/Staging-performance-of-whole-body-DWI%2C-PET-CT-and-Catalano-Daye/2148e4e5855da094a3dda0fabfa665cb6ff169b6",
            "/paper/Diagnostic-Performance-of-Whole-Body-Imaging-to-MRI-Usuda-Sagawa/2abe8fdb9b14dd19612eef88173a5ae3ddd34b4a",
            "/paper/Diffusion-weighted-imaging-as-part-of-hybrid-PET-it-Buchbender-Hartung-Knemeyer/10ab8d4b8e3d978dacc0b57826cdbcf1960a5bdb",
            "/paper/Whole-Body-MRI-with-Diffusion-Weighted-Imaging-in-A-Stecco-Trisoglio/914dee414626701bc1382e7d0caac41e8f2d4a78",
            "/paper/Whole-body-FDG-PET-CT-is-more-accurate-than-imaging-Riegger-Herrmann/548d84c1c5149ca94b61bbc3791e6a274eaa29b4",
            "/paper/Multiparametric-Whole-body-MRI-with-Imaging-and-ADC-Jacobs-Macura/9b7067af9fd0acad2606d1a450c5d1236e82fb5c",
            "/paper/Diagnostic-performance-of-PET-computed-tomography-N-Mooij-Sunen/114dbea274a9273529db54cc754926d02cfaf651",
            "/paper/Correlation-of-the-Apparent-Diffusion-Coefficient-Heusch-Buchbender/6b505d4714f102fa2a93f1e90a74e81bde32e273",
            "/paper/Comparison-of-diagnostic-and-prognostic-of-CT%2C-and-Nagamachi-Wakamatsu/2978e0b1b5bfb075f6d6311b9613a7e2a40cb465",
            "/paper/Non-small-cell-lung-cancer%3A-whole-body-MR-for-for-Ohno-Koyama/8d6c803d97686e7ba199231d6a7fbde6fa850c71",
            "/paper/Diffusion-Weighted-Magnetic-Resonance-Imaging-for-Mori-Nomori/e8c7130b7eba72184abd6b613838ad127290bdcd",
            "/paper/Prostate-carcinoma%3A-diffusion-weighted-imaging-as-Luboldt-K%C3%BCfer/c7efff4fb8202fd11dc224f2d71d9977cc0e64e5",
            "/paper/Value-of-diffusion-weighted-MR-imaging-in-the-and-Holzapfel-Duetsch/e053a6ec5201b6a9ad9ee4c869bacbc6dbedc62b",
            "/paper/Breast-Cancer-Staging-in-a-Single-Session%3A-PET-CT-Heusner-Kuemmel/df5712cfc1bd5c36706d7a12cd7feefbdf9413ca",
            "/paper/Whole-body-MRI-for-detecting-metastatic-bone-tumor%3A-Nakanishi-Kobayashi/ff22342bde3dac417070fd3cb47aaf12ce2911cb",
            "/paper/Diagnostic-value-of-full-dose-FDG-PET-CT-for-lymph-Heusner-Kuemmel/e21988a114ac2d2086b97de1ed8af106b8b5ecc2",
            "/paper/%5BDiffusion-weighted-whole-body-MRI-(virtual-PET)-in-Barcel%C3%B3-Vilanova/16955b753e29f8e98282a3d911090589edd0d37b",
            "/paper/Whole-body-PET-CT-mammography-for-staging-breast-Heusner-Freudenberg/4c6ac6e75cf7ab339e1ed0341cd50abbf36350b6",
            "/paper/Role-of-diffusion-weighted-magnetic-resonance-for-Kanauchi-Oizumi/ae740732e386807d0f4c4b450b6fb76b6d7c0c82"
        ]
    },
    {
        "id": "3b4aef5c352d6301f183388a58e4522130fbb3a1",
        "title": "Determining the clinical applicability of machine learning models through assessment of reporting across skin phototypes and rarer skin cancer types: A systematic review",
        "abstract": "A degree of variability in ML model performance across subgroups is expected, but the current lack of transparency is not justifiable and risks models being used inappropriately in populations in whom accuracy is low. Machine learning (ML) models for skin cancer recognition may have variable performance across different skin phototypes and skin cancer types. Overall performance metrics alone are insufficient to detect poor subgroup performance. We aimed (1) to assess whether studies of ML models reported results separately for different skin phototypes and rarer skin cancers, and (2) to graphically represent the skin cancer training datasets used by current ML models. In this systematic review, we searched PubMed, Embase and CENTRAL. We included all studies in medical journals assessing an ML technique for skin cancer diagnosis that used clinical or dermoscopic images from 1 January 2012 to 22 September 2021. No language restrictions were applied. We considered rarer skin cancers to be skin cancers other than pigmented melanoma, basal cell carcinoma and squamous cell carcinoma. We identified 114 studies for inclusion. Rarer skin cancers were included by 8/114 studies (7.0%), and results for a rarer skin cancer were reported separately in 1/114 studies (0.9%). Performance was reported across all skin phototypes in 1/114 studies (0.9%), but performance was uncertain in skin phototypes I and VI from minimal representation of the skin phototypes in the test dataset (9/3756 and 1/3756, respectively). For training datasets, although public datasets were most frequently used, with the most widely used being the International Skin Imaging Collaboration (ISIC) archive (65/114 studies, 57.0%), the largest datasets were private. Our review identified that most ML models did not report performance separately for rarer skin cancers and different skin phototypes. A degree of variability in ML model performance across subgroups is expected, but the current lack of transparency is not justifiable and risks models being used inappropriately in populations in whom accuracy is low.",
        "publication_year": "2022",
        "authors": [
            "L. Steele",
            "Xiang Li Tan",
            "B. Olabi",
            "Jinglei Gao",
            "R. Tanaka",
            "H. Williams"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "53",
        "references": [
            "/paper/Editor's-Picks-April-2023-Yosis/e8729c3d7060475a9fe09ffea433f469f83a7c65",
            "/paper/Validation-of-artificial-intelligence-prediction-Combalia-Codella/545a775e30398b0acfcb7cd0a01dec61fcbb18dd",
            "/paper/Characteristics-of-publicly-available-skin-cancer-a-Wen-Khan/376fab5f7886ad389cb341a587796ca4df9ba016",
            "/paper/Evaluating-Deep-Neural-Networks-Trained-on-Clinical-Groh-Harris/069b9321f85b6bc005fff3c254d34c791f6c0741",
            "/paper/Estimating-Skin-Tone-and-Effects-on-Classification-Kinyanjui-Odonga/088977ac55f66421fa87393830409b44dbe207d3",
            "/paper/%5BClinical-image-identification-of-basal-cell-and-on-Xie-He/af8a24f11d584b7130c9133d4b2223e295cdd513",
            "/paper/Dermatologist-level-classification-of-skin-cancer-Esteva-Kuprel/e1ec11a1cb3d9745fb18d3bf74247f95a6663d08",
            "/paper/Do-AI-models-recognise-rare%2C-aggressive-skin-of-a-Steele-Velazquez-Pimentel/b7e9452d2813649a218001ff4c7745870eddee06",
            "/paper/Augment-Intelligence-Dermatology-%3A-Deep-Neural-in-Han-Park/bbafa6a55d75493ea65db7b1fe85f25f3548b47e",
            "/paper/Disparities-in-dermatology-AI-performance-on-a-set-Daneshjou-Vodrahalli/954b0b209dea569fff76240521956b212680a41a",
            "/paper/The-Application-of-Deep-Learning-in-the-Risk-of-for-Zhao-Wu/fcc452fc869307614a8b0c24c5eab023f6d1648c"
        ]
    },
    {
        "id": "66e46478fc83b19353a5fd67044f2592ff511a9b",
        "title": "Data-centric artificial intelligence in oncology: a systematic review assessing data quality in machine learning models for head and neck cancer",
        "abstract": "Overall, data quality was infrequently assessed before the construction of ML models in head and neck cancer irrespective of the use of structured or unstructured datasets, and it was found that class imbalance reduced the\u00a0discriminatory performance for models based on structured datasets while higher image resolution and good class overlap resulted in better model performance using unstructures during internal validation. Machine learning models have been increasingly considered to model head and neck cancer outcomes for improved screening, diagnosis, treatment, and prognostication of the disease. As the concept of data-centric artificial intelligence is still incipient in healthcare systems, little is known about the data quality of the models proposed for clinical utility. This is important as it supports the generalizability of the models and data standardization. Therefore, this study overviews the quality of structured and unstructured data used for machine learning model construction in head and neck cancer. Relevant studies reporting on the use of machine learning models based on structured and unstructured custom datasets between January 2016 and June 2022 were sourced from PubMed, EMBASE, Scopus, and Web of Science electronic databases. Prediction model Risk of Bias Assessment (PROBAST) tool was used to assess the quality of individual studies before comprehensive data quality parameters were assessed according to the type of dataset used for model construction. A total of 159 studies were included in the review; 106 utilized structured datasets while 53 utilized unstructured datasets. Data quality assessments were deliberately performed for 14.2% of structured datasets and 11.3% of unstructured datasets\u00a0before model construction. Class imbalance and data fairness were the most common limitations in data quality for both types of datasets while outlier detection and lack of representative outcome classes were common in structured and unstructured datasets respectively. Furthermore, this review found that class imbalance reduced the\u00a0discriminatory performance for models based on structured datasets while higher image resolution and good class overlap resulted in better model performance using unstructured datasets\u00a0during internal validation. Overall, data quality was infrequently assessed before the construction of ML models in head and neck cancer irrespective of the use of structured or unstructured datasets. To improve model generalizability, the assessments discussed in this study should be introduced during model construction to achieve data-centric intelligent\u00a0systems for head and neck cancer management.",
        "publication_year": "2023",
        "authors": [
            "John Adeoye",
            "L. Hui",
            "Yu-xiong Su"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "211",
        "references": [
            "/paper/Machine-learning-and-its-potential-applications-to-Patil-Awan/12fcbb308109ea0f91623a5a5580440f620ebe29",
            "/paper/Applied-machine-learning-in-cancer-research%3A-A-for-Kourou-Exarchos/f6139df578206ac7f1c954d512becb0d4fd30734",
            "/paper/Prediction-models-applying-machine-learning-to-oral-Adeoye-Tan/3b209d729819c233509ab532a2cde8a64f62a6ac",
            "/paper/Machine-Learning-Algorithms-as-a-Computer-Assisted-Chiesa-Estomba-Gra%C3%B1a/0334ebe40ba084d415ee42452b0fb1735959c894",
            "/paper/Construction-of-machine-learning-based-models-for-A-Adeoye-Akinshipo/b7218fabafc6453a305832768fa76688d9e674fc",
            "/paper/Machine-Learning-for-Head-and-Neck-Cancer%3A-A-Safe-Volpe-Pepa/233ce79ed5e94147430fdc67df2b9b45257eaed0",
            "/paper/Predicting-clinical-outcomes-of-radiotherapy-for-Gangil-Shahabuddin/a70c0547f858707e756a4e74b3ae58f21c59bd94",
            "/paper/Comparison-of-machine-learning-algorithms-for-the-Alkhadar-Macluskey/23c20ac8ba9b3718f16cd44cddb11688750410ec",
            "/paper/Artificial-intelligence-based-prediction-for-in-and-Adeoye-Akinshipo/e78c735aae542dfe0286213e212fe28bfb4f3ff1",
            "/paper/Predicting-clinical-outcomes-of-radiotherapy-for-Gangil-Shahabuddin/cc4fd3319bfe4185c9d05a81fdcb5df66e55fa84"
        ]
    },
    {
        "id": "1f377ecffa9c7b6665c02d36513e2216ea660310",
        "title": "An Auto-Encoder Strategy for Adaptive Image Segmentation",
        "abstract": "A novel perspective of segmentation as a discrete representation learning problem is proposed, and a variational autoencoder segmentation strategy that is flexible and adaptive is presented, which can be a single unpaired segmentation image. Deep neural networks are powerful tools for biomedical image segmentation. These models are often trained with heavy supervision, relying on pairs of images and corresponding voxel-level labels. However, obtaining segmentations of anatomical regions on a large number of cases can be prohibitively expensive. Thus there is a strong need for deep learning-based segmentation tools that do not require heavy supervision and can continuously adapt. In this paper, we propose a novel perspective of segmentation as a discrete representation learning problem, and present a variational autoencoder segmentation strategy that is flexible and adaptive. Our method, called Segmentation Auto-Encoder (SAE), leverages all available unlabeled scans and merely requires a segmentation prior, which can be a single unpaired segmentation image. In experiments, we apply SAE to brain MRI scans. Our results show that SAE can produce good quality segmentations, particularly when the prior is good. We demonstrate that a Markov Random Field prior can yield significantly better results than a spatially independent prior. Our code is freely available at this https URL.",
        "publication_year": "2020",
        "authors": [
            "Evan M. Yu",
            "J. E. Iglesias",
            "Adrian V. Dalca",
            "M. Sabuncu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "8",
        "reference_count": "25",
        "references": [
            "/paper/Label-conditioned-segmentation-Ma-Lee/dc47b4dbde9c20b2ff2a244bae504892dc2e7634",
            "/paper/Semi-Supervised-Cardiac-Image-Segmentation-Using-a-Li-Zhang/38e31d2318d851f04ae8a2b3c87a024916264579",
            "/paper/Deep-Learning-Application-for-Analyzing-of-and-in-Ursuleanu-Luca/80bd89852cc9230562c63983c3bcb75619bb14e5",
            "/paper/Unified-Analysis-Specific-to-the-Medical-Field-in-Ursuleanu-Luca/35d837c1207fddbd93c360c0389e94888db440a0",
            "/paper/A-survey-of-deep-learning-models-in-medical-areas-Nogales-Garc%C3%ADa-Tejedor/05f6bc6a885d9e182be6bb22eb9a05b205dd8b0f",
            "/paper/Linear-Variational-State-Space-Filtering-Pfrommer-Matni/6175f0e90488e262a40b27056352027d8d1e419c",
            "/paper/Privacy-Preserving-Generative-Adversarial-Network-Montenegro-Silva/46428a7ca6262df42a9766df56905701353b0417",
            "/paper/Impact-of-quality%2C-type-and-volume-of-data-used-by-Luca-Ursuleanu/737939a9b9e41d37d45648e5639f0b49b3e982b3",
            "/paper/Few-Labeled-Atlases-are-Necessary-for-Segmentation-Lee-Sabuncu/db7d7e0a3060af2bf2c19896b99f2362ea6eff54",
            "/paper/Anatomical-Priors-in-Convolutional-Networks-for-Dalca-Guttag/896a89bf385a3bea1531f8d012172d4fa4393c37",
            "/paper/Unsupervised-deep-learning-for-Bayesian-brain-MRI-Dalca-Yu/72ac55e04f9d36bf837e02615be1f5e884c92274",
            "/paper/Data-Augmentation-Using-Learned-Transformations-for-Zhao-Balakrishnan/a8a742c5bc8eb1f64de7e6b37b146f71317a691f",
            "/paper/U-Net%3A-Convolutional-Networks-for-Biomedical-Image-Ronneberger-Fischer/6364fdaa0a0eccd823a779fcdd489173f938e91a",
            "/paper/Deep-Multi-Class-Segmentation-Without-Ground-Truth-Joyce-Chartsias/b5e5efa7f1cb513f18bb85e11b82390bbc7c8fe4",
            "/paper/A-Generative-Model-for-Image-Segmentation-Based-on-Sabuncu-Yeo/fc433b8590be34d8652af3c7fae690a3753ddb92",
            "/paper/Pulse-Sequence-Resilient-Fast-Brain-Segmentation-Jog-Fischl/3a7312281fdec136af1571e09734db2b0380a05e",
            "/paper/Generalised-Dice-overlap-as-a-deep-learning-loss-Sudre-Li/2d8edc4e38bf9907170238726ec902cb3739393b",
            "/paper/Semi-Supervised-and-Task-Driven-Data-Augmentation-Chaitanya-Karani/82bd8087a5b970670bfc2dd46a559742cca74903"
        ]
    },
    {
        "id": "01fb399644820a97d94f4f5c907a8f1a82a39e19",
        "title": "Calibrated Focal Loss for Semantic Labeling of High-Resolution Remote Sensing Images",
        "abstract": "This article redefine the hard examples in semantic labeling of HRRSIs and proposes the prediction confusion map to measure the classification difficulty, which shows that CFL can achieve outstanding results compared with other commonly used loss functions without increasing model parameters and training iterations, demonstrating the effectiveness of the method. Currently, the most advanced high-resolution remote sensing image (HRRSI) semantic labeling methods rely on deep neural networks. However, HRRSIs naturally have a serious class imbalance problem, which is not yet well solved by the current method. The cross-entropy loss is often used to guide the training of semantic labeling neural networks for HRRSIs, but it is essentially dominated by the major classes in the image, resulting in poor predictions for the minority class. Based on the prediction results, focal loss (FL) effectively suppresses the negative impact of class imbalance in dense object detection by redistributing the loss of each sample. In this article, we thoroughly analyze the inadequacy of FL for semantic labeling, which inevitably introduces confusing-classified examples that are more difficult to classify while suppressing the loss of well-classified examples. Therefore, following the core idea of FL, we redefine the hard examples in semantic labeling of HRRSIs and propose the prediction confusion map to measure the classification difficulty. Based on this, we further propose the calibrated focal loss (CFL) for the semantic labeling of HRRSIs. Finally, we conduct complete experiments on the International Society for Photogrammetry and Remote Sensing Vaihingen and Potsdam datasets to analyze the semantic labeling performance, model uncertainty, and confidence calibration of different loss functions. Experimental results show that CFL can achieve outstanding results compared with other commonly used loss functions without increasing model parameters and training iterations, demonstrating the effectiveness of our method. In the end, combined with our previously proposed HCANet, we further verify the effectiveness of CFL on state-of-the-art network structures.",
        "publication_year": "2022",
        "authors": [
            "Haiwei Bai",
            "Jian Cheng",
            "Yanzhou Su",
            "Siyu Liu",
            "Xin Liu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "73",
        "references": [
            "/paper/Semantic-Labeling-in-Very-High-Resolution-Images-a-Liu-Fan/b246ceb229271375991ec7ce04dc96c50f14e34b",
            "/paper/Semantic-Segmentation-of-Small-Objects-and-Modeling-Kampffmeyer-Salberg/c92ab8d1139290e91d0da9c7b63b69f3b20ebf49",
            "/paper/Dense-Dilated-Convolutions%E2%80%99-Merging-Network-for-Liu-Kampffmeyer/5c2c03f1bd56f3825197962bd73a69def80a0d41",
            "/paper/A-Combined-Loss-Based-Multiscale-Fully-Network-for-Li-He/bc612fbe733d428b9086ab95ac95f460b5c4a26f",
            "/paper/Semantic-Segmentation-of-Very-High-Resolution-via-Su-Cheng/eeb08af4dc849b4e4428a459ca219124b644cadd",
            "/paper/Learning-Aerial-Image-Segmentation-From-Online-Maps-Kaiser-Wegner/e0cd100dbf93befe1d224f5efe63d670595f59a0",
            "/paper/Classification-With-an-Edge%3A-Improving-Semantic-Marmanis-Schindler/62b378f3d8d6d7803ba3a3339949aea9ecdc5749",
            "/paper/Segmentation-of-Imbalanced-Classes-in-Satellite-Bischke-Helber/5fdce1730fcc57563319b263446ded8569fbdb92",
            "/paper/The-Effect-of-Focal-Loss-in-Semantic-Segmentation-Doi-Iwasaki/72c99627f3d6bf9a0b00c32d44a0f15ec35ae5c1",
            "/paper/Hybrid-Multiple-Attention-Network-for-Semantic-in-Niu-Sun/1ab8a8ac4dca82a63a58edc28ceb87f22a0b1c22"
        ]
    },
    {
        "id": "73d5664b988d921d7fe61d8bdbf3e925a7dccde9",
        "title": "Nonlinear vibration of metal foam cylindrical shells reinforced with graphene platelets",
        "abstract": "Semantic Scholar extracted view of \"Nonlinear vibration of metal foam cylindrical shells reinforced with graphene platelets\" by Y. Wang et al.",
        "publication_year": "2019",
        "authors": [
            "Y. Wang",
            "Chaochao Ye",
            "J. Zu"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": "238",
        "reference_count": "55",
        "references": [
            "/paper/Nonlinear-forced-vibration-of-functionally-graded-Ye-Wang/828044139381730e0ff99f221f67999b6d55cb92",
            "/paper/Nonlinear-free-vibration-of-rectangular-plates-with-Teng-Wang/3cd1c7410c8e2c5c93d7d085090d3265a5ddb7f4",
            "/paper/Analytical-solution-approach-for-nonlinear-of-shear-Salehi-Gholami/62812801097f18b039cb09211ff8847ea7f603ec",
            "/paper/Nonlinear-forced-vibration-of-simply-supported-thin-Teng-Wang/df5e0e1319ac3f611738901cebd8b6ef5040ee2c",
            "/paper/Nonlinear-resonance-of-axially-moving-graphene-foam-Ding-She/484638f357a7c91a9e57bca943a1b2b719dc1575",
            "/paper/Nonlinear-primary-resonance-analysis-of-initially-She-Ding/473c15f9ad5f22a61cc8444f56a848796597c057",
            "/paper/Nonlinear-low-velocity-impact-response-of-graphene-Zhang-She/3025c6e4e7ea08addbaf6dba3fdaab1a31e3c722",
            "/paper/Nonlinear-vibrations-of-functionally-graded-panels-Niu-Yao/02cc6756565d38ee61929423dacc209be887bfc9",
            "/paper/Studying-nonlinear-vibrations-of-composite-conical-Ansari-Hassani/8cc19c9f8a602d7af53758eab5c884867280afae",
            "/paper/The-probabilistic-dynamic-stability-analysis-of-Khayat-Baghlani/478ab378b2e58f30d0e7dd3232ee0ccf31447d9d",
            "/paper/Vibration-characteristics-of-functionally-graded-Dong-Li/bbe275b8bcd55bf861039f2582570db43053a83f",
            "/paper/Nonlinear-free-vibration-of-functionally-graded-on-Gao-Gao/2af94572ed20523f3c3d1157c26f33452bd836b2",
            "/paper/Nonlinear-free-vibration-of-graded-graphene-shells%3A-Dong-Zhu/5ebcff19bf684e855ce28a7668c289dadc93543d",
            "/paper/Linear-and-nonlinear-free-and-forced-vibrations-of-Mao-Zhang/572e5b13bde6aadf3d3013e1ed7b4ad4d4662e14",
            "/paper/Buckling-of-spinning-functionally-graded-graphene-Dong-Dong/ed10b39b50519eb1e50dfa6246054c3d41690caa",
            "/paper/Vibration-analysis-of-circular-cylindrical-shells-Wang-Ye/16b128dc0bdc433dc22d2fac2906efa7cedca6ab",
            "/paper/Nonlinear-vibration-and-postbuckling-of-graded-Chen-Yang/f173d1a1186d6273457145d2564507acb88aab8b",
            "/paper/Buckling-and-free-vibration-analyses-of-graded-on-Yang-Chen/52cc2713b991dae071de3e2f4ac5d570c9e7dc15",
            "/paper/Free-vibration-of-functionally-graded-porous-shell-Wang-Wu/2ce34b5d75c0faf33b39613d247eaeccecb85b82",
            "/paper/Post-buckling-analysis-of-refined-shear-deformable-Barati-Zenkour/de17d6dc29708f72660413981136d31d3854f82b"
        ]
    },
    {
        "id": "e00fe237ff1d0daa918a0a203c94b70e9d14b94b",
        "title": "FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction",
        "abstract": "This work presents a resolution-agnostic TSDF supervision strategy to provide the network with a more accurate learning signal during training, avoiding the pitfalls of TSDF interpolation seen in previous work, and develops a novel architecture for the final layers of the network. Recent works on 3D reconstruction from posed images have demonstrated that direct inference of scene-level 3D geometry without iterative optimization is feasible using a deep neural network, showing remarkable promise and high efficiency. However, the reconstructed geometries, typically represented as a 3D truncated signed distance function (TSDF), are often coarse without fine geometric details. To address this problem, we propose three effective solutions for improving the fidelity of inference-based 3D reconstructions. We first present a resolution-agnostic TSDF supervision strategy to provide the network with a more accurate learning signal during training, avoiding the pitfalls of TSDF interpolation seen in previous work. We then introduce a depth guidance strategy using multi-view depth estimates to enhance the scene representation and recover more accurate surfaces. Finally, we develop a novel architecture for the final layers of the network, conditioning the output TSDF prediction on high-resolution image features in addition to coarse voxel features, enabling sharper reconstruction of fine details. Our method produces smooth and highly accurate reconstructions, showing significant improvements across multiple depth and 3D reconstruction metrics.",
        "publication_year": "2023",
        "authors": [
            "Noah Stier",
            "Anurag Ranjan",
            "Alex Colburn",
            "Yajie Yan",
            "Liang Yang",
            "Fangchang Ma",
            "Baptiste Angles"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "26",
        "references": [
            "/paper/Learning-Signed-Distance-Functions-from-Noisy-3D-to-Ma-Liu/27e54d88ba788fd8b4159fc19931dd63a5c1ff87",
            "/paper/SimpleRecon%3A-3D-Reconstruction-Without-3D-Sayed-Gibson/caf96058774d96dd08df799319ef0d5430121518",
            "/paper/VolumeFusion%3A-Deep-Depth-Fusion-for-3D-Scene-Choe-Im/0ba9140dad37b1b3d8cfcdb6d54389edd701c58d",
            "/paper/MonoNeuralFusion%3A-Online-Monocular-Neural-3D-with-Zou-Huang/e605271aefa33cdf4c77e0ea801cd8e2410fceb0",
            "/paper/Atlas%3A-End-to-End-3D-Scene-Reconstruction-from-Murez-As/35e2056e29d5295b2669fe696ae0c27007c94625",
            "/paper/Monocular-Scene-Reconstruction-with-3D-SDF-Yuan-Gu/d9ef1c8cd640d90d948034bce0abf6e976e98e43",
            "/paper/VoRTX%3A-Volumetric-3D-Reconstruction-With-for-View-Stier-Rich/a358c152d177ad4ff4161f8822187d4a3e22bc24",
            "/paper/MonoSDF%3A-Exploring-Monocular-Geometric-Cues-for-Yu-Peng/c6c518ccd441bae6fdc788a58b6d39db1f33f1a4",
            "/paper/NeuralRecon%3A-Real-Time-Coherent-3D-Reconstruction-Sun-Xie/8db0bf0fa406254d4cd57eb413443a0b96bd12b8",
            "/paper/NerfingMVS%3A-Guided-Optimization-of-Neural-Radiance-Wei-Liu/149343c70c8c506c2e91112746549eea4300817b",
            "/paper/SG-NN%3A-Sparse-Generative-Neural-Networks-for-Scene-Dai-Diller/3c3a7cee35a0ff46a99fc45f891f37eb9f1b1e9d"
        ]
    },
    {
        "id": "f05c892a9d86558c6b001379798142ac2afd0b28",
        "title": "Survey on Age Invariant Facial Recognition",
        "abstract": "This paper surveys the different models and the algorithms used for age invariant facial recognition that effectively matches faces of a person irrespective of their age and considers the discriminative approach, which addresses the face aging problem in a more direct way without relying on generative aging model. The research in age-related face recognition has gained a lot of prominence lately due to the challenging problems of human face aging processes and strong demand of robust face recognition system across ages. Such face recognition systems are crucial in practical applications that need the compensation of age, e.g. missing children identification or passport verification, where there is a significant age difference between probe and gallery images. This paper surveys the different models and the algorithms used for age invariant facial recognition that effectively matches faces of a person irrespective of their age. Here we consider two models, first is the generative model that utilizes global features based approaches and second is the discriminative model that uses local features based approaches and is considered better in performance compared to generative model.The discriminative approach addresses the face aging problem in a more direct way without relying on generative aging model. The discriminative model is used in many algorithms like, SIFT (Scale Invariant Feature Transform), PCA (Principle Component Analysis), MFDA (Multi Feature Discriminant Analysis) and others.",
        "publication_year": "2015",
        "authors": [
            "Aman Kumar",
            "Aditya Srivastava",
            "Pavan Singh Yadav"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "9",
        "references": [
            "/paper/Analysis-on-Age-Invariance-Face-Recognition-Study-Dhamija/1ec5efb4f4e4c02286611f0782ca589f5ceab629",
            "/paper/Multi-Featured-Fuzzy-based-Block-Weight-Assignment-Juneja-Rana/873908480905a7777918b8077a1455ddc80c5315",
            "/paper/A-Discriminative-Model-for-Age-Invariant-Face-Li-Park/96350ffbd6e201cf21f509401148ea7674c6e82d",
            "/paper/Efficient-face-recognition-with-compensation-for-Nayak-Indiramma/29616548010ddcdc3644a1cf1e2821a76f3c6a56",
            "/paper/Modeling-self-Principal-Component-Analysis-for-age-Nayak-Indiramma/740290145ea3980602229a62036db76546982c80",
            "/paper/A-Study-of-Face-Recognition-as-People-Age-Ling-Soatto/d5f1886482893adf2f61e3b344d5483c86792ab5",
            "/paper/Face-Verification-Across-Age-Progression-Ramanathan-Chellappa/5af32f9d079bb6d12b1175ee38e83a71b9516fe3",
            "/paper/Investigating-age-invariant-face-recognition-based-Juefei-Xu-Luu/5833547ef408a04b2efc49bcba4854ba3009585c",
            "/paper/Age-invariant-face-recognition-using-graph-matching-Mahalingam-Kambhamettu/7755ded70d23839e9f9eb348605c0a4dc336e95d",
            "/paper/Image-processing-and-face-detection-analysis-on-on-Syambas-Purwanto/257b1233b8066c85705ef869621b6e03b8903c5c",
            "/paper/Combining-Tensor-Space-Analysis-and-Active-Models-Wang-Zhang/8917c621566b3f548dbd78d2eb285766130e5f6a"
        ]
    },
    {
        "id": "291a0754ea1e073ef2706dcb4384e5deb034da96",
        "title": "VisDrone-DET2019: The Vision Meets Drone Object Detection in Image Challenge Results",
        "abstract": "The goal is to advance the state-of-the-art detection algorithms and provide a comprehensive evaluation platform for them and demonstrate that there still remains a large room for improvement for object detection algorithms on drones. Recently, automatic visual data understanding from drone platforms becomes highly demanding. To facilitate the study, the Vision Meets Drone Object Detection in Image Challenge is held the second time in conjunction with the 17-th International Conference on Computer Vision (ICCV 2019), focuses on image object detection on drones. Results of 33 object detection algorithms are presented. For each participating detector, a short description is provided in the appendix. Our goal is to advance the state-of-the-art detection algorithms and provide a comprehensive evaluation platform for them. The evaluation protocol of the VisDrone-DET2019 Challenge and the comparison results of all the submitted detectors on the released dataset are publicly available at the website: http: //www.aiskyeye.com/. The results demonstrate that there still remains a large room for improvement for object detection algorithms on drones.",
        "publication_year": "2018",
        "authors": [
            "Peng Fei Zhu",
            "Longyin Wen",
            "Dawei Du",
            "Xiao Bian",
            "Haibin Ling",
            "Q. Hu",
            "Qinqin Nie",
            "Hao Cheng",
            "Chenfeng Liu",
            "Xiaoyu Liu",
            "Wenya Ma",
            "Haotian Wu",
            "Lianjie Wang",
            "Arne Schumann",
            "Chase Brown",
            "Qiang Chen",
            "Chengzhen Li",
            "Dongdong Li",
            "E. Michail",
            "Fan Zhang",
            "Feng Ni",
            "Feng Zhu",
            "Guanghui Wang",
            "Haipeng Zhang",
            "Han Deng",
            "Hao Liu",
            "Haoran Wang",
            "Heqian Qiu",
            "H. Qi",
            "Humphrey Shi",
            "Hongliang Li",
            "Hongyu Xu",
            "Hu Lin",
            "Y. Kompatsiaris",
            "Jian Cheng",
            "Jianqiang Wang",
            "Jianxiu Yang",
            "Jingkai Zhou",
            "Juanping Zhao",
            "K. J. Joseph",
            "Kaiwen Duan",
            "Karthik Suresh",
            "Bo Ke",
            "Ke Wang",
            "Konstantinos Avgerinakis",
            "L. Sommer",
            "Lei Zhang",
            "Li Yang",
            "Lin Cheng",
            "Lin Ma",
            "Liyu Lu",
            "Lu Ding",
            "Min Huang",
            "Naveen Kumar Vedurupaka",
            "Nehal Mamgain",
            "Nitin Bansal",
            "Oliver Acatay",
            "Panagiotis Giannakeris",
            "Qian Wang",
            "Qijie Zhao",
            "Qingming Huang",
            "Qiong Liu",
            "Qishang Cheng",
            "Q. Sun",
            "R. Lagani\u00e8re",
            "Sheng Jiang",
            "Shengjin Wang",
            "Shubo Wei",
            "Siwei Wang",
            "S. Vrochidis",
            "Sujuan Wang",
            "Timothy Lee",
            "Usman Sajid",
            "V. Balasubramanian",
            "Wei Li",
            "Wei Zhang",
            "Weikun Wu",
            "Wenchi Ma",
            "Wenrui He",
            "Wen-zhu Yang",
            "Xiaoyu Chen",
            "Xin Sun",
            "Xinbin Luo",
            "X. Lian",
            "Xiufang Li",
            "Yangliu Kuai",
            "Yali Li",
            "Yi Luo",
            "Yifan Zhang",
            "Yiling Liu",
            "Y. Li",
            "Yong Wang",
            "Yongtao Wang",
            "Yuanwei Wu",
            "Yue Fan",
            "Yunchao Wei",
            "Yuqin Zhang",
            "Zexin Wang",
            "Zhangyang Wang",
            "Zhaoyue Xia",
            "Zhenxiang Cui",
            "Zhenwei He",
            "Zhipeng Deng",
            "Z. Guo",
            "Zichen Song"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "155",
        "reference_count": "117",
        "references": [
            "/paper/VisDrone-DET2021%3A-The-Vision-Meets-Drone-Object-Cao-He/4f8a7867d083280387c4f4cce778970d352eb29a",
            "/paper/VisDrone-SOT2019%3A-The-Vision-Meets-Drone-Single-Wen-Zhu/73e7090d7ad8af42add824518772cd99d4048611",
            "/paper/YOLODrone%3A-Improved-YOLO-Architecture-for-Object-in-Sahin-Ozer/a314be3266c34bd127aaca6f3d9095048e973b75",
            "/paper/SimplestNet-Drone%3A-An-efficient-and-Accurate-Object/979fecf5778b0da0433d9801dbe845e78574fa53",
            "/paper/RRNet%3A-A-Hybrid-Detector-for-Object-Detection-in-Chen-Zhang/15f59653fbd135b3d0f051d3311c95b8984dbb76",
            "/paper/Vision-Meets-Drones%3A-Past%2C-Present-and-Future-Zhu-Wen/3367dfef9062681c0631b91b4c8b25f5f87d1187",
            "/paper/Multiscale-Object-Detection-from-Drone-Imagery-Walambe-Marathe/80ca284e56b1388c6a1d08a3a7710356c5759cb1",
            "/paper/VistrongerDet%3A-Stronger-Visual-Information-for-in-Wan-Zhang/61b6a63b513cbe3b1d286f9b2f058caace291267",
            "/paper/Detection-and-Tracking-Meet-Drones-Challenge-Zhu-Wen/424c0206424372ae502198bec7a49ca18c0aa636",
            "/paper/Anchor-Free-Small-Object-Detection-Algorithm-Based-Zhang-Zhang/4093ff2ff300e797b94860f5ede069fe613b420b",
            "/paper/Vision-Meets-Drones%3A-A-Challenge-Zhu-Wen/98bf42055160845e6f8f3c022298e3b8e4e55f80",
            "/paper/RRNet%3A-A-Hybrid-Detector-for-Object-Detection-in-Chen-Zhang/15f59653fbd135b3d0f051d3311c95b8984dbb76",
            "/paper/Vision-Meets-Drones%3A-Past%2C-Present-and-Future-Zhu-Wen/3367dfef9062681c0631b91b4c8b25f5f87d1187",
            "/paper/Vehicle-detection-in-aerial-imagery-%3A-A-small-Razakarivony-Jurie/f5dda7e255bd57923f7b009c47cfb94605434d9c",
            "/paper/DOTA%3A-A-Large-Scale-Dataset-for-Object-Detection-in-Xia-Bai/8ecf9e6428edd33986e8c6a143870e57142fd5a1",
            "/paper/Are-we-ready-for-autonomous-driving-The-KITTI-suite-Geiger-Lenz/de5b0fd02ea4f4d67fe3ae0d74603b9822df4e42",
            "/paper/DetNet%3A-A-Backbone-network-for-Object-Detection-Li-Peng/af77faec0f71d934013c1f17b368edc9e845235f",
            "/paper/R-FCN%3A-Object-Detection-via-Region-based-Fully-Dai-Li/b724c3f7ff395235b62537203ddeb710f0eb27bb",
            "/paper/Monocular-Pedestrian-Detection%3A-Survey-and-Enzweiler-Gavrila/5224b79368dba945a9e90506f23a1cfa91f6f404",
            "/paper/ImageNet-Large-Scale-Visual-Recognition-Challenge-Russakovsky-Deng/e74f9b7f8eec6ba4704c206b93bc8079af3da4bd"
        ]
    },
    {
        "id": "8c8031d26553b7394f64686dadf8df289d173b31",
        "title": "Three-dimensional motion analysis of scene containing multiple moving objects from image sequence and depth",
        "abstract": "A method of three-dimensional motion analysis for multiple moving objects which may include an object for which a unique interpretation of the motion is difficult, and the displacement vector can be determined accurately by iterating the estimation of the object motion. Optical flow analysis is a powerful means to extract an object from the dynamic image. Not only can it extract an object, but it can also recover the three-dimensional structure. However, in practice, there are many cases where the optical flow is difficult to obtain or the motion of the object does not permit a unique interpretation. Based on the gradient method, this paper proposes a method of three-dimensional motion analysis for multiple moving objects which may include an object for which a unique interpretation of the motion is difficult. By the gradient method, the three-dimensional motion parameters of a rigid object can be estimated without using the optical flow as a least-square-error solution for the system of linear equations, if the three-dimensional structure of the object is already given. Based on the residual square-sum error in the estimation, the image plane is segmented if it contains regions of different motions. If the motions are recognized as being similar, the regions are merged. Thus, the object is extracted by iterating the segmentation and merge on the image plane. The uniqueness of the motion is evaluated by the condition number for the coefficient matrix of the system equations. If the motion of the extracted object can be interpreted uniquely, the displacement vector can be determined accurately by iterating the estimation of the object motion.",
        "publication_year": "1987",
        "authors": [
            "Masanobu Yamamoto"
        ],
        "related_topics": [
            "Physics"
        ],
        "citation_count": "3",
        "reference_count": "22",
        "references": [
            "/paper/A-General-Aperture-Problem-for-Direct-Estimation-of-Yamamoto/f34b8c52054fe78aa7f497f0161237ab81827244",
            "/paper/Motion-Estimation-of-Real-Object-through-Virtual-Aoki-Mochiduki/d1881c3d609aaf623a50c7404760308c6ed840fa",
            "/paper/A-simple-and-robust-approach-to-drift-reduction-in-Yamamoto/2fa66426a51b0d838c9a51a5cdcdcb5a923f3efc",
            "/paper/Determining-Three-Dimensional-Motion-and-Structure-Adiv/3733124d9fedd380e7e0c6b82eef9f2db3344fc3",
            "/paper/Scene-segmentation-using-motion-information-Potter/7b541511104fd38430812e792e894af164ab3797",
            "/paper/Estimating-three-dimensional-motion-parameters-of-a-Tsai-Huang/59da095ce0ac2127fa02e9cf818ba33931d70070",
            "/paper/A-theoretical-study-of-recognition-of-moving-by-Maruyama-Amari/1e69d39a0b5818ee0afbe29294f990d291215ad3",
            "/paper/Dynamic-Occlusion-Analysis-in-Optical-Flow-Fields-Thompson-Mutch/53ac115c2d8ddb81f1cfb611447e778c45c575e1",
            "/paper/Methods-for-measuring-small-displacements-of-images-Cafforio-Rocca/6063e876447da22e5bdc36d7bfbc04e6ca3a7836",
            "/paper/Tracking-and-segmentation-of-moving-objects-in-line-Tsuji-Osada/d01975d835de5d668769791cd920845687f411ff",
            "/paper/Velocity-as-a-Cue-to-Segmentation-Potter/bd98ad91d611a940f62b6b6d9cfa9303da6e75b6",
            "/paper/Velocity-determination-in-scenes-containing-several-Fennema-Thompson/67301c286439a7c24368300ea13e9785bd666aed",
            "/paper/Combining-motion-and-contrast-for-segmentation-Thompson/e7fc6e292fa14b48bed6fb01ff761683289e3f87"
        ]
    },
    {
        "id": "e9258b5024e636627ecb50aa56e22c8c03d42985",
        "title": "Normalization Enhances Generalization in Visual Reinforcement Learning",
        "abstract": "It is found that, perhaps surprisingly, incorporating suitable normalization techniques is sufficient to enhance the generalization capabilities, without any additional special design, of visual reinforcement learning methods. Recent advances in visual reinforcement learning (RL) have led to impressive success in handling complex tasks. However, these methods have demonstrated limited generalization capability to visual disturbances, which poses a significant challenge for their real-world application and adaptability. Though normalization techniques have demonstrated huge success in supervised and unsupervised learning, their applications in visual RL are still scarce. In this paper, we explore the potential benefits of integrating normalization into visual RL methods with respect to generalization performance. We find that, perhaps surprisingly, incorporating suitable normalization techniques is sufficient to enhance the generalization capabilities, without any additional special design. We utilize the combination of two normalization techniques, CrossNorm and SelfNorm, for generalizable visual RL. Extensive experiments are conducted on DMControl Generalization Benchmark and CARLA to validate the effectiveness of our method. We show that our method significantly improves generalization capability while only marginally affecting sample efficiency. In particular, when integrated with DrQ-v2, our method enhances the test performance of DrQ-v2 on CARLA across various scenarios, from 14% of the training performance to 97%.",
        "publication_year": "2023",
        "authors": [
            "Lu Li",
            "Jiafei Lyu",
            "Guozheng Ma",
            "Zilin Wang",
            "Zhen Yang",
            "Xiu Li",
            "Zhiheng Li"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "59",
        "references": [
            "/paper/A-Comprehensive-Survey-of-Data-Augmentation-in-Ma-Wang/336eea1e32e2b93c63f97931710d3de54d05a336",
            "/paper/Look-where-you-look!-Saliency-guided-Q-networks-for-Bertoin-Zouitine/261c0be2ad10b605d39e0d13d3b28d11d171b4d4",
            "/paper/Pre-Trained-Image-Encoder-for-Generalizable-Visual-Yuan-Xue/0672e63cb2fb7a754e6549598440d3b0c66a002d",
            "/paper/Learning-Better-with-Less%3A-Effective-Augmentation-Ma-Zhang/d2796dec7c1c94f3a8fc22bea03b82d9a0196c1a",
            "/paper/Don't-Touch-What-Matters%3A-Task-Aware-Lipschitz-Data-Yuan-Ma/670472a67a5ab6af73f8a4d452f8eab8ce47160f",
            "/paper/Stabilizing-Deep-Q-Learning-with-ConvNets-and-under-Hansen-Su/390e248117f8bafad8b47b8e799352980c2f3f70",
            "/paper/Automatic-Data-Augmentation-for-Generalization-in-Raileanu-Goldstein/dff2e7721d11826f841ca99f9fedc664c5266d67",
            "/paper/Improving-Generalization-in-Reinforcement-Learning-Wang-Kang/3caf26a81f8b8be27b8ebc09bdb93416b4c40fe6",
            "/paper/Generalization-and-Regularization-in-DQN-Farebrother-Machado/567eb0ab96fb7f2869e9c4c204da9f2cf422d946",
            "/paper/SECANT%3A-Self-Expert-Cloning-for-Zero-Shot-of-Visual-Fan-Wang/7ad1b82507b61c7113c4bde17fa3d89bb256cff3"
        ]
    },
    {
        "id": "e55498cde3be426504a615a7177391596686f896",
        "title": "SVNet: Where SO(3) Equivariance Meets Binarization on Point Cloud Representation",
        "abstract": "This paper designs a general framework to construct 3D learning architectures with SO(3) equivariance and network binarization, and proposes to locate both scalar and vector features in the authors' networks to avoid both cases. Efficiency and robustness are increasingly needed for applications on 3D point clouds, with the ubiquitous use of edge devices in scenarios like autonomous driving and robotics, which often demand real-time and reliable responses. The paper tackles the challenge by designing a general framework to construct 3D learning architectures with SO(3) equivariance and network binarization. However, a naive combination of equivariant networks and binarization either causes sub-optimal computational efficiency or geometric ambiguity. We propose to locate both scalar and vector features in our networks to avoid both cases. Precisely, the presence of scalar features makes the major part of the network binarizable, while vector features serve to retain rich structural information and ensure SO(3) equivariance. The proposed approach can be applied to general backbones like PointNet and DGCNN. Meanwhile, experiments on ModelNet40, ShapeNet, and the real-world dataset ScanObjectNN, demonstrated that the method achieves a great trade-off between efficiency, rotation robustness, and accuracy. The codes are available at https://github.com/zhuoinoulu/svnet.",
        "publication_year": "2022",
        "authors": [
            "Z. Su",
            "M. Welling",
            "M. Pietikainen",
            "Li Liu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3",
        "reference_count": "56",
        "references": [
            "/paper/BPT%3A-Binary-Point-Cloud-Transformer-for-Place-Hou-Shang/f6d857f2bdec575e5ec90134537236d5c4f5f99c",
            "/paper/From-Local-Binary-Patterns-to-Pixel-Difference-for-Su-Pietikainen/6d3000574682bb8b147205f3d81629f85580033d",
            "/paper/Boosting-Convolutional-Neural-Networks-with-Middle-Su-Zhang/0f53757d05f64e055c9c995e879710cf471e70fd",
            "/paper/Equivariant-Point-Network-for-3D-Point-Cloud-Chen-Liu/28b2b34f31238d657ded15a81bbce22e3b96b1ad",
            "/paper/A-Rotation-Invariant-Framework-for-Deep-Point-Cloud-Li-Li/971924e861fe602371eb354d003e19a52bd76301",
            "/paper/ClusterNet%3A-Deep-Hierarchical-Cluster-Network-With-Chen-Li/a8a70942cec397e6b84de252fa3b9cd24948fa57",
            "/paper/PointNet%3A-Deep-Learning-on-Point-Sets-for-3D-and-Qi-Su/d997beefc0922d97202789d2ac307c55c2c52fba",
            "/paper/SE(3)-Transformers%3A-3D-Roto-Translation-Equivariant-Fuchs-Worrall/4e99406c10b61004826a0428634ad5c7b0f2f731",
            "/paper/Rotation-Invariant-Point-Cloud-Classification%3A-Zhao-Yang/a3fb16549e3af0b14a5b70fc0a22f818abcaecdc",
            "/paper/BiPointNet%3A-Binary-Neural-Network-for-Point-Clouds-Qin-Cai/404bf88bd0194e2ad1de79b4730c70bff5c841d4",
            "/paper/A-Closer-Look-at-Rotation-invariant-Deep-Point-Li-Fujiwara/cd6b56319f85d0ae9266711247ad3e22496e8ab4",
            "/paper/PointCNN%3A-Convolution-On-X-Transformed-Points-Li-Bu/6400c36efdb8a66b401b6aef26c057227266fddd",
            "/paper/PointConv%3A-Deep-Convolutional-Networks-on-3D-Point-Wu-Qi/5ee147684b06ffc4db0f6326e0cba017d12ceff3"
        ]
    },
    {
        "id": "059551ea629c70bd2a1939c5bfde74c7f8a39833",
        "title": "Video summarization with a graph convolutional attention network",
        "abstract": "A graph convolutional attention network (GCAN) for video summarization that uses dilated temporal convolution to model local cues and temporal self-attention to exploit global cues for video frames and learns graph embedding via a multi-layer graph Convolutional network to reveal the intrinsic structure of frame samples. Video summarization has established itself as a fundamental technique for generating compact and concise video, which alleviates managing and browsing large-scale video data. Existing methods fail to fully consider the local and global relations among frames of video, leading to a deteriorated summarization performance. To address the above problem, we propose a graph convolutional attention network (GCAN) for video summarization. GCAN consists of two parts, embedding learning and context fusion, where embedding learning includes the temporal branch and graph branch. In particular, GCAN uses dilated temporal convolution to model local cues and temporal self-attention to exploit global cues for video frames. It learns graph embedding via a multi-layer graph convolutional network to reveal the intrinsic structure of frame samples. The context fusion part combines the output streams from the temporal branch and graph branch to create the context-aware representation of frames, on which the importance scores are evaluated for selecting representative frames to generate video summary. Experiments are carried out on two benchmark databases, SumMe and TVSum, showing that the proposed GCAN approach enjoys superior performance compared to several state-of-the-art alternatives in three evaluation settings.",
        "publication_year": "2021",
        "authors": [
            "Ping Li",
            "Chao Tang",
            "Xianghua Xu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "5",
        "reference_count": "42",
        "references": [
            "/paper/A-Hierarchical-Spatial%E2%80%93Temporal-Cross-Attention-for-Teng-Gui/5aab75fc39c1720b14f13d3e688281d7e6d1181a",
            "/paper/SUM-GAN-GEA%3A-Video-Summarization-Using-GAN-with-and-Yu-Yu/1dadedb8535f372af847d11ea7d5de767496eeeb",
            "/paper/A-Video-Summarization-Model-Based-on-Deep-Learning-Wang-Li/2df437fe675c89a00dc0a7c3a31fa46ed3d94c9d",
            "/paper/An-Efficient-Model-for-Dynamic-Video-Summarization-Rashad-El-Nagar/0047fbb84d4f0473d2b98d5fba86b36c49deb3a7",
            "/paper/A-multi-flexible-video-summarization-scheme-using-Teng-Gui/a16669b6e45d8107048f5ee54e6bc8e7d90cbee3",
            "/paper/Exploring-global-diverse-attention-via-pairwise-for-Li-Ye/e3b914637e63c88eec0721e41eca195921476e94",
            "/paper/Video-Summarization-With-Attention-Based-Networks-Ji-Xiong/88a8baa1be5292e62622f1cb8e627fbf759bf741",
            "/paper/Property-Constrained-Dual-Learning-for-Video-Zhao-Li/b83b6069fc6ad861c6d7cb574825488e0fca8521",
            "/paper/Video-Summarization-via-Semantic-Attended-Networks-Wei-Ni/6ec09bad57cc81a71ef7596f57e94ee13b380ae3",
            "/paper/Video-Summarization-by-Learning-Deep-Side-Semantic-Yuan-Mei/493293a9f4b53146ef358269459205281fde96f9",
            "/paper/Video-Summarization-Using-Fully-Convolutional-Rochan-Ye/8c413c2ee66664909d8c194f3f3e08c5f109c3c1",
            "/paper/Discriminative-Feature-Learning-for-Unsupervised-Jung-Cho/69b3b29c6fabaea88d382922346d9157395a3226",
            "/paper/Video-Summarization-with-Long-Short-Term-Memory-Zhang-Chao/1dbc12e54ceb70f2022f956aa0a46e2706e99962",
            "/paper/HSA-RNN%3A-Hierarchical-Structure-Adaptive-RNN-for-Zhao-Li/1ea6c0f1bde1f800e3c9c23573325e0d5283b12c",
            "/paper/Unsupervised-Video-Summarization-with-Adversarial-Mahasseni-Lam/620fe6c786d15efca7f553ad70f295e2b693b391"
        ]
    },
    {
        "id": "dc1485141a67953c51af5a644b1b5107efff2399",
        "title": "Object Tracking Based on a Time-Varying Spatio-Temporal Regularized Correlation Filter With Aberrance Repression",
        "abstract": "An object tracking model based on a time-varying Spatio-temporal regularized correlation filter with aberrance repression is proposed, which outperformed many state-of-the-art trackers based on DCF and deep-based frameworks in terms of tracking accuracy, tracking success rate, and A-R rank. When used for object tracking, the discriminative correlation filter (DCF) is effective, but its performance is often burdened by undesirable boundary effects. Meanwhile, when there is too much background information in training samples of the DCF, it will be easier to learn the area deviating from the tracking object. Further, illumination variation, partial/full occlusion, and appearance variations, render the response map aberrance of the correlation filter (CF) more prone to occur. To overcome these problems, an object tracking model based on a time-varying Spatio-temporal regularized correlation filter with aberrance repression is proposed in this paper. Firstly, by adding a regularized term to the traditional CFs to limit the change rate of the response map generated in the object detection phase, the proposed tracker can obviously repress the aberrance of the response maps; secondly, by adjusting the filter to the object regions suitable for tracking with high confidence scores with a time-varying spatial reliability map, the proposed tracker effectively overcomes the adverse effects caused by the boundary effect; and finally, by introducing a temporal regularized term, the proposed tracker also has superior tracking ability for the partial occluded objects and those with large appearance variations. Significant experiments on the OTB100, VOT2016, TC128, and UAV 123 datasets have revealed that the performance thereof outperformed many state-of-the-art trackers based on DCF and deep-based frameworks in terms of tracking accuracy, tracking success rate, and A-R rank, etc.",
        "publication_year": "2022",
        "authors": [
            "Junnan Wang",
            "Zhen Jia",
            "Huicheng Lai",
            "Jie Yang",
            "N. Kasabov"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "54",
        "references": [
            "/paper/High-bandwidth-performance-of-multimode-polymer-Savovi%C4%87-Simovi%C4%87/66e6ea42caa76dc84f732d0a1897c07d1ee628b1",
            "/paper/Learning-Aberrance-Repressed-Correlation-Filters-Huang-Fu/1b3a7e6ebd89a1fb03c992110952d7bddae3345a",
            "/paper/Learning-Spatial-Temporal-Regularized-Correlation-Li-Tian/9f45b55af027503fab557f55f70e81e43c6c1db7",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/A-Multi-Information-Fusion-Correlation-Filters-Wang-Jia/469027f78fe8cf176eeefe6f6fdac3baab66af3a",
            "/paper/Visual-Tracking-via-Adaptive-Spatially-Regularized-Dai-Wang/e5820233ea8abd27c81c0a83b1b782a9d4ca8db2",
            "/paper/Visual-object-tracking-using-adaptive-correlation-Bolme-Beveridge/70c3c9b9a40ca55264e454586dca2a6cf416f6e0",
            "/paper/Learning-Temporal-Regularized-Correlation-Filter-Pu-Feng/3431f56973622ad04f85b92fa872ac38352e4e33",
            "/paper/Adaptive-Kernel-Correlation-Filter-Tracking-in-Sun-Ding/b39342c3a20bb1cb096bf3b5bb52e1c3f809aa91",
            "/paper/A-multiple-feature-fused-model-for-visual-object-Yuan-Zhang/cc33a5596bb321f5f24ee1d0d36f96a3bf4d200f",
            "/paper/AutoTrack%3A-Towards-High-Performance-Visual-Tracking-Li-Fu/0619650ae0f698bcc38244a6858cc270df9dfaad"
        ]
    },
    {
        "id": "66afdc8abaca12423dc6ee521f53e6bf856afd78",
        "title": "Adaptive Weighted CNN Features Integration for Correlation Filter Tracking",
        "abstract": "An adaptive weighted CNN features-based Siamese network for tracking that derives feature maps by concatenating features of all convolutional layers and combines the holistic-part network with spatial and channel attention mechanisms to adaptively assign weights to each region and channel of the feature maps. Visual object tracking is an active and challenging research topic in computer vision, as objects often undergo significant appearance changes caused by occlusion, deformation, and background clutter. Although convolutional neural network (CNN)-based trackers have achieved competitive results, there are still some limitations. Most existing CNN-based trackers track the object by leveraging high-level semantic features of the highest convolutional layer, which may lead to low-spatial resolution feature maps and degrade the localization precision of tracking. Furthermore, these trackers hardly benefit from end-to-end training since the extraction of features and the learning of classifier are separated. To deal with the above-mentioned issues, we design an adaptive weighted CNN features-based Siamese network for tracking. To capture spatial and semantic information of the object, we design a feature extraction network that derives feature maps by concatenating features of all convolutional layers. To make the features representation more discriminative, we propose a feature integration network. In the feature integration network, we propose a holistic-part network to capture strong visual cues and learn the semantic relations between the holistic object and its parts and combine the holistic-part network with spatial and channel attention mechanisms to adaptively assign weights to each region and channel of the feature maps. In addition, the designed Siamese network can be trained offline end-to-end. The experimental results on the benchmark datasets OTB50 and OTB100 demonstrate that the proposed tracker achieves favorable performance against several state-of-the-art trackers while running at an average speed of 20.5 frames/s.",
        "publication_year": "2019",
        "authors": [
            "Chunbao Li",
            "Bo Yang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "6",
        "reference_count": "49",
        "references": [
            "/paper/Siamese-High-Level-Feature-Refine-Network-for-Rahman-Ahmed/057f0dbb8f94d7ec3b5ce3c42a76a492d5ad727e",
            "/paper/Efficient-Visual-Tracking-With-Stacked-Attention-Rahman-Fiaz/e085fb462789a8bca7933b5e1c7e9aa0ded8a711",
            "/paper/Review-of-recent-advances-in-visual-tracking-JainulRinoshaS-GethsiyalAugasta/9fc420e96048a30f9f15c49ad2ccbfbf875864b9",
            "/paper/Review-of-recent-advances-in-visual-tracking-JainulRinoshaS.-Augasta/98e301d9a9315b5d498d5b0c6c5d24820422e2ce",
            "/paper/Shape-Matching-Based-on-Multi-Scale-Invariant-Kun-Xiao/87b06824a5e242aa20253acf86eb0c42aa5be2a4",
            "/paper/Channel-Compression%3A-Rethinking-Information-Among-Liang-Zhang/46296054a24d1cd3a86f568d1afc9c96be8c95e8",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Hierarchical-Convolutional-Features-for-Visual-Ma-Huang/5c8a6874011640981e4103d120957802fa28f004",
            "/paper/Robust-occlusion-aware-part-based-visual-tracking-Wang-Hou/6f4595e1e1ee6d85f3fd4de89c5837eb618bf044",
            "/paper/Online-Scale-Adaptive-Visual-Tracking-Based-on-Wang-Hou/ff930639198a7c466e24bfa5df73bbc833b18839",
            "/paper/Convolutional-Regression-for-Visual-Tracking-Chen-Tao/5c1e0fc7d63d1e2fd8b8fd8129e800f0c2e5023c",
            "/paper/Augmenting-cascaded-correlation-filters-with-for-Zhao-Xiao/966239d2f03a81526f77746c2e763cc316095184",
            "/paper/Video-tracking-using-learned-hierarchical-features.-Wang-liu/aa7dddc72d108ea0c96c822cbf6ef473e9d6bfbf",
            "/paper/Visual-object-tracking-via-enhanced-structural-Chen-Tao/c1bf72ff7d805a0cb8940b91d1b4ba9e4bcb8e76",
            "/paper/End-to-End-Feature-Integration-for-Correlation-With-Li-Wen/a8157f1d69979026ef78823e873a653a8600f8c9",
            "/paper/Deep-visual-tracking%3A-Review-and-experimental-Li-Wang/26e2ca763087be09e3799ad294302aa91077942d"
        ]
    },
    {
        "id": "75a3f4ba22c36f6dccfdb469cc6b34d4790e090d",
        "title": "A new framework for on-line object tracking based on SURF",
        "abstract": "Semantic Scholar extracted view of \"A new framework for on-line object tracking based on SURF\" by Quan Miao et al.",
        "publication_year": "2011",
        "authors": [
            "Quan Miao",
            "Guijin Wang",
            "Chenbo Shi",
            "Xinggang Lin",
            "Zhiwei Ruan"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "41",
        "reference_count": "36",
        "references": [
            "/paper/On-Line-Rigid-Object-Tracking-via-Discriminative-Miao-Shi/d870399b106983ecdb248a165c7117593ac2a42b",
            "/paper/Feature-Based-On-Line-Object-Tracking-Combining-and-Miao-Zhang/eaf576254fb0558006d27e7eb3493a1756dac869",
            "/paper/Object-Tracking-Method-Based-on-SURF-Shuo-Na/93d5a35e9043dbe4a20a6d30569834fed199660d",
            "/paper/Kernel-Based-On-Line-Object-Tracking-Combining-both-Miao-Wang/ac920e31480cf3eedf36bfd8d651127f35efdc83",
            "/paper/A-combined-visual-tracker-based-on-global-and-local-Yang-Jin/26ef964a1b44132518d99da1d22fd17ac2f076d1",
            "/paper/Kernel-Based-Online-Object-Tracking-via-Gaussian-Miao-Gu/558f7987234fa3e1dd0e1c788b950bbdd7cbf95b",
            "/paper/Particle-Filter-Vehicle-Tracking-Based-on-SURF-Lu-Izumi/be7cf03bc86bf94f7695abe4abc3bd94f4fb3eb7",
            "/paper/Vehicle-tracking-process-based-on-combination-of-Lu-Wang/4d20b5d838686e5c2c4c0e6a70d254b4652d29b6",
            "/paper/A-New-Object-Tracking-Framework-for-Interest-Point-Guler-Cinar/c02255caf3b03d525ca087f0f79f3da85aa685ab",
            "/paper/Surf-points-based-Moving-Target-Detection-and-in-Zhu-Sun/d76ad7f8b59db7e496dda5b3dc5696f74877e98a",
            "/paper/Learning-Features-for-Tracking-Grabner-Grabner/10d2a56557a3760f1d14fe6869e0cad76f0a24f4",
            "/paper/Scale-and-rotation-invariant-feature-based-object-Miao-Wang/cedd1a11148e99a569edaeea59429ee3b532fd58",
            "/paper/SURFTrac%3A-Efficient-tracking-and-continuous-object-Ta-Chen/a503b0b11fd2b49a60fc29545da0cd048edeb1d4",
            "/paper/Online-selection-of-discriminative-tracking-Collins-Liu/9c2f13c1fe9d8b894c62b4037491605cf8e45b89",
            "/paper/Semi-supervised-On-Line-Boosting-for-Robust-Grabner-Leistner/27d69a2d96600efb66fd907d8287ca3b6e734c59",
            "/paper/SURF%3A-Speeded-Up-Robust-Features-Bay-Tuytelaars/490020c0d4fa1eb85fe353add5713e49f08c628d",
            "/paper/Speeded-Up-Robust-Features-(SURF)-Bay-Ess/cdbb606ae47c64049262dfbd3bb147d3f4ba8420",
            "/paper/Ensemble-Tracking-Avidan/82c0dba2a7a175113050a6c0dbd409c93cc48996",
            "/paper/Keypoint-recognition-using-randomized-trees-Lepetit-Fua/aa6557c658aed10ff303aa90fe8d0952332a43c0",
            "/paper/Rapid-object-detection-using-a-boosted-cascade-of-Viola-Jones/dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63"
        ]
    },
    {
        "id": "1a3284e3c7bc58a5f453e6573d9107bfb3686b9e",
        "title": "Hierarchical Patch VAE-GAN: Generating Diverse Videos from a Single Sample",
        "abstract": "This work introduces a novel patch-based variational autoencoder (VAE) which allows for a much greater diversity in generation of videos, and produces diverse samples in both the image domain, and the more challenging video domain. We consider the task of generating diverse and novel videos from a single video sample. Recently, new hierarchical patch-GAN based approaches were proposed for generating diverse images, given only a single sample at training time. Moving to videos, these approaches fail to generate diverse samples, and often collapse into generating samples similar to the training video. We introduce a novel patch-based variational autoencoder (VAE) which allows for a much greater diversity in generation. Using this tool, a new hierarchical video generation scheme is constructed: at coarse scales, our patch-VAE is employed, ensuring samples are of high diversity. Subsequently, at finer scales, a patch-GAN renders the fine details, resulting in high quality videos. Our experiments show that the proposed method produces diverse samples in both the image domain, and the more challenging video domain.",
        "publication_year": "2020",
        "authors": [
            "Shir Gur",
            "Sagie Benaim",
            "Lior Wolf"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "39",
        "reference_count": "42",
        "references": [
            "/paper/SinFusion%3A-Training-Diffusion-Models-on-a-Single-or-Nikankin-Haim/c1ddae8106deaeb15120a6a0f04111a63f4e2a7c",
            "/paper/Diverse-Video-Generation-from-a-Single-Video-Haim-Feinstein/a0274068d613e389a4f14277b49eb879b81c54ef",
            "/paper/An-Overview-of-Variational-Autoencoders-for-Source-Singh-Ogunfunmi/68e1ae4d882114d29450d90ea45840cfe56917b5",
            "/paper/ExSinGAN%3A-Learning-an-Explainable-Generative-Model-Zhang-Han/745297a4d15cbddf4df785c85c8e2ae3b4a650be",
            "/paper/Diverse-Single-Image-Generation-with-Controllable-Mahendren-Edussooriya/33e750eaa49e998b2eb6174ee5aefc4b23fd0694",
            "/paper/FewGAN%3A-Generating-from-the-Joint-Distribution-of-a-Ben-Moshe-Benaim/5edc266d424ca59260a4c23469773bb47c0d873e",
            "/paper/OUR-GAN%3A-One-shot-Ultra-high-Resolution-Generative-Yoon-Oh/0d0cd6493bb1c11671804eb2bff26dec44d51fe0",
            "/paper/SpecSinGAN%3A-Sound-Effect-Variation-Synthesis-Using-Barahona-R'ios-Collins/0028d7eca19ce75925b0cc0911ef42e62c1b5a6e",
            "/paper/Meta-Internal-Learning-Bensadoun-Gur/872562d3255717bbeadb1cdfc0cc061c7e4eb9eb",
            "/paper/Diverse-Generation-from-a-Single-Video-Made-Haim-Feinstein/9b1546bc2ecbc2a699d83f113c549806212a7398",
            "/paper/Improved-Techniques-for-Training-Single-Image-GANs-Hinz-Fisher/1bf6b1aec3403eee7044935345c581a1b6a9f1ed",
            "/paper/Towards-High-Resolution-Video-Generation-with-of-Acharya-Huang/bc4e0bec299f3d9bcb83b9613db914385597766e",
            "/paper/TGANv2%3A-Efficient-Training-of-Large-Models-for-with-Saito-Saito/ef29d5c85b70fd9dbe04a7b839fbc7d413b161e6",
            "/paper/Efficient-Video-Generation-on-Complex-Datasets-Clark-Donahue/24262bd12149b0b8322e0691fa30ec1a4c06b9a8",
            "/paper/SinGAN%3A-Learning-a-Generative-Model-From-a-Single-Shaham-Dekel/ccaf15d4ad006171061508ca0a99c73814671501",
            "/paper/Large-Scale-GAN-Training-for-High-Fidelity-Natural-Brock-Donahue/22aab110058ebbd198edb1f1e7b4f69fb13c0613",
            "/paper/Temporal-Generative-Adversarial-Nets-with-Singular-Saito-Matsumoto/062c41dad67bb68fefd9ff0c5c4d296e796004dc",
            "/paper/Progressive-Growing-of-GANs-for-Improved-Quality%2C-Karras-Aila/744fe47157477235032f7bb3777800f9f2f45e52",
            "/paper/MoCoGAN%3A-Decomposing-Motion-and-Content-for-Video-Tulyakov-Liu/e76edb86f270c3a77ed9f5a1e1b305461f36f96f",
            "/paper/VEEGAN%3A-Reducing-Mode-Collapse-in-GANs-using-Srivastava-Valkov/81d5740cac256489978c2751e867128e97620eae"
        ]
    },
    {
        "id": "4b6a190bb897a626639849c09426d30d5a1462c8",
        "title": "SiamCorners: Siamese Corner Networks for Visual Tracking",
        "abstract": "A simple yet effective anchor-free tracker (named Siamese corner networks, SiamCorners), which is end-to-end trained offline on large-scale image pairs is proposed, which achieves experimental results that are comparable to the state-of-art tracker while maintaining a high running speed. The current Siamese network based on region proposal network (RPN) has attracted great attention in visual tracking due to its excellent accuracy and high efficiency. However, the design of the RPN involves the selection of the number, scale, and aspect ratios of anchor boxes, which will affect the applicability and convenience of the model. Furthermore, these anchor boxes require complicated calculations, such as calculating their intersection-over-union (IoU) with ground truth bounding boxes. Due to the problems related to anchor boxes, we propose a simple yet effective anchor-free tracker (named Siamese corner networks, SiamCorners), which is end-to-end trained offline on large-scale image pairs. Specifically, we introduce a modified corner pooling layer to convert the bounding box estimate of the target into a pair of corner predictions (the bottom-right and the top-left corners). By tracking a target as a pair of corners, we avoid the need to design the anchor boxes. This will make the entire tracking algorithm more flexible and simple than anchor-based trackers. In our network design, we further introduce a layer-wise feature aggregation strategy that enables the corner pooling module to predict multiple corners for a tracking target in deep networks. We then introduce a new penalty term that is used to select an optimal tracking box in these candidate corners. Finally, SiamCorners achieves experimental results that are comparable to the state-of-art tracker while maintaining a high running speed. In particular, SiamCorners achieves a 53.7% AUC on NFS30 and a 61.4% AUC on UAV123, while still running at 42 frames per second (FPS).",
        "publication_year": "2021",
        "authors": [
            "Kai Yang",
            "Zhenyu He",
            "Wenjie Pei",
            "Zikun Zhou",
            "Xin Li",
            "Di Yuan",
            "Haijun Zhang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "40",
        "reference_count": "67",
        "references": [
            "/paper/SiamDA%3A-distribution-aware-Siamese-network-for-Ji-Shi/0bc6b7c8e3d2efdb6cf35a15192029c078bebbc6",
            "/paper/Learning-Localization-aware-Target-Confidence-for-Nie-Wu/14540dc59b5c7b7283f7dfa9357ab3d55fd688a8",
            "/paper/RHL-track%3A-visual-object-tracking-based-on-Meng-Gong/be2c3cb1d4412c71a3d7acff15e886855b211961",
            "/paper/SiamON%3A-Siamese-Tracking-by-Online-Update-Module-Liu-Li/10b8cdc2022c517142b2b886efe6ee31dac086b0",
            "/paper/Siamese-Attention-Networks-with-Adaptive-Templates-Zhang-Liang/0a17d476e9543756e09b9c3240398af2f7f5af9f",
            "/paper/SiamOA%3A-siamese-offset-aware-object-tracking-Zhang-Xie/53201f6a34e5bedd793f0132e3b4c59a098c2274",
            "/paper/MultiBSP%3A-multi-branch-and-multi-scale-perception-Jiang-Yang/56fcec7aeea49fd2797375d2064462de83cff312",
            "/paper/An-efficient-method-to-fool-and-enhance-object-with-Pang-Ma/32fb15dd9a6c5b5095b2b3f492e794f5ea98f382",
            "/paper/Learning-object-uncertainty-policy-for-visual-He-Chen/ca3330ead176d8a5cf30f8c74ab66a4ffb8c9491",
            "/paper/Learning-task-specific-discriminative-for-multiple-Wu-Nie/4d94c3fda9a88a0a6c7c587e3fdd892a82114ade",
            "/paper/Siamese-Box-Adaptive-Network-for-Visual-Tracking-Chen-Zhong/cce1fecc800d2782da638f3060d5b2e887739f74",
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/Fully-Conventional-Anchor-Free-Siamese-Networks-for-Han-Du/25c42e0744384ba761fd380f51fd3e5a52c2d5ff",
            "/paper/SiamRPN%2B%2B%3A-Evolution-of-Siamese-Visual-Tracking-Li-Wu/d1a4135a2edd1af8a1e501109bbf7c2c720f10f8",
            "/paper/Distractor-aware-Siamese-Networks-for-Visual-Object-Zhu-Wang/776bc8955e801f6965e85b35d8e2dd6f2f1498ad",
            "/paper/Deeper-and-Wider-Siamese-Networks-for-Real-Time-Zhang-Peng/2e713509e96daf06e65e10bdc438d00a827c914c",
            "/paper/Ocean%3A-Object-aware-Anchor-free-Tracking-Zhang-Peng/27d52bf3265bea0f9929980f6ffb4c2009eecfee",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437",
            "/paper/Structured-Siamese-Network-for-Real-Time-Visual-Zhang-Wang/4b1965a54a064ac9145b1ce404fe33f0120c8ae3",
            "/paper/Graph-Convolutional-Tracking-Gao-Zhang/53970ae69a73f547a56661fd25f6711746d277fb"
        ]
    },
    {
        "id": "c4d60d547c5bd5d16c916dcdb40ab0d9503230dc",
        "title": "Artificial Special Visual Geometry Group-16(VGG) Learning Model for Analysing Accuracy and Precision of SARS-COV-2 Forecasting",
        "abstract": "This investigation applied the promising Visual Geometry Group16 transfer learning technique to identify SARS-COV-2 more reliably and fast, and provides an outstanding calibration for identification of 0.93% for the knowledge involved in the given random sample. Current research attempts to predict SARS-COV-2 efficiently applying recent artificial intelligence algorithms and lung radiography. In this investigation, we applied the promising Visual Geometry Group16 transfer learning technique to identify SARS-COV-2 more reliably and fast. The technique splits the lung radiograph image into two groups: SARS-COV-2 and normal. The Accuracy, Exactness, Recognize, Support, and Harmonic Mean metrics are used to evaluate the model\u2019s effectiveness. A total of 2000 radiograph samples were used in the study. The recommended VGG16 model provides an outstanding calibration for identification of 0.93% for the knowledge involved in the given random sample, which is greater than any current technique reported even in prior studies. Because the proposed method is highly successful and accurate, it might be used to help and educate radiologists and other medical practitioners in diagnosing SARS-COV-2 using lung radiographs.",
        "publication_year": "2023",
        "authors": [
            "Valaboju Shiva Kumar Chary",
            "Bellamkonda Satya Sai Venkateswarlu",
            "Saketh Vemuri",
            "Venkata Naga Sai Suraj Pasupuleti",
            "Vijaya Babu Burra",
            "Praveen Tumuluru"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "22",
        "references": [
            "/paper/COVID-CLNet%3A-COVID-19-Detection-with-Compressive-Awedat-Essa/e34b19d7008ca5081c3d6e76c98b78e66907c557",
            "/paper/Emphasize-of-Deep-CNN-for-Chest-Radiology-Images-in-Patil-Narawade/dd0b263e837de8404523ddbe0afe49f16cbe94ee",
            "/paper/Self-supervision-and-Multi-task-Learning%3A-in-from-Ridzuan-Bawazir/b48f7a3ba1c3c16e0b70dc867182e549ca90be94",
            "/paper/Radiomics-Guided-Global-Local-Transformer-for-in-Han-Holste/1e9dcd6dea74ae513c2728fa0b284015f86a67cf",
            "/paper/Convolutional-Neural-Networks-for-Medical-Image-and-Parameswari-V./9eb50cc8cc7f343ae7458dfebc15ef3c64246aa3",
            "/paper/Detection-of-COVID-Disease-from-CT-Scan-Images-CNN-Tumuluru-Srinivas/5a06924852c7519edae223d7c3b40c7a8cf57cdf",
            "/paper/Image-Translation-by-Ad-CycleGAN-for-COVID-19-X-Ray-Liang-Huang/252138bb2182e34b780b29e79da13ae77d01875e",
            "/paper/FastDARTSDet%3A-Fast-Differentiable-Architecture-on-Wang-Wang/7285a7684787c63e824d7df40dd6c7ab10566279",
            "/paper/Pixel-Wise-Contrastive-Distillation-Huang-Guo/4eed71bbe5821e41bb118fd144e2686304272538",
            "/paper/CAViaR-WS-based-HAN%3A-conditional-autoregressive-at-Venkateswarlu-Shenoi/0e35f7e3a3b46b34fdcfc5a3f4141fbaa785dd05"
        ]
    },
    {
        "id": "39fc5bbed62308da62a34c896b685e70cef31cd3",
        "title": "Adversarial deep structured nets for mass segmentation from mammograms",
        "abstract": "A novel end-to-end network for mammographic mass segmentation is proposed which employs a fully convolutional network (FCN) to model a potential function, followed by a conditional random field (CRF) to perform structured learning. Mass segmentation provides effective morphological features which are important for mass diagnosis. In this work, we propose a novel end-to-end network for mammographic mass segmentation which employs a fully convolutional network (FCN) to model a potential function, followed by a conditional random field (CRF) to perform structured learning. Because the mass distribution varies greatly with pixel position, the FCN is combined with a position priori. Further, we employ adversarial training to eliminate over-fitting due to the small sizes of mammogram datasets. Multi-scale FCN is employed to improve the segmentation performance. Experimental results on two public datasets, IN breast and DDSM-BCRP, demonstrate that our end-to-end network achieves better performance than state-of-the-art approaches.1",
        "publication_year": "2017",
        "authors": [
            "Wentao Zhu",
            "Xiang Xiang",
            "T. Tran",
            "Gregory Hager",
            "Xiaohui Xie"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "107",
        "reference_count": "19",
        "references": [
            "/paper/Breast-Mass-Segmentation-and-Shape-Classification-Singh-Rashwan/52ae77b5c140f387ecdca06fdd2948e02da4aa3f",
            "/paper/A-Novel-Multi-Scale-Adversarial-Networks-for-of-Chen-Chen/aca85a9c9a6dd928e3b2fceb0cb5aa18bc8ab86f",
            "/paper/Breast-tumor-segmentation-and-shape-classification-Singh-Rashwan/1220fbc61ec47d5978d42c8b5a48b7a33b3c8374",
            "/paper/Improved-Breast-Mass-Segmentation-in-Mammograms-Li-Chen/eb5f164a5f3b9238831f186219169f4da0b565d4",
            "/paper/Cascaded-multi-scale-convolutional-encoder-decoders-Yan-Conze/33ef4f2862aa5542dbe28c5e66ad74e4ee58eb45",
            "/paper/Residual-Deep-Learning-System-for-Mass-Segmentation-Abdelhafiz-Nabavi/1134b99f815d7fffc703f4fbfe211c19da0917c5",
            "/paper/Brain-stroke-lesion-segmentation-using-consistent-Wang-Chen/c30535739d019e35197b775380df7528b040e424",
            "/paper/ASU-Net%3A-U-shape-adaptive-scale-network-for-mass-in-Sun-Xin/c2fc124b05d522aaa9a0d051659b6f8dae2e789e",
            "/paper/Convolutional-neural-network-for-automated-mass-in-Abdelhafiz-Bi/063a8d0d2023e6ec95487191e921b6ecda7efc24",
            "/paper/Mammographic-mass-segmentation-using-multichannel-Xu-Adeli/5f0b1a00267ec9e69a709851ebf6406c25cc91de",
            "/paper/Deep-structured-learning-for-mass-segmentation-from-Dhungel-Carneiro/ab0ef1360df100dfb41953d836814a908590064a",
            "/paper/Automatic-Liver-Segmentation-Using-an-Adversarial-Yang-Xu/8b6b0c1139a3ab405e6df58aaa79305867f453ef",
            "/paper/Tree-RE-weighted-belief-propagation-using-deep-for-Dhungel-Carneiro/cbf80ca27355db47c6d5bcfdd9a6a92ee065dbf3",
            "/paper/Deep-Multi-instance-Networks-with-Sparse-Label-for-Zhu-Lou/a7dfb1dc06bd1b68d8e35c460922f3c7354d921f",
            "/paper/Mammographic-Mass-Segmentation-with-Online-Learned-Jiang-Zhang/1eb5491275ff5dea07057ddd0f23b48799a7aea4",
            "/paper/Unsupervised-Deep-Learning-Applied-to-Breast-and-Kallenberg-Petersen/47c18899d0d264ff72ca2376ed8bb2d821f13a98",
            "/paper/Deep-Learning-and-Structured-Prediction-for-the-of-Dhungel-Carneiro/437fdba28304a2ff03f7f9a6b93be2f4af561371",
            "/paper/Digital-Mammographic-Computer-Aided-Diagnosis-(CAD)-Ball-Bruce/78002c425cae0ebc3e5506a242123e0004baaf25",
            "/paper/Conditional-Random-Fields-as-Recurrent-Neural-Zheng-Jayasumana/ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a",
            "/paper/Fully-convolutional-networks-for-semantic-Shelhamer-Long/317aee7fc081f2b137a85c4f20129007fd8e717e"
        ]
    },
    {
        "id": "ed79b3f0d185e70aad52f7bd49edf1b5ac247770",
        "title": "The Comparison of Whole Body Diffusion MRI with PET-CT in Cancer Diagnosis",
        "abstract": "The DWI was generally correlated withPET/CT with very close specificity and sensitivity values between the two methods, and can be used as an alternative or complementary to PET/CT in investigation and follow-up of oncological cases. Objective: Whole body diffusion-weighted imaging (DWI) has been increasingly used in oncological cases for detection and characterization of tumors and monitoring of treatment response. We compared the tumor detection capacity and diagnostic accuracy of DWI with positron emission tomography/computed tomography (PET/CT), which is regarded as the gold standard.Materials and Methods: The study included 29 adult patients (13 men and 16 women) aged between 38 and 86 years who had various types of cancer, and for whom PET/CT was indicated for staging or evaluating treatment response. A total of 240 lesions that were positive in FDGPET/CT were identified in the DWI images, and DWI signal intensity and apparent diffusion coefficient (ADC) value were measured for each lesion. SUVmax, ADC, and DWI intensity values of PET-positive lesions and lesion areas for both methods were compared using Mann-Whitney U test and Spearman correlation test.Results: SUVmax and DWI intensities of the lesions showed significant correlation (Spearman correlation coefficient= 0.296, p<0.0001). When we analyzed whether lesion size was associated with SUVmax, ADC, or DWI intensity, we found a correlation between lesion diameter and DWI intensity (r=-0.30; p=0.0001). Conclusion: The DWI was generally correlated with PET/CT with very close specificity and sensitivity values between the two methods. Whole body DWI can be used as an alternative or complementary to PET/CT in investigation and follow-up of oncological cases.",
        "publication_year": "2021",
        "authors": [
            "D. Akta\u015f",
            "Rumeysa Akt\u00fcrk",
            "M. Callioglu"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "14",
        "references": [
            "/paper/Diagnostic-value-of-diffusion-weighted-magnetic-to-Heusner-Kuemmel/a5fd2d31880ce230ce425417681ca6d6a00b3a1b",
            "/paper/Whole-body-MR-diffusion-weighted-imaging-in-Wilhelm-Stieltjes/a58f2e81f8e6bc169d0f8383c2608946d8e74d0e",
            "/paper/The-role-of-diffusion-weighted-magnetic-resonance-Bozgeyik-Onur/2bc845552ce7265de0b157a11c8dfc5006fb46de",
            "/paper/Computed-diffusion-weighted-MR-imaging-may-improve-Blackledge-Leach/f869d9b12a61a045a7a7b614c9de01b22ee010c4",
            "/paper/Complementary-Roles-of-Whole-Body-MRI-and-18F-FDG-Kwee-Takahara/c0b503b6e05d7ec08f59b61be598940d6d0bc768",
            "/paper/Diffusion-weighted-MRI-in-the-body%3A-applications-in-Koh-Collins/fc381b65aa18e62c109c2725fe0b8889527dc929",
            "/paper/Comparison-of-quantitative-T2-mapping-and-imaging-Gibbs-Tozer/e015ef0d154465b35a079e05444e818bed2366a9",
            "/paper/Diffusion-weighted-magnetic-resonance-imaging-and-Charles-Edwards-deSouza/2562ee066fd274ad0627551cd6abd031b04120b6",
            "/paper/False-positive-uptake-on-(FDG)-positron-emission-in-Culverwell-Scarsbrook/f7511f01451eb84ed5b89169c81653ca0b963ce5",
            "/paper/Diffusion-weighted-whole-body-imaging-with-body-and-Takahara-Imai/7772ce8c1fff127198a138e9dabf1276b7fb4d2c"
        ]
    },
    {
        "id": "e8729c3d7060475a9fe09ffea433f469f83a7c65",
        "title": "Editor's Picks April 2023",
        "abstract": "Two families with autosomal dominant inheritance of a peculiar clinical phenotype consisting of childhoodonset acral lamellar ichthyosis are described, found that this phenotype is caused by an amino acid substitution in the Cterminus of keratin 2. Inherited ichthyosis comprises a genetically heterogeneous group of disorders with skin dryness and scaling starting at birth or in childhood. Although ichthyoses are easy to recognize as a group of disorders, clinical subclassification and genotype\u2013 phenotype correlations are challenging because of the large number of genes and private diseasecausing variants. Frommherz et al. describe two families with autosomal dominant inheritance of a peculiar clinical phenotype consisting of childhoodonset acral lamellar ichthyosis (affecting the extremities). As shown in Figure 1, this was characterized by brown lamellar scaling without evidence of blistering, moulting or peeling. They found that this phenotype is caused by an amino acid substitution in the Cterminus of keratin 2. This molecular defect was not predicted by clinical or histologic features and the authors propose that it extends the spectrum of phenotypes associated with mutations in the gene for keratin 2. Acral lamellar ichthyosis may thus be genetically heterogeneous. Frommherz L, Komlosi K, Hewel C, et al. Acral lamellar ichthyosis with amino acid substitution in the Cterminus of keratin 2. J Eur Acad Dermatol Venereol 2023; 37:817822. doi: 10.1111/jdv.18719.",
        "publication_year": "2023",
        "authors": [
            "Ichth Yosis"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": 0,
        "reference_count": "2",
        "references": [
            "/paper/Brodalumab-in-plaque-psoriasis%3A-Real%E2%80%90world-data-on-Rompoti-Politou/f180a37e7cb66aeaace4542cfdf280e1c1b21293",
            "/paper/Determining-the-clinical-applicability-of-machine-A-Steele-Tan/3b4aef5c352d6301f183388a58e4522130fbb3a1"
        ]
    },
    {
        "id": "376fab5f7886ad389cb341a587796ca4df9ba016",
        "title": "Characteristics of publicly available skin cancer image datasets: a systematic review.",
        "abstract": "Semantic Scholar extracted view of \"Characteristics of publicly available skin cancer image datasets: a systematic review.\" by David Wen et al.",
        "publication_year": "2021",
        "authors": [
            "David Wen",
            "Saad M. Khan",
            "Antonio Ji Xu",
            "H. Ibrahim",
            "L. Smith",
            "Jose Caballero",
            "Luis Zepeda",
            "Carlos de Blas P\u00e9rez",
            "A. Denniston",
            "Xiaoxuan Liu",
            "R. Matin"
        ],
        "related_topics": [
            "Medicine",
            "Computer Science",
            "Environmental Science"
        ],
        "citation_count": "38",
        "reference_count": "77",
        "references": [
            "/paper/Scarcity-of-publicly-available-oral-cancer-image-Sengupta-Sarode/21e6688151f8210b2a72c73d661f76754b8b479e",
            "/paper/Publicly-available-datasets-of-breast-H%26E-images%3A-A-Tafavvoghi-Bongo/58229c28af74d3a0de745aaad0b8d693c197b7c8",
            "/paper/Absence-of-skin-of-colour-images-in-online-atlases-Ji-Xu-Artounian/9e69157dc5b335510001e18876164361c028e8ea",
            "/paper/Determining-the-clinical-applicability-of-machine-A-Steele-Tan/3b4aef5c352d6301f183388a58e4522130fbb3a1",
            "/paper/Deep-learning-in-computational-dermatopathology-of-Sauter-Lodde/540870d9f43ee30f8cf0d8ce5354b4497b2c765a",
            "/paper/An-Automated-and-Efficient-Deep-Learning-based-of-Ali-Joshi/a2a71d0325947c4992244068572f9434495af4a7",
            "/paper/Artificial-intelligence-and-machine-learning-for-of-Jones-Matin/759586ed8fe87a4dde9a802261b93fda84125218",
            "/paper/Towards-Transparency-in-Dermatology-Image-Datasets-Groh-Harris/2d47fe33b68f36a06e576dee9c13ee38fbd62220",
            "/paper/AI-Powered-Diagnosis-of-Skin-Cancer%3A-A-Contemporary-Melarkode-Srinivasan/164e14a497f49ce5694c48e1c04a3125aeabec6e",
            "/paper/A-Secure-Framework-toward-IoMT-Assisted-Data-and-Islam-Kaushal/7482cfd5700e3a020c525ea4ba4eadebcd13cc0b",
            "/paper/A-global-review-of-publicly-available-datasets-for-Khan-Liu/1b0782dd263fc8a1455f385b7f603a9c1e499cd6",
            "/paper/A-patient-centric-dataset-of-images-and-metadata-Rotemberg-Kurtansky/2ffcf95e4916d3a44c474f7a6cccaabe9d11df83",
            "/paper/The-HAM10000-dataset%2C-a-large-collection-of-images-Tschandl-Rosendahl/f95f5b715eb0441ca4ee1b0fac6b4bcaaba65556",
            "/paper/Comparison-of-the-accuracy-of-human-readers-versus-Tschandl-Codella/4e50a8356f6e8239e08418b260bc6cbe640a512c",
            "/paper/Dermatologist-level-classification-of-skin-cancer-Esteva-Kuprel/e1ec11a1cb3d9745fb18d3bf74247f95a6663d08",
            "/paper/Skin-Lesion-Analysis-Toward-Melanoma-Detection-A-by-Codella-Rotemberg/1f3ff6ca41574a7136f5fe4925b7bc54189837fb",
            "/paper/PAD-UFES-20%3A-A-skin-lesion-dataset-composed-of-data-Pacheco-Lima/6d7a6d578e9fa9b2e488ac98f14cc2bd610477ca",
            "/paper/Assessment-of-deep-neural-networks-for-the-of-and-A-Han-Moon/4caf1a8cb0cf7e49349aa7ec079701a1210b2818",
            "/paper/Multimodal-skin-lesion-classification-using-deep-Yap-Yolland/8196df42cf3383fc531a6630957908af50681fa2",
            "/paper/Computer-Algorithms-Show-Potential-for-Improving-to-Marchetti-Liopyris/77487ddfa40f1e6ae2040b60136a03aacc1501d3"
        ]
    },
    {
        "id": "3b209d729819c233509ab532a2cde8a64f62a6ac",
        "title": "Prediction models applying machine learning to oral cavity cancer outcomes: A systematic review",
        "abstract": "Semantic Scholar extracted view of \"Prediction models applying machine learning to oral cavity cancer outcomes: A systematic review\" by John Adeoye et al.",
        "publication_year": "2021",
        "authors": [
            "John Adeoye",
            "Jiaqing Tan",
            "Siu-Wai Choi",
            "P. Thomson"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "18",
        "reference_count": 0,
        "references": [
            "/paper/Comparison-of-time-to-event-machine-learning-models-Adeoye-Hui/a2673efe56b05b5f251906f5eb82704d7baa1540",
            "/paper/Application-and-Performance-of-Artificial-(AI)-in-A-Khanagar-Alkadi/4e1a7210dc8dedbed521c96691e5c71483904095",
            "/paper/Data-centric-artificial-intelligence-in-oncology%3A-a-Adeoye-Hui/66e46478fc83b19353a5fd67044f2592ff511a9b",
            "/paper/Explainable-ensemble-learning-model-improves-of-for-Adeoye-Zheng/5fc799de1c03d9babaa4355528c6bdaa8986bc30",
            "/paper/Improvement-of-Mucosal-Lesion-Diagnosis-with-Based-Dubuc-Zitouni/128b125d70e43a8e9dfc568c339eea461e243dbe",
            "/paper/A-retrospective-analysis-based-on-multiple-machine-Yang-Mart%C3%ADnez-Useros/7753275a86698ad6c0991526569e384c24a05dfd",
            "/paper/Deep-Learning-Predicts-the-Survival-of-Oral-Adeoye-Koohi-Moghadam/6fd917fa772b59ea2ba94cc0042d71b97bed06ee",
            "/paper/Predicting-oral-cancer-risk-in-patients-with-oral-Adeoye-Koohi-Moghadam/be19476b98e6001a4ea2a61218923892ac4231e4",
            "/paper/Comparison-of-nomogram-with-random-survival-forest-Zhang-Liang/70beb8e19140b7b48c1178f66e12fa3a9386c282",
            "/paper/A-Current-Review-of-Machine-Learning-and-Deep-in-Dixit-Kumar/fb5fcf878c1f390c0eb1b81e30a8672589459117"
        ]
    },
    {
        "id": "80bd89852cc9230562c63983c3bcb75619bb14e5",
        "title": "Deep Learning Application for Analyzing of Constituents and Their Correlations in the Interpretations of Medical Images",
        "abstract": "The novelty in this paper consists primarily in the unitary approach, of the constituent elements of DL models, namely, data, tools used by DL architectures or specifically constructed DL architecture combinations and highlighting their \u201ckey\u201d features, for completion of tasks in current applications in the interpretation of medical images. The need for time and attention, given by the doctor to the patient, due to the increased volume of medical data to be interpreted and filtered for diagnostic and therapeutic purposes has encouraged the development of the option to support, constructively and effectively, deep learning models. Deep learning (DL) has experienced an exponential development in recent years, with a major impact on interpretations of the medical image. This has influenced the development, diversification and increase of the quality of scientific data, the development of knowledge construction methods and the improvement of DL models used in medical applications. All research papers focus on description, highlighting, classification of one of the constituent elements of deep learning models (DL), used in the interpretation of medical images and do not provide a unified picture of the importance and impact of each constituent in the performance of DL models. The novelty in our paper consists primarily in the unitary approach, of the constituent elements of DL models, namely, data, tools used by DL architectures or specifically constructed DL architecture combinations and highlighting their \u201ckey\u201d features, for completion of tasks in current applications in the interpretation of medical images. The use of \u201ckey\u201d characteristics specific to each constituent of DL models and the correct determination of their correlations, may be the subject of future research, with the aim of increasing the performance of DL models in the interpretation of medical images.",
        "publication_year": "2021",
        "authors": [
            "Tudor Florin Ursuleanu",
            "Andreea Roxana Luca",
            "L. Gheorghe",
            "Roxana Grigorovici",
            "Stefan Iancu",
            "Maria Hlusneac",
            "C. Preda",
            "A. Grigorovici"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "7",
        "reference_count": "281",
        "references": [
            "/paper/Compatible-domain-Transfer-Learning-for-Breast-with-Shamshiri-Krzy%C5%BCak/dd9343d271cf76759172d94e29ae8cc9e8c5c89e",
            "/paper/Smart-Visualization-of-Medical-Images-as-a-Tool-in-Simovic-Lutovac-Banduka/1abb539aa428d0539021f33426c241d23007e1c5",
            "/paper/Feature-Selection-Pipeline-based-on-Hybrid-Approach-Kaur-Singh/166858a7c4a6a17f84634a30730620af028ebe89",
            "/paper/The-Applications-of-Artificial-Intelligence-in-Argentiero-Muscogiuri/5fbcc348ee2cb56fa280ab32b356ab0533d9d969",
            "/paper/Breast-Cancer-Prediction-and-Control-Using-BiLSTM-Agana-Agwu/130abeda191e3a7f9399fb972762ebfe69319d97",
            "/paper/Imaging-in-translational-cancer-research-Kurz-Schlemmer/49ca1d3b4f10278621eacc4f04446bc2fb4e1b6a",
            "/paper/Impact-of-quality%2C-type-and-volume-of-data-used-by-Luca-Ursuleanu/737939a9b9e41d37d45648e5639f0b49b3e982b3",
            "/paper/Unified-Analysis-Specific-to-the-Medical-Field-in-Ursuleanu-Luca/35d837c1207fddbd93c360c0389e94888db440a0",
            "/paper/Towards-a-Better-Understanding-of-Transfer-Learning-Alzubaidi-Fadhel/637d46a33ad7f5742c45bab36a60fe2bebde6f85",
            "/paper/Designing-a-High-Performance-Deep-Learning-Model-by-Luca-Ursuleanu/c008552c26d418fe44374544d1043cd3ecfd4190",
            "/paper/Classification-of-Medical-Images-and-Illustrations-Zhang-Xia/4b87374000f88f413145b1fb496b494d1f75e3f3",
            "/paper/An-overview-of-deep-learning-in-medical-imaging-on-Lundervold-Lundervold/0ebc300c16f01a4e94c8551997922fdb67ac1951",
            "/paper/A-survey-on-incorporating-domain-knowledge-into-for-Xie-Niu/426a946928037a7f89599cb11b79e62dd973494c",
            "/paper/A-comprehensive-survey-of-deep-learning-in-the-of-Pandey-Pandey/d8aacc4fcbfdf170a6a9fa764a6b6faf0221d01f",
            "/paper/Curriculum-learning-for-annotation-efficient-image-Jim'enez-S'anchez-Mateus/b237164b5487d4a50b9bf947e203b4dd17650294",
            "/paper/Novel-Transfer-Learning-Approach-for-Medical-with-Alzubaidi-Al-Amidie/7659a03a1b2058f701434509f258b4521c0358b9",
            "/paper/Adaptive-Augmentation-of-Medical-Data-Using-Pesteie-Abolmaesumi/7e44e16d7db23f744ffac5198e8f6748aaee1d9d"
        ]
    },
    {
        "id": "5c2c03f1bd56f3825197962bd73a69def80a0d41",
        "title": "Dense Dilated Convolutions\u2019 Merging Network for Land Cover Classification",
        "abstract": "The proposed DDCM-Net consists of dense dilated image convolutions merged with varying dilation rates, which effectively utilizes rich combinations of dilated convolutions that enlarge the network\u2019s receptive fields with fewer parameters and features compared with the state-of-the-art approaches in the remote sensing domain. Land cover classification of remote sensing images is a challenging task due to limited amounts of annotated data, highly imbalanced classes, frequent incorrect pixel-level annotations, and an inherent complexity in the semantic segmentation task. In this article, we propose a novel architecture called the dense dilated convolutions\u2019 merging network (DDCM-Net) to address this task. The proposed DDCM-Net consists of dense dilated image convolutions merged with varying dilation rates. This effectively utilizes rich combinations of dilated convolutions that enlarge the network\u2019s receptive fields with fewer parameters and features compared with the state-of-the-art approaches in the remote sensing domain. Importantly, DDCM-Net obtains fused local- and global-context information, in effect incorporating surrounding discriminative capability for multiscale and complex-shaped objects with similar color and textures in very high-resolution aerial imagery. We demonstrate the effectiveness, robustness, and flexibility of the proposed DDCM-Net on the publicly available ISPRS Potsdam and Vaihingen data sets, as well as the DeepGlobe land cover data set. Our single model, trained on three-band Potsdam and Vaihingen data sets, achieves better accuracy in terms of both mean intersection over union (mIoU) and F1-score compared with other published models trained with more than three-band data. We further validate our model on the DeepGlobe data set, achieving state-of-the-art result 56.2% mIoU with much fewer parameters and at a lower computational cost compared with related recent work.",
        "publication_year": "2020",
        "authors": [
            "Qinghui Liu",
            "Michael C. Kampffmeyer",
            "R. Jenssen",
            "A. Salberg"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "66",
        "reference_count": "49",
        "references": [
            "/paper/MCFINet%3A-Multidepth-Convolution-Network-With-for-in-Wang-Dong/7a209e1150e56c015ae9791a32957e2c15758617",
            "/paper/Land-cover-classification-from-remote-sensing-based-Li-Zheng/880785e1bb10926d94f191ac01c376d46360fac4",
            "/paper/Class-Wise-Fully-Convolutional-Network-for-Semantic-Tian-Chu/a92cadf5a6f5859b5c71828ddbb629a99aabde5d",
            "/paper/Multiattention-Network-for-Semantic-Segmentation-of-Li-Zheng/561f5bd4dd8db04e8b74c12b225757e0665b707a",
            "/paper/ABCNet%3A-Attentive-Bilateral-Contextual-Network-for-Li-Duan/f2e0cce0fa68a65b2e3cf254505439d325ce092c",
            "/paper/Full-Semantic-Constructed-Network-for-Urban-Use-Dong-Zhuang/f50d880d153a8c23654e622fa239b7d113cc09a1",
            "/paper/Multi-Attention-Network-for-Semantic-Segmentation-Li-Zheng/6ad809ce3cafcf5a83fd236c07092641b610a696",
            "/paper/JSH-Net%3A-joint-semantic-segmentation-and-height-Zhang-Wan/163c8a2233d4cb540fe67e8d9f04cec8b773be0b",
            "/paper/Integrating-Gate-and-Attention-Modules-for-Image-Zheng-Zhang/ebad5fa233a8ceb7b32c5f35a0220923a9d00a71",
            "/paper/A-Synergistical-Attention-Model-for-Semantic-of-Li-Xu/1fde7a08df562c51863798b3226921e144ee72d2",
            "/paper/Dense-Dilated-Convolutions-Merging-Network-for-of-Liu-Kampffmeyer/e4d3d01f876b7518e9a8b924d0d2d01014320b5d",
            "/paper/Semantic-Segmentation-of-Small-Objects-and-Modeling-Kampffmeyer-Salberg/c92ab8d1139290e91d0da9c7b63b69f3b20ebf49",
            "/paper/Fully-Convolutional-Networks-for-Dense-Semantic-of-Sherrah/83ef7de2669bb2827208fd3a64ac910e276fbdb4",
            "/paper/A-Comparison-of-Deep-Learning-Architectures-for-of-Liu-Salberg/66586f1d755362f485f25acfc60153c2a5ed1533",
            "/paper/Gated-Convolutional-Neural-Network-for-Semantic-in-Wang-Wang/8c4ea76e67a2a99339a8c4decd877fe0aa2d8e82",
            "/paper/DeepLab%3A-Semantic-Image-Segmentation-with-Deep-and-Chen-Papandreou/cab372bc3824780cce20d9dd1c22d4df39ed081a",
            "/paper/Dense-Fusion-Classmate-Network-for-Land-Cover-Tian-Li/823f69be011944780b137937dd40b1ce1a19bd45",
            "/paper/Revisiting-Dilated-Convolution%3A-A-Simple-Approach-Wei-Xiao/f875489e265efcab0e0bb958248f3d49ae299c7f",
            "/paper/Feature-Pyramid-Network-for-Multi-class-Land-Seferbekov-Iglovikov/7aae14c686757ba33d49c49aef4193038a4a7529",
            "/paper/Understanding-Convolution-for-Semantic-Segmentation-Wang-Chen/84c1717345dd451e7a61fe89807b4c017754fc4e"
        ]
    },
    {
        "id": "62812801097f18b039cb09211ff8847ea7f603ec",
        "title": "Analytical solution approach for nonlinear vibration of shear deformable imperfect FG-GPLR porous nanocomposite cylindrical shells",
        "abstract": "Abstract This study presents an analytical solution approach to examine the nonlinear vibration of geometrically imperfect functionally graded porous circular cylindrical shells reinforced with graphene platelets (GPL) surrounded on an elastic foundation. First-order shear deformation theory is employed to formulate the considered problem. Four porosity distributions and four GPLs dispersion patterns are considered which vary through the thickness direction. The effective mechanical properties of considered functionally graded graphene platelet-reinforced porous nanocomposites are characterized via a micromechanical model. Governing equations are derived by Hamilton\u2019s principle and then were transformed into a set of ordinary differential equations using the Galerkin method. Afterward, the nonlinear frequency response curves are obtained with the use of the method of multiple scales. Numerical results are provided to explore the effect of parameters such as initial imperfection, geometry, porous distribution, porosity coefficient, and GPLs\u2019 scheme and weight fraction on the nonlinear frequency-response curve.",
        "publication_year": "2021",
        "authors": [
            "Mahdi Salehi",
            "R. Gholami",
            "R. Ansari"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": "10",
        "reference_count": "63",
        "references": [
            "/paper/Numerical-Study-on-the-Buckling-Behavior-of-FG-Caps-Zhou-Wang/a72a09d154c3fb78411ba7f712af26877f79dd83",
            "/paper/Nonlinear-low-velocity-impact-response-of-GRC-beam-Zhang-Guo/0c2525c39e1eb333bb2272f09d6f0204900ea09c",
            "/paper/Free-vibration-and-stability-of-hybrid-shallow-an-Shahmohammadi-Mirfatah/55a135d81a77e2f539cd4cba7690fb7527ceae37",
            "/paper/Nonlinear-buckling-analysis-of-stiffened-FG-GRC-to-Vu-Nguyen/a1559f91f192257f6f92e76143501ecf5ea699a0",
            "/paper/A-Size-Dependent-Finite-Element-Method-for-the-3D-Wu-Tan/3eb5ca007c9aef122061ecfa61ee79e4f216dff7",
            "/paper/Parameter-Interval-Uncertainty-Analysis-of-Internal-Cai-Liu/56165069847cc56a6d255fa39ca20d770d12db6f",
            "/paper/Investigation-on-nonlinear-bending-behavior-of-with-Kumar-Hirwani/f9e293346d97775d48d6bf9656ea54684ae2e80b",
            "/paper/Active-flutter-suppression-of-damaged-variable-with-Sharma-Swain/05c67fe0d168e6246f9315f27a49a9e908f86c3d",
            "/paper/Vibrations-of-graphene-platelet-reinforced-doubly-Esmaeili-Kiani/3535dbc5ac6acd675af8801c61074d1e1b318f57",
            "/paper/Vibration-and-nonlinear-dynamic-response-of-double-Cong-Trung/4540fae54510b975c2fb9fa909b4ab3b3081db9b",
            "/paper/Nonlinear-bending-analysis-of-arbitrary-shaped-a-Ansari-Hassani/8c880fc3d527d8f523bd6dd691bee34df23f9d03",
            "/paper/On-vibration-and-stability-analysis-of-porous-by-Saidi-Bahaadini/3b2bb9fc0443b7c7febcd60e3a89991b7ee2b367",
            "/paper/Nonlinear-vibration-of-metal-foam-cylindrical-with-Wang-Ye/73d5664b988d921d7fe61d8bdbf3e925a7dccde9",
            "/paper/Three-dimensional-static-and-free-vibration-of-Rahimi-Alibeigloo/a9dee1bf9f8ff476c3c6dc4f2776f2aa8947ceef",
            "/paper/Vibration-characteristics-of-functionally-graded-Dong-Li/bbe275b8bcd55bf861039f2582570db43053a83f",
            "/paper/Accurate-nonlinear-buckling-analysis-of-graded-Zhou-Ni/6851f27103819a0bdd7c95183892feef953cc17a",
            "/paper/Geometrically-nonlinear-resonance-of-higher-order-Gholami-Ansari/c677e3dc6d377a1f622ac4d26f7f7edab82f8070",
            "/paper/Nonlinear-harmonically-excited-vibration-of-shear-Gholami-Ansari/46e8188c3efdab3a04028ecfeaef87f8f83decbb",
            "/paper/A-comprehensive-analysis-of-porous-curved-beams-by-Anirudh-Ganapathi/67746765435e915de7d49bd798b5a8683a59cafa",
            "/paper/Free-vibration-of-functionally-graded-porous-shell-Wang-Wu/2ce34b5d75c0faf33b39613d247eaeccecb85b82"
        ]
    },
    {
        "id": "0ba9140dad37b1b3d8cfcdb6d54389edd701c58d",
        "title": "VolumeFusion: Deep Depth Fusion for 3D Scene Reconstruction",
        "abstract": "This paper advocates that replicating the traditional two stages framework with deep neural networks improves both the interpretability and the accuracy of the results of 3D reconstruction techniques. To reconstruct a 3D scene from a set of calibrated views, traditional multi-view stereo techniques rely on two distinct stages: local depth maps computation and global depth maps fusion. Recent studies concentrate on deep neural architectures for depth estimation by using conventional depth fusion method or direct 3D reconstruction network by regressing Truncated Signed Distance Function (TSDF). In this paper, we advocate that replicating the traditional two stages framework with deep neural networks improves both the interpretability and the accuracy of the results. As mentioned, our network operates in two steps: 1) the local computation of the local depth maps with a deep MVS technique, and, 2) the depth maps and images\u2019 features fusion to build a single TSDF volume. In order to improve the matching performance between images acquired from very different viewpoints (e.g., large-baseline and rotations), we introduce a rotation-invariant 3D convolution kernel called PosedConv. The effectiveness of the proposed architecture is underlined via a large series of experiments conducted on the ScanNet dataset where our approach compares favorably against both traditional and deep learning techniques.",
        "publication_year": "2021",
        "authors": [
            "Jaesung Choe",
            "Sunghoon Im",
            "Fran\u00e7ois Rameau",
            "Minjun Kang",
            "In-So Kweon"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "27",
        "reference_count": "44",
        "references": [
            "/paper/SimpleRecon%3A-3D-Reconstruction-Without-3D-Sayed-Gibson/caf96058774d96dd08df799319ef0d5430121518",
            "/paper/FineRecon%3A-Depth-aware-Feed-forward-Network-for-3D-Stier-Ranjan/e00fe237ff1d0daa918a0a203c94b70e9d14b94b",
            "/paper/VisFusion%3A-Visibility-aware-Online-3D-Scene-from-Gao-Mao/a97cb7fbb308e9a8f020b38e1b091dcfe4763a43",
            "/paper/Cross-Dimensional-Refined-Learning-for-Real-Time-3D-Hong-Yue/a3272f80526e0ea32f5985e640fb5a388fbc02e6",
            "/paper/MonoNeuralFusion%3A-Online-Monocular-Neural-3D-with-Zou-Huang/e605271aefa33cdf4c77e0ea801cd8e2410fceb0",
            "/paper/Heightfields-for-Efficient-Scene-Reconstruction-for-Watson-Vicente/fbd4f728870ce1da8d9b0fb7a18be762fef466b0",
            "/paper/3D-Surface-Reconstruction-in-the-Wild-by-Deforming-Hani-Chao/33a4c0c6f3171b2909039546f3973aa45ee52ff5",
            "/paper/Learning-Continuous-Depth-Representation-via-Wang-Chen/9dc77eb07e11d3fd09f2dd5d2225ded4291f6d3b",
            "/paper/ESLAM%3A-Efficient-Dense-SLAM-System-Based-on-Hybrid-Johari-Carta/4029ade0264485431f7ed98e76b2f789208e3028",
            "/paper/Temporally-Consistent-Online-Depth-Estimation-Using-Khan-Penner/f9a0cd54d5feed0ab8726af2eb108845bb86d58f",
            "/paper/Atlas%3A-End-to-End-3D-Scene-Reconstruction-from-Murez-As/35e2056e29d5295b2669fe696ae0c27007c94625",
            "/paper/NeuralRecon%3A-Real-Time-Coherent-3D-Reconstruction-Sun-Xie/8db0bf0fa406254d4cd57eb413443a0b96bd12b8",
            "/paper/DPSNet%3A-End-to-end-Deep-Plane-Sweep-Stereo-Im-Jeon/01dd15f339e8da4eb9a23e47983c1b7c480c7196",
            "/paper/Normal-Assisted-Stereo-Depth-Estimation-Kusupati-Cheng/6a775d40093d0cca245d2922d5b5417524b1ac1e",
            "/paper/TransformerFusion%3A-Monocular-RGB-Scene-using-Bovzivc-Palafox/64ac6c834bee275eb17f9a34d58ebf18e0f98e6a",
            "/paper/Point-Based-Multi-View-Stereo-Network-Chen-Han/58a309e2ab84c66450480806e09b8a55f455c218",
            "/paper/RoutedFusion%3A-Learning-Real-Time-Depth-Map-Fusion-Weder-Sch%C3%B6nberger/9a99e0bf8f8192b2e144d28f46ef1a415646ad10",
            "/paper/MVSNet%3A-Depth-Inference-for-Unstructured-Multi-view-Yao-Luo/87ca28235555f7e70cf1edc2a63cda4aef7fee42",
            "/paper/MVDepthNet%3A-Real-Time-Multiview-Depth-Estimation-Wang-Shen/c1c8980e423d63942d4ffc31dbffbd3aeb9213a7",
            "/paper/NeuralFusion%3A-Online-Depth-Fusion-in-Latent-Space-Weder-Sch%C3%B6nberger/766d74b0a611f06e66af539a1543f003c94f931a"
        ]
    },
    {
        "id": "29616548010ddcdc3644a1cf1e2821a76f3c6a56",
        "title": "Efficient face recognition with compensation for aging variations",
        "abstract": "A novel self-PCA based approach in order to consider distinctiveness of the effects of aging of a person for age invariant face recognition using the region around the eyes is used as the input feature instead of the entire face as it is more stable part of the face with respect to aging and also requires less space. Face recognition is identifying a person based on facial characteristics. Automated face recognition is identifying a given query face called probe from a target population known as gallery. The face recognition algorithms perform well when the interpersonal images have more discriminating features than intra personal images. The changes in the face bring down the similarity of the intrapersonal images. The variations in the face can be due to pose, expression, illumination changes and aging of a person. Face recognition accuracy is largely influenced by the age related changes in face. Aging effects on face are not uniform and depend on both intrinsic as well as external factors like geographic location, race, food habits etc. The facial changes are exclusive for each person in spite of aging being an apparent phenomenon among all individuals. Hence there are many challenges still open in compensating age related variations. In this paper we have proposed a novel self-PCA based approach in order to consider distinctiveness of the effects of aging of a person for age invariant face recognition. The region around the eyes is used as the input feature instead of the entire face as it is more stable part of the face with respect to aging and also requires less space. The proposed approach is tested using the images of the FG-NET database.",
        "publication_year": "2012",
        "authors": [
            "J. S. Nayak",
            "M. Indiramma"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "14",
        "reference_count": "18",
        "references": [
            "/paper/Survey-on-Age-Invariant-Facial-Recognition-Kumar-Srivastava/f05c892a9d86558c6b001379798142ac2afd0b28",
            "/paper/Study-on-Age-Invariant-Face-Recognition-System-Singh-Medhi/52966dbdb8e2d4a560e3848b13b0e87267c16226",
            "/paper/Age-invariant-face-recognition-using-Frangi2D-Afroze-Beham/13e15d950ae479d0cc40270a067f9bde8434d6a9",
            "/paper/Analysis-on-Age-Invariance-Face-Recognition-Study-Dhamija/1ec5efb4f4e4c02286611f0782ca589f5ceab629",
            "/paper/Face-Detection-and-Recognition-using-Viola-Jones-of-Deshpande-Ravishankar/c5cfc1f5a430ad9c103b381d016adb4cba20ce4e",
            "/paper/SECURITY-SYSTEM-USING-VIOLA-JONES-FACE-DETECTION-pathare-saini/bef3cc76c5cbbffb2ad9b561dd903b495e940d41",
            "/paper/Pose-Estimation-Using-LBP-Solai-Raajan/bbbeb643ff9414d66b84bb96af9bb8b9ca1ad621",
            "/paper/Developmental-network-and-its-application-to-face-Wang-Zheng/dff9a5640e7bdf88199096be4b520e04e0b9730e",
            "/paper/Face-Detection-and-Recognition-using-Viola-Jones-of-Deshpande/f7d5c2927aac01868a3e6be459809dedb17a5891",
            "/paper/An-Age-Variant-Face-Recognition-System-Walia-Chaudhary/398bd6a2f2b24e7f763eea617cd417e16d204faf",
            "/paper/Age-Invariant-Face-Recognition-Park-Tong/f2264d05eaf42572bfdd779f72cfadcafcc0c5f3",
            "/paper/Face-Verification-Across-Age-Progression-Ramanathan-Chellappa/5af32f9d079bb6d12b1175ee38e83a71b9516fe3",
            "/paper/Toward-Automatic-Simulation-of-Aging-Effects-on-Lanitis-Taylor/ca5b01a31a8653cc048344b0806c756fb53e9cde",
            "/paper/A-Non-generative-Approach-for-Face-Recognition-Biswas-Aggarwal/0989d838fe4d707151ee1a45b3f4b01e0b50d1b9",
            "/paper/A-Study-of-Face-Recognition-as-People-Age-Ling-Soatto/d5f1886482893adf2f61e3b344d5483c86792ab5",
            "/paper/Face-Recognition-using-Principle-Component-Analysis-Dabhade-Bewoor/a88dcb7563f7964c281bd93f35492f17abb4c691",
            "/paper/Face-Recognition-Algorithms-Surpass-Humans-Matching-O%E2%80%99Toole-Phillips/e7893c0ac5120b3a14ebfc5a4aad69562ce821c4",
            "/paper/Investigating-age-invariant-face-recognition-based-Juefei-Xu-Luu/5833547ef408a04b2efc49bcba4854ba3009585c",
            "/paper/Effects-of-Aging-over-Facial-Feature-Analysis-and-Esme-Sankur/fcd3d69b418d56ae6800a421c8b89ef363418665",
            "/paper/Fully-automatic-pose-invariant-face-recognition-via-Asthana-Marks/3957b51b44f8727fe008162ea8a142a8c7917dea"
        ]
    },
    {
        "id": "a314be3266c34bd127aaca6f3d9095048e973b75",
        "title": "YOLODrone: Improved YOLO Architecture for Object Detection in Drone Images",
        "abstract": "This paper introduces an improved YOLO algorithm: YOLODrone for detecting objects in drone images, and evaluates the algorithm on VisDrone2019 dataset and reports improved results when compared to Y OLOv3 algorithm. Recent advances in robotics and computer vision fields yield emerging new applications for camera equipped drones. One such application is aerial-based object detection. However, despite the recent advances in the relevant literature, object detection remains as a challenging task in computer vision. Existing object detection algorithms demonstrate even lower performance on drone (or aerial) images since the object detection problem is a more challenging problem in aerial images, when compared to the detection task in ground-taken images. There are many reasons for that including: (i) the lack of large drone datasets with large object variance, (ii) the larger variance in both scale and orientation in drone images, and (iii) the difference in shape and texture features between the ground and the aerial images. In this paper, we introduce an improved YOLO algorithm: YOLODrone for detecting objects in drone images. We evaluate our algorithm on VisDrone2019 dataset and report improved results when compared to YOLOv3 algorithm.",
        "publication_year": "2021",
        "authors": [
            "Oyku Sahin",
            "S. Ozer"
        ],
        "related_topics": [
            "Computer Science",
            "Environmental Science"
        ],
        "citation_count": "11",
        "reference_count": "26",
        "references": [
            "/paper/YOLODrone%2B%3A-Improved-YOLO-Architecture-for-Object-Sahin-Ozer/66bcc4ae4829ff707c4594b1ca8a2b32e94662b4",
            "/paper/Target-Detection-Method-of-UAV-Aerial-Imagery-Based-Luo-Wu/ec8c9e9b450bd1ef413844f08c6f278f95bb3385",
            "/paper/An-Upgraded-YOLO-with-Object-Augmentation%3A-Mini-UAV-Delleji-Slimeni/4cfa533e84cc958dd4948403f3c82b251a02c070",
            "/paper/Object-Detection-Algorithm-based-on-Structural-in-Singh-Nene/ce557be248ed74100f2da0f91236326927dae8db",
            "/paper/Influence-of-Insufficient-Dataset-Augmentation-on-Bo%C5%BCko-Ambroziak/7b965205db1de57e060b9287926d9b6415260111",
            "/paper/Cross-Layer-Triple-Branch-Parallel-Fusion-Network-Liang-Su/13d90c151c9c5029dd696f1e210db7a4d07dc1dc",
            "/paper/Offloading-Deep-Learning-Powered-Vision-Tasks-From-Ozer-Ilhan/93781858dbb771fb71c4bfbc2ac721fd1ed6b395",
            "/paper/A-Comprehensive-Review-of-YOLO%3A-From-YOLOv1-and-Terven-C%C3%B3rdova-Esparza/ef321ff571c234ca2396aa6d1f9cc1928ca7cb90",
            "/paper/A-deep-learning%2C-vision-based-framework-for-testing-Sarkar-Johnson/5661f4ec9e48687d18a109041962fa488775eff7",
            "/paper/Using-Deep-Compression-on-PyTorch-Models-for-Dogan-Ugurdag/0e64698cac49bee13d6dd15d77ab99d656729c0b",
            "/paper/SyNet%3A-An-Ensemble-Network-for-Object-Detection-in-Albaba-Ozer/415e276ef5f308361b7a1c22b587372b71b456b4",
            "/paper/VisDrone-DET2019%3A-The-Vision-Meets-Drone-Object-in-Zhu-Wen/291a0754ea1e073ef2706dcb4384e5deb034da96",
            "/paper/Car-Detection-using-Unmanned-Aerial-Vehicles%3A-R-CNN-Benjdira-Khursheed/6bf5593d678592eda47b0a26d3dbed6fdfe998f0",
            "/paper/Visual-Object-Tracking-in-Drone-Images-with-Deep-G%C3%B6zen-Ozer/86293304e7fb8137d2278592bf804d0a643f42b3",
            "/paper/DOTA%3A-A-Large-Scale-Dataset-for-Object-Detection-in-Xia-Bai/8ecf9e6428edd33986e8c6a143870e57142fd5a1",
            "/paper/Object-Detection-of-UAV-for-Anti-UAV-Based-on-YOLO-Hu-Wu/d572a4077036b3684502b9cb0fd0d44bcbabe9c1",
            "/paper/YOLO9000%3A-Better%2C-Faster%2C-Stronger-Redmon-Farhadi/7d39d69b23424446f0400ef603b2e3e22d0309d6",
            "/paper/Orientation-robust-object-detection-in-aerial-using-Zhu-Chen/ffc2f8c1bf568b913427d1702d75306311df3d5b",
            "/paper/You-Only-Look-Once%3A-Unified%2C-Real-Time-Object-Redmon-Divvala/f8e79ac0ea341056ef20f2616628b3e964764cfd",
            "/paper/xView%3A-Objects-in-Context-in-Overhead-Imagery-Lam-Kuzma/7b3a63d030d03e536ddcbc217bc8d6fd630e3b53"
        ]
    },
    {
        "id": "2fa66426a51b0d838c9a51a5cdcdcb5a923f3efc",
        "title": "A simple and robust approach to drift reduction in motion estimation of human body",
        "abstract": "This paper proposes a simple and robust method of drift reduction that measures a pose by model fitting even to the final frame, and calculates the drift between the final pose and the pose obtained as a result of accumulation. Incremental tracking of the human body is performed by accumulating pose displacements frame by frame over the initial pose at the starting frame. The initial pose is measured by fitting an articulated model to the human body in an image, and the pose displacement can be estimated from the intensity difference between successive frames. However, the incremental approach faces a difficulty, namely, drift by accumulation of estimation errors of pose displacements. This paper proposes a simple and robust method of drift reduction. The proposed method measures a pose by model fitting even to the final frame, and calculates the drift between the final pose and the pose obtained as a result of accumulation. The mean drift between the frames is then calculated from the drift at the final frame, and the pose is corrected by subtracting the mean drift from the pose by accumulation in turn. Several iterations of motion estimation and pose correction make the model fit the human body in the intermediate frames. Pose correction can be performed simply by solving a system of linear equations in three unknowns. Several experiments in body tracking prove the proposed method to be effective. \u00a9 2006 Wiley Periodicals, Inc. Electron Comm Jpn Pt 3, 89(8): 39\u201352, 2006; Published online in Wiley InterScience (www.interscience.wiley.com). DOI 10.1002/ecjc.20264",
        "publication_year": "2006",
        "authors": [
            "Masanobu Yamamoto"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3",
        "reference_count": "16",
        "references": [
            "/paper/Pan-zoom-Motion-Capture-in-Wide-Scenes-using-Yamamoto/85b1502c51b9b6d74dc0de5d95cb404993ea290b",
            "/paper/Real-time-Upper-Body-Pose-Detection-using-Stereo-Jeong-Shin/8edead66ea057a753cb45252d0d7460ffb3fdd49",
            "/paper/Motion-Capture-for-Skillful-Handworks-in-Production-Miura-Ohsawa/346297d1782d73982633403f3b280f3032d220be",
            "/paper/Incremental-tracking-of-human-actions-from-multiple-Yamamoto-Sato/64356216ce400a0845e95d464718d85babb3a6e6",
            "/paper/Human-action-tracking-guided-by-key-frames-Yamamoto-Ohta/d1d9f0155ec3c32e42f2cbc464b58be9879a5bd0",
            "/paper/3-D-model-based-tracking-of-humans-in-action%3A-a-Gavrila-Davis/cc9b263c1af95ea803c4f5c8888ef8e37f0cef80",
            "/paper/Model-based-estimation-of-3D-human-motion-with-on-Kakadiaris-Metaxas/4d24117d83ad925d837ed0b7d6aa065140fb0248",
            "/paper/Estimating-anthropometry-and-pose-from-a-single-Barron-Kakadiaris/8de5f2df3e1afd14b3fe8a003be483fc099b2e1f",
            "/paper/Tracking-people-with-twists-and-exponential-maps-Bregler-Malik/8f6a3dea66b539d75c30fb24ecefe627bbb0c3a9",
            "/paper/Reducing-drift-in-parametric-motion-tracking-Rahimi-Morency/98785561e7de53da356007d8e79e6d0016ce1e03",
            "/paper/Reconstruction-of-3D-Human-Movement-Using-Inverse-Amaya-Hara/2fc9ac1fa5a7fa6270dd642f09a61c3267176e4e",
            "/paper/Three-dimensional-motion-analysis-of-scene-multiple-Yamamoto/8c8031d26553b7394f64686dadf8df289d173b31",
            "/paper/A-Survey-of-Computer-Vision-Based-Human-Motion-Moeslund-Granum/5ee709de9c13034bcae1fff96b5abb7312bd097e"
        ]
    },
    {
        "id": "d2796dec7c1c94f3a8fc22bea03b82d9a0196c1a",
        "title": "Learning Better with Less: Effective Augmentation for Sample-Efficient Visual Reinforcement Learning",
        "abstract": "This work conducts comprehensive experiments to assess the impact of DA's attributes on its efficacy and reveals that both ample spatial diversity and slight hardness are indispensable and introduces Random PadResize (Rand PR), a new DA operation that offers abundant spatial diversity with minimal hardness. Data augmentation (DA) is a crucial technique for enhancing the sample efficiency of visual reinforcement learning (RL) algorithms. Notably, employing simple observation transformations alone can yield outstanding performance without extra auxiliary representation tasks or pre-trained encoders. However, it remains unclear which attributes of DA account for its effectiveness in achieving sample-efficient visual RL. To investigate this issue and further explore the potential of DA, this work conducts comprehensive experiments to assess the impact of DA's attributes on its efficacy and provides the following insights and improvements: (1) For individual DA operations, we reveal that both ample spatial diversity and slight hardness are indispensable. Building on this finding, we introduce Random PadResize (Rand PR), a new DA operation that offers abundant spatial diversity with minimal hardness. (2) For multi-type DA fusion schemes, the increased DA hardness and unstable data distribution result in the current fusion schemes being unable to achieve higher sample efficiency than their corresponding individual operations. Taking the non-stationary nature of RL into account, we propose a RL-tailored multi-type DA fusion scheme called Cycling Augmentation (CycAug), which performs periodic cycles of different DA operations to increase type diversity while maintaining data distribution consistency. Extensive evaluations on the DeepMind Control suite and CARLA driving simulator demonstrate that our methods achieve superior sample efficiency compared with the prior state-of-the-art methods.",
        "publication_year": "2023",
        "authors": [
            "Guozheng Ma",
            "Linrui Zhang",
            "Haoyu Wang",
            "Lu Li",
            "Zilin Wang",
            "Zhen Wang",
            "Li Shen",
            "Xueqian Wang",
            "Dacheng Tao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "62",
        "references": [
            "/paper/Normalization-Enhances-Generalization-in-Visual-Li-Lyu/e9258b5024e636627ecb50aa56e22c8c03d42985",
            "/paper/A-Comprehensive-Survey-of-Data-Augmentation-in-Ma-Wang/336eea1e32e2b93c63f97931710d3de54d05a336",
            "/paper/Improving-Sample-Efficiency-in-Model-Free-Learning-Yarats-Zhang/88dd6594c9ddd4c4bb7f9b407b162e283907f4f3",
            "/paper/Sample-efficient-Reinforcement-Learning-Learning-Nguyen-Luu/d2d3dd6188f98e5a9102bd50045df7f33b5f7104",
            "/paper/Reinforcement-Learning-with-Augmented-Data-Laskin-Lee/744139d65c3bf6da6a6acd384a32d94a06f44f62",
            "/paper/Efficient-Scheduling-of-Data-Augmentation-for-Deep-Ko-Ok/3b9c93876303d2a0f96d44e2392f54c7cc61b376",
            "/paper/Does-Self-supervised-Learning-Really-Improve-from-Li-Shang/5e9b0756a2f1eb70494093e0fb68ce9177a36be5",
            "/paper/Robust-Deep-Reinforcement-Learning-via-Multi-View-Fan-Li/09f36087d9dae1ca5ccc1d75bb326954ec30239a",
            "/paper/Data-Efficient-Reinforcement-Learning-with-Schwarzer-Anand/7c4356ec0dca6e6df0af7a882e2cd1571c8bf3dc",
            "/paper/Mask-based-Latent-Reconstruction-for-Reinforcement-Yu-Zhang/073ae688d98abc9b25120b039c70653a93f3ed68",
            "/paper/SECANT%3A-Self-Expert-Cloning-for-Zero-Shot-of-Visual-Fan-Wang/7ad1b82507b61c7113c4bde17fa3d89bb256cff3"
        ]
    },
    {
        "id": "0f53757d05f64e055c9c995e879710cf471e70fd",
        "title": "Boosting Convolutional Neural Networks with Middle Spectrum Grouped Convolution",
        "abstract": "This paper proposes a novel module called middle spectrum grouped convolution (MSGC) for efficient deep convolutional neural networks (DCNNs) with the mechanism of grouped Convolution that acts as a booster that can reduce the computational cost of the host backbones for general image recognition with even improved predictive accuracy. This paper proposes a novel module called middle spectrum grouped convolution (MSGC) for efficient deep convolutional neural networks (DCNNs) with the mechanism of grouped convolution. It explores the broad\"middle spectrum\"area between channel pruning and conventional grouped convolution. Compared with channel pruning, MSGC can retain most of the information from the input feature maps due to the group mechanism; compared with grouped convolution, MSGC benefits from the learnability, the core of channel pruning, for constructing its group topology, leading to better channel division. The middle spectrum area is unfolded along four dimensions: group-wise, layer-wise, sample-wise, and attention-wise, making it possible to reveal more powerful and interpretable structures. As a result, the proposed module acts as a booster that can reduce the computational cost of the host backbones for general image recognition with even improved predictive accuracy. For example, in the experiments on ImageNet dataset for image classification, MSGC can reduce the multiply-accumulates (MACs) of ResNet-18 and ResNet-50 by half but still increase the Top-1 accuracy by more than 1%. With 35% reduction of MACs, MSGC can also increase the Top-1 accuracy of the MobileNetV2 backbone. Results on MS COCO dataset for object detection show similar observations. Our code and trained models are available at https://github.com/hellozhuo/msgc.",
        "publication_year": "2023",
        "authors": [
            "Z. Su",
            "Jiehua Zhang",
            "Tianpeng Liu",
            "Zhen Liu",
            "Shuanghui Zhang",
            "M. Pietikainen",
            "Li Liu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "82",
        "references": [
            "/paper/Dynamic-Group-Convolution-for-Accelerating-Neural-Su-Fang/4a819d20abbc171b8bca370fcf1b298b1166e839",
            "/paper/Self-grouping-Convolutional-Neural-Networks-Guo-Wu/cbbdef7778d6413a08aa3e385ea42f16adaf0e9a",
            "/paper/Differentiable-Learning-to-Group-Channels-via-Zhang-Li/9297fa2b614884ee65fc6f03e692ee100add375a",
            "/paper/ChannelNets%3A-Compact-and-Efficient-Convolutional-Gao-Wang/0cb18f817ba9e9888cf2438933b7349093f51303",
            "/paper/An-Efficient-Sharing-Grouped-Convolution-via-Chen-Duan/30be54791bec3a413ff8b97b41db9a136d7d3c54",
            "/paper/Dynamic-Channel-Pruning%3A-Feature-Boosting-and-Gao-Zhao/a055b9917759dd75811edbc8500ca247b457c5b2",
            "/paper/IGCV3%3A-Interleaved-Low-Rank-Group-Convolutions-for-Sun-Li/054db727adc1a3a877aae6ad4ac835a3a9b872ba",
            "/paper/Carrying-out-CNN-Channel-Pruning-in-a-White-Box-Zhang-Lin/42b61ce5ae3d6ed1aa994ba9c57de66f0531ed1f",
            "/paper/ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "/paper/Squeeze-and-Excitation-Networks-Hu-Shen/df67d46e78aae0d2fccfb6212d101a342259c01b"
        ]
    },
    {
        "id": "2df437fe675c89a00dc0a7c3a31fa46ed3d94c9d",
        "title": "A Video Summarization Model Based on Deep Reinforcement Learning with Long-Term Dependency",
        "abstract": "This paper introduces an unsupervised auxiliary summarization loss module with LSTM and a swish activation function to capture the long-term dependencies for video summarization, which can be easily integrated with various networks. Deep summarization models have succeeded in the video summarization field based on the development of gated recursive unit (GRU) and long and short-term memory (LSTM) technology. However, for some long videos, GRU and LSTM cannot effectively capture long-term dependencies. This paper proposes a deep summarization network with auxiliary summarization losses to address this problem. We introduce an unsupervised auxiliary summarization loss module with LSTM and a swish activation function to capture the long-term dependencies for video summarization, which can be easily integrated with various networks. The proposed model is an unsupervised framework for deep reinforcement learning that does not depend on any labels or user interactions. Additionally, we implement a reward function (R(S)) that jointly considers the consistency, diversity, and representativeness of generated summaries. Furthermore, the proposed model is lightweight and can be successfully deployed on mobile devices and enhance the experience of mobile users and reduce pressure on server operations. We conducted experiments on two benchmark datasets and the results demonstrate that our proposed unsupervised approach can obtain better summaries than existing video summarization methods. Furthermore, the proposed algorithm can generate higher F scores with a nearly 6.3% increase on the SumMe dataset and a 2.2% increase on the TVSum dataset compared to the DR-DSN model.",
        "publication_year": "2022",
        "authors": [
            "Xu Wang",
            "Yujie Li",
            "Haoyu Wang",
            "Longzhao Huang",
            "Shuxue Ding"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3",
        "reference_count": "61",
        "references": [
            "/paper/Unsupervised-Video-Summarization-Based-on-Deep-with-Yoon-Hong/8ab6e5ae92a83f593baeff10d68d87b124aeffbe",
            "/paper/Automatic-video-summarization-and-classification-by-Vinta-Singh/4e89cac6601ac2c9ac98ee84b435a41e168d9f5b",
            "/paper/Automatic-Spoiler-Sensitive-Video-Preview-with-Saxena-Saxena/5f54906fe3eac5929680f4b916b8527f67b823b6",
            "/paper/Video-Summarization-with-Long-Short-Term-Memory-Zhang-Chao/1dbc12e54ceb70f2022f956aa0a46e2706e99962",
            "/paper/Using-independently-recurrent-networks-for-learning-Yal%C4%B1n%C4%B1z-Ikizler-Cinbis/87e5f876111fe8d6c47aeceb1dacc2f967f0bd7d",
            "/paper/Video-Summarisation-by-Classification-with-Deep-Zhou-Xiang/c65232de4ca3809b0765577a1197bd6027f98a39",
            "/paper/Hierarchical-Recurrent-Neural-Network-for-Video-Zhao-Li/454e65c2a9b019a00790a1d6029dc5539edad35d",
            "/paper/Video-summarization-with-a-graph-convolutional-Li-Tang/059551ea629c70bd2a1939c5bfde74c7f8a39833",
            "/paper/Video-Summarization-Using-Deep-Neural-Networks%3A-A-Apostolidis-Adamantidou/40ffdf58932db8284a33a56b6ce8bded2f5a829b",
            "/paper/Discriminative-Feature-Learning-for-Unsupervised-Jung-Cho/69b3b29c6fabaea88d382922346d9157395a3226",
            "/paper/Property-Constrained-Dual-Learning-for-Video-Zhao-Li/b83b6069fc6ad861c6d7cb574825488e0fca8521",
            "/paper/Unsupervised-Reinforcement-Learning-For-Video-Wang-Zhu/2f77d37aa0082b45ca8749d68d1312ae52986028",
            "/paper/Interp-SUM%3A-Unsupervised-Video-Summarization-with-Yoon-Hong/9f3ee820302c3e23ca342cbdef801144f880e27b"
        ]
    },
    {
        "id": "9f45b55af027503fab557f55f70e81e43c6c1db7",
        "title": "Learning Spatial-Temporal Regularized Correlation Filters for Visual Tracking",
        "abstract": "The spatial-temporal regularized correlation filters (STRCF) formulation can not only serve as a reasonable approximation to SRDCF with multiple training samples, but also provide a more robust appearance model thanSRDCF in the case of large appearance variations. Discriminative Correlation Filters (DCF) are efficient in visual tracking but suffer from unwanted boundary effects. Spatially Regularized DCF (SRDCF) has been suggested to resolve this issue by enforcing spatial penalty on DCF coefficients, which, inevitably, improves the tracking performance at the price of increasing complexity. To tackle online updating, SRDCF formulates its model on multiple training images, further adding difficulties in improving efficiency. In this work, by introducing temporal regularization to SRDCF with single sample, we present our spatial-temporal regularized correlation filters (STRCF). The STRCF formulation can not only serve as a reasonable approximation to SRDCF with multiple training samples, but also provide a more robust appearance model than SRDCF in the case of large appearance variations. Besides, it can be efficiently solved via the alternating direction method of multipliers (ADMM). By incorporating both temporal and spatial regularization, our STRCF can handle boundary effects without much loss in efficiency and achieve superior performance over SRDCF in terms of accuracy and speed. Compared with SRDCF, STRCF with hand-crafted features provides a 5\u00c3\u2014 speedup and achieves a gain of 5.4% and 3.6% AUC score on OTB-2015 and Temple-Color, respectively. Moreover, STRCF with deep features also performs favorably against state-of-the-art trackers and achieves an AUC score of 68.3% on OTB-2015.",
        "publication_year": "2018",
        "authors": [
            "Feng Li",
            "Cheng Tian",
            "W. Zuo",
            "Lei Zhang",
            "Ming-Hsuan Yang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "548",
        "reference_count": "42",
        "references": [
            "/paper/Learning-spatial-temporally-regularized-kernelized-Su-Li/746b31b391f4714b709399f6dcbe7278107d5fb9",
            "/paper/Correlation-Tracking-via-Spatial-Temporal-and-Tian-Zang/e501195e993da1ea8d8b7c954bc2f7b4e3e0035f",
            "/paper/Fast-Learning-of-Spatially-Regularized-and-Content-Han-Feng/a57335f8db4c7a490713261511421b3b37d3ec19",
            "/paper/Learning-an-Orientation-and-Scale-Adaptive-Tracker-Tan-Wei/da83daa968923a32eaf3f2d554a16518d7f562fa",
            "/paper/Learning-Multi-feature-Based-Spatially-Regularized-She-Yi/18cf7984a7c1a0e19eeb0c022714cca857747378",
            "/paper/Learning-reliable-spatial-and-spatial-variation-for-Fu-Zhang/28927936bcb7798141671913570f341d83ef6205",
            "/paper/Learning-Spatial-Corrected-Regularized-Correlation-Yang-Wu/edc4f3ae92ba1e23300d982c23a48348dc79225f",
            "/paper/Learning-Aberrance-Repressed-and-Temporal-Filters-Wang-Fan/0b4114ebdfcc42b1b971bdf9030b6296bbae6e3d",
            "/paper/Learning-adaptive-spatial-temporal-regularized-for-Zhao-Li/2618cd556accbf95933d648c4bdf25ba9ccdeff9",
            "/paper/Learning-Spatial%E2%80%93Temporal-Background-Aware-Based-Gu-Liu/64863e04ce0a89e448a9bb7f0d8465f50273600b",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "/paper/ECO%3A-Efficient-Convolution-Operators-for-Tracking-Danelljan-Bhat/a87cc499cf101b3697cacc65094b4b6590e0d061",
            "/paper/Robust-and-real-time-deep-tracking-via-multi-scale-Wang-Li/2ac7ab669a56af6ada5cc1459f2c7e93dcdb025a",
            "/paper/Structural-Correlation-Filter-for-Robust-Visual-Liu-Zhang/3eedcf9302f299a1eebbc0f543a366454d6dcefb",
            "/paper/Integrating-Boundary-and-Center-Correlation-Filters-Li-Yao/650ff51384273e52bdc9f3d1d8a27a3910c9b446",
            "/paper/Target-Response-Adaptation-for-Correlation-Filter-Bibi-Mueller/f5eae0841111c814c977dd691c072a3aa57e6ad8",
            "/paper/Learning-Support-Correlation-Filters-for-Visual-Zuo-Wu/7688f4535f62db2d6da8573e7e0829591ae5d1db",
            "/paper/Staple%3A-Complementary-Learners-for-Real-Time-Bertinetto-Valmadre/0f12a3aaf3851078d93a9bba4e3ebece6d4bcfe5",
            "/paper/Adaptive-Decontamination-of-the-Training-Set%3A-A-for-Danelljan-H%C3%A4ger/992efcca992733f4b170309fbdcd26219f1c0fbb"
        ]
    },
    {
        "id": "9fc420e96048a30f9f15c49ad2ccbfbf875864b9",
        "title": "Review of recent advances in visual tracking techniques",
        "abstract": "This study analyses and tabulates the methodologies applied in every recently proposed visual tracking method and provides a detailed insight to the reader with the different aspects of tracking methodologies and future direction of tracking researches. Visual tracking is the widely emerging research in computer vision applications. Nowadays, researchers have proposed various novel tracking methodologies to attain the excellence in terms of performance. In this review, several recent visual tracking methodologies have been clearly examined and categorised into four different categories such as Discriminative Trackers, Generative Trackers, Correlation Filter Based Trackers and Combined Trackers. Moreover, this study analyses and tabulates the methodologies applied in every recently proposed visual tracking method. The main objective of this review is to provide a detailed insight to the reader with the different aspects of tracking methodologies and future direction of tracking researches. The experimental evaluations on recent trackers have been documented for the better understanding of the performance of existing visual trackers on different benchmark datasets such as OTB 2015, VOT 2016 and MOT 2020.",
        "publication_year": "2021",
        "authors": [
            "Jainul Rinosha S M",
            "Gethsiyal Augasta M"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "74",
        "references": [
            "/paper/A-survey-on-online-learning-for-visual-tracking-Abbass-Kwon/93bde1d02ce505b8d17b3078afe9c6d246138370",
            "/paper/Single-online-visual-object-tracking-with-enhanced-Yi-Luo/b6087cb68b610aabcf3113ef7038308d16b37f4b",
            "/paper/Real-time-part-based-visual-tracking-via-adaptive-Liu-Wang/6410c97ae03d356e14544c8e95f5367fb7ebb6e6",
            "/paper/Visual-Tracking-Based-on-Multi-Feature-and-Fast-Zeng-Xu/aa043777132a88d23e6f5967c1e3dda4ee45af2e",
            "/paper/A-Review-of-Visual-Tracking-Cannons/257cfe2995243b2a5f91a7a423bf2853e1c05420",
            "/paper/Tracking-by-Sampling-Trackers-Kwon-Lee/68cc57640bfd04f697048534f82d16bf10a002ec",
            "/paper/The-Visual-Object-Tracking-VOT2016-Challenge-Kristan-Leonardis/966aad492f75b17f698e981e008b73b51816c6aa",
            "/paper/Reliable-Patch-Trackers%3A-Robust-visual-tracking-by-Li-Zhu/a9448de3265ed9ecdc11c79273aee92daa31f79a",
            "/paper/Robust-long-term-correlation-tracking-with-multiple-Wang-Liu/11276d260a97d43effde3f1b38b3f03556434747",
            "/paper/The-Visual-Object-Tracking-VOT2015-Challenge-Kristan-Matas/15c3d43d1e7ca086bb8ea7f3958b6d4d6abb7a3d"
        ]
    },
    {
        "id": "93d5a35e9043dbe4a20a6d30569834fed199660d",
        "title": "Object Tracking Method Based on SURF",
        "abstract": "Semantic Scholar extracted view of \"Object Tracking Method Based on SURF\" by Hu Shuo et al.",
        "publication_year": "2012",
        "authors": [
            "Hu Shuo",
            "Wu Na",
            "Song Hua-jun"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "19",
        "reference_count": "12",
        "references": [
            "/paper/Classification-of-Features-Extracted-from-Image-and-%D9%84%D8%B7%D8%A8%D9%81-%D8%B2%D8%A7%D8%AF%D9%87/211d9234104e2c8541e61912477a046cb8ba1127",
            "/paper/Robust-tracking-of-moving-objects-using-thermal-and-Vlahovi%C4%87-Djurovic/89e1bb9e43a023e23ed9eeeef396b56f3db5a625",
            "/paper/OBJECT-DETECTION-AND-TRACKING-IN-VIDEOS-%3A-A-REVIEW-Nagalakshmi-Shobha/62d1868154b87e19f1e84f02b90f0cf2e908d71b",
            "/paper/Training-Based-Methods-for-Comparison-of-Object-for-Delforouzi-Pamarthi/b43d77ce4bf8b482598df86b64fc9c0261359b75",
            "/paper/Multi-person-tracking-using-SURF-and-background-for-Yu-Lee/51483313bc33e90d9faa57d3aa8d51fb4730eca8",
            "/paper/RIGID-TRACKING-FOR-SCALE-AND-ROTATION-VARYING-FROM-Pragadeeswari-Yamuna/085a7c0cff16b263b34ec184fb5b0dabeda99ebc",
            "/paper/Camera-Handoff-for-Multi-camera-Surveillance-Jirafe-Jibhe/d580aa896d69710f03afdfe96b495d1ffed37e0a",
            "/paper/Supervised-Machine-Learning-Approaches-for-Moving-A-Mondal/2e95f15d1b0f7d6a27c22199d447173d758b8275",
            "/paper/Object-Identification-using-Graph-Theory-Dessai-Araujo/b429e19a249d34ad09f3c912fe0db86e0e9a6cca",
            "/paper/Comparative-Analysis-of-Different-Keypoint-Based-Kaur-Walia/420c478c9274671a1111fc8d1fc4d0d21a587632",
            "/paper/A-new-framework-for-on-line-object-tracking-based-Miao-Wang/75a3f4ba22c36f6dccfdb469cc6b34d4790e090d",
            "/paper/SURFTrac%3A-Efficient-tracking-and-continuous-object-Ta-Chen/a503b0b11fd2b49a60fc29545da0cd048edeb1d4",
            "/paper/Object-tracking-using-SIFT-features-and-mean-shift-Zhou-Yuan/260449f28ff2dfc7174c210ed603bde52487492e",
            "/paper/SURF%3A-Speeded-Up-Robust-Features-Bay-Tuytelaars/490020c0d4fa1eb85fe353add5713e49f08c628d",
            "/paper/Visual-Tracking-by-Adaptive-Kalman-Filtering-and-Karavasilis-Nikou/a4818d02f88be7ba5217f844b6a34cf4d0e1ad93",
            "/paper/Recent-advances-and-trends-in-visual-tracking%3A-A-Yang-Shao/2bcf2bd59219d89f335cbc8d1dd4f431076b4c4c",
            "/paper/Distinctive-Image-Features-from-Scale-Invariant-Lowe/8c04f169203f9e55056a6f7f956695babe622a38",
            "/paper/Speeded-Up-Robust-Features-(SURF)-Bay-Ess/cdbb606ae47c64049262dfbd3bb147d3f4ba8420",
            "/paper/An-Affine-Invariant-Interest-Point-Detector-Mikolajczyk-Schmid/9c7a96155f10f152cae0866102c061cdf6da02e8",
            "/paper/Boosting-a-weak-learning-algorithm-by-majority-Freund/b824cb051ffbdd81b529c4b82379a3af270fb6f7"
        ]
    },
    {
        "id": "68e1ae4d882114d29450d90ea45840cfe56917b5",
        "title": "An Overview of Variational Autoencoders for Source Separation, Finance, and Bio-Signal Applications",
        "abstract": "Applications of variational autoencoders for finance, speech/audio source separation, and biosignal applications are presented, and possible areas of research in improving performance of VAEs in particular and deep generative models in general are identified. Autoencoders are a self-supervised learning system where, during training, the output is an approximation of the input. Typically, autoencoders have three parts: Encoder (which produces a compressed latent space representation of the input data), the Latent Space (which retains the knowledge in the input data with reduced dimensionality but preserves maximum information) and the Decoder (which reconstructs the input data from the compressed latent space). Autoencoders have found wide applications in dimensionality reduction, object detection, image classification, and image denoising applications. Variational Autoencoders (VAEs) can be regarded as enhanced Autoencoders where a Bayesian approach is used to learn the probability distribution of the input data. VAEs have found wide applications in generating data for speech, images, and text. In this paper, we present a general comprehensive overview of variational autoencoders. We discuss problems with the VAEs and present several variants of the VAEs that attempt to provide solutions to the problems. We present applications of variational autoencoders for finance (a new and emerging field of application), speech/audio source separation, and biosignal applications. Experimental results are presented for an example of speech source separation to illustrate the powerful application of variants of VAE: VAE, \u03b2-VAE, and ITL-AE. We conclude the paper with a summary, and we identify possible areas of research in improving performance of VAEs in particular and deep generative models in general, of which VAEs and generative adversarial networks (GANs) are examples.",
        "publication_year": "2021",
        "authors": [
            "Aman Singh",
            "T. Ogunfunmi"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "7",
        "reference_count": "138",
        "references": [
            "/paper/Examining-the-Size-of-the-Latent-Space-of-Trained-Ahmed-Longo/f5d1ca7f04b829e45fe6c7e03dce4bd00e2c9a16",
            "/paper/Interpretation-for-Variational-Autoencoder-Used-to-Wu-Plataniotis/36c79469c969f741e3792017692f62b4917e6dff",
            "/paper/State-of-the-Art-Analysis-of-Deep-Learning-Based-Soni-Yadav/7f982e3e79bbf74355c86aae191a7d080c7d5c0d",
            "/paper/Evaluation-of-Synthetic-Data-Generation-Techniques-Cullen-Halladay/6e831d42b1536d58f542afab2441a9141b650562",
            "/paper/Automatic-Classification-of-Magnetic-Resonance-of-a-Csore-Karmonik/012d4adbf73af4c592f7470c80147d8043d09750",
            "/paper/Adaptive-Signal-Processing-and-Machine-Learning-and-Ogunfunmi/1f6c46ba6d6a65eecd871ec97cfb11b9fc0a2342",
            "/paper/A-review-of-automatic-recognition-technology-for-in-Xie-Zhong/53226c393c738b6aed31cdd244d939155bb49c36",
            "/paper/Adaptive-Neural-Speech-Enhancement-with-a-Denoising-Bando-Sekiguchi/1f62a7fbd4ade2fab054269c3ecba121b292ee8c",
            "/paper/A-Survey-on-Variational-Autoencoders-from-a-Green-Asperti-Evangelista/8223fd4888f86712267b7ecb292a93225241d0b5",
            "/paper/NVAE%3A-A-Deep-Hierarchical-Variational-Autoencoder-Vahdat-Kautz/853de0e00ac5ac257a622ae678ed373b8e086404",
            "/paper/MADE%3A-Masked-Autoencoder-for-Distribution-Germain-Gregor/90f72fbbe5f0a29e627db28999e01a30a9655bc6",
            "/paper/On-the-Information-Plane-of-Autoencoders-Tapia-Est'evez/33a6cfd502ac8460bec4d6512663b98c0c01fbc2",
            "/paper/VAE-with-a-VampPrior-Tomczak-Welling/5ea2cdab68c69d7aef5a004495783ae7628193f2",
            "/paper/Fast-MVAE%3A-Joint-Separation-and-Classification-of-Li-Kameoka/667d2be16e20cbe3179354531464ff0bd71333db",
            "/paper/Stacked-Denoising-Autoencoders%3A-Learning-Useful-in-Vincent-Larochelle/e2b7f37cd97a7907b1b8a41138721ed06a0b76cd",
            "/paper/Neural-Discrete-Representation-Learning-Oord-Vinyals/f466157848d1a7772fb6d02cdac9a7a5e7ef982e",
            "/paper/Supervised-Determined-Source-Separation-with-Kameoka-Li/ab41dc5094b24778a225a90bdb4e2091e25c91b7"
        ]
    },
    {
        "id": "be2c3cb1d4412c71a3d7acff15e886855b211961",
        "title": "RHL-track: visual object tracking based on recurrent historical localization",
        "abstract": "This paper proposes an advanced tracking network based on recurrent historical localization information that utilizes two convolution layers to perform target classification that predicts the initial target center and applies a gated recurrent unit that fuses multi-resolution features with historical localized information to yield the final optimized target position. Visual object tracking (VOT) is a fundamental and complex problem in computer vision field. In the past few years, the research focus has been shifted from template matching to deep learning models. Especially, the Siamese networks dominate tracking domain in recent years, which take the first frame as the reference and perform object detection and localization in the following frames. However, most of them could not capture target changes due to the lack of strong feature representation abilities. To address these issue, we propose an advanced tracking network in this paper based on recurrent historical localization information. Unlike traditional symmetric structures, we utilize two convolution layers to perform target classification that predicts the initial target center. Then, we apply a gated recurrent unit that fuses multi-resolution features with historical localization information to yield the final optimized target position. Extensive experiments have been conducted on six mainstream datasets: OTB100, GOT-10k, TrackingNet, LaSOT, VOT2018 and NFS, where our tracker exhibits state-of-the-art performances.",
        "publication_year": "2023",
        "authors": [
            "Feiyu Meng",
            "X. Gong",
            "Yi Zhang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "57",
        "references": [
            "/paper/High-Performance-Visual-Tracking-with-Siamese-Li-Yan/320d05db95ab42ade69294abe46cd1aca6aca602",
            "/paper/TRAT%3A-Tracking-by-Attention-Using-Spatio-Temporal-Saribas-Cevikalp/5fa30736fb132c3a021b1e4227d212d4b8f6bc79",
            "/paper/GradNet%3A-Gradient-Guided-Network-for-Visual-Object-Li-Chen/47a58f8bec1d34004a7d7cf837e27a26de64f0f7",
            "/paper/Hedging-Deep-Features-for-Visual-Tracking-Qi-Zhang/719b882d8efe6fb801fc821e6ec3a6bcc4b6c3ec",
            "/paper/SiamET%3A-a-Siamese-based-visual-tracking-network-Zhou-Zhang/cf1559f1dc37fb77705efee31f01b2dda49e5ced",
            "/paper/Siam-R-CNN%3A-Visual-Tracking-by-Re-Detection-Voigtlaender-Luiten/069ccdbab6ea6ca2d9c3b75c76360ca1e4e9a5e9",
            "/paper/Graph-Convolutional-Tracking-Gao-Zhang/53970ae69a73f547a56661fd25f6711746d277fb",
            "/paper/SiamFC%2B%2B%3A-Towards-Robust-and-Accurate-Visual-with-Xu-Wang/be412c7c7128cf91455233b652d6c94a6001a7c8",
            "/paper/ROAM%3A-Recurrently-Optimizing-Tracking-Model-Yang-Xu/e1cfc317da9268897cd8e9a594178ef63f6d5ddf",
            "/paper/Dynamic-Template-Selection-Through-Change-Detection-Kiran-Nguyen-Meidine/2f02729d35704bbec4ccd42fc59e80953f1c3f6b"
        ]
    },
    {
        "id": "b48f7a3ba1c3c16e0b70dc867182e549ca90be94",
        "title": "Self-supervision and Multi-task Learning: Challenges in Fine-Grained COVID-19 Multi-class Classification from Chest X-rays",
        "abstract": "This work investigates the impact of multi-task learning (classification and segmentation) on the ability of CNNs to differentiate between various appearances of COVID-19 infections in the lung and employs self-supervised pre-training approaches, namely MoCo and inpainting-CXR, to eliminate the dependence on expensive ground truth annotations. Quick and accurate diagnosis is of paramount importance to mitigate the effects of COVID-19 infection, particularly for severe cases. Enormous effort has been put towards developing deep learning methods to classify and detect COVID-19 infections from chest radiography images. However, recently some questions have been raised surrounding the clinical viability and effectiveness of such methods. In this work, we investigate the impact of multi-task learning (classification and segmentation) on the ability of CNNs to differentiate between various appearances of COVID-19 infections in the lung. We also employ self-supervised pre-training approaches, namely MoCo and inpainting-CXR, to eliminate the dependence on expensive ground truth annotations for COVID-19 classification. Finally, we conduct a critical evaluation of the models to assess their deploy-readiness and provide insights into the difficulties of fine-grained COVID-19 multi-class classification from chest X-rays.",
        "publication_year": "2022",
        "authors": [
            "Muhammad Ridzuan",
            "Ameera Ali Bawazir",
            "Ivo Gollini Navarette",
            "Ibrahim Almakky",
            "Mohammad Yaqub"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3",
        "reference_count": "38",
        "references": [
            "/paper/Artificial-Special-Visual-Geometry-Group-16(VGG)-of-Chary-Venkateswarlu/c4d60d547c5bd5d16c916dcdb40ab0d9503230dc",
            "/paper/Enhanced-feature-selection-algorithm-for-pneumonia-Abdullah-Abedi/4e41f2f80c2b4b242cd29fa9e14b298cfbe5f37b",
            "/paper/MGMT-promoter-methylation-status-prediction-using-Saeed-Ridzuan/1da0908e1df871e2fdec0abb6eb7efe38d7357ed",
            "/paper/Classification-of-COVID-19-chest-X-rays-with-deep-Pham/cab0e0c5f39c7ad5fd7d1c2ade1616f46e2a4e6f",
            "/paper/Self-supervised-deep-convolutional-neural-network-Gazda-Gazda/d3533e73d0cf34f5e907b468f0ac544eac61953a",
            "/paper/Self-Supervised-Learning-for-Few-Shot-Medical-Image-Ouyang-Biffi/c5925f4bb074468e91be8563f521d8e2124394f3",
            "/paper/Detection-and-analysis-of-COVID-19-in-medical-using-Yang-Martinez/d1f0c37d5c7e6eee185537caac413ad8c28cc6c9",
            "/paper/Classification-of-COVID-19-chest-X-Ray-and-CT-using-Jia-Lam/453c4382b4fa26e3bdf7ef11e93ae1bb482f53d4",
            "/paper/Deep-Learning-Approaches-for-Detecting-COVID-19-A-Alghamdi-Amoudi/bf0af2a5865597285a617f8d096f75a4ac34ab56",
            "/paper/Rethinking-Computer-Aided-Tuberculosis-Diagnosis-Liu-Wu/c8da769833fa7dafc3546030ce090a092169f8df",
            "/paper/Deep-Transfer-Learning-Based-Classification-Model-Pathak-Shukla/eb533b2830af187a1e0954b08ff7d249a80673a9",
            "/paper/MoCo-Pretraining-Improves-Representation-and-of-Sowrirajan-Yang/5985e77cce2a6a3720682c81dbeaef7b417e0459",
            "/paper/Self-Supervised-Deep-Learning-to-Enhance-Breast-on-Miller-Arasu/06cab0049907b996d0da800910fcef30553d4dce"
        ]
    },
    {
        "id": "1220fbc61ec47d5978d42c8b5a48b7a33b3c8374",
        "title": "Breast tumor segmentation and shape classification in mammograms using generative adversarial and convolutional neural network",
        "abstract": "Semantic Scholar extracted view of \"Breast tumor segmentation and shape classification in mammograms using generative adversarial and convolutional neural network\" by V. Singh et al.",
        "publication_year": "2018",
        "authors": [
            "V. Singh",
            "Hatem A. Rashwan",
            "S. Roman\u00ed",
            "Farhan Akram",
            "Nidhi Pandey",
            "Md. Mostafa Kamal Sarker",
            "Adel Saleh",
            "M. Arenas",
            "M. Arquez",
            "D. Puig",
            "J. Torrents-Barrena"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "124",
        "reference_count": "59",
        "references": [
            "/paper/Connected-UNets%3A-a-deep-learning-architecture-for-Baccouche-Garcia-Zapirain/7aec930be9263254b1f3c7901f62a2533aeb8e16",
            "/paper/Connected-UNets%3A-a-deep-learning-architecture-for-Baccouche-Garcia-Zapirain/5269e5dac2c63932f9793b6ae36909c02bb6c65b",
            "/paper/Dual-Convolutional-Neural-Networks-for-Breast-Mass-Li-Chen/9c7bd7fbbe680aa64b379222191d632a9cc5e927",
            "/paper/Breast-tumor-segmentation-in-ultrasound-images-deep-Singh-Abdel-Nasser/819c9162f6411c72a0e788e6f3ea45b106407d3b",
            "/paper/Architectural-Distortion-Based-Digital-Mammograms-Rehman-Li/282289945150093481aa4aa4e3648bf03c7e44f1",
            "/paper/Segmentation-of-malignant-tumours-in-mammogram-A-Roy-Singh/32320a5c9a49868b851ef6727cc3d0bd80f9f011",
            "/paper/Two-stage-breast-mass-detection-and-segmentation-Yan-Conze/62658b7ecda20d221dfcd80abeea393b2851e221",
            "/paper/A-Novel-Method-For-Segmentation-Of-Breast-Masses-On-Cao-Pu/22bdfd6329040b5501408b318722a7f5fc8f77a3",
            "/paper/An-Automated-In-Depth-Feature-Learning-Algorithm-Mahmood-Li/96217a4d3764e76754c1281c456df42818552711",
            "/paper/YOLO-LOGO%3A-A-transformer-based-YOLO-segmentation-in-Su-Liu/e10f7e643779ff0ff01448721e4ae9bdc908c7fb",
            "/paper/Conditional-Generative-Adversarial-and-Networks-for-Singh-Roman%C3%AD/71520c61037479c7bc742a109e30b1c8812cd8c6",
            "/paper/Adversarial-deep-structured-nets-for-mass-from-Zhu-Xiang/39fc5bbed62308da62a34c896b685e70cef31cd3",
            "/paper/A-parasitic-metric-learning-net-for-breast-mass-on-Jiao-Gao/6420bae0949cd9a2d0f31bb14fd20bce8071a6db",
            "/paper/ICADx%3A-interpretable-computer-aided-diagnosis-of-Kim-Lee/c3e32869e80b1231b9925b260e20e8b4d667bde4",
            "/paper/Automatic-Liver-Segmentation-Using-an-Adversarial-Yang-Xu/8b6b0c1139a3ab405e6df58aaa79305867f453ef",
            "/paper/Mass-segmentation-in-mammograms%3A-A-cross-sensor-of-Cardoso-Marques/85f14b7992bdef32a852d03c30d41dd7875f2809",
            "/paper/Preprocessing-filters-for-mammogram-images%3A-A-Kshema-George/8967ea71b194a28744c15a3be93fa6e32ddac1e9",
            "/paper/Fusing-texture%2C-shape-and-deep-model-learned-at-for-Xie-Zhang/529d10764cafcc57afe5b8ad84a6b1921b41c207",
            "/paper/Benign-and-malignant-breast-tumors-classification-Rouhi-Jafari/c6db34ade32b3681a92068b22a354903b2953d52",
            "/paper/A-deep-learning-approach-for-the-analysis-of-masses-Dhungel-Carneiro/e247423b6d191da75f8d6ce88b1698c1b9629f0f"
        ]
    },
    {
        "id": "2bc845552ce7265de0b157a11c8dfc5006fb46de",
        "title": "The role of diffusion weighted magnetic resonance imaging in oncologic settings.",
        "abstract": "Oncologic applications of diffusion-weighted-imaging in the parts of the body are presented and it is thought that DWI can be added to conventional magnetic resonance imaging (MRI) sequences. There is growing interest in the applications of diffusion-weighted-imaging (DWI) in oncologic area for last ten years. DWI has important advantages as do not require contrast medium, very quick technique and it provides qualitative and quantitative information that can be helpful for tumor assessment. In this article, we present oncologic applications of DWI in the parts of the body. DWI has been applied to the evaluation of central nervous system (CNS) pathologies. Some technologic advances lead to using of DWI in the extracranial sites such as abdomen and pelvis. As well as tumor detection and characterization, DWI has been widely used for predicting and monitoring response to therapy. One of the most prominent contributions of DWI is differentiation of between malignant and benign tumoral process. Apparent-diffusion-coefficient (ADC) value is quantitative parameter of DWI which reflects diffusion movements of water molecules in various tissues. Most of the studies suggested that malignant tumors had lower ADC values than benign ones. DWI may be a routine sequence in oncologic settings and it provides much useful information about tumoral tissue. We think it can be added to conventional magnetic resonance imaging (MRI) sequences.",
        "publication_year": "2013",
        "authors": [
            "Z. Bozgeyik",
            "M. Onur",
            "A. K. Poyraz"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "50",
        "reference_count": "58",
        "references": [
            "/paper/The-Comparison-of-Whole-Body-Diffusion-MRI-with-in-Akta%C5%9F-Akt%C3%BCrk/ed79b3f0d185e70aad52f7bd49edf1b5ac247770",
            "/paper/Role-of-Various-DW-MRI-and-DCE-MRI-Parameters-as-of-Kumar-Sharma/d088dafe5a7a19c2c93e270a208f73f3bcd6d6ed",
            "/paper/Role-Of-Diffusion-Weighted-Magnetic-Resonance-in-of-Shabana-Refaat/723428b10eb6e4172a94df068fd7b8ce1c548862",
            "/paper/The-Role-of-Diffusion-Weighted-Magnetic-Resonance-A-Monisha-Gowda/6cd928e9a21e6ac09bfd998a08ef4a8dd19c4269",
            "/paper/Diagnosed-chest-lesion-on-diffusion-weighted-images-Lu-Hung/efed938096878207891f05df73a36a414b23f34a",
            "/paper/The-utility-of-diffusion-weighted-magnetic-imaging-Ko%C3%A7-Serhatlioglu/c7f83c0cc9e4c926656c8e337527e747069aaec9",
            "/paper/Evaluation-of-Apparent-Diffusion-Coefficient-Values-Madhok-Sachdeva/b7683e648180b802b6f3f376815453acd2156e47",
            "/paper/The-value-of-diffusion-weighted-MR-imaging-in-T-and-Sherif/2b6e6f7e226603183e18f7312acf94e5e1549c5f",
            "/paper/Multiparametric-Assessment-of-Treatment-Response-in-Soldatos-Ahlawat/d9e61a7de5494c904da206454171150ab78b8823",
            "/paper/Magnetic-resonance-imaging-in-lung%3A-a-review-of-its-Kumar-Liney/e1d8ecfdbb508a8825ecef6cb08d0ffda2a65b28",
            "/paper/The-role-of-diffusion-weighted-imaging-in-patients-Kono-Inoue/523c8243e2933e0c0ee28150273c59f14d19fb22",
            "/paper/Diffusion-weighted-MRI-in-the-body%3A-applications-in-Koh-Collins/fc381b65aa18e62c109c2725fe0b8889527dc929",
            "/paper/Diffusion-weighted-imaging-of-metastatic-brain-with-Hayashida-Hirai/ea5f7eca5ae6c8930309e3a43fc5744a5f357eff",
            "/paper/The-value-of-diffusion-weighted-MRI-for-prostate-Ya%C4%9Fc%C4%B1-Ozari/6274ed92c4c2e03104d38e4b7b2a62621f065703",
            "/paper/Diffusion-and-perfusion-imaging-of-bone-marrow.-Biffar-Dietrich/3afa70b6a02cc61cbe926c1e30d6b6b2f7821acc",
            "/paper/Diffusion-Weighted-Magnetic-Resonance-Imaging-for-Mori-Nomori/e8c7130b7eba72184abd6b613838ad127290bdcd",
            "/paper/Diffusion-Weighted-Echo-Planar-Imaging-of-Ovarian-Katayama-Masui/b819f17b121c805a12ab0fcdb699dce1bd6817ae",
            "/paper/Quantitative-diffusion-MR-imaging-of-cerebral-tumor-Eis-Els/112ebe484d74057e2003bd0a404b377656ec0ee8",
            "/paper/Head-and-neck-lesions%3A-characterization-with-MR-Wang-Takashima/7905b738b15c87510f629434640eb32ff70ad18d",
            "/paper/Renal-lesions%3A-characterization-with-imaging-versus-Taouli-Thakur/da716cc11cc3cd055c16a99ea48b35e5ec1831b0"
        ]
    },
    {
        "id": "21e6688151f8210b2a72c73d661f76754b8b479e",
        "title": "Scarcity of publicly available oral cancer image datasets for machine learning research.",
        "abstract": "Semantic Scholar extracted view of \"Scarcity of publicly available oral cancer image datasets for machine learning research.\" by Namrata Sengupta et al.",
        "publication_year": "2022",
        "authors": [
            "Namrata Sengupta",
            "S. Sarode",
            "G. Sarode",
            "Urmi Ghone"
        ],
        "related_topics": [
            "Medicine",
            "Computer Science",
            "Political Science"
        ],
        "citation_count": "6",
        "reference_count": "10",
        "references": [
            "/paper/Supremacy-of-attention-based-convolution-neural-in-Deo-Pal/f122b34b7271a0dfaab33b48d6547d5f548b2fe1",
            "/paper/An-ensemble-deep-learning-model-with-empirical-for-Deo-Pal/7952a2f8354186182d0c61626b3d6491b48833bb",
            "/paper/Machine-Learning-Heuristics-on-Gingivobuccal-Cancer-Singh-Malik/dee8f3d8948db9d63a16ba10e46d6326b7c913ea",
            "/paper/NDB-UFES%3A-An-oral-cancer-and-leukoplakia-dataset-of-Assis-Soares/784fc7611f2df38c5bb6fa317ba987c9e96d5fd8",
            "/paper/On-the-importance-of-complementary-data-to-image-of-Lima-Assis/24303fd0b683ea8ab4100e90d15fe9c2d02f3871",
            "/paper/Oral-cancer-histopathology-images-and-artificial-A-Sarode-Kumari/897f147d7209d39121d0a523858c1cb7af72230b",
            "/paper/Characteristics-of-publicly-available-skin-cancer-a-Wen-Khan/376fab5f7886ad389cb341a587796ca4df9ba016",
            "/paper/A-deep-learning-algorithm-for-detection-of-oral-A-Fu-Chen/9244964828f717dc94ad4adb24dc16d8dd899542",
            "/paper/Automatic-classification-and-detection-of-oral-in-Warin-Limprasert/a932aa4621106478255a5b1defd17236ca2e6c35",
            "/paper/Bayesian-deep-learning-for-reliable-oral-cancer-Song-Sunny/ade486fc9e856793a7347aa1402e8c11724a2973",
            "/paper/Oral-cancer-databases%3A-A-comprehensive-review-Sarode-Sarode/f5d80b2ff52d41b6d3f4c5116c6d9aa2dc3a4b04",
            "/paper/Machine-learning-applications-in-cancer-prognosis-Kourou-Exarchos/95274ca3be569765960464d24f898c6fe025bac9",
            "/paper/A-comparison-of-deep-learning-performance-against-a-Liu-Faes/34f360a2bc49027bba77385be6c02179ba6875d8",
            "/paper/Epidemiologic-aspects-of-oral-cancer.-Sarode-Maniyar/cf47cb4cd08a117306895ec70cd982faf9392fbb",
            "/paper/Applications-of-Machine-Learning-in-Real-Life-of-Triantafyllidis-Tsanas/f7f2d6450d50ee11d3c5827ca1997daafef0a8fb",
            "/paper/Deep-learning-based-survival-prediction-of-oral-Kim-Lee/2e07896dabe7faef35918d6418d9f8ba80ebb742"
        ]
    },
    {
        "id": "a2673efe56b05b5f251906f5eb82704d7baa1540",
        "title": "Comparison of time-to-event machine learning models in predicting oral cavity cancer prognosis",
        "abstract": "Semantic Scholar extracted view of \"Comparison of time-to-event machine learning models in predicting oral cavity cancer prognosis\" by John Adeoye et al.",
        "publication_year": "2021",
        "authors": [
            "John Adeoye",
            "L. Hui",
            "M. Koohi-Moghadam",
            "Jiaqing Tan",
            "Siu-Wai Choi",
            "Peter Thomson"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "11",
        "reference_count": "46",
        "references": [
            "/paper/Machine-Learning%E2%80%93Based-Overall-Survival-Prediction-Bao-Wang/fdbcd183f6e113b1d0dd603c0b5f93d8bca7ae02",
            "/paper/Survival-analysis-of-breast-cancer-patients-using-KerenEvangeline-Kirubha/c4128b87c98547fe0209834164e8644c4063401f",
            "/paper/A-deep-learning-based-model-predicts-survival-for-a-Liao-Wang/f06f416573439acd0901e401ebeed031a19ae9e1",
            "/paper/Deep-Learning-Predicts-the-Survival-of-Oral-Adeoye-Koohi-Moghadam/6fd917fa772b59ea2ba94cc0042d71b97bed06ee",
            "/paper/Predicting-oral-cancer-survival-Development-and-of-Wang-Zhang/b0fd4a47928d8cf1f0eee26d9f62e4cc028977bc",
            "/paper/Machine-Learning-based-Lung-and-Colon-Cancer-using-Talukder-Islam/0e67255fdec1d4f463d05380064058f960f0e69e",
            "/paper/A-systematic-review-on-machine-learning-and-deep-in-D.-Gunavathi/105ad59fb1655048cbc8f7a53c9884f0b38b71aa",
            "/paper/Data-centric-artificial-intelligence-in-oncology%3A-a-Adeoye-Hui/66e46478fc83b19353a5fd67044f2592ff511a9b",
            "/paper/Development-and-validation-of-questionnaire-based-a-Li-Yang/4dda929faa60faa3689baf2aa36eb2501d488431",
            "/paper/Prediction-of-University-Patent-Transfer-Cycle-on-Deng-Chen/b5b26c3dad8e815a4a5c14880aaeb4d129761797",
            "/paper/Comparison-of-machine-learning-algorithms-for-the-Alkhadar-Macluskey/23c20ac8ba9b3718f16cd44cddb11688750410ec",
            "/paper/Machine-Learning-and-Treatment-Outcome-Prediction-Chu-Lee/881ef3c9e064c589d17eaf329e36a37780f03fe4",
            "/paper/Prediction-models-applying-machine-learning-to-oral-Adeoye-Tan/3b209d729819c233509ab532a2cde8a64f62a6ac",
            "/paper/A-Deep-Learning-Risk-Prediction-Model-for-Overall-A-Zhang-Dong/8aed46317e89bb2bf40d8874d442747d6807cac6",
            "/paper/Comparison-of-nomogram-with-machine-learning-for-of-Alabi-M%C3%A4kitie/9472ab3d04455070f95583e524493c9c86286440",
            "/paper/Survival-model-in-oral-squamous-cell-carcinoma-on-Rosado-Lequerica-Fern%C3%A1ndez/6cf1a9f8f18765470471f6130e44cfb931d5c700",
            "/paper/Development-of-a-Machine-Learning-Model-for-Risk-of-Tseng-Wang/fa88288bda89f87b66a539f7b17ace6a87e6e045",
            "/paper/Comparison-of-supervised-machine-learning-in-of-in-Alabi-Elmusrati/e18f7b47da494fd8cfac52e3c6eded3d8a9e71ab",
            "/paper/Comparison-between-artificial-neural-network-and-in-Zhu-Luo/eb00feb8d60560d95f42e21e06584a56f12199f2",
            "/paper/The-Application-of-Data-Mining-Techniques-to-Oral-Tseng-Chiang/5c421033f3eeb8b41fa5b3441a5d00f21796e8d4"
        ]
    },
    {
        "id": "dd9343d271cf76759172d94e29ae8cc9e8c5c89e",
        "title": "Compatible-domain Transfer Learning for Breast Cancer Classification with Limited Annotated Data",
        "abstract": "Semantic Scholar extracted view of \"Compatible-domain Transfer Learning for Breast Cancer Classification with Limited Annotated Data\" by Mohammad Amin Shamshiri et al.",
        "publication_year": "2023",
        "authors": [
            "Mohammad Amin Shamshiri",
            "A. Krzy\u017cak",
            "Marek Kowal",
            "J. Korbicz"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "55",
        "references": [
            "/paper/Label-Efficient-Breast-Cancer-Histopathological-Qi-Li/083a044422233f9e428b95192d579e58c2748af9",
            "/paper/Breast-Cancer-Classification-Using-Deep-Learning-A-Shahidi-Daud/9e505d09d3df9f1786d86c7817f2c3d1a86d6cdd",
            "/paper/Optimizing-the-Performance-of-Breast-Cancer-by-the-Alzubaidi-Al-Shamma/2b18e8af1b9c007f1c04d93c0f0b1642c1c4e4f9",
            "/paper/Deep-classification-of-breast-cancer-in-ultrasound-Behboodi-Rasaee/09d3acbd12d0afde882a81b58077f5499ebf701c",
            "/paper/Towards-a-Better-Understanding-of-Transfer-Learning-Alzubaidi-Fadhel/637d46a33ad7f5742c45bab36a60fe2bebde6f85",
            "/paper/Digital-mammographic-tumor-classification-using-Huynh-Li/c21b644926a1391d403b0a74e003a0e593de4464",
            "/paper/Representation-learning-for-mammography-mass-lesion-Arevalo-Gonz%C3%A1lez/a4360f362168a6107e1014b7fc61bed32038fc70",
            "/paper/A-Dataset-for-Breast-Cancer-Histopathological-Image-Spanhol-Oliveira/86f0b58404a264a6216e29c78a5c113d900ca461",
            "/paper/Computer-aided-diagnosis-for-breast-cancer-using-Aljuaid-Alturki/688bc5b6b171019afb7c56887c374a77253d5782",
            "/paper/Attention-by-Selection%3A-A-Deep-Selective-Attention-Xu-Liu/e065748cafb1ef439db92bd85d6eed8b92f924ec"
        ]
    },
    {
        "id": "7a209e1150e56c015ae9791a32957e2c15758617",
        "title": "MCFINet: Multidepth Convolution Network With Shallow-Deep Feature Integration for Semantic Labeling in Remote Sensing Images",
        "abstract": "This work proposes a multidepth convolution network with shallow-deep feature integration, called MCFINet, which could effectively integrate multiscale contexts and shallow-layer/deep-layer features for labeling various objects in remote sensing images. Semantic labeling in remote sensing images is an important and challenging technique, which has attracted increasing attention recently in earth detection, environmental protection, land utilization, and so on. However, it remains a challenge on how to effectively label objects with varied scales and similar textures in literature. Addressing this challenge, we propose a multidepth convolution network with shallow-deep feature integration, called MCFINet, which could effectively integrate multiscale contexts and shallow-layer/deep-layer features for labeling various objects. In the proposed network, we design two new modules\u2014a multidepth convolutional module (MDCM) and an adaptive feature integration module (AFIM). The MDCM employs multilayer convolutions with varied layer numbers but fixed small-sized kernels in parallel to capture multiscale contexts, while the AFIM adaptively integrates the shallow-layer and deep-layer features of the proposed network to capture more discriminant features for segmenting objects with similar textures. Extensive experimental results on two benchmark data sets demonstrate that MCFINet could achieve better performances than seven existing methods in most cases.",
        "publication_year": "2022",
        "authors": [
            "Dongjiao Wang",
            "Qiulei Dong"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "26",
        "references": [
            "/paper/BSSNet%3A-Building-Subclass-Segmentation-From-Images-Xie-Hu/cafd6fdefa130abed03c966ee7f05266596cc700",
            "/paper/Enhanced-contextual-representation-with-deep-neural-Cheng-He/01dd2c7dacd521b1185cfdeb9d6067e0d1c8a7f9",
            "/paper/Semantic-Labeling-in-Very-High-Resolution-Images-a-Liu-Fan/b246ceb229271375991ec7ce04dc96c50f14e34b",
            "/paper/Dense-Dilated-Convolutions%E2%80%99-Merging-Network-for-Liu-Kampffmeyer/5c2c03f1bd56f3825197962bd73a69def80a0d41",
            "/paper/Encoder-Decoder-with-Atrous-Separable-Convolution-Chen-Zhu/9217e28b2273eb3b26e4e9b7b498b4661e6e09f5",
            "/paper/Supervised-Classification-of-Multisensor-Remotely-a-Piramanayagam-Saber/9905cc8e2d60dd44b1880ac52440dee631671664",
            "/paper/Accurate-cloud-detection-in-high-resolution-remote-Li-Wei/770a02f93febfeba2f5e6ceeb29b7e252930f919",
            "/paper/Understanding-Convolution-for-Semantic-Segmentation-Wang-Chen/84c1717345dd451e7a61fe89807b4c017754fc4e",
            "/paper/Dense-Semantic-Labeling-of-Subdecimeter-Resolution-Volpi-Tuia/ef500cd3c401cd0632656c9fddccb1655c6659f9",
            "/paper/SegNet%3A-A-Deep-Convolutional-Encoder-Decoder-for-Badrinarayanan-Kendall/b0c065cd43aa7280e766b5dcbcc7e26abce59330",
            "/paper/Semantic-Image-Segmentation-with-Deep-Convolutional-Chen-Papandreou/39ad6c911f3351a3b390130a6e4265355b4d593b",
            "/paper/Road-Segmentation-for-Remote-Sensing-Images-Using-Shamsolmoali-Zareapoor/4de8aa957fba89472e2ed9cb6d660abe0640004d"
        ]
    },
    {
        "id": "a72a09d154c3fb78411ba7f712af26877f79dd83",
        "title": "Numerical Study on the Buckling Behavior of FG Porous Spherical Caps Reinforced by Graphene Platelets",
        "abstract": "The buckling response of functionally graded (FG) porous spherical caps reinforced by graphene platelets (GPLs) is assessed here, including both symmetric and uniform porosity patterns in the metal matrix, together with five different GPL distributions. The Halpin\u2013Tsai model is here applied, together with an extended rule of mixture to determine the elastic properties and mass density of the selected shells, respectively. The equilibrium equations of the pre-buckling state are here determined according to a linear three-dimensional (3D) elasticity basics and principle of virtual work, whose solution is determined from classical finite elements. The buckling load is, thus, obtained based on the nonlinear Green strain field and generalized geometric stiffness concept. A large parametric investigation studies the sensitivity of the natural frequencies of FG porous spherical caps reinforced by GPLs to different parameters, namely, the porosity coefficients and distributions, together with different polar angles and stiffness coefficients of the elastic foundation, but also different GPL patterns and weight fractions of graphene nanofillers. Results denote that the maximum and minimum buckling loads are reached for GPL-X and GPL-O distributions, respectively. Additionally, the difference between the maximum and minimum critical buckling loads for different porosity distributions is approximately equal to 90%, which belong to symmetric distributions. It is also found that a high weight fraction of GPLs and a high porosity coefficient yield the highest and lowest effects of the structure on the buckling loads of the structure for an amount of 100% and 12.5%, respectively.",
        "publication_year": "2023",
        "authors": [
            "Zhimin Zhou",
            "Yun Wang",
            "Suying Zhang",
            "R. Dimitri",
            "Francesco Tornabene",
            "K. Asemi"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": 0,
        "reference_count": "61",
        "references": [
            "/paper/Static-Response-of-Nanocomposite-Electromagnetic-a-Mukahal/4a12a4007ecdbcd907544be2a97ba2c9da6bd451",
            "/paper/Torsional-buckling-analyses-of-functionally-graded-Shahgholian-Ghahfarokhi-Safarpour/3d64f3b8a49965afa4035a2d9ff9cec6ef730b3b",
            "/paper/Buckling-analyses-of-FG-porous-nanocomposite-shells-Shahgholian-Ghahfarokhi-Rahimi/5ccb69ea3a9dd57774a368ac1f114903d4fe2ef5",
            "/paper/Transient-Thermal-Stresses-in-FG-Porous-Rotating-by-Babaei-Kiarasi/37ebc2fa166be0d9f5248455c793131b2f7d8052",
            "/paper/Vibration-Analysis-of-Functionally-Graded-Graphene-Bahaadini-Saidi/4d9fe4f5032b3ecbc87973450dc7ccfce2f7530d",
            "/paper/Nonlinear-axisymmetric-dynamic-buckling-of-graded-Haboussi-Sankar/493a46419e5427c771147144655341b3560f6496",
            "/paper/Stability-and-dynamic-behavior-of-porous-FGM-beam%3A-Priyanka-Twinkle/5ce266c7b0e33cf0c8029f6c890678232949821f",
            "/paper/Nonlinear-free-vibration-of-functionally-graded-on-Gao-Gao/2af94572ed20523f3c3d1157c26f33452bd836b2",
            "/paper/Free-vibration-and-elastic-buckling-of-functionally-Kitipornchai-Chen/017928ae80bda5891199f4be6a8efa1e8f7b9c1e",
            "/paper/Buckling-and-free-vibration-analyses-of-graded-on-Yang-Chen/52cc2713b991dae071de3e2f4ac5d570c9e7dc15",
            "/paper/Vibration-analysis-of-FG-porous-rectangular-plates-Zhou-Zhang/0cd4421e021f3acd733929bc1c1fb089787fc124"
        ]
    },
    {
        "id": "a97cb7fbb308e9a8f020b38e1b091dcfe4763a43",
        "title": "VisFusion: Visibility-aware Online 3D Scene Reconstruction from Videos",
        "abstract": "The VisFusion model, a visibility-aware online 3D scene reconstruction approach from posed monocular videos, aims to improve the feature fusion by explicitly inferring its visibility from a similarity matrix, computed from its projected features in each image pair. We propose VisFusion, a visibility-aware online 3D scene reconstruction approach from posed monocular videos. In particular, we aim to reconstruct the scene from volumetric features. Unlike previous reconstruction methods which aggregate features for each voxel from input views without considering its visibility, we aim to improve the feature fusion by explicitly inferring its visibility from a similarity matrix, computed from its projected features in each image pair. Following previous works, our model is a coarse-to-fine pipeline including a volume sparsification process. Different from their works which sparsify voxels globally with a fixed occupancy threshold, we perform the sparsification on a local feature volume along each visual ray to preserve at least one voxel per ray for more fine details. The sparse local volume is then fused with a global one for online reconstruction. We further propose to predict TSDF in a coarse-to-fine manner by learning its residuals across scales leading to better TSDF predictions. Experimental results on benchmarks show that our method can achieve superior performance with more scene details. Code is available at: https://github.com/huiyu-gao/VisFusion",
        "publication_year": "2023",
        "authors": [
            "Huiyu Gao",
            "Wei Mao",
            "Miaomiao Liu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "36",
        "references": [
            "/paper/VoRTX%3A-Volumetric-3D-Reconstruction-With-for-View-Stier-Rich/a358c152d177ad4ff4161f8822187d4a3e22bc24",
            "/paper/SimpleRecon%3A-3D-Reconstruction-Without-3D-Sayed-Gibson/caf96058774d96dd08df799319ef0d5430121518",
            "/paper/VolumeFusion%3A-Deep-Depth-Fusion-for-3D-Scene-Choe-Im/0ba9140dad37b1b3d8cfcdb6d54389edd701c58d",
            "/paper/Atlas%3A-End-to-End-3D-Scene-Reconstruction-from-Murez-As/35e2056e29d5295b2669fe696ae0c27007c94625",
            "/paper/TransformerFusion%3A-Monocular-RGB-Scene-using-Bovzivc-Palafox/64ac6c834bee275eb17f9a34d58ebf18e0f98e6a",
            "/paper/3DVNet%3A-Multi-View-Depth-Prediction-and-Volumetric-Rich-Stier/534ff47a5bd665ae38398053a90f7295a0e0b30b",
            "/paper/MVSNet%3A-Depth-Inference-for-Unstructured-Multi-view-Yao-Luo/87ca28235555f7e70cf1edc2a63cda4aef7fee42",
            "/paper/Consistent-video-depth-estimation-Luo-Huang/43550e50110d6a500ac827e30f54f391aae5f8c7",
            "/paper/PlanarRecon%3A-Realtime-3D-Plane-Detection-and-from-Xie-Gadelha/b38bbf2b21aa7a25a4918b1dbb8dc4514e617399",
            "/paper/NeuralRecon%3A-Real-Time-Coherent-3D-Reconstruction-Sun-Xie/8db0bf0fa406254d4cd57eb413443a0b96bd12b8"
        ]
    },
    {
        "id": "52966dbdb8e2d4a560e3848b13b0e87267c16226",
        "title": "Study on Age Invariant Face Recognition System",
        "abstract": "The use of appropriate Pre processing techniques for improving the quality of the input image and the use of Periocular region as the template for recognition as this region remains unchanged or stable for the lifetime in an individual is focused on. At present the Computer automated Face recognition systems are used for personal identification, but the Age variations of an individual poses a serious problem for it. Designing an appropriate feature representation and an effective matching framework for age invariant face recognition remains an open problem. In this paper we have focused on the use of appropriate Pre processing techniques for improving the quality of the input image and the use of Periocular region as the template for recognition as this region remains unchanged or stable for the lifetime in an individual. The modified PCA algorithm called the Self-PCA is used for creating the Eigen Face for feature extraction for the recognition process. The Self-PCA can be used in order to consider distinctiveness of the effects of aging of a person for age invariant face recognition. KeywordsBiometrics, Eigen Space, Face Recognition, Illumination Normalization, Periocular, Preprocessing, Pose correction, Self-PCA.",
        "publication_year": "2013",
        "authors": [
            "Wangjam Niranjan Singh",
            "D. Medhi",
            "M. Nimbarte"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "25",
        "references": [
            "/paper/Face-Recognition-across-Age-Using-Auto-Encoder-Rahman-Naiyar/984880c2af318521110e70d25f03f0e455c61d42",
            "/paper/Modeling-self-Principal-Component-Analysis-for-age-Nayak-Indiramma/740290145ea3980602229a62036db76546982c80",
            "/paper/A-Discriminative-Model-for-Age-Invariant-Face-Li-Park/96350ffbd6e201cf21f509401148ea7674c6e82d",
            "/paper/Efficient-face-recognition-with-compensation-for-Nayak-Indiramma/29616548010ddcdc3644a1cf1e2821a76f3c6a56",
            "/paper/Investigating-age-invariant-face-recognition-based-Juefei-Xu-Luu/5833547ef408a04b2efc49bcba4854ba3009585c",
            "/paper/Face-Recognition-using-Principle-Component-Analysis-Dabhade-Bewoor/a88dcb7563f7964c281bd93f35492f17abb4c691",
            "/paper/Age-invariant-face-recognition-using-graph-matching-Mahalingam-Kambhamettu/7755ded70d23839e9f9eb348605c0a4dc336e95d",
            "/paper/Image-processing-and-face-detection-analysis-on-on-Syambas-Purwanto/257b1233b8066c85705ef869621b6e03b8903c5c",
            "/paper/A-Study-of-Face-Recognition-as-People-Age-Ling-Soatto/d5f1886482893adf2f61e3b344d5483c86792ab5",
            "/paper/An-Optimized-Illumination-Normalization-Method-for-Holappa-Ahonen/a964903746a1067984fb1881438f7e5cc6468a3d",
            "/paper/Face-Verification-Across-Age-Progression-Ramanathan-Chellappa/5af32f9d079bb6d12b1175ee38e83a71b9516fe3"
        ]
    },
    {
        "id": "66bcc4ae4829ff707c4594b1ca8a2b32e94662b4",
        "title": "YOLODrone+: Improved YOLO Architecture for Object Detection in UAV Images",
        "abstract": "Novel architectural improvements to the YO-LOv5 architecture are proposed, increasing the number of detection layers and use of transformers in the model to improve the detection accuracy. The performance of object detection algorithms running on images taken from Unmanned Aerial Vehicles (UAVs) remains limited when compared to the object detection algorithms running on ground taken images. Due to its various features, YOLO based models, as a part of one-stage object detectors, are preferred in many UAV based applications. In this paper, we are proposing novel architectural improvements to the YO-LOv5 architecture. Our improvements include: (i) increasing the number of detection layers and (ii) use of transformers in the model. In order to train and test the performance of our proposed model, we used VisDrone and SkyData datasets in our paper. Our test results suggest that our proposed solutions can improve the detection accuracy.",
        "publication_year": "2022",
        "authors": [
            "Oyku Sahin",
            "S. Ozer"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "26",
        "references": [
            "/paper/YOLODrone%3A-Improved-YOLO-Architecture-for-Object-in-Sahin-Ozer/a314be3266c34bd127aaca6f3d9095048e973b75",
            "/paper/SyNet%3A-An-Ensemble-Network-for-Object-Detection-in-Albaba-Ozer/415e276ef5f308361b7a1c22b587372b71b456b4",
            "/paper/VisDrone-MOT2019%3A-The-Vision-Meets-Drone-Multiple-Wen-Zhang/7dd67487a33b813514f63367354dcfcf5a7fd28f",
            "/paper/Visual-Object-Tracking-in-Drone-Images-with-Deep-G%C3%B6zen-Ozer/86293304e7fb8137d2278592bf804d0a643f42b3",
            "/paper/TPH-YOLOv5%3A-Improved-YOLOv5-Based-on-Transformer-on-Zhu-Lyu/129bf8f4a1c9693ba140da7591293b9808c61a4e",
            "/paper/YOLO9000%3A-Better%2C-Faster%2C-Stronger-Redmon-Farhadi/7d39d69b23424446f0400ef603b2e3e22d0309d6",
            "/paper/ViT-YOLO%3ATransformer-Based-YOLO-for-Object-Zhang-Lu/4ba435bca5a006ee5b63a1355c5f323d8a8359b3",
            "/paper/Object-Detection-in-20-Years%3A-A-Survey-Zou-Shi/bd040c9f76d3b0b77e2065089b8d344c9b5d83d6",
            "/paper/You-Only-Look-Once%3A-Unified%2C-Real-Time-Object-Redmon-Divvala/f8e79ac0ea341056ef20f2616628b3e964764cfd",
            "/paper/Focal-Loss-for-Dense-Object-Detection-Lin-Goyal/72564a69bf339ff1d16a639c86a764db2321caab"
        ]
    },
    {
        "id": "85b1502c51b9b6d74dc0de5d95cb404993ea290b",
        "title": "Pan-zoom Motion Capture in Wide Scenes using Panoramic Background",
        "abstract": "In this research, the viewpoint of the camera keeps to be fixed an d the pan and zoom functions are used to operate the cameras so that the subject body could be always shot near the center of the image. Measuring a subject three-dimensionally from multiple cam eras, the measurable area is a common field of view from cameras. When the subject goes out of the field of vie w, the cameras must follow the subject. In this research, the viewpoint of the camera keeps to be fixed an d the pan and zoom functions are used to operate the cameras so that the subject body could be always shot near the center of the image. The problem is camera calibration. Our approach is to use a panoramic image. Each c mera pans in advance to take a background image and create a panoramic image of the background. Then, t he background image around the subject body is collated with the panoramic image, the pan rotation angle and the zoom ratio are obtained from the matching position, and the camera is calibrated. The body motion is ca ptured from the multi-view motion image using the camera parameters obtained in this way. Since the viewpo nt of the camera is fixed, the shooting range is not so wide, but it is still possible to capture an athlete\u2019s fl oor exercise in the gymnasium.",
        "publication_year": "2022",
        "authors": [
            "Masanobu Yamamoto"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "25",
        "references": [
            "/paper/Wide-range-motion-capture-from-panning-multi-view-Kobayashi-Yamamoto/6550ac72229c6fb7ff396c6f458ebbd630e19fa9",
            "/paper/Pan-tilt-zoom-camera-calibration-and-mosaic-Sinha-Pollefeys/e6cba419538e94e099ddbd7ffef3b63d683bf6bb",
            "/paper/Keeping-a-Pan-Tilt-Zoom-Camera-Calibrated-Wu-Radke/41b1473919d2e5b056d41f9875a2ea2bd0d3a269",
            "/paper/PANORAMA-BASED-CAMERA-CALIBRATION-Cannelle-Paparoditis/686543082137c7a39387beb19a7f01ff3d130a37",
            "/paper/Systems-and-Experiment-Paper%3A-Construction-of-Image-Shum-Szeliski/f52040154c9db4ce2f3741a53b7f1896f523948e",
            "/paper/EgoCap%3A-egocentric-marker-less-motion-capture-with-Rhodin-Richardt/ec2d6d3bfa6a342c215c4f1ee1ae22a5c4ca82ae",
            "/paper/FlyCap%3A-Markerless-Motion-Capture-Using-Multiple-Xu-Liu/2d3d7c6531d78ed18efe3d0f87f636da2b27e6f7",
            "/paper/Markerless-Outdoor-Human-Motion-Capture-Using-Micro-Saini-Price/6169963b0de4ab59284c8c8297fe343c4a673f2d",
            "/paper/Markerless-Motion-Capture-using-Multiple-Cameras-Sundaresan-Chellappa/2b24b17afd6db56de6adc41464df35d93f53f814",
            "/paper/Distinctive-Image-Features-from-Scale-Invariant-Lowe/8c04f169203f9e55056a6f7f956695babe622a38"
        ]
    },
    {
        "id": "88dd6594c9ddd4c4bb7f9b407b162e283907f4f3",
        "title": "Improving Sample Efficiency in Model-Free Reinforcement Learning from Images",
        "abstract": "A simple approach capable of matching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks and demonstrating robustness to observational noise, surpassing existing approaches in this setting. Training an agent to solve control tasks directly from high-dimensional images with model-free reinforcement learning (RL) has proven difficult. A promising approach is to learn a latent representation together with the control policy. However, fitting a high-capacity encoder using a scarce reward signal is sample inefficient and leads to poor performance.\nPrior work has shown that auxiliary losses, such as image reconstruction, can aid efficient representation learning. \nHowever, incorporating reconstruction loss into an off-policy learning algorithm often leads to training instability. We explore the underlying reasons and \nidentify variational autoencoders, used by previous investigations, as the cause of the divergence. \nFollowing these findings, we propose effective techniques to improve training stability. \nThis results in a simple approach capable of\nmatching state-of-the-art model-free and model-based algorithms on MuJoCo control tasks. Furthermore, our approach demonstrates robustness to observational noise, surpassing existing approaches in this setting. Code, results, and videos are anonymously available at https://sites.google.com/view/sac-ae/home.",
        "publication_year": "2019",
        "authors": [
            "Denis Yarats",
            "Amy Zhang",
            "Ilya Kostrikov",
            "Brandon Amos",
            "Joelle Pineau",
            "R. Fergus"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "260",
        "reference_count": "39",
        "references": [
            "/paper/Improving-Computational-Efficiency-in-Visual-via-Chen-Lee/95f59cbace81a7871d46fd0510a3884eef0dc0e9",
            "/paper/Image-Augmentation-Is-All-You-Need%3A-Regularizing-Kostrikov-Yarats/6568423cfaca7e24c88ea208cb0e67129e43aa9b",
            "/paper/RLAD%3A-Reinforcement-Learning-from-Pixels-for-in-Coelho-Oliveira/4f327f6a8e8dbc730cc350f458ddf0753f57945e",
            "/paper/Learning-Better-with-Less%3A-Effective-Augmentation-Ma-Zhang/d2796dec7c1c94f3a8fc22bea03b82d9a0196c1a",
            "/paper/Policy-Gradient-Methods-in-the-Presence-of-and-Panangaden-Rezaei-Shoshtari/3960ea842b8b89819f80cf2cd77adc321ff744a1",
            "/paper/Hierarchical-State-Abstraction-Based-on-Structural-Zeng-Peng/6d6ee72a43bac634e6f023c04724f3903b446e00",
            "/paper/Temporal-Disentanglement-of-Representations-for-in-Dunion-McInroe/103edca5e51ce35bdfe7afcbe6e83b7213acc454",
            "/paper/Stabilizing-Off-Policy-Deep-Reinforcement-Learning-Cetin-Ball/02d9dc238ae825e45e728607c3c83b77d07f4017",
            "/paper/Learning-Representations-for-Pixel-based-Control%3A-Tomar-Mishra/79419a74d1e9295c9a21659076adca3540af677f",
            "/paper/Making-Curiosity-Explicit-in-Vision-based-RL-Aljalbout-Ulmer/72ec17c5be2b6200a92319708dbf6d1ad89544d1",
            "/paper/Loss-is-its-own-Reward%3A-Self-Supervision-for-Shelhamer-Mahmoudieh/cdfb8f75c8f6459961359b483f1c017dbeec8282",
            "/paper/Deep-auto-encoder-neural-networks-in-reinforcement-Lange-Riedmiller/36086ff255207cc1adb818c4d0cd62287d437d38",
            "/paper/Stochastic-Latent-Actor-Critic%3A-Deep-Reinforcement-Lee-Nagabandi/69d1ee8a99f55e9228f33fdb3a0339541ad1201c",
            "/paper/From-Variational-to-Deterministic-Autoencoders-Ghosh-Sajjadi/622e392f8c5da161cf61582af434f6976094dfc4",
            "/paper/Learning-Latent-Dynamics-for-Planning-from-Pixels-Hafner-Lillicrap/fea3e63c97c7292dc6fbcb3ffe7131eb54053986",
            "/paper/Visual-Reinforcement-Learning-with-Imagined-Goals-Nair-Pong/3aadab924520c58be81781aafd51e6807e9c4576",
            "/paper/DeepMind-Control-Suite-Tassa-Doron/a9a3ed69c94a3e1c08ef1f833d9199f57736238b",
            "/paper/DARLA%3A-Improving-Zero-Shot-Transfer-in-Learning-Higgins-Pal/a2141a5ec0c65ea0a9861ae562f4c9fb8020d197",
            "/paper/Learning-Visual-Feature-Spaces-for-Robotic-with-Finn-Tan/6e9f1f05b7bef4b5afaf25df6952bc6bf0662179",
            "/paper/Auto-Encoding-Variational-Bayes-Kingma-Welling/5f5dc5b9a2ba710937e2c413b37b053cd673df02"
        ]
    },
    {
        "id": "4a819d20abbc171b8bca370fcf1b298b1166e839",
        "title": "Dynamic Group Convolution for Accelerating Convolutional Neural Networks",
        "abstract": "This paper proposes dynamic group convolution (DGC) that adaptively selects which part of input channels to be connected within each group for individual samples on the fly, and has similar computational efficiency as the conventional group Convolution simultaneously. Replacing normal convolutions with group convolutions can significantly increase the computational efficiency of modern deep convolutional networks, which has been widely adopted in compact network architecture designs. However, existing group convolutions undermine the original network structures by cutting off some connections permanently resulting in significant accuracy degradation. In this paper, we propose dynamic group convolution (DGC) that adaptively selects which part of input channels to be connected within each group for individual samples on the fly. Specifically, we equip each group with a small feature selector to automatically select the most important input channels conditioned on the input images. Multiple groups can adaptively capture abundant and complementary visual/semantic features for each input image. The DGC preserves the original network structure and has similar computational efficiency as the conventional group convolution simultaneously. Extensive experiments on multiple image classification benchmarks including CIFAR-10, CIFAR-100 and ImageNet demonstrate its superiority over the existing group convolution techniques and dynamic execution methods. The code is available at this https URL.",
        "publication_year": "2020",
        "authors": [
            "Z. Su",
            "Linpu Fang",
            "Wenxiong Kang",
            "D. Hu",
            "M. Pietik\u00e4inen",
            "Li Liu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "25",
        "reference_count": "61",
        "references": [
            "/paper/Pruning-Dynamic-Group-Convolution-with-Static-Che-Wang/fd3013c1b28218f28f7c039601c1a89492229c40",
            "/paper/Boosting-Convolutional-Neural-Networks-with-Middle-Su-Zhang/0f53757d05f64e055c9c995e879710cf471e70fd",
            "/paper/Parameter-Efficient-Dynamic-Convolution-via-Tensor-Hou-Kung/b79af6e9f7053de74b292b80859a5782cb6bee53",
            "/paper/Revisiting-Dynamic-Convolution-via-Matrix-Li-Chen/7cda4cc432db2797758d8fd1f2c4703523fd36d0",
            "/paper/Dynamic-Dual-Gating-Neural-Networks-Li-Li/e52f20cb4dee2cec6152a4088f9847094feac199",
            "/paper/Pixel-Difference-Networks-for-Efficient-Edge-Su-Liu/6560df566443a3634b7674942aebe0f0e0af3b00",
            "/paper/Automatic-Group-Based-Structured-Pruning-for-Deep-Wei-Wang/689cb70f1348df140d8b6566ec89741a43bc0e19",
            "/paper/An-Efficient-Sharing-Grouped-Convolution-via-Chen-Duan/30be54791bec3a413ff8b97b41db9a136d7d3c54",
            "/paper/S2-aware-network-for-visual-recognition-Zhao-Yang/f313df88b135a60b9b4d038cf0e172c2f9a01998",
            "/paper/Hierarchically-Structured-Network-With-Attention-Xiong-Gong/f5c5c6666729e0563b6bdde24f76d369160dc89a",
            "/paper/Dynamic-Channel-Pruning%3A-Feature-Boosting-and-Gao-Zhao/a055b9917759dd75811edbc8500ca247b457c5b2",
            "/paper/Fully-Learnable-Group-Convolution-for-Acceleration-Wang-Kan/79ca601c150eb576fccc87881d07332471b32be3",
            "/paper/Learning-Efficient-Convolutional-Networks-through-Liu-Li/90a16f34d109b63d95ab4da2d491cbe3a1c8b656",
            "/paper/Interleaved-Group-Convolutions-Zhang-Qi/b9fd6c8ae5c3dce4a7a40989d6dbf62f0093dc6e",
            "/paper/IGCV3%3A-Interleaved-Low-Rank-Group-Convolutions-for-Sun-Li/054db727adc1a3a877aae6ad4ac835a3a9b872ba",
            "/paper/Densely-Connected-Convolutional-Networks-Huang-Liu/5694e46284460a648fe29117cbc55f6c9be3fa3c",
            "/paper/Channel-Pruning-for-Accelerating-Very-Deep-Neural-He-Zhang/ee53c9480132fc0d09b1192226cb2c460462fd6d",
            "/paper/Xception%3A-Deep-Learning-with-Depthwise-Separable-Chollet/5b6ec746d309b165f9f9def873a2375b6fb40f3d",
            "/paper/Speeding-up-Convolutional-Neural-Networks-with-Low-Jaderberg-Vedaldi/021fc345d40d3e6332cd2ef276e2eaa5e71102e4",
            "/paper/CondenseNet%3A-An-Efficient-DenseNet-Using-Learned-Huang-Liu/efbac99adf8628aae7f070e5b4388a295956f9d2"
        ]
    },
    {
        "id": "8ab6e5ae92a83f593baeff10d68d87b124aeffbe",
        "title": "Unsupervised Video Summarization Based on Deep Reinforcement Learning with Interpolation",
        "abstract": "This paper presents a lightweight video summarization network with transformer and CNN networks to capture the global and local contexts to efficiently predict the keyframe-level importance score of the video in a short length and calculates the reward based on the reward functions, which helped select interesting keyframes efficiently and uniformly. Individuals spend time on online video-sharing platforms searching for videos. Video summarization helps search through many videos efficiently and quickly. In this paper, we propose an unsupervised video summarization method based on deep reinforcement learning with an interpolation method. To train the video summarization network efficiently, we used the graph-level features and designed a reinforcement learning-based video summarization framework with a temporal consistency reward function and other reward functions. Our temporal consistency reward function helped to select keyframes uniformly. We present a lightweight video summarization network with transformer and CNN networks to capture the global and local contexts to efficiently predict the keyframe-level importance score of the video in a short length. The output importance score of the network was interpolated to fit the video length. Using the predicted importance score, we calculated the reward based on the reward functions, which helped select interesting keyframes efficiently and uniformly. We evaluated the proposed method on two datasets, SumMe and TVSum. The experimental results illustrate that the proposed method showed a state-of-the-art performance compared to the latest unsupervised video summarization methods, which we demonstrate and analyze experimentally.",
        "publication_year": "2023",
        "authors": [
            "Ui-Nyoung Yoon",
            "Myung-Duk Hong",
            "G. Jo"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "27",
        "references": [
            "/paper/Interp-SUM%3A-Unsupervised-Video-Summarization-with-Yoon-Hong/9f3ee820302c3e23ca342cbdef801144f880e27b",
            "/paper/Self-Attention-Recurrent-Summarization-Network-with-Phaphuangwittayakul-Guo/2f3c304201e69a412f80d873d6dfacc20d922787",
            "/paper/Discriminative-Feature-Learning-for-Unsupervised-Jung-Cho/69b3b29c6fabaea88d382922346d9157395a3226",
            "/paper/Deep-Reinforcement-Learning-for-Unsupervised-Video-Zhou-Qiao/9e9e5f0c36548cfe2855aae46b519b146aa8c9ae",
            "/paper/Video-Summarization-With-Attention-Based-Networks-Ji-Xiong/88a8baa1be5292e62622f1cb8e627fbf759bf741",
            "/paper/A-Video-Summarization-Model-Based-on-Deep-Learning-Wang-Li/2df437fe675c89a00dc0a7c3a31fa46ed3d94c9d",
            "/paper/Video-Summarization-Using-Fully-Convolutional-Rochan-Ye/8c413c2ee66664909d8c194f3f3e08c5f109c3c1",
            "/paper/Extractive-Video-Summarizer-with-Memory-Augmented-Feng-Li/c4218e08002ffa8c2988c3d96af232440591e724",
            "/paper/Video-Summarization-with-Long-Short-Term-Memory-Zhang-Chao/1dbc12e54ceb70f2022f956aa0a46e2706e99962",
            "/paper/Unsupervised-Video-Summarization-with-Adversarial-Mahasseni-Lam/620fe6c786d15efca7f553ad70f295e2b693b391"
        ]
    },
    {
        "id": "746b31b391f4714b709399f6dcbe7278107d5fb9",
        "title": "Learning spatial-temporally regularized complementary kernelized correlation filters for visual tracking",
        "abstract": "A novel spatial-temporally regularized complementary kernelized CFs (STRCKCF) based tracking approach that formulates its model with only one training image, which can not only facilitate exploiting the circulant structure in learning, but also reasonably approximate the SRDCF with multiple training images. Despite excellent performance shown by spatially regularized discriminative correlation filters (SRDCF) for visual tracking, some issues remain open that hinder further boosting their performance: first, SRDCF utilizes multiple training images to formulate its model, which makes it unable to exploit the circulant structure of the training samples in learning, leading to high computational burden; second, SRDCF is unable to efficiently exploit the powerfully discriminative nonlinear kernels, further negatively affecting its performance. In this paper, we present a novel spatial-temporally regularized complementary kernelized CFs (STRCKCF) based tracking approach. First, by introducing spatial-temporal regularization to the filter learning, the STRCKCF formulates its model with only one training image, which can not only facilitate exploiting the circulant structure in learning, but also reasonably approximate the SRDCF with multiple training images. Furthermore, by incorporating two types of kernels whose matrices are circulant, the STRCKCF is able to fully take advantage of the complementary traits of the color and HOG features to learn a robust target representation efficiently. Besides, our STRCKCF can be efficiently optimized via the alternating direction method of multipliers (ADMM). Extensive evaluations on OTB100 and VOT2016 visual tracking benchmarks demonstrate that the proposed method achieves favorable performance against state-of-the-art trackers with a speed of 40 fps on a single CPU. Compared with SRDCF, STRCKCF provides a 8 \u00d7 speedup and achieves a gain of 5.5% AUC score on OTB100 and 8.4% EAO score on VOT2016.",
        "publication_year": "2020",
        "authors": [
            "Zhenyang Su",
            "Jing Li",
            "Junfei Chang",
            "Chengfang Song",
            "Yafu Xiao",
            "Jun Wan"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "67",
        "references": [
            "/paper/Multiple-Cues-Based-Robust-Visual-Object-Tracking-Khan-Jalil/e22d9cf2fc938a5a8514a44b7f4ece8f545adc0e",
            "/paper/Learning-Spatial-Temporal-Regularized-Correlation-Li-Tian/9f45b55af027503fab557f55f70e81e43c6c1db7",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Complementary-Tracking-via-Dual-Color-Clustering-Fan-Song/5425d41c227dd5b95304f6e3389e0c428bdfe1dc",
            "/paper/Learning-Adaptive-Discriminative-Correlation-via-Xu-Feng/bd5ecaea14eeb27c776b2c153b3e4716448bad4e",
            "/paper/Learning-Compact-Target-Oriented-Feature-for-Visual-Li-Huang/2477523d851af6e53caf2636a902937af670396e",
            "/paper/Dynamically-Spatiotemporal-Regularized-Correlation-Zheng-Song/83b556a05616cc8fbfa0b5e90da51eab4c1093e6",
            "/paper/Robust-Visual-Tracking-via-Convolutional-Networks-Zhang-Liu/9cf3c67529085d31c646091b97be1a1e3dc191f2",
            "/paper/Robust-visual-tracking-via-patch-based-kernel-with-Chen-Zhang/8bf62de8a90b419f68c4f53c4474299fe1406c2b",
            "/paper/High-Speed-Tracking-with-Kernelized-Correlation-Henriques-Caseiro/65c9b4b1d49f46b3f8f64a5f617acfc14f85d031",
            "/paper/Visual-tracking-using-spatio-temporally-nonlocally-Zhang-Li/2c175fd84a197e14a4d59b5e5a2acfc60393eae6"
        ]
    },
    {
        "id": "93bde1d02ce505b8d17b3078afe9c6d246138370",
        "title": "A survey on online learning for visual tracking",
        "abstract": "An in-depth overview of recent object tracking research is provided and the latest research trend in object tracking based on convolutional neural networks, which is receiving growing attention is reviewed. Visual object tracking has become one of the most active research topics in computer vision, which has been growing in commercial development as well as academic research. Many visual trackers have been proposed in the last two decades. Recent studies of computer vision for dynamic scenes include motion detection, object classification, environment modeling, tracking of moving objects, understanding of object behaviors, object identification, and data fusion from multiple sensors. This paper provides an in-depth overview of recent object tracking research. Object tracking tasks in realistic scenario often face challenging problems such as camera motion, occlusion, illumination effect, clutter, and similar appearance. A variety of tracker techniques have been published, which combine multiple techniques to solve multiple visual tracking sub-problems. This paper also reviews the latest research trend in object tracking based on convolutional neural networks, which is receiving growing attention. Finally, the paper discusses the future challenges and research directions for the object tracking problems that still need extensive studies in coming years.",
        "publication_year": "2020",
        "authors": [
            "M. Y. Abbass",
            "Ki-Chul Kwon",
            "Nam Kim",
            "S. A. Abdelwahab",
            "F. El-Samie",
            "A. Khalaf"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "32",
        "reference_count": "124",
        "references": [
            "/paper/Visual-tracking-using-convolutional-features-with-Abbass-Kwon/76bdd4422e298bf970c270e416c32fbdd32b112c",
            "/paper/Discriminative-appearance-model-with-template-for-Vadamala-Aklak/7ba5f8a3b9e4d2edecb593363fdadf9d17e28902",
            "/paper/Review-of-recent-advances-in-visual-tracking-JainulRinoshaS.-Augasta/98e301d9a9315b5d498d5b0c6c5d24820422e2ce",
            "/paper/Review-of-recent-advances-in-visual-tracking-JainulRinoshaS-GethsiyalAugasta/9fc420e96048a30f9f15c49ad2ccbfbf875864b9",
            "/paper/An-Improved-Visual-Tracking-Approach-Based-on-Zeng-Zhang/884b1f6a00a51d50358f90ba2b7e39c06b57f93f",
            "/paper/Siamese-Visual-Object-Tracking%3A-A-Survey-Ondra%C5%A1ovi%C4%8D-Tar%C3%A1bek/177d12634a4df3a6f67a4aecd03714ff39845d0e",
            "/paper/Object-Tracking-Based-on-Global-Context-Attention-Wang-Chen/94d79709b5b079ec7a3f2508b565d03b5e0e4c77",
            "/paper/Context-and-saliency-aware-correlation-filter-for-Wang-Yin/1e32c5176952cfd7a9b353ff8e203d19137a6133",
            "/paper/Object-Tracking-in-Video-Using-the-TLD-and-CMT-Tran/79d4f95b5b5bb81c8bd0aacfce75a9db6d3005b1",
            "/paper/Visual-tracking-via-dynamic-saliency-discriminative-Gao-Liu/bbc97484fe622da1e07cf5816ecd8e6b32235d78",
            "/paper/Efficient-object-tracking-using-hierarchical-model-Abbass-Kwon/588d4467f4324f211c8d981eefe4797c03227442",
            "/paper/Survey-of-single-target-visual-tracking-methods-on-Liu-Zhao/49060414918d5e893c4b734322d1080c6bbf7f25",
            "/paper/Visual-Tracking%3A-An-Experimental-Survey-Smeulders-Chu/eda3368a5198ca55768b07b6f5667aea28baf2cd",
            "/paper/Online-learning-of-robust-object-detectors-during-Kalal-Matas/03f9c789ae36196e3e6669c49370b8d48fcf26a2",
            "/paper/Real-time-visual-tracking-via-Incremental-Tensor-Wu-Cheng/da199480427da6b4c3800b11a91ef7f9bbbc90ee",
            "/paper/Adaptive-Probabilistic-Visual-Tracking-with-Update-Ross-Lim/fbd1a9f177eabe9817610d6dbbeec368611d89bb",
            "/paper/Robust-Visual-Tracking-with-Deep-Convolutional-on-Zhu-Porikli/1bb80ad79b9dafdbd68db40737cff3378b002e37",
            "/paper/Hierarchical-Convolutional-Features-for-Visual-Ma-Huang/5c8a6874011640981e4103d120957802fa28f004",
            "/paper/Once-for-All%3A-A-Two-Flow-Convolutional-Neural-for-Chen-Tao/3dc60732c1c08165c9d4e7b334ce66e511474bb2",
            "/paper/Learning-Dynamic-Siamese-Network-for-Visual-Object-Guo-Feng/7574b7e5a75fdd338c27af5aeb77ab79460c4437"
        ]
    },
    {
        "id": "211d9234104e2c8541e61912477a046cb8ba1127",
        "title": "Classification of Features Extracted from Image Foreground and Background for Tracking of Aerial Moving Targets",
        "abstract": "Experimental results show the effectiveness of the proposed method against KLT and SURF tracking algorithms in term of accuracy and the main challenge is classification of extracted features from background and foreground regions. Moving target tracking is a process in which an object is tracked and its location is determine in each frame. The goal of this process is facilitating the subsequent processing to analyze the behavior or detect moving objects. In this paper a new approach for aerial moving targets tracking based on feature matching algorithms have been proposed. The main challenge is classification of extracted features from background and foreground regions. To solve this problem, key points and their corresponding on the patterns extracted from consecutive frames, is calculated by the KLT algorithm. Then for each of this points, six attributes such as color, mean, variance and range of variation are calculated. Using these attributes and Bayesian discriminant function, extracted key points classified. In addition to resisting the proposed algorithm to scale change of target the object history scale in the 10 previous frames is used. Proposed algorithm was performed on an AIRCRAFT TRACKING standard database. Experimental results show the effectiveness of the proposed method against KLT and SURF tracking algorithms in term of accuracy.",
        "publication_year": "2016",
        "authors": [
            "\u0639\u0644\u06cc \u0645\u062d\u0645\u062f \u0644\u0637\u0628\u0641",
            "\u0648\u062d\u06cc\u062f \u0622\u0632\u0627\u062f \u0632\u0627\u062f\u0647"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "9",
        "references": [
            "/paper/Object-Tracking-Method-Based-on-SURF-Shuo-Na/93d5a35e9043dbe4a20a6d30569834fed199660d",
            "/paper/Online-adaptive-radial-basis-function-networks-for-Babu-Suresh/f362a8e6853113f28dfd012ef46a423f47bedb42",
            "/paper/K-means-Tracker%3A-A-General-Algorithm-for-Tracking-Hua-Wu/d9470464e5f94826ce8127a036056312b5c2c129",
            "/paper/K-means-Clustering-Based-Pixel-wise-Object-Tracking-Hua-Wu/b04094c1d30b5ee6ee0b97a9fb8bc842a3e937ef",
            "/paper/LOW-DISTANCE-AIRPLANES-DETECTION-AND-TRACKING-USING-Mohammad-Ramin/b9da0c4bbfe557fcae7ced305776c2cf64299be4",
            "/paper/Fast-Tracking-via-Spatio-Temporal-Context-Learning-Zhang-Zhang/5d22a122272e7cc728e6d344a0b8a070c24c2999",
            "/paper/Online-Object-Tracking-With-Sparse-Prototypes-Wang-Lu/a65c76169bdb8479353806556f61bf94fdec7e10",
            "/paper/Lucas-Kanade-20-Years-On%3A-A-Unifying-Framework-Baker-Matthews/389110a28961ebe80d8856cd204f8d8305a260ef",
            "/paper/Object-tracking%3A-A-survey-Yilmaz-Javed/1b5e87c442680e43203cbf8742561a7addf34e02"
        ]
    },
    {
        "id": "420c478c9274671a1111fc8d1fc4d0d21a587632",
        "title": "Comparative Analysis of Different Keypoint Based Copy-Move Forgery Detection Methods",
        "abstract": "Keypoint based features are chosen as they are computationally less complex as compared to block based features and turn out to be unsuitable for copy-move forgery detection due to the fact that Harris corner points are not scale invariant and detect only corners instead of edges. Copy-move forgery is the most commonly performed type of forgery. For copy-move forgery detection, block based and keypoint based methods are available. In this paper, keypoint based features are chosen as they are computationally less complex as compared to block based features. Four different keypoint based feature extraction algorithms i.e. SURF, KAZE, Harris corner points and BRISK are analyzed in order to check their efficiency for copy-move forgery detection. The method used involves four basic stages: Image pre-processing, interest point detector, feature vector description and feature matching. The results are compared on the basis of accuracy, fl-score and precision which are calculated using a threshold parameter for matching algorithm. It has been concluded that KAZE features give best results in all performance metrics and Harris corner points turn out to be unsuitable for copy move forgery detection due to the fact that Harris corner points are not scale invariant and detect only corners instead of edges.",
        "publication_year": "2018",
        "authors": [
            "Amanpreet Kaur",
            "Savita Walia",
            "Krishan Kumar"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "16",
        "references": [
            "/paper/Forgery-Detection-For-High-Resolution-Digital-Using-Kaur-Bhatla/b11ada6271daa81df49f384508b4d9bc545c478e",
            "/paper/Copy-Move-Forgery-Detection%3A-The-Current-and-Gajjar-Saxena/f37b0dcdc0016d24932dfea9c063412a4a0b4a86",
            "/paper/Unveiling-digital-image-forgeries-using-Markov-in-Walia-Kumar/df346feff93387d7b48c5eeeefbebb7f84cb8c09",
            "/paper/Muzzle-Based-Identification-of-Cattle-Using-KAZE-Kaushik-Reddy/11e11a0108a843d67b9d1d594b8ede3615ef294b",
            "/paper/Copy-move-forgery-detection-based-on-hybrid-Yang-Li/6352a1bb02013f4ef240b99b2dc429024d18edf7",
            "/paper/A-Survey-on-Keypoint-Based-Copy-paste-Forgery-Warbhe-Dharaskar/6ced83ed59e34c15980211ea423893c39e6547eb",
            "/paper/SURF-based-Detection-of-Copy-Move-Forgery-in-Flat-Zhang-Wang/5e5a5f50b7c0aef9a999c1a342d4446d8883f74c",
            "/paper/Passive-Forensics-for-Region-Duplication-Image-on-Zhao-Zhao/57b0e350def7a01ec8c4ceb2c9027403ef3bf47d",
            "/paper/Recent-Block-based-Methods-of-Copy-Move-Forgery-in-Sekhar-ChithraA./023927ba190a3fff893df89499bc121cffb23e41",
            "/paper/Nonoverlapping-Blocks-Based-Copy-Move-Forgery-Sun-Ni/e68a884cb920433887a2aaf53bc2a988738447d4",
            "/paper/A-survey-on-block-based-copy-move-image-forgery-Mahmood-Nawaz/7b62839fb9557a804503a111d85f3ffb2bdde801",
            "/paper/Copy-Move-Image-Forgery-Detection-Using-SURF-Point-Pun-Yuan/1a9a3ca027a34e334c1fe2310a80a9d9c230219d",
            "/paper/Digital-image-forgery-detection-using-passive-A-Birajdar-Mankar/6f9be47ae591b37a20a940883359b113ec5f3609",
            "/paper/An-Eagle-Eye-View-of-Recent-Digital-Image-Forgery-Walia-Kumar/dd8e31da0e60e6b04aa85b300ec426057a3516e3"
        ]
    },
    {
        "id": "853de0e00ac5ac257a622ae678ed373b8e086404",
        "title": "NVAE: A Deep Hierarchical Variational Autoencoder",
        "abstract": "NVAE is the first successful VAE applied to natural images as large as 256$\\times$256 pixels and achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256$\\times$256 pixels. The source code is available at this https URL .",
        "publication_year": "2020",
        "authors": [
            "Arash Vahdat",
            "J. Kautz"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "481",
        "reference_count": "81",
        "references": [
            "/paper/NCP-VAE%3A-Variational-Autoencoders-with-Noise-Priors-Aneja-Schwing/c97dfad7a023fbec97a901dae02b73e2e8e0fff1",
            "/paper/Very-Deep-VAEs-Generalize-Autoregressive-Models-and-Child/3e577c9bdc82cb7fed337a74f90bbc4505fdfb69",
            "/paper/A-Survey-on-Variational-Autoencoders-from-a-Green-Asperti-Evangelista/8223fd4888f86712267b7ecb292a93225241d0b5",
            "/paper/VAEBM%3A-A-Symbiosis-between-Variational-Autoencoders-Xiao-Kreis/0d79c6737849bea78d5bd96c0894d9ec61190089",
            "/paper/Spatial-Dependency-Networks%3A-Neural-Layers-for-Miladinovi'c-Stani'c/0019f027ef77749d95fa2751185046302dc6b264",
            "/paper/Deep-Generative-Modelling%3A-A-Comparative-Review-of-Bond-Taylor-Leach/bc519f58ae61afbf6318d6e4239d2d565c7ba467",
            "/paper/Cauchy-Schwarz-Regularized-Autoencoder-Tran-Pantic/220be271b6e3c2c8c57b47222f79370f4fc0e17e",
            "/paper/Dual-Contradistinctive-Generative-Autoencoder-Parmar-Li/a53e73139d9d6474ef8a002ee9c1dea49755ebc6",
            "/paper/Self-Supervised-Variational-Auto-Encoders-Gatopoulos-Tomczak/0ecdf84b5af6907481f9d67695b6df7c221de32e",
            "/paper/Learning-deep-autoregressive-models-for-data-Andersson-Wahlstrom/4e95e66ecf069a2f7ccc4c62e17043d0b268b9a6",
            "/paper/BIVA%3A-A-Very-Deep-Hierarchy-of-Latent-Variables-for-Maal%C3%B8e-Fraccaro/d4463fe262306eaac336fa5cae38e98811bffa80",
            "/paper/VAE-with-a-VampPrior-Tomczak-Welling/5ea2cdab68c69d7aef5a004495783ae7628193f2",
            "/paper/MAE%3A-Mutual-Posterior-Divergence-Regularization-for-Ma-Zhou/93729569ff90298ba4a130254c3be73c589806f4",
            "/paper/Generating-Diverse-High-Fidelity-Images-with-Razavi-Oord/6be216d93421bf19c1659e7721241ae73d483baf",
            "/paper/Importance-Weighted-Autoencoders-Burda-Grosse/3e47c4c2dd98c49b7771c7228812d5fd9eee56a3",
            "/paper/Ladder-Variational-Autoencoders-S%C3%B8nderby-Raiko/64d698ecd01eab99e81e586400e86d3d70b9cba7",
            "/paper/Variational-Lossy-Autoencoder-Chen-Kingma/590c9a1ff422b03477f7830b20609f212c85aa13",
            "/paper/PixelSNAIL%3A-An-Improved-Autoregressive-Generative-Chen-Mishra/d1c424c261c577958917055f72fb9e2ad0348865",
            "/paper/MaCow%3A-Masked-Convolutional-Generative-Flow-Ma-Hovy/d81edc320369c9c26e26a6801e08d68c4daf766c",
            "/paper/Flow%2B%2B%3A-Improving-Flow-Based-Generative-Models-with-Ho-Chen/c8b25a128f4bfd0c79de82c174dd403b2ef6eeb1"
        ]
    },
    {
        "id": "2f02729d35704bbec4ccd42fc59e80953f1c3f6b",
        "title": "Dynamic Template Selection Through Change Detection for Adaptive Siamese Tracking",
        "abstract": "Results indicate that integrating the proposed method into state-of-art adaptive Siamese trackers can increase the potential benefits of a template update strategy, and significantly improve performance. Deep Siamese trackers have recently gained much attention in recent years since they can track visual objects at high speed. Additionally, adaptive tracking methods, where target samples collected by the tracker are employed for online learning, have achieved state-of-the-art accuracy. However, single object tracking (SOT) remains a challenging task in real-world application due to changes and deformations in a target object's appearance. Learning on all the collected samples may lead to catastrophic forgetting, and thereby corrupt the tracking model. In this paper, SOT is formulated as an online incremental learning problem. A new method is proposed for dynamic sample selection and memory replay, preventing template corruption. In particular, we propose a change detection mechanism to detect gradual changes in object appearance, and select the corresponding samples for online adaption. In addition, an entropy-based sample selection strategy is introduced to maintain a diversified auxiliary buffer for memory replay. Our proposed method can be integrated into any object tracking algorithm that leverages online learning for model adaptation. Extensive experiments conducted on the OTB-100, LaSOT, UAV123, and TrackingNet datasets highlight the cost-effectiveness of our method, along with the contribution of its key components. Results indicate that integrating our proposed method into state-of-art adaptive Siamese trackers can increase the potential benefits of a template update strategy, and significantly improve performance. Code: https://github.com/madhukiranets/Adaptive-Siamese-Dimp",
        "publication_year": "2022",
        "authors": [
            "M. Kiran",
            "Le Thanh Nguyen-Meidine",
            "R. Sahay",
            "Rafael M. O. Cruz",
            "Louis-Antoine Blais-Morin",
            "Eric Granger"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "56",
        "references": [
            "/paper/RHL-track%3A-visual-object-tracking-based-on-Meng-Gong/be2c3cb1d4412c71a3d7acff15e886855b211961",
            "/paper/Concept-Drift-Detection-in-Data-Stream-Mining-%3A-A-Agrahari-Singh/71af002316d1df8d2170c65320c74968a66a0d64",
            "/paper/Online-Object-Tracking%3A-A-Benchmark-Wu-Lim/bfba194dfd9c7c27683082aa8331adc4c5963a0d",
            "/paper/Transformer-Meets-Tracker%3A-Exploiting-Temporal-for-Wang-Zhou/75284d5e4dfe1cd8a9ce69085210319e14fcfa3d",
            "/paper/Probabilistic-Regression-for-Visual-Tracking-Danelljan-Gool/6b6d31b022b7984a25fa9ee7fef64086ce7c464d",
            "/paper/Learning-Discriminative-Model-Prediction-for-Bhat-Danelljan/2c8315ae713b3e27c6e9f291a158134d9c516166",
            "/paper/STMTrack%3A-Template-free-Visual-Tracking-with-Memory-Fu-Liu/811ffb185bc90ac5d02d6dbfbcdb6173756b52ef",
            "/paper/Entropy-based-Sample-Selection-for-Online-Continual-Wiewel-Yang/031792258f21262b8b90df466a8c0201bfb4bc46",
            "/paper/Joint-Representation-Learning-with-Deep-Quadruplet-Zhang/9d52423a924ba7d1e5bcd1a4d5a439c15a7943cb",
            "/paper/Ocean%3A-Object-aware-Anchor-free-Tracking-Zhang-Peng/27d52bf3265bea0f9929980f6ffb4c2009eecfee",
            "/paper/High-Performance-Long-Term-Tracking-With-Dai-Zhang/adacccd99a42c3145ec6392a1a6b08878376e38b"
        ]
    },
    {
        "id": "c8da769833fa7dafc3546030ce090a092169f8df",
        "title": "Rethinking Computer-Aided Tuberculosis Diagnosis",
        "abstract": "A large-scale TB dataset is established, namely Tuberculosis X-ray (TBX11K) dataset, which enables the training of sophisticated detectors for high-quality CTD and reform the existing object detectors to adapt them to simultaneous image classification and TB area detection. As a serious infectious disease, tuberculosis (TB) is one of the major threats to human health worldwide, leading to millions of death every year. Although early diagnosis and treatment can greatly improve the chances of survival, it remains a major challenge, especially in developing countries. Computer-aided tuberculosis diagnosis (CTD) is a promising choice for TB diagnosis due to the great successes of deep learning. However, when it comes to TB diagnosis, the lack of training data has hampered the progress of CTD. To solve this problem, we establish a large-scale TB dataset, namely Tuberculosis X-ray (TBX11K) dataset. This dataset contains 11200 X-ray images with corresponding bounding box annotations for TB areas, while the existing largest public TB dataset only has 662 X-ray images with corresponding image-level annotations. The proposed dataset enables the training of sophisticated detectors for high-quality CTD. We reform the existing object detectors to adapt them to simultaneous image classification and TB area detection. These reformed detectors are trained and evaluated on the proposed TBX11K dataset and served as the baselines for future research.",
        "publication_year": "2020",
        "authors": [
            "Yun Liu",
            "Yu-Huan Wu",
            "Yunfeng Ban",
            "Huifang Wang",
            "Ming-Ming Cheng"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "57",
        "reference_count": "40",
        "references": [
            "/paper/EfficientNet-Based-Convolutional-Neural-Networks-Ravi-Narasimhan/ac1ff733028f9cb175d9ccece59e851fd18c96bd",
            "/paper/Computer-aided-Tuberculosis-Diagnosis-with-Pan-Zhao/74f4b38cd73496c31642de66948e969409a0f603",
            "/paper/E-TBNet%3A-Light-Deep-Neural-Network-for-Automatic-of-An-Peng/480b0c78dfc5da5d2e86dc12a98257af8c218b7a",
            "/paper/An-Improved-Deep-Network-of-Pulmonary-Tuberculosis-An-Peng/d769afe3fc27afa70c1e40b1e1656050409d9996",
            "/paper/Detection-of-Tuberculosis-based-on-Deep-Learning-Puttagunta-Ravi/e27e02a299e37a1c5bc7fd7ecccd4ec8a674f31c",
            "/paper/Identifying-Drug-Resistant-Tuberculosis-in-Chest-of-Karki-Kantipudi/e101b2abc5bb48de67f560449745bdc5c7e408ea",
            "/paper/AI-Assisted-Tuberculosis-Detection-and-from-Chest-a-Acharya-Dhiman/a47db68c236680f09e78e9a265fd7a87f18ec9af",
            "/paper/Detection-of-Lung-Lesions-in-Chest-X-ray-Images-on-Wei-Ou/e3bd7bc291b2969f25fdd1a706d58b382a0e4b28",
            "/paper/Object-Detection-and-Instance-Segmentation-in-Chest-Griffin-Cao/0b7867849e3960b54bf4aa3f1f14d13091f9142e",
            "/paper/Generative-Adversarial-Network-based-Chest-Disease-Junaid-Anwar/ebe10d3189518bb879f56bbac4968f37bdba5b40",
            "/paper/A-novel-approach-for-tuberculosis-screening-based-Hwang-Kim/212388cada6b67f0937eedc90f99d4fd70ad612f",
            "/paper/Pre-trained-convolutional-neural-networks-as-for-Lopes-Valiati/41240f3f1eccf75a6bcb105fd453bdb1675e8ce1",
            "/paper/Automatic-Tuberculosis-Screening-Using-Chest-Jaeger-Karargyris/bc7fdd91d351203f2820240b43b9aa4a66d0a8e3",
            "/paper/Deep-Learning-at-Chest-Radiography%3A-Automated-of-by-Lakhani-Sundaram/28bab81994b60eadc04033885d1023a9116f8e95",
            "/paper/Role-of-Gist-and-PHOG-Features-in-Computer-Aided-of-Chauhan-Chauhan/7582c82b69d73b52a28aa324596d38594d603a55",
            "/paper/FCOS%3A-Fully-Convolutional-One-Stage-Object-Tian-Shen/e2751a898867ce6687e08a5cc7bdb562e999b841",
            "/paper/Lung-Segmentation-in-Chest-Radiographs-Using-With-Candemir-Jaeger/86bf80d54ef110149398c5830606547c0537641f",
            "/paper/Faster-R-CNN%3A-Towards-Real-Time-Object-Detection-Ren-He/424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "/paper/Focal-Loss-for-Dense-Object-Detection-Lin-Goyal/72564a69bf339ff1d16a639c86a764db2321caab",
            "/paper/ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff"
        ]
    },
    {
        "id": "e10f7e643779ff0ff01448721e4ae9bdc908c7fb",
        "title": "YOLO-LOGO: A transformer-based YOLO segmentation model for breast mass detection and segmentation in digital mammograms",
        "abstract": "Semantic Scholar extracted view of \"YOLO-LOGO: A transformer-based YOLO segmentation model for breast mass detection and segmentation in digital mammograms\" by Yongye Su et al.",
        "publication_year": "2022",
        "authors": [
            "Yongye Su",
            "Qian Liu",
            "W. Xie",
            "P. Hu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "12",
        "reference_count": "31",
        "references": [
            "/paper/Vision-Transformer-Based-Transfer-Learning-for-Ayana-Dese/e7b94521b38c05f66f6a337c80be5956084585ac",
            "/paper/Multiple-level-thresholding-for-breast-mass-Yu-Wang/a5001d5daba1f539c4452cbe7b31373a3a679288",
            "/paper/Multi-Head-Feature-Pyramid-Networks-for-Breast-Mass-Zhang-Xu/3886a97d7e24582214100ac736d7fd86ca5654ce",
            "/paper/Transformer-based-mass-detection-in-digital-Tarifa-Marrocco/1d5d7985e8773a4ecc6290ac94386df1b0b9b499",
            "/paper/Ensemble-Technique-Coupled-with-Deep-Transfer-for-Kotei-Thirunavukarasu/00a727146015be6ba355a398fc1fc566ad40af07",
            "/paper/Learning-with-Explicit-Shape-Priors-for-Medical-You-He/413cc55b90887f27e63079ede1c2350776458641",
            "/paper/One-stage-and-lightweight-CNN-detection-approach-to-Han-Huang/2402f8a0c4b3d82e8bb75d9fec0dca5fb7c4ce84",
            "/paper/Breast-cancer-detection%3A-Shallow-convolutional-deep-Das-Das/01a16ca737db3f60afae38228374c11b61b9a429",
            "/paper/YOLO-Based-Deep-Learning-Model-for-Segmenting-the-Rasi-Antobennet/9ef7a25e8e0d7a9ef7eeedfd0a1df0d7e4743910",
            "/paper/Automatic-reorientation-by-deep-learning-to-SPECT-Zhu-Wang/12006a47d3759bc87196036fbdec7cf37cd0c9b5",
            "/paper/Simultaneous-Detection-and-Classification-of-Breast-Al-masni-Al-antari/3cbe19d5e217d1bc27d45dd867a6e185db9970d3",
            "/paper/Integrating-segmentation-information-into-CNN-for-Tsochatzidis-Koutla/1b5d4cf4c09acfeb2662ed10ac2ab33aceb3f3ad",
            "/paper/Connected-UNets%3A-a-deep-learning-architecture-for-Baccouche-Garcia-Zapirain/5269e5dac2c63932f9793b6ae36909c02bb6c65b",
            "/paper/Breast-cancer%3A-One-stage-automated-detection%2C-and-Soulami-Kaabouch/ba4a787088f7e1fc278e45bc1663d402c5922d4e",
            "/paper/Two-stage-multi-scale-breast-mass-segmentation-for-Yan-Conze/c8e7667f1dfbd117f1e461feca147ac2435d2677",
            "/paper/Breast-tumor-segmentation-and-shape-classification-Singh-Rashwan/1220fbc61ec47d5978d42c8b5a48b7a33b3c8374",
            "/paper/Deep-learning-for-mass-detection-in-Full-Field-Agarwal-D%C3%ADaz/64ad8fc9aef099396aaf7bd2d5030cd577e44a34",
            "/paper/A-fully-integrated-computer-aided-diagnosis-system-Al-antari-Al-masni/0f3ca7895bebbad7a0f4b7ddcc3febeeff9811ef",
            "/paper/Evaluation-of-deep-learning-detection-and-towards-Al-antari-Han/f36b3ee8d83ec14808a268183e28f05f4b7b3bc3",
            "/paper/Breast-Mass-Detection-With-Faster-R-CNN%3A-On-the-of-Famouri-Morra/1d72fb2b1d979c7159ab406c7b78b3f3036a1206"
        ]
    },
    {
        "id": "e1d8ecfdbb508a8825ecef6cb08d0ffda2a65b28",
        "title": "Magnetic resonance imaging in lung: a review of its potential for radiotherapy.",
        "abstract": "While there are some limitations for the adoption of MRI in RT-planning process for lung cancer, MRI has shown the potential to compete with both CT and PET for tumour delineation and motion definition, with the added benefit of functional information. MRI has superior soft-tissue definition compared with existing imaging modalities in radiation oncology; this has the added benefit of functional as well as anatomical imaging. This review aimed to evaluate the current use of MRI for lung cancer and identify the potential of a MRI protocol for lung radiotherapy (RT). 30 relevant studies were identified. Improvements in MRI technology have overcome some of the initial limitations of utilizing MRI for lung imaging. A number of commercially available and novel sequences have shown image quality to be adequate for the detection of pulmonary nodules with the potential for tumour delineation. Quantifying tumour motion is also feasible and may be more representative than that seen on four-dimensional CT. Functional MRI sequences have shown correlation with flu-deoxy-glucose positron emission tomography (FDG-PET) in identifying malignant involvement and treatment response. MRI can also be used as a measure of pulmonary function. While there are some limitations for the adoption of MRI in RT-planning process for lung cancer, MRI has shown the potential to compete with both CT and PET for tumour delineation and motion definition, with the added benefit of functional information. MRI is well placed to become a significant imaging modality in RT for lung cancer.",
        "publication_year": "2016",
        "authors": [
            "Shivani Kumar",
            "G. Liney",
            "R. Rai",
            "L. Holloway",
            "D. Moses",
            "S. Vinod"
        ],
        "related_topics": [
            "Medicine",
            "Physics"
        ],
        "citation_count": "40",
        "reference_count": "87",
        "references": [
            "/paper/Magnetic-resonance-imaging-in-precision-radiation-Bainbridge-Salem/c2683f27ba55b2046848ffdff62b64b31241c07c",
            "/paper/Chest-Magnetic-Resonance-Imaging-Decreases-of-Gross-Basson-Jarraya/8a428e20abff674b70a18235aa56bb67c7a49f6b",
            "/paper/Chest-Magnetic-Resonance-Imaging-Decreases-of-Gross-Basson-Jarraya/55d3fa2cb0b0d7a415fecd7c4edba307b9259df6",
            "/paper/Feasibility-of-pulmonary-MRI-for-nodule-detection-Yu-Yang/6750d7976633d4309df19bd010e6b1ca586a097e",
            "/paper/The-role-of-imaging-in-screening-special-feaTure%3A-Ronot-Pommier/5f1d0ecfdfc39dc05055baf3d49b68f15fc11209",
            "/paper/Whole-Body-MRI-and-oncology%3A-recent-major-advances.-Pasoglou-Michoux/8ff0ddc11d704685106a70df457b4261bb796b55",
            "/paper/State-of-the-Art%3A-Lung-Cancer-Staging-Using-Updated-Batouty-Saleh/4237ee5a57bc0370d564269907b8e8ee4428314c",
            "/paper/Functional-Image-guided-Radiotherapy-Planning-for-Ireland-Tahir/860c0e8b6dada9da03e79d0c19abb2637c457301",
            "/paper/Comparison-of-gross-tumor-volumes-of-pulmonary-by-T-Hama-Tate/94341b4f5b24ee5730c697f28495fc30439f88a9",
            "/paper/PET-and-MRI-guided-adaptive-radiotherapy%3A-Rational%2C-Thureau-Briens/db82ae34ad951629c1818bd5fb00c3f9d0a52a16",
            "/paper/Magnetic-resonance-imaging-(MRI)%3A-considerations-in-Khoo-Dearnaley/2f77df319827925f8267ab54eab89cf726d58d1b",
            "/paper/Diffusion-and-perfusion-MRI-of-the-lung-and-Henzler-Schmid%E2%80%90Bindert/11f7bc0138ca82c1d10c7e753830095ff21cfea5",
            "/paper/Clinical-oncologic-applications-of-PET-MRI%3A-a-new-Partovi-Kohan/3351fe1ee79316d0d3905d17047569a07f3b220d",
            "/paper/Clinical-applications-for-diffusion-magnetic-in-Tsien-Cao/e3a0ebbdf254d00cdaa1ab3adea31cebb8e6eb0a",
            "/paper/MRI-of-the-lung-(2-3).-Why-%E2%80%A6-when-%E2%80%A6-how-Biederer-Beer/5efb25c6c36edc04ff2e962f68d219bbcf60a640",
            "/paper/The-role-of-positron-emission-tomography-computed-Manus-Hicks/9f38b1aeed3bbb55376375cb16add948118205da",
            "/paper/MRI-of-the-lung-(1-3)%3A-methods-Wild-Marshall/68ea0270537656318bae74e5c1ea2a658334c293",
            "/paper/New-developments-in-MRI-for-target-volume-in-Khoo-Joon/e385c0d9ddc60ac19dcfed987c5e8529fc899dcc",
            "/paper/MRI-of-the-lung%3A-state-of-the-art.-Wielp%C3%BCtz-Kauczor/5d94bca9a246fa969c923e609e77cadb9106e5e4",
            "/paper/Detection-of-radiation-induced-lung-injury-in-cell-Ireland-Din/ed24d2fe8b514111a8fb72515a69090319fcde46"
        ]
    },
    {
        "id": "ade486fc9e856793a7347aa1402e8c11724a2973",
        "title": "Bayesian deep learning for reliable oral cancer image classification.",
        "abstract": "The experiments show the model is capable of identifying difficult cases needing further inspection and improved accuracy by uncertainty-informed referral and can be further improved by referring more patients. In medical imaging, deep learning-based solutions have achieved state-of-the-art performance. However, reliability restricts the integration of deep learning into practical medical workflows since conventional deep learning frameworks cannot quantitatively assess model uncertainty. In this work, we propose to address this shortcoming by utilizing a Bayesian deep network capable of estimating uncertainty to assess oral cancer image classification reliability. We evaluate the model using a large intraoral cheek mucosa image dataset captured using our customized device from high-risk population to show that meaningful uncertainty information can be produced. In addition, our experiments show improved accuracy by uncertainty-informed referral. The accuracy of retained data reaches roughly 90% when referring either 10% of all cases or referring cases whose uncertainty value is greater than 0.3. The performance can be further improved by referring more patients. The experiments show the model is capable of identifying difficult cases needing further inspection.",
        "publication_year": "2021",
        "authors": [
            "Bofan Song",
            "Sumsum P. Sunny",
            "Shaobai Li",
            "Keerthi Gurushanth",
            "Pramila Mendonca",
            "Nirza Mukhia",
            "Sanjana Patrick",
            "S. Gurudath",
            "S. Raghavan",
            "Imchen Tsusennaro",
            "Shirley T. Leivon",
            "T. Kolur",
            "V. Shetty",
            "Vidya R. Bushan",
            "R. Ramesh",
            "Tyler Peterson",
            "V. Pillai",
            "P. Wilder-Smith",
            "A. Sigamani",
            "A. Suresh",
            "M. Kuriakose",
            "Praveen Birur",
            "Rongguang Liang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "16",
        "reference_count": "24",
        "references": [
            "/paper/Exploring-uncertainty-measures-in-convolutional-for-Song-Li/4efcca204f430fd5fe49bde14f27d343b0544dd0",
            "/paper/Trustworthy-clinical-AI-solutions%3A-a-unified-review-Lambert-Forbes/b18cb2b9c1a605c72d9e4c8d045c945a06fab384",
            "/paper/Uncertainty-informed-deep-learning-models-enable-Dolezal-Srisuwananukorn/09bb95dc51bd58c8fefc237a1dc22848394f641c",
            "/paper/Application-of-simultaneous-uncertainty-for-image-a-Sahlsten-Jaskari/cf3359000887bace9f37d2db52a933883c9381b3",
            "/paper/Interpretable-and-Reliable-Oral-Cancer-Classifier-Song-Zhang/01e5e9f6fccfb58d772f7cb002e47ffe2a5c33f6",
            "/paper/A-Review-on-Bayesian-Deep-Learning-in-Healthcare%3A-Abdullah-Hassan/916325c49aafb6891f404250781530ca8ab043b6",
            "/paper/Scarcity-of-publicly-available-oral-cancer-image-Sengupta-Sarode/21e6688151f8210b2a72c73d661f76754b8b479e",
            "/paper/Machine-learning-in-point-of-care-automated-of-oral-Ferro-Kotecha/6f33724b68d09e8811337a481cb6d4d13f270462",
            "/paper/A-Current-Review-of-Machine-Learning-and-Deep-in-Dixit-Kumar/fb5fcf878c1f390c0eb1b81e30a8672589459117",
            "/paper/Uncertainty-Quantification-for-MLP-Mixer-Using-Deep-Abdullah-Hassan/37db33c8e96717e07478909977e87390caa2516d",
            "/paper/Identifying-Medical-Diagnoses-and-Treatable-by-Deep-Kermany-Goldbaum/364128bcce9836d60e685bb717b80f30e25092e0",
            "/paper/Reliable-deep-learning-based-phase-imaging-with-Xue-Cheng/e02ad71fc29eaba88f5d8603ad5dd6266ae78488",
            "/paper/Clinically-applicable-deep-learning-for-diagnosis-Fauw-Ledsam/b2d952fbd6951cbed68ea13003a045300970731a",
            "/paper/Automatic-classification-of-dual-modalilty%2C-oral-Song-Sunny/eeebfa2343ef9180d647d7bbec008e249d250d88",
            "/paper/An-Observational-Study-of-Deep-Learning-and-of-for-Hu-Bell/e852a030f89e5212d2b172bd59c7bc5914dcb60a",
            "/paper/Automatic-Classification-of-Cancerous-Tissue-in-of-Aubreville-Knipfer/6d23efb8c59592b29f876f310ab480f303c20d8a",
            "/paper/Diagnostic-Assessment-of-Deep-Learning-Algorithms-Bejnordi-Veta/ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba",
            "/paper/Classification-and-mutation-prediction-from-cell-Coudray-Ocampo/769149c0dc0ed308eca8bc916f4326b2e2f57a1f",
            "/paper/Accuracy-and-Efficiency-of-Deep-Learning%E2%80%93Based-of-Wentzensen-Lahrmann/40652dcf936bce11d027aed220902b5367fbcf7c",
            "/paper/Dermatologist-level-classification-of-skin-cancer-Esteva-Kuprel/e1ec11a1cb3d9745fb18d3bf74247f95a6663d08"
        ]
    },
    {
        "id": "b5b26c3dad8e815a4a5c14880aaeb4d129761797",
        "title": "Prediction of University Patent Transfer Cycle Based on Random Survival Forest",
        "abstract": "Taking the invention patents of the C9 League from 2002 to 2020 as samples, a random survival forest model is established to predict the dynamic time-point of patent transfer cycle. By ranking the variables based on importance, it is found that the countries citing, the non-patent citations and the backward citations have significant impacts on the patent transfer cycle. C-index, Brier score and integrated Brier score are used to measure the discrimination and calibration ability of the four different survival models respectively. It is found that the prediction accuracy of the random survival forest model is higher than that of the Cox proportional risk model, Cox model based on lasso penalty and random forest model. In addition, the survival function and cumulative risk function under the random survival forest are adopted to predict and analyze the individual university patent transfer cycle, which shows that the random survival forest model has good prediction performance and is able to help universities as well as enterprises to identify the patent transfer opportunities effectively, thereby shortening the patent transfer cycle and improving the patent transfer efficiency.",
        "publication_year": "2022",
        "authors": [
            "Disha Deng",
            "Tao Chen"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": 0,
        "reference_count": "35",
        "references": [
            "/paper/A-Predictive-Model-for-Patent-Registration-Time-Jun-Uhm/ab73be92c3add3de3cfce526228599bb99dcbf54",
            "/paper/Determinants-of-patent-survival-in-emerging-from-in-Danish-Ranjan/7e81abd7693536ecdf85ee8ebad1af81d8b3eb74",
            "/paper/Novel-mixed-encoding-for-forecasting-patent-grant-Dutt-Rathi/b8507b2b43b5cb31a683e56f0fbe3e928c75abd9",
            "/paper/A-patent-quality-analysis-for-innovative-technology-Trappey-Trappey/1ba9605f4c47928a0a4a756ab080fa04ab4c5c7e",
            "/paper/Faculty-patent-assignment-in-the-Chinese-mainland%3A-Fong-Chang/29f3d761cc286eff7fd9d6f0503546b0bca2d6bc",
            "/paper/Evolution-of-High-Value-Patents-in-Reverse-Focus-on-Wei-Liu/a7af9da2834fce0b393ea2c6ffdba94c7b66987d",
            "/paper/Delay-from-patent-filing-to-technology-transfer%3A-A-Llor/9222c770fe0ed0f01e3e75c929895248f2b5e2a9",
            "/paper/Advancing-In%E2%80%90Hospital-Clinical-Deterioration-Models-Jeffery-Dietrich/2ae22d08ad04a47b88099e218e08a4648c78fead",
            "/paper/Individual-risk-prediction%3A-Comparing-random-with-a-Baralou-Kalpourtzi/028dd239a12b06a4b5914e46f8f4de8f898411b0",
            "/paper/A-Selective-Review-on-Random-Survival-Forests-for-Wang-Li/b38c5d62a7e6fcae049be3822e386f7b9342b393"
        ]
    },
    {
        "id": "e065748cafb1ef439db92bd85d6eed8b92f924ec",
        "title": "Attention by Selection: A Deep Selective Attention Approach to Breast Cancer Classification",
        "abstract": "This work proposes a deep selective attention approach that aims to select valuable regions in the original images for classification, and demonstrates superior performance compared to state-of-the-art deep learning approaches. Deep learning approaches are widely applied to histopathological image analysis due to the impressive levels of performance achieved. However, when dealing with high-resolution histopathological images, utilizing the original image as input to the deep learning model is computationally expensive, while resizing the original image to achieve low resolution incurs information loss. Some hard-attention based approaches have emerged to select possible lesion regions from images to avoid processing the original image. However, these hard-attention based approaches usually take a long time to converge with weak guidance, and valueless patches may be trained by the classifier. To overcome this problem, we propose a deep selective attention approach that aims to select valuable regions in the original images for classification. In our approach, a decision network is developed to decide where to crop and whether the cropped patch is necessary for classification. These selected patches are then trained by the classification network, which then provides feedback to the decision network to update its selection policy. With such a co-evolution training strategy, we show that our approach can achieve a fast convergence rate and high classification accuracy. Our approach is evaluated on a public breast cancer histopathological image database, where it demonstrates superior performance compared to state-of-the-art deep learning approaches, achieving approximately 98% classification accuracy while only taking 50% of the training time of the previous hard-attention approach.",
        "publication_year": "2019",
        "authors": [
            "Bolei Xu",
            "Jingxin Liu",
            "Xianxu Hou",
            "Bozhi Liu",
            "J. Garibaldi",
            "I. Ellis",
            "Andy Green",
            "Linlin Shen",
            "G. Qiu"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "31",
        "reference_count": "40",
        "references": [
            "/paper/A-multi-level-feature-fusion-based-approach-to-Ding-Zhu/61766d9c5dcb3df66a18f99849e7e163fe48f6a0",
            "/paper/Multi-Classification-of-Breast-Cancer-Lesions-in-Ukwuoma-Hossain/34f95ca512c3601b2b0805bb742e74a46da46eea",
            "/paper/Region-based-feature-enhancement-using-channel-wise-Rashmi-Prasad/a406064020b1d3f43c404996414f8c7203078d45",
            "/paper/MTRRE-Net%3A-A-deep-learning-model-for-detection-of-Chattopadhyay-Dey/d46fb91a06364209d5caab6aa1bf06460cecefdc",
            "/paper/Compatible-domain-Transfer-Learning-for-Breast-with-Shamshiri-Krzy%C5%BCak/dd9343d271cf76759172d94e29ae8cc9e8c5c89e",
            "/paper/A-Deep-Reinforcement-Learning-Framework-for-Rapid-Zheng-Chen/26c141dffe58e39f0806bbf862a891d295f3cf47",
            "/paper/Classification-of-breast-tumors-by-using-a-novel-on-Kutluer-Solmaz/32299d9894a4b50aa61701c5af3ec6265ac6ee9e",
            "/paper/Mimicking-a-Pathologist%3A-Dual-Attention-Model-for-Raza-Awan/273cb794830874afd8a86aaf086ebbd890e58714",
            "/paper/BCHisto-Net%3A-Breast-histopathological-image-by-and-Rashmi-Prasad/f3ede91ae0f59c1a2ee2445fa5e126ff56a26e10",
            "/paper/Classification-of-breast-cancer-histology-images-Liu-Feng/4ac514936f6a1d3b2319f8460a04601d8bf36e84",
            "/paper/Look%2C-Investigate%2C-and-Classify%3A-A-Deep-Hybrid-for-Xu-Liu/759963fa85fd0f9be88db8e470bd9175d4deb0d6",
            "/paper/Sequential-Modeling-of-Deep-Features-for-Breast-Gupta-Bhavsar/3262598e2c4447d972bdb40a1e7c958c49caade5",
            "/paper/Partially-Independent-Framework-for-Breast-Cancer-Gupta-Bhavsar/bbe1268f17a01b8197a31c1fbc998ebaf8139058",
            "/paper/Attention-Residual-Learning-for-Skin-Lesion-Zhang-Xie/e485fbf1fab3e3910d301989ba98d6509627cfd2",
            "/paper/Breast-cancer-histopathological-image-using-Neural-Spanhol-Oliveira/3dbb621f1c35c659f3ce6efadad7aa16308fae13",
            "/paper/Multi-Class-Breast-Cancer-Classification-using-Deep-Nawaz-Sewissy/1a19f55bc7f5d3d90bd5cb239c0202b3428b6aa2",
            "/paper/Breast-cancer-histopathological-image-using-neural-Jiang-Chen/4e8a53da0e76b29b33a209f8df79902b2b1f77f7",
            "/paper/Breast-Cancer-Histopathological-Image-A-Deep-Jannesari-Habibzadeh/bba934f088c8520265682c75e2374e412b56e4d4",
            "/paper/Deep-features-for-breast-cancer-histopathological-Spanhol-Oliveira/9e338dd767703ab0a14edeb2a784798f4e169975",
            "/paper/Deep-Convolutional-Neural-Networks-for-Breast-Image-Rakhlin-Shvets/198d308169e7b95aced6e6b65918a548be20235d"
        ]
    },
    {
        "id": "b0c065cd43aa7280e766b5dcbcc7e26abce59330",
        "title": "SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation",
        "abstract": "Quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures, including FCN and DeconvNet. We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.",
        "publication_year": "2015",
        "authors": [
            "Vijay Badrinarayanan",
            "Alex Kendall",
            "R. Cipolla"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "11,315",
        "reference_count": "74",
        "references": [
            "/paper/Weakly-Supervised-Learning-of-a-Deep-Convolutional-Feng-Wang/8b5e2a1cf32e65f283fcee865e9168459348cfd4",
            "/paper/SegNetRes-CRF%3A-A-Deep-Convolutional-Encoder-Decoder-Junior-Medeiros/991673d4f9dd08893723549ff3ea866b2dc18047",
            "/paper/A-Novel-Upsampling-and-Context-Convolution-for-Sediqi-Lee/52eeaa7a84be61874defaec134be8ccf79852cd1",
            "/paper/Detailed-Dense-Inference-with-Convolutional-Neural-Ma-St%C3%BCckler/041ac91c85276f61bec3f0f3c42782e4f9a31f88",
            "/paper/Encoder-Decoder-with-Atrous-Separable-Convolution-Chen-Zhu/9217e28b2273eb3b26e4e9b7b498b4661e6e09f5",
            "/paper/Improvement-in-Accuracy-and-Speed-of-Image-Semantic-Zamanian/18dc45245754efdd9370e447cbf2e35c0412f88b",
            "/paper/Hybrid-connection-network-for-semantic-segmentation-Liang-Kamata/a98859e39b2c785aa9d8da455fdaade69d52be25",
            "/paper/Squeeze-SegNet%3A-a-new-fast-deep-convolutional-for-Nanfack-Elhassouny/a77401e30510f9775b1bf9a1cb6f010b0e5b571f",
            "/paper/A-novel-weight-initialization-with-adaptive-for-Haq-Khan/115f7fe31259332b2108f17371ddb1cdd9cd8568",
            "/paper/PPEDNet%3A-Pyramid-Pooling-Encoder-Decoder-Network-Tan-Liu/583e2dab221ad5d54c1b1cc0a9df4f1254bf3942",
            "/paper/SegNet%3A-A-Deep-Convolutional-Encoder-Decoder-for-Badrinarayanan-Handa/6f9f143ec602aac743e07d092165b708fa8f1473",
            "/paper/Fully-convolutional-networks-for-semantic-Shelhamer-Long/317aee7fc081f2b137a85c4f20129007fd8e717e",
            "/paper/Learning-Deconvolution-Network-for-Semantic-Noh-Hong/cf986bfe13a24d4739f95df3a856a3c6e4ed4c1c",
            "/paper/Recurrent-Convolutional-Neural-Networks-for-Scene-Pinheiro-Collobert/1a9658c0b7bea22075c0ea3c229b8c70c1790153",
            "/paper/Semantic-Image-Segmentation-with-Deep-Convolutional-Chen-Papandreou/39ad6c911f3351a3b390130a6e4265355b4d593b",
            "/paper/Conditional-Random-Fields-as-Recurrent-Neural-Zheng-Jayasumana/ca5c766b2d31a1f5ce8896a0a42b40a2bff9323a",
            "/paper/Learning-a-Deep-Convolutional-Network-for-Image-Dong-Loy/0504945cc2d03550fecb6ff02e637f9421107c25",
            "/paper/Efficient-Piecewise-Training-of-Deep-Structured-for-Lin-Shen/4cef5476f9da50c1a8fefdcb7114863966f61d67",
            "/paper/Semantic-Image-Segmentation-via-Deep-Parsing-Liu-Li/55a3b288b6885cd8dcc247212de9e82e713702db",
            "/paper/Bayesian-SegNet%3A-Model-Uncertainty-in-Deep-for-Kendall-Badrinarayanan/2daa90492b5509b33567eaf49360926f0e79f286"
        ]
    },
    {
        "id": "52cc2713b991dae071de3e2f4ac5d570c9e7dc15",
        "title": "Buckling and free vibration analyses of functionally graded graphene reinforced porous nanocomposite plates based on Chebyshev-Ritz method",
        "abstract": "Semantic Scholar extracted view of \"Buckling and free vibration analyses of functionally graded graphene reinforced porous nanocomposite plates based on Chebyshev-Ritz method\" by Jie Yang et al.",
        "publication_year": "2018",
        "authors": [
            "Jie Yang",
            "Da Chen",
            "S. Kitipornchai"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": "294",
        "reference_count": "57",
        "references": [
            "/paper/Free-vibration-and-buckling-behavior-of-graded-by-Anamagh-Bediz/1947358cadb7b89e9e38c1b6adb6315ba05e169e",
            "/paper/Free-vibration-and-stability-of-graphene-platelet-Twinkle-Pitchaimani/e2e858f557c3dd78f1c6705d64fc41eb95f3003b",
            "/paper/Buckling-analyses-of-functionally-graded-porous-the-Shahgholian-Safarpour/8d41bbca9b87885e0ee9b414d70811dc1bceaa1d",
            "/paper/Buckling-analyses-of-functionally-graded-porous-the-Shahgholian-Safarpour/3423dca3224c030b2c2d5e0a53ef7875452be4c9",
            "/paper/Torsional-buckling-analyses-of-functionally-graded-Shahgholian-Ghahfarokhi-Safarpour/3d64f3b8a49965afa4035a2d9ff9cec6ef730b3b",
            "/paper/Nonlinear-free-vibration-of-functionally-graded-on-Gao-Gao/2af94572ed20523f3c3d1157c26f33452bd836b2",
            "/paper/Buckling-analyses-of-FG-porous-nanocomposite-shells-Shahgholian-Ghahfarokhi-Rahimi/5ccb69ea3a9dd57774a368ac1f114903d4fe2ef5",
            "/paper/Stability-and-dynamic-behavior-of-porous-FGM-beam%3A-Priyanka-Twinkle/5ce266c7b0e33cf0c8029f6c890678232949821f",
            "/paper/Nonlinear-vibration-and-dynamic-buckling-analyses-Li-Wu/966239f713e2d05702e0de1731d71ea91e616a9d",
            "/paper/On-vibration-and-stability-analysis-of-porous-by-Saidi-Bahaadini/3b2bb9fc0443b7c7febcd60e3a89991b7ee2b367",
            "/paper/Free-vibration-and-elastic-buckling-of-functionally-Kitipornchai-Chen/017928ae80bda5891199f4be6a8efa1e8f7b9c1e",
            "/paper/Nonlinear-vibration-and-postbuckling-of-graded-Chen-Yang/f173d1a1186d6273457145d2564507acb88aab8b",
            "/paper/Free-and-forced-vibrations-of-functionally-graded-Song-Kitipornchai/baebbcec5c8157cc7d4bed9e6e42d932ba5ec1e5",
            "/paper/3D-thermo-mechanical-bending-solution-of-graded-and-Yang-Kitipornchai/484341f6451c043df9c26c8ec395af9fb700e340",
            "/paper/Parametric-instability-of-thermo-mechanically-Wu-Yang/49b35661f1753e269f1a95042e989998bb5c5de7",
            "/paper/Free-vibration-analysis-of-functionally-graded-the-Lei-Lei/0d602982142e00a9332795a42c4b7f9698fd2ee4",
            "/paper/Static-and-free-vibration-analyses-of-carbon-plates-Zhu-Lei/0b215e34a2bb50cc235575ce35a8aaa769bcebce",
            "/paper/An-element-free-IMLS-Ritz-framework-for-buckling-of-Zhang-Lei/8206afe1b7960403fc047e217355b08212f544c8",
            "/paper/Bending-and-buckling-analyses-of-functionally-with-Song-Yang/2bea415c47f69de83286e830ef8ae277e0f31b64",
            "/paper/Buckling-and-postbuckling-of-functionally-graded-Yang-Wu/3d2e8ad05aefae58a8c9cbfc8bc387e3ec53c898"
        ]
    },
    {
        "id": "8db0bf0fa406254d4cd57eb413443a0b96bd12b8",
        "title": "NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video",
        "abstract": "To the best of the knowledge, this is the first learning-based system that is able to reconstruct dense coherent 3D geometry in real-time and outperforms state-of-the-art methods in terms of both ac-curacy and speed. We present a novel framework named NeuralRecon for real-time 3D scene reconstruction from a monocular video. Unlike previous methods that estimate single-view depth maps separately on each key-frame and fuse them later, we propose to directly reconstruct local surfaces represented as sparse TSDF volumes for each video fragment sequentially by a neural network. A learning-based TSDF fusion module based on gated recurrent units is used to guide the network to fuse features from previous fragments. This de-sign allows the network to capture local smoothness prior and global shape prior of 3D surfaces when sequentially reconstructing the surfaces, resulting in accurate, coherent, and real-time surface reconstruction. The experiments on ScanNet and 7-Scenes datasets show that our system outperforms state-of-the-art methods in terms of both ac-curacy and speed. To the best of our knowledge, this is the first learning-based system that is able to reconstruct dense coherent 3D geometry in real-time. Code is available at the project page: https://zju3dv.github.io/neuralrecon/.",
        "publication_year": "2021",
        "authors": [
            "Jiaming Sun",
            "Yiming Xie",
            "Linghao Chen",
            "Xiaowei Zhou",
            "H. Bao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "110",
        "reference_count": "57",
        "references": [
            "/paper/PlanarRecon%3A-Realtime-3D-Plane-Detection-and-from-Xie-Gadelha/b38bbf2b21aa7a25a4918b1dbb8dc4514e617399",
            "/paper/Cross-Dimensional-Refined-Learning-for-Real-Time-3D-Hong-Yue/a3272f80526e0ea32f5985e640fb5a388fbc02e6",
            "/paper/MonoNeuralFusion%3A-Online-Monocular-Neural-3D-with-Zou-Huang/e605271aefa33cdf4c77e0ea801cd8e2410fceb0",
            "/paper/VolumeFusion%3A-Deep-Depth-Fusion-for-3D-Scene-Choe-Im/0ba9140dad37b1b3d8cfcdb6d54389edd701c58d",
            "/paper/3DVNet%3A-Multi-View-Depth-Prediction-and-Volumetric-Rich-Stier/534ff47a5bd665ae38398053a90f7295a0e0b30b",
            "/paper/SimpleRecon%3A-3D-Reconstruction-Without-3D-Sayed-Gibson/caf96058774d96dd08df799319ef0d5430121518",
            "/paper/SST%3A-Real-time-End-to-end-Monocular-3D-via-Sparse-Zhang-Lou/5f55592ce1f12c7e405d76faf062825094f6fbaa",
            "/paper/Neural-3D-Scene-Reconstruction-with-the-Assumption-Guo-Peng/283a4e3e45f65d825c5640d582d30aa4d6dddf7e",
            "/paper/CVRecon%3A-Rethinking-3D-Geometric-Feature-Learning-Feng-Yang/c736a949dac2a67219788ab2f48b02068865872a",
            "/paper/FineRecon%3A-Depth-aware-Feed-forward-Network-for-3D-Stier-Ranjan/e00fe237ff1d0daa918a0a203c94b70e9d14b94b",
            "/paper/RoutedFusion%3A-Learning-Real-Time-Depth-Map-Fusion-Weder-Sch%C3%B6nberger/9a99e0bf8f8192b2e144d28f46ef1a415646ad10",
            "/paper/Consistent-video-depth-estimation-Luo-Huang/43550e50110d6a500ac827e30f54f391aae5f8c7",
            "/paper/MVDepthNet%3A-Real-Time-Multiview-Depth-Estimation-Wang-Shen/c1c8980e423d63942d4ffc31dbffbd3aeb9213a7",
            "/paper/3D-R2N2%3A-A-Unified-Approach-for-Single-and-3D-Choy-Xu/a9032d632e04399e46efe0668d8b19993aff6dbc",
            "/paper/Multiview-Neural-Surface-Reconstruction-by-Geometry-Yariv-Kasten/2854af340fffab4f6de0bcdcf21d1d33ece08ae8",
            "/paper/Atlas%3A-End-to-End-3D-Scene-Reconstruction-from-Murez-As/35e2056e29d5295b2669fe696ae0c27007c94625",
            "/paper/Occupancy-Networks%3A-Learning-3D-Reconstruction-in-Mescheder-Oechsle/2e689bdce24cf3644432505ce2783f03a1445ed2",
            "/paper/MVSNet%3A-Depth-Inference-for-Unstructured-Multi-view-Yao-Luo/87ca28235555f7e70cf1edc2a63cda4aef7fee42",
            "/paper/DeMoN%3A-Depth-and-Motion-Network-for-Learning-Stereo-Ummenhofer-Zhou/19842f051ec5aba00cabf58f84ce8db4894d1be6",
            "/paper/Learning-a-Multi-View-Stereo-Machine-Kar-H%C3%A4ne/61ce67533d2dd6605c907146658ccdbc4778a5d8"
        ]
    },
    {
        "id": "a964903746a1067984fb1881438f7e5cc6468a3d",
        "title": "An Optimized Illumination Normalization Method for Face Recognition",
        "abstract": "This paper presents a new preprocessing approach that uses custom filters obtained through an optimization procedure striving for most suitable preprocessing filters for the selected feature extractor and distance measure. Differences in illumination conditions cause significant challenges for any 2-D face recognition algorithm. One of the methods to counter these effects is image preprocessing before feature extraction. In this paper we present a new preprocessing approach that uses custom filters obtained through an optimization procedure striving for most suitable preprocessing filters for the selected feature extractor and distance measure. We experiment with it using Local Binary Pattern texture features and X2 histogram distance metric. Results are provided for Face Recognition Grand Challenge (FRGC) 1.0.4 dataset.",
        "publication_year": "2008",
        "authors": [
            "J. Holappa",
            "T. Ahonen",
            "M. Pietikainen"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "26",
        "reference_count": "27",
        "references": [
            "/paper/Illumination-Invariant-Face-Recognition-with-Filter-Li-Long/337eb9cea869ba22b7d684b34642fb6d90e32997",
            "/paper/Illumination-Processing-in-Face-Recognition-Chen-Liu/4e50fd221d06d2b114be1bc33c7ac4cfd4f71e4d",
            "/paper/Study-on-Age-Invariant-Face-Recognition-System-Singh-Medhi/52966dbdb8e2d4a560e3848b13b0e87267c16226",
            "/paper/Recognition-of-Faces-Using-Discriminative-Features-Bhele-Mankar/e724c9a69613bef36f67ae7ed6850b1942918804",
            "/paper/Next-Generation-Face-Recognition-System-under-Zeenathunisa/82b01d6a56737626a9796c78d09ca98df90518c0",
            "/paper/Dual-Transform-based-Feature-Extraction-for-Face-Ramesha-Raja/26e0174a8bacb05492bc2875e95440eb1ff324bf",
            "/paper/Face-and-texture-image-analysis-with-quantized-Ahonen/9cad729fb861cd853517cf5ae08b327cdeaf2c1b",
            "/paper/A-biometric-approach-towards-recognizing-face-in-Zeenathunisa-Jaya/0237c6778f1d72d651676a537b9e3caf6653f3f1",
            "/paper/Face-Identification-Using-Large-Feature-Sets-Schwartz-Guo/a14ae81609d09fed217aa12a4df9466553db4859",
            "/paper/Illumination-Processing-in-Face-Recognition-187-in-Li-Wang/ccdea57234d38c7831f1e9231efcb6352c801c55",
            "/paper/An-Image-Preprocessing-Algorithm-for-Illumination-Gross-Brajovic/07c90e85ac0f74b977babe245dea0f0abcf177e3",
            "/paper/Enhanced-Local-Texture-Feature-Sets-for-Face-Under-Tan-Triggs/a99e76f6debc3eb6b8f654321a6f76200bc1086d",
            "/paper/Face-Description-with-Local-Binary-Patterns%3A-to-Ahonen-Hadid/e7c4665ce36a53484f8a7b7dfa821a9f6273eab4",
            "/paper/A-comparison-of-photometric-normalisation-for-face-Short-Kittler/89f4bcbfeb29966ab969682eae235066a89fc151",
            "/paper/Face-recognition-under-varying-lighting-conditions-Wang-Li/ecac3da2ff8bc2ba55981467f7fdea9de80e2092",
            "/paper/Boosting-Local-Binary-Pattern-(LBP)-Based-Face-Zhang-Huang/52818433a95c7dc357a6348b458255e06c1b00b2",
            "/paper/Face-recognition-in-the-presence-of-multiple-Aggarwal-Chellappa/7d90d5d82e92414a6df6e72c97696500a67799be",
            "/paper/Multispectral-Local-Binary-Pattern-Histogram-for-Chan-Kittler/70fa17afd5a2f5953032ed1ecb66ca0c3e11b150",
            "/paper/Face-Recognition%3A-The-Problem-of-Compensating-for-Adini-Moses/59e9ef8b61182acace9e37f41f9c2a03db69c15b",
            "/paper/Total-variation-models-for-variable-lighting-face-Chen-Yin/84ca8611f9051789aefeaedaaeb8a2188f8e53a7"
        ]
    },
    {
        "id": "72564a69bf339ff1d16a639c86a764db2321caab",
        "title": "Focal Loss for Dense Object Detection",
        "abstract": "This paper proposes to address the extreme foreground-background class imbalance encountered during training of dense detectors by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples, and develops a novel Focal Loss, which focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.",
        "publication_year": "2017",
        "authors": [
            "Tsung-Yi Lin",
            "Priya Goyal",
            "Ross B. Girshick",
            "Kaiming He",
            "Piotr Doll\u00e1r"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "14,227",
        "reference_count": "41",
        "references": [
            "/paper/Towards-General-Purpose-Object-Detection%3A-Deep-Grid-Tesema-Bourennane/3a24360c0566bfc3cfe301a91aba8a47f13b2756",
            "/paper/Cascade-R-CNN%3A-Delving-Into-High-Quality-Object-Cai-Vasconcelos/04957e40d47ca89d38653e97f728883c0ad26e5d",
            "/paper/Learning-Gaussian-Maps-for-Dense-Object-Detection-Kant/86fa0d1a5450556b3de513d0b5512271274c7656",
            "/paper/Multiple-step-Sampling-for-Dense-Object-Detection-Deng-Yang/f01a1ebe4c673f37f7d245023e0d23ed112c4849",
            "/paper/Soft-Sampling-for-Robust-Object-Detection-Wu-Bodla/af12144e6f113c5de20c74eff5c179a97065eabe",
            "/paper/DATNet%3A-Dense-Auxiliary-Tasks-for-Object-Detection-Levinshtein-Sereshkeh/6b1eaa72d7533e52cc6646a7e02f21a191fbee8f",
            "/paper/DetNet%3A-A-Backbone-network-for-Object-Detection-Li-Peng/af77faec0f71d934013c1f17b368edc9e845235f",
            "/paper/Optimisation-of-Deep-Learning-Small-Object-with-Mohamed-Sirlantzis/3b7ed3947bb36c75d934b99c358d998d67791eeb",
            "/paper/Generalized-Focal-Loss%3A-Towards-Efficient-Learning-Li-Lv/4b457ca003b4d05b2fd6af22c283fe8b4e211a2f",
            "/paper/Near-duplicated-Loss-for-Accurate-Object-Liu-Yang/cedfe3e7cb81d12b8550d9ce39faaba4caa3ae12",
            "/paper/Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue/2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "/paper/R-FCN%3A-Object-Detection-via-Region-based-Fully-Dai-Li/b724c3f7ff395235b62537203ddeb710f0eb27bb",
            "/paper/Learning-to-Segment-Object-Candidates-Pinheiro-Collobert/6b8d0df903496699e52b4daee5d1815b7b784cf7",
            "/paper/You-Only-Look-Once%3A-Unified%2C-Real-Time-Object-Redmon-Divvala/f8e79ac0ea341056ef20f2616628b3e964764cfd",
            "/paper/Training-Region-Based-Object-Detectors-with-Online-Shrivastava-Gupta/63333669bcf694aba2e1928f6060ab1d6a5161fe",
            "/paper/SSD%3A-Single-Shot-MultiBox-Detector-Liu-Anguelov/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
            "/paper/What-Makes-for-Effective-Detection-Proposals-Hosang-Benenson/6c016579af5becc230fb9efc1f885f2afa65a46e",
            "/paper/Faster-R-CNN%3A-Towards-Real-Time-Object-Detection-Ren-He/424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "/paper/Feature-Pyramid-Networks-for-Object-Detection-Lin-Doll%C3%A1r/b9b4e05faa194e5022edd9eb9dd07e3d675c2b36",
            "/paper/Scalable-Object-Detection-Using-Deep-Neural-Erhan-Szegedy/67fc0ec1d26f334b05fe66d2b7e0767b60fb73b6"
        ]
    },
    {
        "id": "8c04f169203f9e55056a6f7f956695babe622a38",
        "title": "Distinctive Image Features from Scale-Invariant Keypoints",
        "abstract": "This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene and can robustly identify objects among clutter and occlusion while achieving near real-time performance. This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.",
        "publication_year": "2004",
        "authors": [
            "D. Lowe"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "15,181",
        "reference_count": "52",
        "references": [
            "/paper/Evaluation-of-Clustering-Configurations-for-Object-Fern%C3%A1ndez-Robles-Castej%C3%B3n-Limas/10a19896e6c524535a75b4c940a4e0cb6692920c",
            "/paper/Relative-scale-method-to-locate-an-object-in-Islam-Sluzek/56dfa546aee1aed1fcc08daaf16191451f03a140",
            "/paper/Geometric-features-extraction-Farag/44b71b68f9e4bc161427603f02b14b70e743ad25",
            "/paper/Improved-SIFT-Features-Matching-for-Object-Alhwarin-Wang/73927a98036e08523f962da2336145b007ddf7f5",
            "/paper/Incorporating-Background-Invariance-into-Object-Stein-Hebert/2f0fbfdc9a72e4b475234b551a6054d8cf26b3c2",
            "/paper/A-Method-to-Enhance-Homogeneous-Distribution-of-for-G%C3%BClmez-Demirtas/b790473eb64f02adaea83e0b4ebd7a6717d70e56",
            "/paper/Shape-Signature-Matching-for-Object-Identification-Giannarou-Stathaki/8d389d1407efd4c33254b2b7d1bc35c195646b9d",
            "/paper/SMD%3A-A-Locally-Stable-Monotonic-Change-Invariant-Gupta-Mittal/75b1d8339085ab03f45c0316b976755b6c5da9e9",
            "/paper/Scene-analysis-using-scale-invariant-feature-and-Buckles-Guturu/dd9582b1d0542b09ecea10307c3d80284da1dcdd",
            "/paper/On-the-Repeatability-and-Quality-of-Keypoints-for-Mian-Bennamoun/bd47b1f3e2839af705187de7fe7d51a514997765",
            "/paper/Object-recognition-from-local-scale-invariant-Lowe/f9f836d28f52ad260213d32224a6d227f8e8849a",
            "/paper/Shape-recognition-with-edge-based-features-Mikolajczyk-Zisserman/5e80772f40e8ef924727c6c24168cadc3be0b856",
            "/paper/Local-feature-view-clustering-for-3D-object-Lowe/9188b661f2ae65a080676617cc83b7d09773c59f",
            "/paper/Invariant-Features-from-Interest-Point-Groups-Brown-Lowe/45cb5262b1fb9f149e8f4171e0b1e52d748e062d",
            "/paper/Reliable-feature-matching-across-widely-separated-Baumberg/67f693427d956c0dbc822e7f3452aee8ca36204b",
            "/paper/Recognition-Using-Region-Correspondences-Basri-Jacobs/2a7c89fcfc49c56ec3557dc507f0477676a2dd6a",
            "/paper/Probabilistic-Models-of-Appearance-for-3-D-Object-Pope-Lowe/6d1e696747a5452364579bea583196ed8bc2b254",
            "/paper/Object-class-recognition-by-unsupervised-learning-Fergus-Perona/62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5",
            "/paper/Robust-wide-baseline-stereo-from-maximally-stable-Matas-Chum/9d5ea177c7fcaf88ec6f56cbeb3e9b74c08e98a3",
            "/paper/Phase-Based-Local-Features-Carneiro-Jepson/f691349c928b1d65546f243075d5e8995b82f958"
        ]
    },
    {
        "id": "72ec17c5be2b6200a92319708dbf6d1ad89544d1",
        "title": "Making Curiosity Explicit in Vision-based RL",
        "abstract": "This work presents an approach to improve the sample diversity and enhances the exploration capability of the RL algorithms by taking advantage of the SRL setup and shows that the presented approach outperforms the baseline for all tested environments. Vision-based reinforcement learning (RL) is a promising technique to solve control tasks involving images as the main observation. State-of-the-art RL algorithms still struggle in terms of sample efficiency, especially when using image observations. This has led to an increased attention on integrating state representation learning (SRL) techniques into the RL pipeline. Work in this field demonstrates a substantial improvement in sample efficiency among other benefits. However, to take full advantage of this paradigm, the quality of samples used for training plays a crucial role. More importantly, the diversity of these samples could affect the sample efficiency of vision-based RL, but also its generalization capability. In this work, we present an approach to improve the sample diversity. Our method enhances the exploration capability of the RL algorithms by taking advantage of the SRL setup. Our experiments show that the presented approach outperforms the baseline for all tested environments. These results are most apparent for environments where the baseline method struggles. Even in simple environments, our method stabilizes the training, reduces the reward variance and boosts sample efficiency.",
        "publication_year": "2021",
        "authors": [
            "Elie Aljalbout",
            "Maximilian Ulmer",
            "Rudolph Triebel"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "26",
        "references": [
            "/paper/Seeking-Visual-Discomfort%3A-Curiosity-driven-for-Aljalbout-Ulmer/e55840992c735daa4b18fe80af7ab19d94bbbaad",
            "/paper/Dual-Arm-Adversarial-Robot-Learning-Aljalbout/f87a4a2649b55998af55e045f3f3c2519dea92c2",
            "/paper/Improving-Sample-Efficiency-in-Model-Free-Learning-Yarats-Zhang/88dd6594c9ddd4c4bb7f9b407b162e283907f4f3",
            "/paper/DARLA%3A-Improving-Zero-Shot-Transfer-in-Learning-Higgins-Pal/a2141a5ec0c65ea0a9861ae562f4c9fb8020d197",
            "/paper/Decoupling-Representation-Learning-from-Learning-Stooke-Lee/17985b57240bfaea02a6098a7a34e71e780180eb",
            "/paper/Deep-auto-encoder-neural-networks-in-reinforcement-Lange-Riedmiller/36086ff255207cc1adb818c4d0cd62287d437d38",
            "/paper/State-Entropy-Maximization-with-Random-Encoders-for-Seo-Chen/0292d2f80b52bda7cb5c2432ea35bc3af6822a8a",
            "/paper/Curiosity-Driven-Exploration-by-Self-Supervised-Pathak-Agrawal/225ab689f41cef1dc18237ef5dab059a49950abf",
            "/paper/Learning-Latent-Dynamics-for-Planning-from-Pixels-Hafner-Lillicrap/fea3e63c97c7292dc6fbcb3ffe7131eb54053986",
            "/paper/CURL%3A-Contrastive-Unsupervised-Representations-for-Srinivas-Laskin/79e14a09ff070e06ab9df598ccd885b929164ef9",
            "/paper/VIME%3A-Variational-Information-Maximizing-Houthooft-Chen/317cd4522b1f4a6f889743578143bb8823623f8b",
            "/paper/Unsupervised-Learning-of-Visual-Representations-by-Noroozi-Favaro/2ec8f7e0257a07d3914322b36072d1bbcd58a1e0"
        ]
    },
    {
        "id": "f5c5c6666729e0563b6bdde24f76d369160dc89a",
        "title": "Hierarchically Structured Network With Attention Convolution and Embedding Propagation for Imbalanced Few-Shot Learning",
        "abstract": "A novel few-shot classification model that uses learning balance variables to decide how much to learn from the imbalance dataset, which dynamically generates the convolution kernel based on each input is proposed. Generally, a few-shot distribution shift will lead to a poor generalization. Furthermore, while the number of instances of each class in the real world may significantly different, the existing few-shot classification methods are based on the assumption that the number of samples in each class is equal, which causes the trained classifier invalid. Moreover, through ResNet and WRN (Wide Residual Network) have achieved great success in the image processing field, the depth and width of CNNs constrain the conventional convolution layer performance. Thus, to overcome the above problems, the model of this paper proposes a novel few-shot classification model that uses learning balance variables to decide how much to learn from the imbalance dataset, which dynamically generates the convolution kernel based on each input. In our model, to extend the decision boundaries and enhance the class representations, this paper uses embedding propagation as a regularizer for manifold smoothing. Manifold smoothing can effectively solve the above problems of transductive learning. The interpolations between neural network features based on similarity graphs are used by embedding propagation. Experiments show that embedding propagation can produce a better embedding manifold and our model in standard few-shot datasets, such as miniImagenet, tieredImagenet, CUB has state-of-the-art results. It significantly outperforms the existing few-shot approaches, which consistently improves the accuracy of the models by about 11%.",
        "publication_year": "2021",
        "authors": [
            "Wei Xiong",
            "Yu Gong",
            "Weihua Niu",
            "Ruonan Wang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "45",
        "references": [
            "/paper/Generating-Classification-Weights-With-GNN-for-Gidaris-Komodakis/4732ef442a5d0f2ec77600ddd44aaa288a043ace",
            "/paper/Multi-Scale-Decision-Network-With-Feature-Fusion-Wang-Ma/11cebac22ee19cdf28dad8128a201351bcee6844",
            "/paper/Robust-Compare-Network-for-Few-Shot-Learning-Yang-Li/e4c924590f026f6ac8768673649a3cf3e20647dc",
            "/paper/Learning-to-Propagate-Labels%3A-Transductive-Network-Liu-Lee/a3d7257cf0ba5c501d2f6b8ddab931bcd588dfbf",
            "/paper/A-Closer-Look-at-Few-shot-Classification-Chen-Liu/9d5ec23154fb278a765f47ba5ee5150bd441d0de",
            "/paper/Learning-to-Compare%3A-Relation-Network-for-Few-Shot-Sung-Yang/bfe284e4338e62f0a61bb33398353efd687f206f",
            "/paper/Charting-the-Right-Manifold%3A-Manifold-Mixup-for-Mangla-Singh/ccbe37b5c4d8a5687153df6fedbb44d6f1c46d7d",
            "/paper/Discriminative-k-shot-learning-using-probabilistic-Bauer-Rojas-Carulla/fd7cbaec285fd9fae70a71f30be7833855d01035",
            "/paper/Squeeze-and-Excitation-Networks-Hu-Shen/df67d46e78aae0d2fccfb6212d101a342259c01b",
            "/paper/Exploiting-Unsupervised-Inputs-for-Accurate-Hu-Gripon/e72273f31b31d3ba369ed3e23a46a2b28949f5b2"
        ]
    },
    {
        "id": "620fe6c786d15efca7f553ad70f295e2b693b391",
        "title": "Unsupervised Video Summarization with Adversarial LSTM Networks",
        "abstract": "This paper addresses the problem of unsupervised video summarization, formulated as selecting a sparse subset of video frames that optimally represent the input video, with a novel generative adversarial framework. This paper addresses the problem of unsupervised video summarization, formulated as selecting a sparse subset of video frames that optimally represent the input video. Our key idea is to learn a deep summarizer network to minimize distance between training videos and a distribution of their summarizations, in an unsupervised way. Such a summarizer can then be applied on a new video for estimating its optimal summarization. For learning, we specify a novel generative adversarial framework, consisting of the summarizer and discriminator. The summarizer is the autoencoder long short-term memory network (LSTM) aimed at, first, selecting video frames, and then decoding the obtained summarization for reconstructing the input video. The discriminator is another LSTM aimed at distinguishing between the original video and its reconstruction from the summarizer. The summarizer LSTM is cast as an adversary of the discriminator, i.e., trained so as to maximally confuse the discriminator. This learning is also regularized for sparsity. Evaluation on four benchmark datasets, consisting of videos showing diverse events in first-and third-person views, demonstrates our competitive performance in comparison to fully supervised state-of-the-art approaches.",
        "publication_year": "2017",
        "authors": [
            "Behrooz Mahasseni",
            "Michael Lam",
            "S. Todorovic"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "369",
        "reference_count": "43",
        "references": [
            "/paper/Unsupervised-Video-Summarization-with-a-Attentive-Liang-Lv/4881c8c48881f28b7d29226f1a54f38b71481048",
            "/paper/Unsupervised-Video-Summarization-With-Adversarial-Yuan-Tay/f5ddbb4c316cab504edbe2306cb8753ec6efaa6f",
            "/paper/Adversarial-Unsupervised-Video-Summarization-With-Kaseris-Mademlis/15c4f8cf4e9323f674692819374002e2055a4c40",
            "/paper/Unsupervised-Video-Summarization-with-Attentive-He-Hua/81b4f76a767b4393b2db876e3b7fd85d3f7cd820",
            "/paper/Reinforcement-Learning-Versus-Generative-Networks-Aboelenien-Salem/fcfca33403129facbc5126f16b08bff79bd9a181",
            "/paper/Cycle-SUM%3A-Cycle-consistent-Adversarial-LSTM-for-Yuan-Tay/cef078462df7ca5f022fd654ae7d5764e7936eb0",
            "/paper/Attentive-and-Adversarial-Learning-for-Video-Fu-Tai/b0e6921c84263c5c961cd286e3145ca5eda352e0",
            "/paper/Recurrent-generative-adversarial-networks-for-WCE-Lan-Ye/8c5823fef044dfd287672a7ab639e92bb9d3657d",
            "/paper/ERA%3A-Entity-Relationship-Aware-Video-Summarization-Wu-Lin/2a375ed6e53a49d89b2d2a5c6dcb941b72dd7692",
            "/paper/Video-Summarization-With-Attention-Based-Networks-Ji-Xiong/88a8baa1be5292e62622f1cb8e627fbf759bf741",
            "/paper/Video-Summarization-with-Long-Short-Term-Memory-Zhang-Chao/1dbc12e54ceb70f2022f956aa0a46e2706e99962",
            "/paper/Diverse-Sequential-Subset-Selection-for-Supervised-Gong-Chao/25da1b119ba1e0bb602be6ce8492d1e33dbac9ff",
            "/paper/Video-summarization-by-learning-submodular-mixtures-Gygli-Grabner/cfcb9bcc1e8b4d3451578398aeb37f0fa5614632",
            "/paper/Unsupervised-Learning-of-Video-Representations-Srivastava-Mansimov/829510ad6f975c939d589eeb01a3cf6fc6c8ce4d",
            "/paper/Quasi-Real-Time-Summarization-for-Consumer-Videos-Zhao-Xing/91ee61105749d187f980a1995ace548516969a83",
            "/paper/Unsupervised-Extraction-of-Video-Highlights-via-Yang-Wang/faafe2a76dbb9a5a1468b1a02b1f0f09ced8587e",
            "/paper/Deep-multi-scale-video-prediction-beyond-mean-error-Mathieu-Couprie/17fa1c2a24ba8f731c8b21f1244463bc4b465681",
            "/paper/Summary-Transfer%3A-Exemplar-Based-Subset-Selection-Zhang-Chao/e3db6c8086082acd55f6c95070ad309ecb834517",
            "/paper/Generating-Videos-with-Scene-Dynamics-Vondrick-Pirsiavash/ee091ccf24c4f053c5c3dfbefe4a7975ed3447c1",
            "/paper/TVSum%3A-Summarizing-web-videos-using-titles-Song-Vallmitjana/cbf89cb4e107fb59e119ae619bcfe48e1964e033"
        ]
    },
    {
        "id": "65c9b4b1d49f46b3f8f64a5f617acfc14f85d031",
        "title": "High-Speed Tracking with Kernelized Correlation Filters",
        "abstract": "A new kernelized correlation filter is derived, that unlike other kernel algorithms has the exact same complexity as its linear counterpart, which is called dual correlation filter (DCF), which outperform top-ranking trackers such as Struck or TLD on a 50 videos benchmark, despite being implemented in a few lines of code. The core component of most modern trackers is a discriminative classifier, tasked with distinguishing between the target and the surrounding environment. To cope with natural image changes, this classifier is typically trained with translated and scaled sample patches. Such sets of samples are riddled with redundancies\u2014any overlapping pixels are constrained to be the same. Based on this simple observation, we propose an analytic model for datasets of thousands of translated patches. By showing that the resulting data matrix is circulant, we can diagonalize it with the discrete Fourier transform, reducing both storage and computation by several orders of magnitude. Interestingly, for linear regression our formulation is equivalent to a correlation filter, used by some of the fastest competitive trackers. For kernel regression, however, we derive a new kernelized correlation filter (KCF), that unlike other kernel algorithms has the exact same complexity as its linear counterpart. Building on it, we also propose a fast multi-channel extension of linear correlation filters, via a linear kernel, which we call dual correlation filter (DCF). Both KCF and DCF outperform top-ranking trackers such as Struck or TLD on a 50 videos benchmark, despite running at hundreds of frames-per-second, and being implemented in a few lines of code (Algorithm 1). To encourage further developments, our tracking framework was made open-source.",
        "publication_year": "2014",
        "authors": [
            "Jo\u00e3o F. Henriques",
            "Rui Caseiro",
            "P. Martins",
            "Jorge Batista"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4,603",
        "reference_count": "39",
        "references": [
            "/paper/Robust-Visual-Tracking-via-Constrained-Multi-Kernel-Huang-Xu/cad2fb5036f2de25defd13158d05349a382152f1",
            "/paper/Real-Time-Scalable-Visual-Tracking-via-Quadrangle-Ding-Chen/80efbeca0f3250fe99a899e1122583eeacd3ddc2",
            "/paper/Multi-scale-kernel-correlation-filter-for-visual-Chen-Ding/ff2d8945d5e155ab6430e146baf25a111033bfe1",
            "/paper/Fast-Visual-Tracking-With-Robustifying-Kernelized-Liu-Hu/8046cfa1c834f966cfc88aa3dca56fb828f241d0",
            "/paper/A-Fast-Adaptive-Multi-Scale-Kernel-Correlation-for-Zheng-Zhang/6592f38b9c533ab0ff3819efeb8ce759978e556a",
            "/paper/Fast-Kernelized-Correlation-Filter-without-Boundary-Tang-Zheng/49824b83ac8be0bc3f52779417b10bb034281412",
            "/paper/Target-Response-Adaptation-for-Correlation-Filter-Bibi-Mueller/f5eae0841111c814c977dd691c072a3aa57e6ad8",
            "/paper/Asymmetric-discriminative-correlation-filters-for-Li-Jiang/e2694307c375d316049a5b78999b928a37f94018",
            "/paper/High-Speed-Tracking-With-A-Fourier-Domain-Filter-Guan-Li/f25ffdb4098270fe2026a44dd8d466a8df8ccace",
            "/paper/A-compressed-multiple-feature-and-adaptive-scale-Wang-Cai/df369a0a5f679b6c378f9bcf4320e1eb9ff2cf57",
            "/paper/Exploiting-the-Circulant-Structure-of-with-Kernels-Henriques-Caseiro/5b4e50860d61095bb5fb65eaa367b131923917be",
            "/paper/Real-Time-Compressive-Tracking-Zhang-Zhang/9d57723b4908397654fb1846d37db403d8b2b56a",
            "/paper/Visual-object-tracking-using-adaptive-correlation-Bolme-Beveridge/70c3c9b9a40ca55264e454586dca2a6cf416f6e0",
            "/paper/Multi-channel-Correlation-Filters-Galoogahi-Sim/21765df4c0224afcc25eb780bef654cbe6f0bc3a",
            "/paper/Distribution-fields-for-tracking-Sevilla-Lara-Learned-Miller/20402c2004cfb2e9caa433d9ca2fb61ff30b302e",
            "/paper/Fast-FFT-based-distortion-invariant-kernel-filters-Patnaik-Casasent/06e78273b4dbfe6b7ff56df16892782f01fac5bb",
            "/paper/Visual-Tracking%3A-An-Experimental-Survey-Smeulders-Chu/eda3368a5198ca55768b07b6f5667aea28baf2cd",
            "/paper/Fast-Feature-Pyramids-for-Object-Detection-Doll%C3%A1r-Appel/84e0d68e41788644c78cfdc3f4ac3cbea7854a5c",
            "/paper/Robust-Object-Tracking-with-Online-Multiple-Babenko-Yang/201631fbc3f7d7cb2c1ddfaf82278cad5e44f2f9",
            "/paper/Analysis-of-kernel-distortion-invariant-filters-Casasent-Patnaik/dc1e53f2d6aa320e4142348292a05119cbf971dd"
        ]
    },
    {
        "id": "bbc97484fe622da1e07cf5816ecd8e6b32235d78",
        "title": "Visual tracking via dynamic saliency discriminative correlation filter",
        "abstract": "A novel dynamic saliency discriminative correlation filter for visual tracking that effectively highlights the target by further increasing the number of positive samples to alleviate the boundary effect and applies a novel update approach to prevent filter model degradation. The discriminative correlation filter (DCF) is one of the crucial visual tracking methods, and it has outstanding performance. Nevertheless, DCF-based methods have an unavoidable boundary effect, which results in poor tracking performance in an abrupt scene, such as fast motion or deformation. To address this problem, we propose a novel dynamic saliency discriminative correlation filter for visual tracking. In our approach, a response guided saliency map is constructed to introduce saliency information into the filter. The method effectively highlights the target by further increasing the number of positive samples to alleviate the boundary effect. We also investigate an effective multifeature integration method to extract the target feature by employing the Felzenszwalb Histograms of Oriented Gradients (fHOG) from each color space. Finally, we apply a novel update approach to prevent filter model degradation, which uses a temporal regularization term to update the filter model. Extensive experiments on the standard OTB-2015 benchmark validate that our approach achieves competitive performance compared to other state-of-the-art trackers. Moreover, we conducted an ablation study to evaluate the effectiveness of the components in our tracker.",
        "publication_year": "2021",
        "authors": [
            "Lina Gao",
            "Bing Liu",
            "P. Fu",
            "Mingzhu Xu",
            "Junbao Li"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "17",
        "reference_count": "59",
        "references": [
            "/paper/Learning-discriminative-correlation-filters-via-for-Ma-Zhao/21375e2f1dec58df23d4b721b9cb2f31564a76fd",
            "/paper/SiamET%3A-a-Siamese-based-visual-tracking-network-Zhou-Zhang/cf1559f1dc37fb77705efee31f01b2dda49e5ced",
            "/paper/RHL-track%3A-visual-object-tracking-based-on-Meng-Gong/be2c3cb1d4412c71a3d7acff15e886855b211961",
            "/paper/A-robust-spatial-temporal-correlation-filter-for-Chen-Liu/1e318864509718478dbf363fee229594a7fda5d3",
            "/paper/Correlation-filter-tracking-algorithm-based-on-and-Wu-Xu/61e42100d211dee373447591e744a19764a7bfe3",
            "/paper/A-practical-evaluation-of-correlation-filter-based-Mohamed-Elhenawy/0d9281a58c437b53bb3fd18e8783859b13b90ecb",
            "/paper/SiamOT%3A-An-Improved-Siamese-Network-with-Online-for-Gong-Zhou/d1df68cd747f999fa69564d75a82e0560e260fa2",
            "/paper/PSINet%3A-Progressive-Saliency-Iteration-Network-for-Duan-Xia/4b5d57d49ba825903a7abd84932d56c77b4eb060",
            "/paper/Non-linear-target-trajectory-prediction-for-robust-Xu-Diao/f049c6b463502d802b4c5aa12e01e848ccfb2a90",
            "/paper/Cross-Modality-Global-Correlational-based-Visual-Gao-Fu/11bd49de8c091104f874db142befc937e744de11",
            "/paper/Dynamic-Saliency-Aware-Regularization-for-Object-Feng-Han/60863b09592df8ef41bd616ac9df947eb212246a",
            "/paper/Robust-occlusion-aware-part-based-visual-tracking-Wang-Hou/6f4595e1e1ee6d85f3fd4de89c5837eb618bf044",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Non-rigid-Object-Tracking-via-Deep-Multi-scale-Maps-Zhang-Wang/75272d131929bd4dfab7beedc4d1b34647644181",
            "/paper/MC-HOG-Correlation-Tracking-with-Saliency-Proposal-Zhu-Wang/7d5146e3ddb13a157799f0bdca1ac7d822b1fc82",
            "/paper/Object-Saliency-Aware-Dual-Regularized-Correlation-Fu-Xu/bb1e5031d0b5f905fa88ed248c44aa7165f10714",
            "/paper/Augmenting-cascaded-correlation-filters-with-for-Zhao-Xiao/966239d2f03a81526f77746c2e763cc316095184",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "/paper/Siamese-Attentional-Keypoint-Network-for-High-Gao-Ma/fce3655dc22a783b1f82f09190410f070c7bf42c",
            "/paper/Robust-Visual-Tracking-Using-Multi-Frame-Joint-Zhang-Yu/1114c2aba97a5782a48341817811df2438d0fdbf"
        ]
    },
    {
        "id": "b11ada6271daa81df49f384508b4d9bc545c478e",
        "title": "Forgery Detection For High-Resolution Digital Images Using FCM And PBFOAAlgorithm",
        "abstract": "The PBFOA method has been implemented to optimize and extract the features using the component analysis method and performance analysis depends on the proposed metrics FAR, FRR, ACC, Precision, Recall, and compared with the existing methods. Image forgery detection is the area of research in the field of biometric and forensics. Digital pictures are the resource of data. In the present world of technology, image processing software tools have developed to generate and modify digital images from one location to another. With the current technology, it is simple to establish image forgery by addition and subtraction of the components from the pictures that lead to image interfering. Copy-move image forgery is created by copying and pasting the element in a similar image. Hence, copy-move forgery has become an area of research in the image forensic unit. Various methods have been implemented to detect digital image forgery. Some issues still required to resolve like time complexity, fake, and blurred image. In existing research, the block and feature-based approach used to remove a forged area from the image using SIFT and RANSAC algorithm. The forgery dataset of the 80 pictures collected to achieve accuracy of up to 95%. In the research work, the PBFOA method has been implemented to optimize and extract the features using the component analysis method. FCM is used for image segmentation in the input image. PBFOA is based on an optimization process to select valuable features based on the calculation of the fitness function. In this method, two steps are used to re-verify the instance, features (i) Slower and faster condition. BFOA steps are described in detail in this research paper. Initial steps, Spread the feature set in the whole system. In the rapid condition selected and to eliminate the valuable features one at a time, then reproduction phase is implemented with the help of the fitness function to recover the feature values and detect the forgery information in the uploaded image. The simulation setup using MATLAB 2016a version and improve the accuracy rate and image quality parameter. Performance analysis depends on the proposed metrics FAR, FRR, ACC, Precision, Recall, and compared with the existing methods.",
        "publication_year": "2020",
        "authors": [
            "S. Kaur",
            "Nidhi Bhatla"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "27",
        "references": [
            "/paper/Detection-of-Copy-Move-Forgery-in-Digital-Images-Fridrich/6be1c27bb9f7f45854af8dcd4b2d3217d6bd71c5",
            "/paper/Detection-of-Digital-Image-Forgery-using-Fast-and-Kanwal-Girdhar/c2e72ed0e26446160f0de31843bb2da5d76d8d7e",
            "/paper/Comparison-between-image-forgery-detection-Nazli-Maghari/162f031525ede8662537b237adb337aef597e3cc",
            "/paper/Blind-Approach-for-Digital-Image-Forgery-Detection-Thakur-Singh/7e2a06c7c77e64c559a0118f40188349bededeec",
            "/paper/Copy-move-forgery-detection-exploiting-statistical-Dixit-Naskar/cf2c47ee24bdcef6a4b111874eb68826b4d5d7a8",
            "/paper/A-new-approach-for-detecting-copy-move-forgery-in-Shabanian-Mashhadi/09002ca4f9f8125ee7ed2c6695d32e0165f19780",
            "/paper/Improving-SURF-Based-Copy-Move-Forgery-Detection-Al-Hammadi-Emmanuel/42530e54732e944aaba6cbdd077fcbbcc5324074",
            "/paper/Image-Copy-Move-Forgery-Detection-Based-on-SURF-Bo-Junwen/55113052879e183b31a503d42b7d521c51556c59",
            "/paper/Robust-Detection-of-Region-Duplication-Forgery-in-Luo-Huang/635ffbfb91ac6ea4c7593bf2ae77a5b9be09e1fb",
            "/paper/An-efficient-and-robust-method-for-detecting-Bayram-Sencar/fd75f3efb885cf2b4aea76958be4e868126354c0"
        ]
    },
    {
        "id": "c97dfad7a023fbec97a901dae02b73e2e8e0fff1",
        "title": "NCP-VAE: Variational Autoencoders with Noise Contrastive Priors",
        "abstract": "Noise contrastive priors are proposed that improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in various domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets.",
        "publication_year": "2020",
        "authors": [
            "J. Aneja",
            "A. Schwing",
            "J. Kautz",
            "Arash Vahdat"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "29",
        "reference_count": "72",
        "references": [
            "/paper/D2C%3A-Diffusion-Denoising-Models-for-Few-shot-Sinha-Song/2e613574b6c3a3422728d4478440d922162a4fa0",
            "/paper/Score-based-Generative-Modeling-in-Latent-Space-Vahdat-Kreis/ad14e11bc97cc2fed0fd344e7cb7d7ce4205bfc6",
            "/paper/Dual-Contradistinctive-Generative-Autoencoder-Parmar-Li/a53e73139d9d6474ef8a002ee9c1dea49755ebc6",
            "/paper/DiffuseVAE%3A-Efficient%2C-Controllable-and-Generation-Pandey-Mukherjee/ce8e3fa6fa6d45b8b92169a2e181dafb20749a2f",
            "/paper/Catch-Missing-Details%3A-Image-Reconstruction-with-Lin-Li/2f11b0950505b3c68903d3d221465a9440682c64",
            "/paper/Pythae%3A-Unifying-Generative-Autoencoders-in-Python-Chadebec-Vincent/ac21deac5d63600f2b6464459751149ebc1fd8f3",
            "/paper/InfoNCE-is-a-variational-autoencoder-Aitchison/263cc7861dd15c25796cbeace50736bdc3a80ce4",
            "/paper/Generative-Modeling-with-Optimal-Transport-Maps-Rout-Korotin/987a879358bdc2150965d50396c0eb0159ffdf86",
            "/paper/A-Geometric-Perspective-on-Variational-Autoencoders-Chadebec-Allassonni%C3%A8re/ee9f509be30014026cf586a2eae9674b0cb38bb2",
            "/paper/Conjugate-Energy-Based-Models-Wu-Esmaeili/d321394b0c0343383ce8df9483496681f8bbee03",
            "/paper/VAE-with-a-VampPrior-Tomczak-Welling/5ea2cdab68c69d7aef5a004495783ae7628193f2",
            "/paper/From-Variational-to-Deterministic-Autoencoders-Ghosh-Sajjadi/622e392f8c5da161cf61582af434f6976094dfc4",
            "/paper/Generating-Diverse-High-Fidelity-Images-with-Razavi-Oord/6be216d93421bf19c1659e7721241ae73d483baf",
            "/paper/Perceptual-Generative-Autoencoders-Zhang-Zhang/eb64ed452b7ab7f989d857909e1759a20f7909b6",
            "/paper/NVAE%3A-A-Deep-Hierarchical-Variational-Autoencoder-Vahdat-Kautz/853de0e00ac5ac257a622ae678ed373b8e086404",
            "/paper/On-the-Necessity-and-Effectiveness-of-Learning-the-Xu-Chen/f25496122d98e117db03e91cecc94ac638e48631",
            "/paper/Diagnosing-and-Enhancing-VAE-Models-Dai-Wipf/d3dd00e24f96bae7ad780ac5fdb0c14194d5cb74",
            "/paper/Variational-Autoencoder-with-Implicit-Optimal-Takahashi-Iwata/2a69f4d3dd7a71a8c593e5fd30167febef8365a5",
            "/paper/Resampled-Priors-for-Variational-Autoencoders-Bauer-Mnih/4eb82d0ae0b60311779f2b1e1fa2d8789fb38cb2",
            "/paper/Adversarial-Autoencoders-Makhzani-Shlens/c8c04ed972d38e2326a53d322a6f2d7e0f8218c1"
        ]
    },
    {
        "id": "71af002316d1df8d2170c65320c74968a66a0d64",
        "title": "Concept Drift Detection in Data Stream Mining : A literature review",
        "abstract": "Semantic Scholar extracted view of \"Concept Drift Detection in Data Stream Mining : A literature review\" by Supriya Agrahari et al.",
        "publication_year": "2021",
        "authors": [
            "Supriya Agrahari",
            "A. Singh"
        ],
        "related_topics": [
            "Art"
        ],
        "citation_count": "27",
        "reference_count": "74",
        "references": [
            "/paper/Dynamic-Template-Selection-Through-Change-Detection-Kiran-Nguyen-Meidine/2f02729d35704bbec4ccd42fc59e80953f1c3f6b",
            "/paper/Detecting-Dataset-Drift-and-Non-IID-Sampling-via-Cummings-Snorrason/46a6794b8888a2cf81b2aa16740d406cba4ad075",
            "/paper/An-Experimental-Analysis-of-Drift-Detection-Methods-Palli-Jaafar/2cd3f8e0df3ffbd272bf24ed7b1604e837f076fd",
            "/paper/The-Data-Stream-Principles%2C-Tools-and-Applications%3A-Sheet-Alazeez/fd5ef7bff1c3e8452e1fd6c6a0da8b4cf347ecb2",
            "/paper/DARWIN%3A-An-online-deep-learning-approach-to-handle-Pasquadibisceglie-Appice/dc8d37596b08507d92cc574264a11d5b9500da63",
            "/paper/Concept-Drift-Adaptation-Methods-under-the-Deep-A-Xiang-Zi/d0e4f72ffb94a52850f5e979a665632b1247dffe",
            "/paper/Review-of-Anomaly-Detection-Algorithms-for-Data-Lu-Wang/873beda4580fa606cd4a611cd7508dbee04937f2",
            "/paper/Concept-drift-from-1980-to-2020%3A-a-comprehensive-Bab%C3%BCro%C4%9Flu-Durmu%C5%9Fo%C4%9Flu/182e2c5bda5d37a7b2b3ed85ec8de169dc46a5e0",
            "/paper/Enhanced-classification-of-hydraulic-testing-of-Neunzig-M%C3%B6llensiep/c58fb0cbf944e5848eba6d3b0012eea5d070ae2d",
            "/paper/A-survey-on-detecting-healthcare-concept-drift-in-a-A.M.-NirmalaC./2285236e3de1b908a14b11eec7a9a0ce7d28ff9b",
            "/paper/Aquila-Optimizer%3A-A-novel-meta-heuristic-algorithm-Abualigah-Yousri/425dee1b48a7a1f6847ea2c5885a5567a3f55b68",
            "/paper/Concept-learning-using-one-class-classifiers-for-in-G%C3%B6z%C3%BCa%C3%A7ik-Can/ff1e8d3895641ce8157f6cc5d312e3a2837c997a",
            "/paper/Concept-drift-detection-on-stream-data-for-revising-Miyata-Ishikawa/bd7aaf7c022cdafd03e411efbeffde8ceed34519",
            "/paper/Concept-drift-detection-with-False-Positive-rate-in-Wang-Jin/47d2a7b6224f720fc3f118d8388436ee2ad99f5c",
            "/paper/Diversity-measure-as-a-new-drift-detection-method-Mahdi-Pardede/ec28cad6b19ee5fcdbb4192b25e6b0e879e70b92",
            "/paper/Self-adaption-neighborhood-density-clustering-for-Xu-Feng/fc07a6e37ee4e127b5e87d1efc209e4e1d27ea34",
            "/paper/STDS%3A-self-training-data-streams-for-mining-limited-Khezri-Tanha/1bf2bb4bde7ac8da1072d200540510bd025a28c3",
            "/paper/A-Novel-Concept-Drift-Detection-Method-for-Learning-Yang-Al-Dahidi/0ed672ccf1920d7f9779817d276cb4dee61c703b",
            "/paper/Experimenting-with-prequential-variations-for-data-Hidalgo-Maciel/5d454b93e17a1ef6fe15c647a3f09fc0ce75244f",
            "/paper/Data-driven-decision-support-under-concept-drift-in-Lu-Liu/231a48f1bd1e9e70c76ef1c3273d269f310f4552"
        ]
    },
    {
        "id": "ac1ff733028f9cb175d9ccece59e851fd18c96bd",
        "title": "EfficientNet-Based Convolutional Neural Networks for Tuberculosis Classification",
        "abstract": "A detailed investigation and analysis of 26 pretrained convolutional neural network (CNN) models using a recently released and large public database of TB X-ray shows that ENet-based models can be effectively used as a useful tool for TB classification. Tuberculosis (TB) is an infectious disease that remained as a major health threat in the world. The computer-aided diagnosis (CAD) system for TB is one of the automated methods in early diagnosis and treatment, particularly used in developing countries. Literature survey shows that many methods exist based on machine learning for TB classification using X-ray images. Recently, deep learning approaches have been used instead of machine learning in many applications. This is mainly due to the reason that deep learning can learn optimal features from the raw dataset implicitly and obtains better performances. Due to the lack of X-ray image TB datasets, there are a small number of works on deep learning addressing the image-based classification of TB. In addition, the existing works can only classify X-ray images of a patient as TB or Healthy. This work presents a detailed investigation and analysis of 26 pretrained convolutional neural network (CNN) models using a recently released and large public database of TB X-ray. The proposed models have the capability to classify X-ray of a patient as TB, Healthy, or Sick but non-TB. Various visualization methods are adopted to show the optimal features learnt by the pretrained CNN models. Most of the pretrained CNN models achieved above 99% accuracy and less than 0.005 loss with 15 epochs during the training. All 7 different types of EfficientNet (ENet)-based CNN models performed better in comparison to other models in terms of accuracy, average and macro precision, recall, F1 score. Moreover, the proposed ENet-based CNN models performed better than other existing methods such as VGG16 and ResNet-50 for TB classification tasks. These results demonstrate that ENet-based models can be effectively used as a useful tool for TB classification.",
        "publication_year": "2021",
        "authors": [
            "Vinayakumar Ravi",
            "Harini Narasimhan",
            "T. Pham"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "7",
        "reference_count": "33",
        "references": [
            "/paper/Optimal-Deep-Transfer-Learning-Model-for-Automated-Manivannan-Sathiamoorthy/6847d5107a4ea78c351a03fbb4d956285bd84f2d",
            "/paper/A-deep-learning%E2%80%90based-approach-for-diagnosing-on-a-Sevli/4da66209b6f2e9c30c1a9d67d1e19cc661d96dfa",
            "/paper/Pulmonary-nodules-recognition-based-on-parallel-Hu-Zhan/b6d3b25205d1fc9ea4aaf121e0c32cf662d5bc83",
            "/paper/Feature-extraction-and-fusion-network-for-salient-Dai-Pan/25557a14cf60f7ab4de6a8f522fde0a5afe79705",
            "/paper/A-hybrid-feature-fusion-strategy-for-early-fusion-Singh-Janghel/704198362c3ef1b1f8792b74eaff6469a27e7c49",
            "/paper/Vision-Transformers-in-Medical-Computer-Vision-A-Parvaiz-Khalid/224ecc21a917dd246420d2da0b3edee5834f3391",
            "/paper/Optimized-deformable-convolution-network-for-and-of-Devulapalli-Chanamallu/444b2218f31629617840ac1f99c2f50914d7227a",
            "/paper/Deep-learning%3A-A-potential-method-for-tuberculosis-Hooda-Sofat/a87f545ed1ebf88e003c4b8d39d0324ded1a0be6",
            "/paper/Rethinking-Computer-Aided-Tuberculosis-Diagnosis-Liu-Wu/c8da769833fa7dafc3546030ce090a092169f8df",
            "/paper/Tuberculosis-Diagnostics-and-Localization-in-Chest-Guo-Passi/80ffb35701d6d69cc3092a7f2c603fe65e531ff2",
            "/paper/Pre-trained-convolutional-neural-networks-as-for-Lopes-Valiati/41240f3f1eccf75a6bcb105fd453bdb1675e8ce1",
            "/paper/Image-Enhancement-for-Tuberculosis-Detection-Using-Munadi-Muchtar/5a1cce0bb153c54e13707ee613d885bbfbb2bcd5",
            "/paper/Reliable-Tuberculosis-Detection-Using-Chest-X-Ray-Rahman-Khandakar/b00de2e18011659a26ffcc51c8e5f24d76efa8dd",
            "/paper/Ensembles-of-Convolutional-Neural-Networks-on-Evangelista-Guedes/e2db14d03b54381e98312f9a2e4b86c79302664f",
            "/paper/Deep-Learning-for-Automated-Classification-of-Chest-Sathitratanacheewin-Pongpirul/708c2d89aa2925b7f3d2ec27cd8703d7eaf18519",
            "/paper/TBNet%3A-Pulmonary-Tuberculosis-Diagnosing-System-Ghorakavi/13562d58553a4cd2c83993211dbf25409deaceed",
            "/paper/ImageCLEF-2017%3A-ImageCLEF-Tuberculosis-Task-the-Sun-Chong/734627bbb47516d66947cd8fda0ccdb2039d23b9"
        ]
    },
    {
        "id": "e7b94521b38c05f66f6a337c80be5956084585ac",
        "title": "Vision-Transformer-Based Transfer Learning for Mammogram Classification",
        "abstract": "A transfer learning technique based on vision transformers to classify breast mass mammograms that outperforms the CNN-based transfer-learning models and vision transformer models trained from scratch and can be applied in a clinical setting, to improve the early diagnosis of breast cancer. Breast mass identification is a crucial procedure during mammogram-based early breast cancer diagnosis. However, it is difficult to determine whether a breast lump is benign or cancerous at early stages. Convolutional neural networks (CNNs) have been used to solve this problem and have provided useful advancements. However, CNNs focus only on a certain portion of the mammogram while ignoring the remaining and present computational complexity because of multiple convolutions. Recently, vision transformers have been developed as a technique to overcome such limitations of CNNs, ensuring better or comparable performance in natural image classification. However, the utility of this technique has not been thoroughly investigated in the medical image domain. In this study, we developed a transfer learning technique based on vision transformers to classify breast mass mammograms. The area under the receiver operating curve of the new model was estimated as 1 \u00b1 0, thus outperforming the CNN-based transfer-learning models and vision transformer models trained from scratch. The technique can, hence, be applied in a clinical setting, to improve the early diagnosis of breast cancer.",
        "publication_year": "2023",
        "authors": [
            "Gelan Ayana",
            "K. Dese",
            "Yisak Dereje",
            "Yonas Kebede",
            "Hika Barki",
            "Dechassa Amdissa",
            "Nahimiya Husen",
            "Fikadu Mulugeta",
            "Bontu Habtamu",
            "S. Choe"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "2",
        "reference_count": "60",
        "references": [
            "/paper/BC2NetRF%3A-Breast-Cancer-Classification-from-Images-Jabeen-Khan/b0925e7744cb305c4a6b1fcf012b66eec249fc69",
            "/paper/Recent-Progress-in-Transformer-based-Medical-Image-Liu-Lv/5e768672936864225e4cae6f1b8fb45e9f9b478d",
            "/paper/Patchless-Multi-Stage-Transfer-Learning-for-Breast-Ayana-Park/f4cefb08a98d781dd73472ff98b6a1c7f36ebcb2",
            "/paper/Detection-of-Breast-Cancer-from-Mammograms-using-a-Agrawal-Rangnekar/f9f89d9f71fc6a00fe1323fe952ca85cfb183c0f",
            "/paper/MAMMO%3A-A-Deep-Learning-Solution-for-Facilitating-in-Kyono-Gilbert/6796df2623b8265fb6cfe3336d0a1d41acac5697",
            "/paper/A-Multi-scale-CNN-and-Curriculum-Learning-Strategy-Lotter-Sorensen/64fc4501736f4be0869838456eb54d5e5b4d886b",
            "/paper/BUViTNet%3A-Breast-Ultrasound-Detection-via-Vision-Ayana-Choe/4502b311cbbe916f513ef74f51d91bc4c1d1e204",
            "/paper/Large-scale-deep-learning-for-computer-aided-of-Kooi-Litjens/2a863a1ab6df3ca9a55573befcb89e1ed7b7df74",
            "/paper/Hierarchical-Fused-Model-With-Deep-Learning-and-for-Shen-Wang/dc808e6da49d094b76f7848d5b39a90b9b807bbc",
            "/paper/YOLO-LOGO%3A-A-transformer-based-YOLO-segmentation-in-Su-Liu/e10f7e643779ff0ff01448721e4ae9bdc908c7fb",
            "/paper/Transformers-Improve-Breast-Cancer-Diagnosis-from-Chen-Zhang/6ef1a9af57f6119f4bd4b13013e218d38b7a7ec4",
            "/paper/A-Novel-Multistage-Transfer-Learning-for-Ultrasound-Ayana-Park/7cbb2af0640372e2668407a1e870220aee162a0d"
        ]
    },
    {
        "id": "c2683f27ba55b2046848ffdff62b64b31241c07c",
        "title": "Magnetic resonance imaging in precision radiation therapy for lung cancer.",
        "abstract": "MRI sequences are rapidly developing and although the issue of intra-thoracic motion has historically hindered the quality of MRI due to the effect of motion, progress is being made in this field and the clinical benefits of MR-guided radiotherapy will be derived from the ability to adapt treatment on the fly for each fraction and in real-time, using 'beam-on' imaging. Radiotherapy remains the cornerstone of curative treatment for inoperable locally advanced lung cancer, given concomitantly with platinum-based chemotherapy. With poor overall survival, research efforts continue to explore whether integration of advanced radiation techniques will assist safe treatment intensification with the potential for improving outcomes. One advance is the integration of magnetic resonance imaging (MRI) in the treatment pathway, providing anatomical and functional information with excellent soft tissue contrast without exposure of the patient to radiation. MRI may complement or improve the diagnostic staging accuracy of F-18 fluorodeoxyglucose position emission tomography and computerized tomography imaging, particularly in assessing local tumour invasion and is also effective for identification of nodal and distant metastatic disease. Incorporating anatomical MRI sequences into lung radiotherapy treatment planning is a novel application and may improve target volume and organs at risk delineation reproducibility. Furthermore, functional MRI may facilitate dose painting for heterogeneous target volumes and prediction of normal tissue toxicity to guide adaptive strategies. MRI sequences are rapidly developing and although the issue of intra-thoracic motion has historically hindered the quality of MRI due to the effect of motion, progress is being made in this field. Four-dimensional MRI has the potential to complement or supersede 4D CT and 4D F-18-FDG PET, by providing superior spatial resolution. A number of MR-guided radiotherapy delivery units are now available, combining a radiotherapy delivery machine (linear accelerator or cobalt-60 unit) with MRI at varying magnetic field strengths. This novel hybrid technology is evolving with many technical challenges to overcome. It is anticipated that the clinical benefits of MR-guided radiotherapy will be derived from the ability to adapt treatment on the fly for each fraction and in real-time, using 'beam-on' imaging. The lung tumour site group of the Atlantic MR-Linac consortium is working to generate a challenging MR-guided adaptive workflow for multi-institution treatment intensification trials in this patient group.",
        "publication_year": "2017",
        "authors": [
            "H. Bainbridge",
            "A. Salem",
            "R. Tijssen",
            "M. Dubec",
            "A. Wetscherek",
            "C. V. van Es",
            "J. Belderbos",
            "C. Faivre-Finn",
            "F. McDonald"
        ],
        "related_topics": [
            "Medicine",
            "Physics"
        ],
        "citation_count": "52",
        "reference_count": "134",
        "references": [
            "/paper/Magnetic-resonance%E2%80%90guided-radiation-therapy%3A-A-Chin-Eccles/df8bcc74868724d33f85e4b5de3212eb394b6390",
            "/paper/MRI-guidance-for-motion-management-in-external-beam-Paganelli-Whelan/5cd35f8269dadb695d142ccaf8630406148a9b8e",
            "/paper/Initial-Clinical-Experience-of-MR-Guided-for-Cell-Crockett-Samson/00be274aa8e4049d4bf159d58be16e38644a151d",
            "/paper/The-Path-Toward-PET-Guided-Radiation-Therapy-for-in-Donche-Verhoeven/9eadfbf7ba0efc091052c66ce09dea41dc0ee430",
            "/paper/Introducing-magnetic-resonance-imaging-into-the-An-Bellhouse-Brown/6b838dbfbf65aa8d06325330e07e071501f3cce4",
            "/paper/Anatomical-Adaptation-Early-Clinical-Evidence-of-in-Kavanaugh-Hugo/7e0b62ae2f6ce2e97318e19abbe32b50d3a45ad5",
            "/paper/A-modern-review-of-the-uncertainties-in-volumetric-Vergalasova-Cai/0e6c71521d0f6f335c47c3ec9d3286cb22417948",
            "/paper/Current-radiotherapy-techniques-in-NSCLC%3A-and-Giaj-Levra-Borghetti/742cc5631236aebdb1a7b73c6f1d28f2844aadd1",
            "/paper/Technological-advancements-and-future-perspectives-Fozza-Rose/ea33722afa9038a72d28dd4a0a28f46e686e6914",
            "/paper/Diffusion-Weighted-Magnetic-Resonance-Imaging-for-Liu-Lv/108c1eba5f90dd7cd2dc8a5fce61ff5e68c1d894",
            "/paper/MRI-guided-lung-SBRT%3A-Present-and-future-Menten-Wetscherek/0ed5b73172bdcc98b7d60946b5dab12b7644e144",
            "/paper/Multimodal-hypoxia-imaging-and-intensity-modulated-Askoxylakis-Dinkel/49c429f6b4503608dd628eceb85ecd36e2b5923f",
            "/paper/New-radiotherapy-approaches-in-locally-advanced-Christodoulou-Bayman/1a941c9f4f6d0d35c49e492d2f1ed9dc866e1fa2",
            "/paper/Magnetic-resonance-imaging-in-lung%3A-a-review-of-its-Kumar-Liney/e1d8ecfdbb508a8825ecef6cb08d0ffda2a65b28",
            "/paper/PET-in-the-management-of-locally-advanced-and-NSCLC-Grootjans-Geus-Oei/c1cc8af40aed47110154527f4da2f0ccfcc15443",
            "/paper/Effect-of-Midtreatment-PET-CT-Adapted-Radiation-in-Kong-Haken/8305959780a50ed93a936c57b75587163a22deb3",
            "/paper/Motion-management-for-radical-radiotherapy-in-cell-Cole-Hanna/f1cf1fb55a8471ddfd977d67476045fca7dc7e87",
            "/paper/PET-CT-versus-MRI-for-diagnosis%2C-staging%2C-and-of-Kim-Lee/4c9f8604bf74a0423cc72a575ec65806265aa72b",
            "/paper/MRI-guided-prostate-adaptive-radiotherapy-A-review.-McPartlin-Li/f9cc7ad76fa29d2b0c74e66eb05e9c44117f8b87",
            "/paper/Adaptive-radiotherapy-in-lung-cancer%3A-dosimetric-Kataria-Gupta/8d144d869481b849cb4d4d398924884a32584be9"
        ]
    },
    {
        "id": "4efcca204f430fd5fe49bde14f27d343b0544dd0",
        "title": "Exploring uncertainty measures in convolutional neural network for semantic segmentation of oral cancer images",
        "abstract": "This study demonstrates the UNet-based BDL model not only can perform potentially malignant and malignant oral lesion segmentation, but also can provide informative pixel-level uncertainty estimation. Abstract. Significance Oral cancer is one of the most prevalent cancers, especially in middle- and low-income countries such as India. Automatic segmentation of oral cancer images can improve the diagnostic workflow, which is a significant task in oral cancer image analysis. Despite the remarkable success of deep-learning networks in medical segmentation, they rarely provide uncertainty quantification for their output. Aim We aim to estimate uncertainty in a deep-learning approach to semantic segmentation of oral cancer images and to improve the accuracy and reliability of predictions. Approach This work introduced a UNet-based Bayesian deep-learning (BDL) model to segment potentially malignant and malignant lesion areas in the oral cavity. The model can quantify uncertainty in predictions. We also developed an efficient model that increased the inference speed, which is almost six times smaller and two times faster (inference speed) than the original UNet. The dataset in this study was collected using our customized screening platform and was annotated by oral oncology specialists. Results The proposed approach achieved good segmentation performance as well as good uncertainty estimation performance. In the experiments, we observed an improvement in pixel accuracy and mean intersection over union by removing uncertain pixels. This result reflects that the model provided less accurate predictions in uncertain areas that may need more attention and further inspection. The experiments also showed that with some performance compromises, the efficient model reduced computation time and model size, which expands the potential for implementation on portable devices used in resource-limited settings. Conclusions Our study demonstrates the UNet-based BDL model not only can perform potentially malignant and malignant oral lesion segmentation, but also can provide informative pixel-level uncertainty estimation. With this extra uncertainty information, the accuracy and reliability of the model\u2019s prediction can be improved.",
        "publication_year": "2022",
        "authors": [
            "Bofan Song",
            "Shaobai Li",
            "Sumsum P. Sunny",
            "Keerthi Gurushanth",
            "Pramila Mendonca",
            "Nirza Mukhia",
            "Sanjana Patrick",
            "Tyler Peterson",
            "S. Gurudath",
            "S. Raghavan",
            "Imchen Tsusennaro",
            "Shirley T. Leivon",
            "T. Kolur",
            "V. Shetty",
            "V. Bushan",
            "R. Ramesh",
            "V. Pillai",
            "P. Wilder-Smith",
            "A. Suresh",
            "M. Kuriakose",
            "Praveen Birur",
            "Rongguang Liang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "28",
        "references": [
            "/paper/Bayesian-deep-learning-for-reliable-oral-cancer-Song-Sunny/ade486fc9e856793a7347aa1402e8c11724a2973",
            "/paper/Uncertainty-quantification-in-skin-cancer-using-Abdar-Samami/466f101a55619f100ffa02c7531fb2ec52ea395c",
            "/paper/Exploring-Uncertainty-Measures-in-Bayesian-Deep-for-Liu-Yang/f4a12d50579eb11335c5ae3d06b46cfc23de3641",
            "/paper/Her2Net%3A-A-Deep-Framework-for-Semantic-Segmentation-Saha-Chakraborty/2aca6293ee2c9d442fe1b10bbb9866e21df5410f",
            "/paper/ARA%3A-accurate%2C-reliable-and-active-image-framework-R%C4%85czkowski-Mo%C5%BCejko/e7b445e4bcf4dbb91aa20fe670a53223be29bcb3",
            "/paper/Interpretable-deep-learning-systems-for-multi-class-Thomas-Lefevre/c51d00c236e7ec16e35c7d11ec8c7b2e458db326",
            "/paper/Glaucoma-diagnosis-in-the-Chinese-context%3A-An-deep-Chai-Bian/97098aff126d1d188c4bf6146cb8bd04ec0aea57",
            "/paper/A-deep-learning-based-framework-for-segmenting-with-Balagopal-Nguyen/fb33105075d024dc42cd436e2647a6f787338e18",
            "/paper/Evaluating-Bayesian-Deep-Learning-Methods-for-Mukhoti-Gal/e9a88a25b4d6332de95d829b5a892275dc3ffcfc",
            "/paper/Automatic-classification-of-dual-modalilty%2C-oral-Song-Sunny/eeebfa2343ef9180d647d7bbec008e249d250d88"
        ]
    },
    {
        "id": "ab73be92c3add3de3cfce526228599bb99dcbf54",
        "title": "A Predictive Model for Patent Registration Time Using Survival Analysis",
        "abstract": "A predictive model of the patent registration time is proposed using survival analysis w ith a Weibull survival regression model and a Cox's propor- tional hazard model, modeled on the number of international patent classifications (IPC) codes and keywords. The infringements and suits of patents have been increased in many technological fields. Since a patent is an intellectual property to protect inventors' exclusive rights of developed technolog ies, technological pre-occupancy is very important to the devel- opment of companies. For the technological pre-occupancy a patent registration time is surely important. In this paper we propose a predictive model of the patent registration time using survival analysis w ith a Weibull survival regression model and a Cox's propor- tional hazard model. They are modeled on the number of international patent classifications (IPC) codes and keywords. To verify the proposed models, we perform a case study using the retrieved patent documents of 'hybrid vehicle' from the website of United State Patent and Trademark Office (USPTO).",
        "publication_year": "2013",
        "authors": [
            "Sunghae Jun",
            "Daiho Uhm"
        ],
        "related_topics": [
            "Economics"
        ],
        "citation_count": "4",
        "reference_count": "16",
        "references": [
            "/paper/Prediction-of-University-Patent-Transfer-Cycle-on-Deng-Chen/b5b26c3dad8e815a4a5c14880aaeb4d129761797",
            "/paper/Patent-Research-in-a-Period-of-Industry-A-Focus-on-Qu/6988cb30337787a8f982b04a4e1d3be39d7573f7",
            "/paper/Data-Analysis-Regression-with-CSR-Files-in-Data-Chinnaiah-Saxena/edab69e71c91af1305c0ea1bb32128b0237a545b",
            "/paper/A-Divided-Regression-Analysis-for-Big-Data-Jun-Lee/84163acfd57000af2202f49256b201f063ba2506",
            "/paper/Patent-Registration-Prediction-Methodology-Using-Jung-Park/d00369a2624eff522ae14a4a6367260f4294f17a",
            "/paper/Nonparametric-Estimation-from-Incomplete-Kaplan-Meier/7b261c11533a4bdcb14f10d840660bd60fc5130a",
            "/paper/Theory-and-Applications-of-Hazard-Plotting-for-Data-Nelson/7cbd0304295aa4f7a060ce908857df3452e7f0f2",
            "/paper/Text-Mining-Infrastructure-in-R-Feinerer-Hornik/693914b7f38c19585e35668fd626aecf62d4c5e7",
            "/paper/Survival-Analysis%3A-Techniques-for-Censored-and-Data-Klein-Moeschberger/f8d65d3d73bcb4cdc99ab96a049e567b98520e32",
            "/paper/R%3A-A-language-and-environment-for-statistical-Team/659408b243cec55de8d0a3bc51b81173007aa89b",
            "/paper/Regression-models-and-life-tables-(with-discussion-Cox/2e2da6822c026dee4114921cc8a0babc7251b862",
            "/paper/Survival-Analysis%3A-Techniques-for-Censored-and-Data-Langner/5c8141ff184529fe30c46cbd86c7f2c591508081",
            "/paper/Nonparametric-Inference-for-a-Family-of-Counting-Aalen/472fb298331c83a6791f2f3b26595e26177d35d8",
            "/paper/A-Statistical-Distribution-Function-of-Wide-Weibull/88c37770028e7ed61180a34d6a837a9a4db3b264"
        ]
    },
    {
        "id": "61766d9c5dcb3df66a18f99849e7e163fe48f6a0",
        "title": "A multi-level feature-fusion-based approach to breast histopathological image classification",
        "abstract": "This work proposes a multi-level feature fusion method for breast histopathology image classification that fuse shallow features and deep semantic features by attention mechanism and convolutions to deal with the misjudgment of false negative and false positive in images. Previously, convolutional neural networks mostly used deep semantic feature information obtained from several convolutions for image classification. Such deep semantic features have a larger receptive field, and the features extracted are more effective as the number of convolutions increases, which helps in the classification of targets. However, this method tends to lose the shallow local features, such as the spatial connectivity and correlation of tumor region texture and edge contours in breast histopathology images, which leads to its recognition accuracy not being high enough. To address this problem, we propose a multi-level feature fusion method for breast histopathology image classification. First, we fuse shallow features and deep semantic features by attention mechanism and convolutions. Then, a new weighted cross entropy loss function is used to deal with the misjudgment of false negative and false positive. And finally, the correlation of spatial information is used to correct the misjudgment of some patches. We have conducted experiments on our own datasets and compared with the base network Inception-ResNet-v2, which has a high accuracy. The proposed method achieves an accuracy of 99.0% and an AUC of 99.9%.",
        "publication_year": "2022",
        "authors": [
            "Weilong Ding",
            "Xiaojie Zhu",
            "Kui Zheng",
            "Jinlong Liu",
            "Qinghua You"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "19",
        "references": [
            "/paper/Image-Classification-Learning-Method-Incorporating-Sun-Diao/c6360bc7b7e1e34debd6a233a5f11edb9ad7a10a",
            "/paper/Attention-by-Selection%3A-A-Deep-Selective-Attention-Xu-Liu/e065748cafb1ef439db92bd85d6eed8b92f924ec",
            "/paper/Automatic-cell-nuclei-segmentation-and-of-breast-Wang-Hu/be9c59ec7ca1363828091cdd5e003e4e6e51b600",
            "/paper/Computer-Aided-Cancer-Regions-Detection-of-in-based-Diao-Luo/bdc40a9ead6a87acab8888bb2e6c496cd5999fa1",
            "/paper/Remote-Computer-Aided-Breast-Cancer-Detection-and-George-Zayed/e1c878dd76b2352d8e3c3ef15b08763e97410f21",
            "/paper/Large-scale-computations-on-histology-images-reveal-Petushi-Garcia/d0ad4142e4e7fb052935b349dc67bac288c83ff3",
            "/paper/Automated-gland-and-nuclei-segmentation-for-grading-Naik-Doyle/856a3c76453a798556096cd23848402d1351c1f9",
            "/paper/Cost-Sensitive-Learning-of-Deep-Feature-From-Data-Khan-Hayat/e60b8cd6cddb08e27e0e4158eed5fad3680f177e",
            "/paper/Recognition-and-Classification-of-Glomerular-Images-Meng-Chen/f2f7f8c32651ede9d35fbfadb70d3e9e13891885",
            "/paper/Diagnostic-Assessment-of-Deep-Learning-Algorithms-Bejnordi-Veta/ba913e2c03ece1c75f0af4d16dd11c7ffbc6e3ba",
            "/paper/A-New-Performance-Evaluation-Metric-for-Polygon-Aydemir/3173b8d824099e9b25a016a0b7a09d71816f5b7a"
        ]
    },
    {
        "id": "8b5e2a1cf32e65f283fcee865e9168459348cfd4",
        "title": "Weakly-Supervised Learning of a Deep Convolutional Neural Networks for Semantic Segmentation",
        "abstract": "A DCNNs model for generating the pixel-level labels using the image-level annotation based on the orthogonal non-negative matrix factorization (NMF) technology that outperforms some recently developed methods. Deep convolutional neural networks (DCNNs) trained on the pixel-wise annotated images have dramatically improved the state-of-the-art in semantic segmentation. However, due to the high cost of labeling training data, its application has great limitation. In this paper, we propose a DCNNs model for generating the pixel-level labels using the image-level annotation. The model consists of an encoder-decoder, a feature decomposer, and a multi-label classifier. The encoder extracts the deep convolutional feature maps of the input image. The feature decomposer can decompose the convolutional feature extracted by the encoder into feature components of different semantics. The decomposer is based on the orthogonal non-negative matrix factorization (NMF) technology. The function of the decoder is to map the feature components of different semantics to the input resolution images. The decoder uses the position index of maximum pooling provided by the corresponding encoder to perform non-linear up-sampling, which eliminates the learning requirement of up-sampling. Since the image reconstruction is conducted according to the semantic categories, image regions of different semantics are restored to different images. Then, the regions of different semantics can be segmented through the posted-processing algorithms. The experimental results on open data set show that the proposed model outperforms some recently developed methods.",
        "publication_year": "2019",
        "authors": [
            "Yanqing Feng",
            "Lunwen Wang",
            "Mengbo Zhang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "8",
        "reference_count": "28",
        "references": [
            "/paper/Weakly-Supervised-Image-Annotation-and-Segmentation-Naik-Jaidhar/bd5e6d3381992ec221e719ca0513a8a248a75aaf",
            "/paper/Different-Approaches-for-Semantic-Segmentation-GreeshmaP/51862add82a9c1ecb65f89acf27386cba1c6e778",
            "/paper/Different-Approaches-for-Semantic-Segmentation-GreeshmaP/3396cccf980e317c58e82635a414cffb0e6490d7",
            "/paper/Extracting-Structured-Supervision-From-Captions-for-Vilar-Perez/069639b08303c07e57a477412b15f9f91d51d366",
            "/paper/Hide-CAM%3A-Finding-Multiple-Discriminative-Regions-Xu-Sheng/a40500025395604685864d138199179ba3347777",
            "/paper/Performing-Weakly-Supervised-Retail-Instance-via-Bi-Wang/548804d066d8b4beb067eee38dd2ca687bdd8e18",
            "/paper/Generative-Adversarial-Network-based-Chest-Disease-Junaid-Anwar/ebe10d3189518bb879f56bbac4968f37bdba5b40",
            "/paper/Change-Detection-Method-of-High-Resolution-Remote-Zhao-Liu/2d67742714b0c6ba463d42e722c171c864a2d90c",
            "/paper/SegNet%3A-A-Deep-Convolutional-Encoder-Decoder-for-Badrinarayanan-Kendall/b0c065cd43aa7280e766b5dcbcc7e26abce59330",
            "/paper/Weakly-and-Semi-Supervised-Learning-of-a-Deep-for-Papandreou-Chen/da2f85e313160992b1a0e3db70eb02b58ec740c0",
            "/paper/Fully-convolutional-networks-for-semantic-Shelhamer-Long/317aee7fc081f2b137a85c4f20129007fd8e717e",
            "/paper/From-image-level-to-pixel-level-labeling-with-Pinheiro-Collobert/d46bc623f5eecb44c5a053587f841dac5cc9b743",
            "/paper/Weakly-Supervised-Semantic-Segmentation-Using-Kwak-Hong/265379eb654eda5a5916da034c310ec2d5ce408f",
            "/paper/Weakly-supervised-semantic-segmentation-with-a-Vezhnevets-Ferrari/c2932a9c686ede327f41069e17962d330a7a3ebf",
            "/paper/HCP%3A-A-Flexible-CNN-Framework-for-Multi-Label-Image-Wei-Xia/32d850e556f39f6bbedcdef0e38f5cd295a6144f",
            "/paper/BoxSup%3A-Exploiting-Bounding-Boxes-to-Supervise-for-Dai-He/f084f0126d48a0793cf7e60830089b93ef09c844",
            "/paper/Exploiting-Saliency-for-Object-Segmentation-from-Oh-Benenson/3bef0e31f88e5ff528741ce442e1aa0d765646bd",
            "/paper/Deep-Structured-Scene-Parsing-by-Learning-with-Lin-Wang/4fbbc962c4cb54380301e21a5636d7f5f1fdf9fa"
        ]
    },
    {
        "id": "1947358cadb7b89e9e38c1b6adb6315ba05e169e",
        "title": "Free vibration and buckling behavior of functionally graded porous plates reinforced by graphene platelets using spectral Chebyshev approach",
        "abstract": "Semantic Scholar extracted view of \"Free vibration and buckling behavior of functionally graded porous plates reinforced by graphene platelets using spectral Chebyshev approach\" by Mirmeysam Rafiei Anamagh et al.",
        "publication_year": "2020",
        "authors": [
            "Mirmeysam Rafiei Anamagh",
            "Bekir Bediz"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": "52",
        "reference_count": "38",
        "references": [
            "/paper/Free-Vibration-Analysis-of-Spinning-Sandwich-Plates-Huang-Ma/283d9b2d3cd72c5e96d9f39d25da56de66911285",
            "/paper/Thermal-buckling-analysis-of-porous-functionally-by-Yas-Rahimi/372fa6275ba695cd6de4bd24f2520790a24184da",
            "/paper/Theoretical-and-Numerical-Solution-for-the-Bending-Safarpour-Forooghi/e6680d773a626054214761d164285cb377874cc1",
            "/paper/Vibration-and-flutter-characteristics-of-graded-to-Zhou-Wang/3851021ccbb45603d2223b01296448586fdf7832",
            "/paper/Bending-and-free-vibration-analysis-of-functionally-Yin-Gao/c779cea8510c4865910273f4f42d6e4c221b3b3a",
            "/paper/Numerical-Study-on-the-Buckling-Behavior-of-FG-Caps-Zhou-Wang/a72a09d154c3fb78411ba7f712af26877f79dd83",
            "/paper/Influence-of-porosity-distribution-on-static-and-of-Dhuria-Grover/c6966ff16d514cf59a594d04dc71d9b577dc1c71",
            "/paper/Stability-of-functionally-graded-composite-thick-in-Ma-Jin/870a03bce1b8d6d0893c51e3e00e10a42b931186",
            "/paper/Buckling-and-free-vibration-analyses-of-pultruded-Madenci-%C3%96zk%C4%B1l%C4%B1%C3%A7/ffec13a4a6a9f0cb1aba0bb37042c5b8f9dfa0fb",
            "/paper/Buckling-of-porous-FG-sandwich-plates-subjected-to-Chaabani-Mesmoudi/6594d1b0f4b99355cbeff982473da5877b85cfdb",
            "/paper/Buckling-and-free-vibration-analyses-of-graded-on-Yang-Chen/52cc2713b991dae071de3e2f4ac5d570c9e7dc15",
            "/paper/Free-vibration-and-elastic-buckling-of-functionally-Kitipornchai-Chen/017928ae80bda5891199f4be6a8efa1e8f7b9c1e",
            "/paper/On-vibration-and-stability-analysis-of-porous-by-Saidi-Bahaadini/3b2bb9fc0443b7c7febcd60e3a89991b7ee2b367",
            "/paper/Nonlinear-free-vibration-of-functionally-graded-on-Gao-Gao/2af94572ed20523f3c3d1157c26f33452bd836b2",
            "/paper/Isogeometric-Analysis-of-functionally-graded-porous-Li-Wu/3f6291ba7e226983353a022d8ead96156f58eaee",
            "/paper/Nonlinear-vibration-and-dynamic-buckling-analyses-Li-Wu/966239f713e2d05702e0de1731d71ea91e616a9d",
            "/paper/Bending-and-buckling-analyses-of-functionally-with-Song-Yang/2bea415c47f69de83286e830ef8ae277e0f31b64",
            "/paper/Dynamic-characteristics-of-functionally-graded-on-Ganapathi-Anirudh/cfbf2622080198fdf47bc9851b2aaa267aa2d41e",
            "/paper/Static-and-free-vibration-analyses-of-carbon-plates-Zhu-Lei/0b215e34a2bb50cc235575ce35a8aaa769bcebce",
            "/paper/Elastic-buckling-and-free-vibration-analyses-of-and-Thang-Nguyen-Thoi/f3542a3a93c17859f6a4e804ed24123ca0e80e36"
        ]
    },
    {
        "id": "b38bbf2b21aa7a25a4918b1dbb8dc4514e617399",
        "title": "PlanarRecon: Realtime 3D Plane Detection and Reconstruction from Posed Monocular Videos",
        "abstract": "PlanarRecon is presented - a novel framework for globally coherent detection and reconstruction of 3D planes from a posed monocular video that achieves state-of-the-art performances on the ScanNet dataset while being real-time. We present PlanarRecon - a novel framework for globally coherent detection and reconstruction of 3D planes from a posed monocular video. Unlike previous works that detect planes in 2D from a single image, PlanarRecon incrementally detects planes in 3D for each video fragment, which consists of a set of key frames, from a volumetric representation of the scene using neural networks. A learning-based tracking and fusion module is designed to merge planes from previous fragments to form a coherent global plane reconstruction. Such design allows Planar-Recon to integrate observations from multiple views within each fragment and temporal information across different ones, resulting in an accurate and coherent reconstruction of the scene abstraction with low-polygonal geometry. Experiments show that the proposed approach achieves state-of-the-art performances on the ScanNet dataset while being real-time. Code is available at the project page: https://neu-vi.github.io/planarrecon/.",
        "publication_year": "2022",
        "authors": [
            "Yiming Xie",
            "Matheus Gadelha",
            "Fengting Yang",
            "Xiaowei Zhou",
            "Huaizu Jiang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "50",
        "references": [
            "/paper/VisFusion%3A-Visibility-aware-Online-3D-Scene-from-Gao-Mao/a97cb7fbb308e9a8f020b38e1b091dcfe4763a43",
            "/paper/Cross-Dimensional-Refined-Learning-for-Real-Time-3D-Hong-Yue/a3272f80526e0ea32f5985e640fb5a388fbc02e6",
            "/paper/Structural-Multiplane-Image%3A-Bridging-Neural-View-Zhang-Wang/5dcb4333bbcb9928843df728ae82805682c0ab16",
            "/paper/A-local-tangent-plane-distance-based-approach-to-3D-Chen-Xie/907d1bd9f4ef80ebd8ddcef09eeed6d7fa2a5260",
            "/paper/NeuralRecon%3A-Real-Time-Coherent-3D-Reconstruction-Sun-Xie/8db0bf0fa406254d4cd57eb413443a0b96bd12b8",
            "/paper/PlaneRCNN%3A-3D-Plane-Detection-and-Reconstruction-a-Liu-Kim/d6b1c09b98d0c815952c3dbac4b00c419cdbfac9",
            "/paper/Recovering-3D-Planes-from-a-Single-Image-via-Neural-Yang-Zhou/66b37797286952e7735901e152b4cdea171e8567",
            "/paper/DPPTAM%3A-Dense-piecewise-planar-tracking-and-mapping-Concha-Civera/680f5900c01eb8c55708bd9cfbd432ff89f9bd77",
            "/paper/Indoor-Panorama-Planar-3D-Reconstruction-via-Divide-Sun-Hsiao/31f9c1159c60cf9ac617ce4e99eab13e4311378b",
            "/paper/Automatic-reconstruction-of-piecewise-planar-models-Baillard-Zisserman/625d24014e0d22d9ad63131476a6d88926ffd174",
            "/paper/Multi-view-Depth-Estimation-using-Epipolar-Networks-Long-Liu/1b945b488732aff7583319b8e962854c5c327926",
            "/paper/PlaneNet%3A-Piece-Wise-Planar-Reconstruction-from-a-Liu-Yang/1b9a83b05d57dd7bcdff9a51b81d1d66a27734a7",
            "/paper/Single-Image-Piece-Wise-Planar-3D-Reconstruction-Yu-Zheng/a3c059979783ae3c4cc262423ee62c9ea0b37097",
            "/paper/Atlas%3A-End-to-End-3D-Scene-Reconstruction-from-Murez-As/35e2056e29d5295b2669fe696ae0c27007c94625"
        ]
    },
    {
        "id": "337eb9cea869ba22b7d684b34642fb6d90e32997",
        "title": "Illumination Invariant Face Recognition with Whitening Filter and Local Binary Patterns",
        "abstract": "An approach for the illumination invariant face recognition, which combines 2D spatial whitening filter with the LBP descriptor is presented, which proves the out performance of the proposed method compared to other existing methods. In this paper we present an approach for the illumination invariant face recognition, which combines 2D spatial whitening filter with the LBP descriptor. Firstly, we preprocess each face image by applying whitening filter in the logarithm domain. Then LBP features are extracted based on the filtered images. After representing image with a LBP operator, the image will be divided into non-overlapped blocks and histograms are used to represent each block. Finally the concatenated histograms are used for recognition. The experimental results on the Yale B and Extended Yale B prove the out performance of the proposed method compared to other existing methods.",
        "publication_year": "2013",
        "authors": [
            "Shun Li",
            "Fei Long",
            "Xuefeng Cheng"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "16",
        "references": [
            "/paper/Recognizing-human-faces-under-varying-degree-of-A-Pai-Fernandes/928b6bc4492740e8eba83366e9b21eb47ba59067",
            "/paper/Face-Recognition-with-Local-Binary-Patterns-Ahonen-Hadid/32420c65f8ef0c5bd83b14c8ae662cbce73e6781",
            "/paper/Illumination-robust-face-recognition-using-retina-Vu-Caplier/5c624382057b55e46af4dc4c055a33c90e8bf08a",
            "/paper/Illumination-compensation-and-normalization-for-in-Chen-Er/d5dbf02dfbf78f0bdd447952a87da06b58e97499",
            "/paper/A-Novel-Local-Illumination-Normalization-Approach-Lian-Er/a3609d94339367b5bbd7a1d1f6a02af7d07eb8f2",
            "/paper/An-Optimized-Illumination-Normalization-Method-for-Holappa-Ahonen/a964903746a1067984fb1881438f7e5cc6468a3d",
            "/paper/Enhanced-Local-Texture-Feature-Sets-for-Face-Under-Tan-Triggs/a99e76f6debc3eb6b8f654321a6f76200bc1086d",
            "/paper/Face-recognition-under-varying-lighting-conditions-Wang-Li/ecac3da2ff8bc2ba55981467f7fdea9de80e2092",
            "/paper/Illumination-normalization-for-robust-face-against-Shan-Gao/4a18adc7f5a090a041528a88166671248703f6e0",
            "/paper/Face-Recognition%3A-The-Problem-of-Compensating-for-Adini-Moses/59e9ef8b61182acace9e37f41f9c2a03db69c15b",
            "/paper/From-Few-to-Many%3A-Illumination-Cone-Models-for-Face-Georghiades-Belhumeur/6642e9c6cf7432e2d11b7edf7cd47f1285acd54e"
        ]
    },
    {
        "id": "3a24360c0566bfc3cfe301a91aba8a47f13b2756",
        "title": "Towards General Purpose Object Detection: Deep Dense Grid Based Object Detection",
        "abstract": "A fine grained and equally spaced dense grid cells throughout an input image be responsible of detecting an object and increase YOLOv2\u2019s performance on Pascal VOC 2007 and COCO datasets by +2.3% and +7.2% mean average precision (mAP) respectively. Object detection is one of the most challenging and very important branch of computer vision. Some of the challenging aspect of a detection network is the fact that an object can appear anywhere in the image, be partially occluded by another object, might appear in crowd or have greatly varying scales. Consequently, we propose a fine grained and equally spaced dense grid cells throughout an input image be responsible of detecting an object. We re-purpose an already existing deep state-of-the-art detector or classifier into deep and dense detector. Our dense object detector uses binary class encoding and hence suitable for very large multi-class object detector. We also propose a more flexible and robust non-max suppression implementation to filter out redundant detection of same object. As a result of our dense object detection implementation we have managed to increase YOLOv2\u2019s performance on Pascal VOC 2007 and COCO datasets by +2.3% and +7.2% mean average precision (mAP) respectively.",
        "publication_year": "2020",
        "authors": [
            "Solomon Negussie Tesema",
            "E. Bourennane"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "2",
        "reference_count": "24",
        "references": [
            "/paper/Multi-Grid-Redundant-Bounding-Box-Annotation-for-Tesema-Bourennane/a9d28021588042becb0ab76d0bc74c2d7aaf075a",
            "/paper/Resource-and-Power-Efficient-High-Performance-Using-Tesema-Bourennane/3ed92c6061ed85fc8e57b84ddd9e4e2d19e2857e",
            "/paper/Focal-Loss-for-Dense-Object-Detection-Lin-Goyal/72564a69bf339ff1d16a639c86a764db2321caab",
            "/paper/A-Survey-of-Deep-Learning-Based-Object-Detection-Jiao-Zhang/dfbeb3ca7a01fe80c49b76baa50bf092f71eef4a",
            "/paper/You-Only-Look-Once%3A-Unified%2C-Real-Time-Object-Redmon-Divvala/f8e79ac0ea341056ef20f2616628b3e964764cfd",
            "/paper/Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue/2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "/paper/Feature-Pyramid-Networks-for-Object-Detection-Lin-Doll%C3%A1r/b9b4e05faa194e5022edd9eb9dd07e3d675c2b36",
            "/paper/Rapid-object-detection-using-a-boosted-cascade-of-Viola-Jones/dc6ea0e30e46163b706f2f8bdc9c67ca87f83d63",
            "/paper/SSD%3A-Single-Shot-MultiBox-Detector-Liu-Anguelov/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
            "/paper/Training-Region-Based-Object-Detectors-with-Online-Shrivastava-Gupta/63333669bcf694aba2e1928f6060ab1d6a5161fe",
            "/paper/Faster-R-CNN%3A-Towards-Real-Time-Object-Detection-Ren-He/424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "/paper/Soft-NMS-%E2%80%94-Improving-Object-Detection-with-One-Line-Bodla-Singh/53c0aa8d33d240197caff824a6225fb223c1181c"
        ]
    },
    {
        "id": "10a19896e6c524535a75b4c940a4e0cb6692920c",
        "title": "Evaluation of Clustering Configurations for Object Retrieval Using SIFT Features",
        "abstract": "Different configurations for clustering sets of keypoints according to their pose parameters are presented and evaluated: x and y coordinates location, scale and orientation based on Lowe\u2019s approach. Scale-Invariant Feature Transform (SIFT) features have been widely accepted as an effective local keypoint descriptor for its robust description of digital image content. This method extracts distinctive invariant features from images that can be used to perform reliable matching between different views of an object. Object recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbour algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. Nonetheless, reasoning for the choice of this clustering approach is not provided and a lack of its theoretical insight is noticed. Here, we present and evaluate different configurations for clustering sets of keypoints according to their pose parameters: x and y coordinates location, scale and orientation based on Lowe\u2019s approach.",
        "publication_year": "2015",
        "authors": [
            "Laura Fern\u00e1ndez-Robles",
            "M. Castej\u00f3n-Limas",
            "Javier Alfonso-Cend\u00f3n",
            "Enrique Alegre"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "3",
        "reference_count": "16",
        "references": [
            "/paper/Recognition-and-retrieval-of-objects-in-diverse-Fern%C3%A1ndez-Robles/e500eb7c2e630f7faae7d39f4cede2bc6ffd1497",
            "/paper/Application-of-Multi-Camera-Tracking-and-Technology-Zhang-Tian/00a5d73ae7b457cce5e8497aea19ac26551646ed",
            "/paper/Object-Detection-for-Crime-Scene-Evidence-Analysis-Saikia-FIDALGO/d9cc8bc5c4a4b29ab40f75b721bd9e5140d2baf6",
            "/paper/Distinctive-Image-Features-from-Scale-Invariant-Lowe/8c04f169203f9e55056a6f7f956695babe622a38",
            "/paper/Object-recognition-from-local-scale-invariant-Lowe/f9f836d28f52ad260213d32224a6d227f8e8849a",
            "/paper/BRISK%3A-Binary-Robust-invariant-scalable-keypoints-Leutenegger-Chli/86d27422aac2398cfe132ae8e312a6f2d190f754",
            "/paper/SURF%3A-Speeded-Up-Robust-Features-Bay-Tuytelaars/490020c0d4fa1eb85fe353add5713e49f08c628d",
            "/paper/Fast-Keypoint-Recognition-Using-Random-Ferns-%C3%96zuysal-Calonder/3ea54da8ba5deba902e34199b63956edf68906ff",
            "/paper/ORB%3A-An-efficient-alternative-to-SIFT-or-SURF-Rublee-Rabaud/976e29fe6c9baeb39732bca0e35f66f84d5bdd90",
            "/paper/Machine-Learning-for-High-Speed-Corner-Detection-Rosten-Drummond/e0408181bccb7e3754dd5e6785ec47d8beb8b6bd",
            "/paper/FREAK%3A-Fast-Retina-Keypoint-Alahi-Ortiz/bfe2e9b42cca691dcddca8ca90a0d75c67cead58",
            "/paper/Adaptive-and-Generic-Corner-Detection-Based-on-the-Mair-Hager/8cda08ead0ae18b552f69469fcbf68099e498a9c",
            "/paper/BRIEF%3A-Binary-Robust-Independent-Elementary-Calonder-Lepetit/2145f5cbac48df8ed9f7695606c34e98b26cf5a9"
        ]
    },
    {
        "id": "e55840992c735daa4b18fe80af7ab19d94bbbaad",
        "title": "Seeking Visual Discomfort: Curiosity-driven Representations for Reinforcement Learning",
        "abstract": "The proposed method enhances the exploration capability of RL algorithms, by taking advantage of the SRL setup, and contributes to stabilizing the training, reducing the reward variance, and improving sample efficiency. Vision-based reinforcement learning (RL) is a promising approach to solve control tasks involving images as the main observation. State-of-the-art RL algorithms still struggle in terms of sample efficiency, especially when using image observations. This has led to increased attention on integrating state representation learning (SRL) techniques into the RL pipeline. Work in this field demonstrates a substantial improvement in sample efficiency among other benefits. However, to take full advantage of this paradigm, the quality of samples used for training plays a crucial role. More importantly, the diversity of these samples could affect the sample efficiency of vision-based RL, but also its generalization capability. In this work, we present an approach to improve sample diversity for state representation learning. Our method enhances the exploration capability of RL algorithms, by taking advantage of the SRL setup. Our experiments show that our proposed approach boosts the visitation of problematic states, improves the learned state representation, and outperforms the baselines for all tested environments. These results are most apparent for environments where the baseline methods struggle. In simple environments, our method contributes to stabilizing the training, reducing the reward variance, and improving sample efficiency.",
        "publication_year": "2021",
        "authors": [
            "Elie Aljalbout",
            "Maximilian Ulmer",
            "Rudolph Triebel"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "42",
        "references": [
            "/paper/An-Information-Theoretic-Perspective-on-Intrinsic-A-Aubret-Matignon/c64a99ec780d5e1c6ba242a82e72e96eb0c3ef8a",
            "/paper/CURL%3A-Contrastive-Unsupervised-Representations-for-Srinivas-Laskin/79e14a09ff070e06ab9df598ccd885b929164ef9",
            "/paper/Soft-Actor-Critic%3A-Off-Policy-Maximum-Entropy-Deep-Haarnoja-Zhou/811df72e210e20de99719539505da54762a11c6d",
            "/paper/Integrating-State-Representation-Learning-Into-Deep-Bruin-Kober/84bb62e3f40434a1e367d24783bd81432a5396d6",
            "/paper/Making-Curiosity-Explicit-in-Vision-based-RL-Aljalbout-Ulmer/72ec17c5be2b6200a92319708dbf6d1ad89544d1",
            "/paper/State-Entropy-Maximization-with-Random-Encoders-for-Seo-Chen/0292d2f80b52bda7cb5c2432ea35bc3af6822a8a",
            "/paper/Robust-Policies-via-Mid-Level-Visual-An-Study-in-Chen-Sax/e6486c08c54133ed92dad42dd41f277c047cd843",
            "/paper/Learning-Vision-based-Reactive-Policies-for-Aljalbout-Chen/bf979b536cbc8c87e58e52ddb0653b241ef8afd3",
            "/paper/Decoupling-Representation-Learning-from-Learning-Stooke-Lee/17985b57240bfaea02a6098a7a34e71e780180eb",
            "/paper/Learning-Precise-3D-Manipulation-from-Multiple-Akinola-Varley/44ef2860f3778dadbc373cad901b4ac647b46f57"
        ]
    },
    {
        "id": "4732ef442a5d0f2ec77600ddd44aaa288a043ace",
        "title": "Generating Classification Weights With GNN Denoising Autoencoders for Few-Shot Learning",
        "abstract": "This work proposes the use of a Denoising Autoencoder network that takes as input a set of classification weights corrupted with Gaussian noise and learns to reconstruct the target-discriminative classification weights, and proposes to implement the DAE model as a Graph Neural Network (GNN). Given an initial recognition model already trained on a set of base classes, the goal of this work is to develop a meta-model for few-shot learning. The meta-model, given as input some novel classes with few training examples per class, must properly adapt the existing recognition model into a new model that can correctly classify in a unified way both the novel and the base classes. To accomplish this goal it must learn to output the appropriate classification weight vectors for those two types of classes. To build our meta-model we make use of two main innovations: we propose the use of a Denoising Autoencoder network (DAE) that (during training) takes as input a set of classification weights corrupted with Gaussian noise and learns to reconstruct the target-discriminative classification weights. In this case, the injected noise on the classification weights serves the role of regularizing the weight generating meta-model. Furthermore, in order to capture the co-dependencies between different classes in a given task instance of our meta-model, we propose to implement the DAE model as a Graph Neural Network (GNN). In order to verify the efficacy of our approach, we extensively evaluate it on ImageNet based few-shot benchmarks and we report state-of-the-art results.",
        "publication_year": "2019",
        "authors": [
            "Spyros Gidaris",
            "N. Komodakis"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "194",
        "reference_count": "44",
        "references": [
            "/paper/LDAnet%3Aa-discriminant-subspace-for-metric-based-Chen-Liu/25e16606d42829610a0c35a89af5fda609519f5a",
            "/paper/Self-Augmentation%3A-Generalizing-Deep-Networks-to-Seo-Jung/ac2d8263981eca1026a7eb8c7c8806f669097f9d",
            "/paper/Self-Denoising-Neural-Networks-for-Few-Shot-Schwarcz-Rambhatla/55345706d97e75209bd4f5d4c4effc84e6280724",
            "/paper/Hierarchically-Structured-Network-With-Attention-Xiong-Gong/f5c5c6666729e0563b6bdde24f76d369160dc89a",
            "/paper/AdarGCN%3A-Adaptive-Aggregation-GCN-for-Few-Shot-Zhang-Zhang/c5a5a2c5a4c5bdc490c8d69f1005b7ba88e9be20",
            "/paper/Mixture-based-Feature-Space-Learning-for-Few-shot-Afrasiyabi-Lalonde/c38af920ef584336a9b3b454165f6b4116459fbf",
            "/paper/Boosting-Few-Shot-Classification-with-Contrastive-Luo-Chen/02262e27b1b31779e987bb1ca1f875ad52e6167d",
            "/paper/Class-Discriminative-Feature-Embedding-For-based-Rahimpour-Qi/034cfd74bf49688decde034c81fa3f8b71b5b424",
            "/paper/Loss-Architecture-Search-for-Few-Shot-Object-Yue-Miao/8b35b6a80b3abcb7734166a2eb8e46218ab02e26",
            "/paper/Few-Shot-Learning-as-Domain-Adaptation%3A-Algorithm-Guan-Lu/9739cd8149a9a659d8e637ccca3da104d05fbe19",
            "/paper/Meta-Learning-for-Semi-Supervised-Few-Shot-Ren-Triantafillou/df093d69cd98cf4b26542f53614a79754754eb78",
            "/paper/Transductive-Propagation-Network-for-Few-shot-Liu-Lee/9df950fd79304000dfd23c7c497e8912a613027c",
            "/paper/Siamese-Neural-Networks-for-One-Shot-Image-Koch/f216444d4f2959b4520c61d20003fa30a199670a",
            "/paper/Optimization-as-a-Model-for-Few-Shot-Learning-Ravi-Larochelle/29c887794eed2ca9462638ff853e6fe1ab91d5d8",
            "/paper/Low-Shot-Learning-with-Imprinted-Weights-Qi-Brown/f1951f3c86493542be597182d194abcd7b936b9b",
            "/paper/Learning-to-Compare%3A-Relation-Network-for-Few-Shot-Sung-Yang/bfe284e4338e62f0a61bb33398353efd687f206f",
            "/paper/Few-Shot-Image-Recognition-by-Predicting-Parameters-Qiao-Liu/3e08a3912ebe494242f6bcd772929cc65307129c",
            "/paper/Model-Agnostic-Meta-Learning-for-Fast-Adaptation-of-Finn-Abbeel/c889d6f98e6d79b89c3a6adf8a921f88fa6ba518",
            "/paper/Dynamic-Few-Shot-Visual-Learning-Without-Forgetting-Gidaris-Komodakis/a40f97770296c7fca2e5361cbceba3f4aae399e0",
            "/paper/Matching-Networks-for-One-Shot-Learning-Vinyals-Blundell/be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6"
        ]
    },
    {
        "id": "4881c8c48881f28b7d29226f1a54f38b71481048",
        "title": "Unsupervised Video Summarization with a Convolutional Attentive Adversarial Network",
        "abstract": "Semantic Scholar extracted view of \"Unsupervised Video Summarization with a Convolutional Attentive Adversarial Network\" by Guoqiang Liang et al.",
        "publication_year": "2021",
        "authors": [
            "Guoqiang Liang",
            "Yanbing Lv",
            "Shucheng Li",
            "Shizhou Zhang",
            "Yanning Zhang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "5",
        "reference_count": "34",
        "references": [
            "/paper/SUM-GAN-GEA%3A-Video-Summarization-Using-GAN-with-and-Yu-Yu/1dadedb8535f372af847d11ea7d5de767496eeeb",
            "/paper/A-comprehensive-study-of-automatic-video-techniques-Gupta-Sharma/abe4ea0e1f1d04288406818048bc3f973d930ec9",
            "/paper/Summarization-of-Videos-with-the-Signature-Curt%C3%B2-Zarz%C3%A0/9a91b85f304030e8a290120d53aa9a9d32604538",
            "/paper/Locating-X-ray-coronary-angiogram-keyframes-via-Zhang-Qin/ea0da5577d3b1dfc544d6668963fe6ca5051bd64",
            "/paper/Topic-aware-video-summarization-using-multimodal-Zhu-Zhao/094e35564f937c1cefc2fa68e8d2fadd5363e95d",
            "/paper/Unsupervised-Video-Summarization-with-Attentive-He-Hua/81b4f76a767b4393b2db876e3b7fd85d3f7cd820",
            "/paper/Unsupervised-Video-Summarization-with-Adversarial-Mahasseni-Lam/620fe6c786d15efca7f553ad70f295e2b693b391",
            "/paper/Unsupervised-Video-Summarization-With-Adversarial-Yuan-Tay/f5ddbb4c316cab504edbe2306cb8753ec6efaa6f",
            "/paper/Video-Summarization-With-Attention-Based-Networks-Ji-Xiong/88a8baa1be5292e62622f1cb8e627fbf759bf741",
            "/paper/Discriminative-Feature-Learning-for-Unsupervised-Jung-Cho/69b3b29c6fabaea88d382922346d9157395a3226",
            "/paper/Video-Summarization-by-Learning-Relationships-and-Park-Lee/62d6e66c6a97540064c3de51b455cdc8fd7f0bdc",
            "/paper/Exploring-global-diverse-attention-via-pairwise-for-Li-Ye/e3b914637e63c88eec0721e41eca195921476e94",
            "/paper/Video-Summarization-with-Long-Short-Term-Memory-Zhang-Chao/1dbc12e54ceb70f2022f956aa0a46e2706e99962",
            "/paper/Video-Summarization-via-Semantic-Attended-Networks-Wei-Ni/6ec09bad57cc81a71ef7596f57e94ee13b380ae3",
            "/paper/Video-Summarization-Using-Fully-Convolutional-Rochan-Ye/8c413c2ee66664909d8c194f3f3e08c5f109c3c1"
        ]
    },
    {
        "id": "cad2fb5036f2de25defd13158d05349a382152f1",
        "title": "Robust Visual Tracking via Constrained Multi-Kernel Correlation Filters",
        "abstract": "This paper proposes a novel Constrained Multi-Kernel Correlation tracking Filter (CMKCF), which applies spatial constraints to address the unwanted boundary effects in the kernelized correlation models. Discriminative Correlation Filter (DCF) based trackers are quite efficient in tracking objects by exploiting the circulant structure. The kernel trick further improves the performance of such trackers. The unwanted boundary effects, however, are difficult to solve in the kernelized correlation models. In this paper, we propose a novel Constrained Multi-Kernel Correlation tracking Filter (CMKCF), which applies spatial constraints to address this drawback. We build the multi-kernel models for multi-channel features with three different attributes, and then employ a spatial cropping operator on the semi-kernel matrix to address the boundary effects. For the constrained optimization solution, we develop an Alternating Direction Method of Multipliers (ADMM) based algorithm to learn our multi-kernel filters efficiently in the frequency domain. In particular, we suggest an adaptive updating mechanism by exploiting the feedback from high-confidence tracking results to avoid corruption in the model. Extensive experimental results demonstrate that the proposed method performs favorably on OTB-2013, OTB-2015, VOT-2016 and VOT-2018 dataset against several state-of-the-art methods.",
        "publication_year": "2020",
        "authors": [
            "Bo Huang",
            "Tingfa Xu",
            "Shenwang Jiang",
            "Yiwen Chen",
            "Yu Bai"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "20",
        "reference_count": "48",
        "references": [
            "/paper/Correlation-Tracking-via-Spatial-Temporal-and-Tian-Zang/e501195e993da1ea8d8b7c954bc2f7b4e3e0035f",
            "/paper/Improving-Object-Tracking-by-Added-Noise-and-Fiaz-Mahmood/93742493c26652d4a95627de699869795698a554",
            "/paper/Learning-Soft-Mask-Based-Feature-Fusion-with-and-Fiaz-Mahmood/9fe233b41ddb40188187dc7c4122aa59f6809ae8",
            "/paper/ARTracker%3A-Compute-a-More-Accurate-and-Robust-for-Chen-Xu/e2314ae29f51774c5ddd53a0e465e5a781110e51",
            "/paper/Foreground-aware-Siamese-tracker-with-dynamic-in-Guo-Xu/775db57b474720b0fa224bf5fd8affdaa81590bf",
            "/paper/Principal-sample-based-learning-of-deep-network-for-Rinosha-GethsiyalAugasta/85471b730a5ad2cbd396bf0295b469d4d874d34a",
            "/paper/A-Reliable-Sample-Selection-Strategy-for-Weakly-Liu-Xu/95f085e640934b608f5d0ed27cdd9249a9e39e05",
            "/paper/BSCF%3A-Learning-background-suppressed-correlation-Huang-Xu/81d092ae2a3a0eb1ea62ef101abc164dc6c84612",
            "/paper/Learning-Spatio-Temporal-Attention-Based-Siamese-in-Chen-Huang/01c253cb49860cf8658be52ae22a185a087b8699",
            "/paper/Efficient-Anchor-Learning-based-Multi-view-A-Late-Zhang-Liu/ee5c4a295c8cb5842b541df5054129aac5460bb9",
            "/paper/Multi-kernel-Correlation-Filter-for-Visual-Tracking-Tang-Feng/d767886def23657557f23c4eb7ede399a13085ee",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/High-Speed-Tracking-with-Kernelized-Correlation-Henriques-Caseiro/65c9b4b1d49f46b3f8f64a5f617acfc14f85d031",
            "/paper/Large-Margin-Object-Tracking-with-Circulant-Feature-Wang-Liu/ece7625a346edbc5f6fab541c0c246ec06939121",
            "/paper/High-Speed-Tracking-with-Multi-kernel-Correlation-Tang-Yu/0dccbfe5a91e1d5610c46585270af0c648d2aa25",
            "/paper/A-Scale-Adaptive-Kernel-Correlation-Filter-Tracker-Li-Zhu/0cae491292feccbc9ad1d864cf8b7144923ce6de",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "/paper/Discriminative-Correlation-Filter-with-Channel-and-Luke%C5%BEi%C4%8D-Voj%C3%ADr/b16a583ee173f222c690242aaff7925838893fe8",
            "/paper/Multi-Correlation-Filters-With-Triangle-Structure-Ruan-Chen/326aa3eac826a9f5be5d6cc087f36e8ca0ad8556",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1"
        ]
    },
    {
        "id": "21375e2f1dec58df23d4b721b9cb2f31564a76fd",
        "title": "Learning discriminative correlation filters via saliency-aware channel selection for robust visual object tracking",
        "abstract": "This work researches the correlation between multi-channel deep features and target saliency information and proposes a novel DCF tracking method based on saliency-aware and adaptive channel selection and uses the alternating direction method of multipliers (ADMM) to optimize the proposed tracker model. In recent years, discriminative correlation filters (DCF) with deep features have achieved excellent results in visual object tracking tasks. These trackers usually use multi-channel features of the fixed layer of the pre-trained network model to represent the target. However, the multi-channel features contain many interfering channels that are not conducive to object representation, resulting in overfitting and high computational complexity. To solve this problem, we research the correlation between multi-channel deep features and target saliency information and propose a novel DCF tracking method based on saliency-aware and adaptive channel selection. Specifically, we adaptively select the most representative feature channels to represent the target by calculating the energy mean ratio of the saliency-aware region to the search region, reducing the feature dimension and improving the tracking efficiency. Then, according to the feedback, the selected channels are given different weights to further enhance the discrimination of the filter. In addition, an adaptive update strategy is designed to alleviate the model degradation problem according to the fluctuation of feature maps in the recent frames. Finally, we use the alternating direction method of multipliers (ADMM) to optimize the proposed tracker model. Extensive experimental results on five well-known tracking benchmark datasets have verified the superiority of the proposed tracker with many state-of-the-art deep features-based trackers, and the running speed of the algorithm can basically meet the real-time requirements.",
        "publication_year": "2023",
        "authors": [
            "Sugang Ma",
            "Zhixian Zhao",
            "Lei Pu",
            "Zhiqiang Hou",
            "Lei Zhang",
            "Xiang Zhao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "58",
        "references": [
            "/paper/Robust-visual-tracking-via-adaptive-feature-channel-Ma-Zhang/ce00552254e1dc200548d5eb885d71f9c3c97098",
            "/paper/Visual-tracking-via-dynamic-saliency-discriminative-Gao-Liu/bbc97484fe622da1e07cf5816ecd8e6b32235d78",
            "/paper/Adaptive-Channel-Selection-for-Robust-Visual-Object-Xu-Feng/08dda937ef87aa6933fd40d6dde18b909ef78d85",
            "/paper/Robust-visual-tracking-via-spatio-temporal-adaptive-Liang-Liu/5dc3b582ebd95f029b1daa8622cf56bb192ef018",
            "/paper/Learning-Adaptive-Discriminative-Correlation-via-Xu-Feng/bd5ecaea14eeb27c776b2c153b3e4716448bad4e",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Non-rigid-Object-Tracking-via-Deep-Multi-scale-Maps-Zhang-Wang/75272d131929bd4dfab7beedc4d1b34647644181",
            "/paper/Joint-Channel-Reliability-and-Correlation-Filters-Du-Liu/1d317427f8008adfab927be909f5a1836f687c9b",
            "/paper/Discriminative-Correlation-Filter-with-Channel-and-Luke%C5%BEi%C4%8D-Voj%C3%ADr/b16a583ee173f222c690242aaff7925838893fe8",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c"
        ]
    },
    {
        "id": "6be1c27bb9f7f45854af8dcd4b2d3217d6bd71c5",
        "title": "Detection of Copy-Move Forgery in Digital Images",
        "abstract": "This paper investigates the problem of detecting the copy-move forgery and describes an efficient and reliable detection method that may successfully detect the forged part even when the copied area is enhanced/retouched to merge it with the background and when the forged image is saved in a lossy format, such as JPEG. Digital images are easy to manipulate and edit due to availability of powerful image processing and editing software. Nowadays, it is possible to add or remove important features from an image without leaving any obvious traces of tampering. As digital cameras and video cameras replace their analog counterparts, the need for authenticating digital images, validating their content, and detecting forgeries will only increase. Detection of malicious manipulation with digital images (digital forgeries) is the topic of this paper. In particular, we focus on detection of a special type of digital forgery \u2013 the copy-move attack in which a part of the image is copied and pasted somewhere else in the image with the intent to cover an important image feature. In this paper, we investigate the problem of detecting the copy-move forgery and describe an efficient and reliable detection method. The method may successfully detect the forged part even when the copied area is enhanced/retouched to merge it with the background and when the forged image is saved in a lossy format, such as JPEG. The performance of the proposed method is demonstrated on several forged images. 1. The Need for Detection of Digital Forgeries The availability of powerful digital image processing programs, such as PhotoShop, makes it relatively easy to create digital forgeries from one or multiple images. An example of a digital forgery is shown in Figure 1. As the newspaper cutout shows, three different photographs were used in creating the composite image: Image of the White House, Bill Clinton, and Saddam Hussein. The White House was rescaled and blurred to create an illusion of an out-of-focus background. Then, Bill Clinton and Saddam were cut off from two different images and pasted on the White House image. Care was taken to bring in the speaker stands with microphones while preserving the correct shadows and lighting. Figure 1 is, in fact, an example of a very realisticlooking forgery. Another example of digital forgeries was given in the plenary talk by Dr. Tomaso A. Poggio at Electronic Imaging 2003 in Santa Clara. In his talk, Dr. Poggio showed how engineers can learn the lip movements of any person from a short video clip and then digitally manipulate the lips to arbitrarily alter the spoken content. In a nice example, a video segment showing a TV anchor announcing evening news was altered to make the anchor appear singing a popular song instead, while preserving the match between the sound and lip movement. The fact that one can use sophisticated tools to digitally manipulate images and video to create non-existing situations threatens to diminish the credibility and value of video tapes and images presented as evidence in court independently of the fact whether the video is in a digital or analog form. To tamper an analogue video, one can easily digitize the analog video stream, upload it into a computer, perform the forgeries, and then save the result in the NTSC format on an ordinary videotape. As one can expect, the situation will only get worse as the tools needed to perform the forgeries will move from research labs to commercial software. Figure 1 Example of a digital forgery. Despite the fact that the need for detection of digital forgeries has been recognized by the research community, very few publications are currently available. Digital watermarks have been proposed as a means for fragile authentication, content authentication, detection of tampering, localization of changes, and recovery of original content [1]. While digital watermarks can provide useful information about the image integrity and its processing history, the watermark must be present in the image before the tampering occurs. This limits their application to controlled environments that include military systems or surveillance cameras. Unless all digital acquisition devices are equipped with a watermarking chip, it will be unlikely that a forgery-inthe-wild will be detectable using a watermark. It might be possible, but very difficult, to use unintentional camera \u201cfingerprints\u201d related to sensor noise, its color gamut, and/or its dynamic range to discover tampered areas in images. Another possibility for blind forgery detection is to classify textures that occur in natural images using statistical measures and find discrepancies in those statistics between different portions of the image ([2], [3]). At this point, however, it appears that such approaches will produce a large number of missed detections as well as false positives. In the next section, we introduce one common type of digital forgeries \u2013 the copy-move forgery \u2013 and show a few examples. Possible approaches to designing a detector are discussed in Section 3. In Section 4, we describe the detection method based on approximate block matching. This approach proved to be by far the most reliable and efficient. The method is tested in the last Section 5 on a few forgeries. In the same section, we summarize the paper and outline future research directions. 2. Copy-Move Forgery Because of the extraordinary difficulty of the problem and its largely unexplored character, the authors believe that the research should start with categorizing forgeries by their mechanism, starting with the simple ones, and analyzing each forgery type separately. In doing so, one will build a diverse Forensic Tool Set (FTS). Even though each tool considered separately may not be reliable enough to provide sufficient evidence for a digital forgery, when the complete set of tools is used, a human expert can fuse the collective evidence and hopefully provide a decisive answer. In this paper, the first step towards building the FTS is taken by identifying one very common class of forgeries, the Copy-Move forgery, and developing efficient algorithms for its detection. In a Copy-Move forgery, a part of the image itself is copied and pasted into another part of the same image. This is usually performed with the intention to make an object \u201cdisappear\u201d from the image by covering it with a segment copied from another part of the image. Textured areas, such as grass, foliage, gravel, or fabric with irregular patterns, are ideal for this purpose because the copied areas will likely blend with the background and the human eye cannot easily discern any suspicious artifacts. Because the copied parts come from the same image, its noise component, color palette, dynamic range, and most other important properties will be compatible with the rest of the image and thus will not be detectable using methods that look for incompatibilities in statistical measures in different parts of the image. To make the forgery even harder to detect, one can use the feathered crop or the retouch tool to further mask any traces of the copied-and-moved segments. Examples of the Copy-Move forgery are given in Figures 2\u20134. Figure 2 is an obvious forgery that was created solely for testing purposes. In Figure 3, you can see a less obvious forgery in which a truck was covered with a portion of the foliage left of the truck (compare the forged image with its original). It is still not too difficult to identify the forged area visually because the original and copied parts of the foliage bear a suspicious similarity. Figure 4 shows another Copy-Move forgery that is much harder to identify visually. This image has been sent to the authors by a third party who did not disclose the nature or extent of the forgery. We used this image as a real-life test for evaluating our detection tools. A visual inspection of the image did not reveal the presence of anything suspicious. Figure 2 Test image \u201cHats\u201d. Figure 3 Forged test image \u201cJeep\u201d (above) and its original version (below). Figure 4 Test image \u201cGolf\u201d with an unknown original. 3. Detection of Copy-Move Forgery Any Copy-Move forgery introduces a correlation between the original image segment and the pasted one. This correlation can be used as a basis for a successful detection of this type of forgery. Because the forgery will likely be saved in the lossy JPEG format and because of a possible use of the retouch tool or other localized image processing tools, the segments may not match exactly but only approximately. Thus, we can formulate the following requirements for the detection algorithm: 1. The detection algorithm must allow for an approximate match of small image segments 2. It must work in a reasonable time while introducing few false positives (i.e., detecting incorrect matching areas). 3. Another natural assumption that should be accepted is that the forged segment will likely be a connected component rather than a collection of very small patches or individual pixels. In this section, two algorithms for detection of the Copy-Move forgery are developed \u2013 one that uses an exact match for detection and one that is based on an approximate match. Before describing the best approach based on approximate block matching that produced the best balance between performance and complexity, two other approaches were investigated \u2013 Exhaustive search and Autocorrelation. 3.1 Exhaustive search This is the simplest (in priciple) and most obvious approach. In this method, the image and its circularly shifted version (see Figure 5) are overlaid looking for closely matching image segments. Let us assume that xij is the pixel value of a grayscale image of size M\u00d7N at the position i, j. In the exhaustive search, the following differences are examined: | xij \u2013 xi+k mod(M) j+l mod(N) |, k = 0, 1, ..., M\u20131, l = 0, 1, ..., N\u20131 for all i and j. It is easy to see that comparing xij with its cyclical shift [k,l] is the same as comparing xij with its cyclical shift [k\u2019,l\u2019], where k\u2019=M\u2013k and l\u2019=N\u2013l. Thus, it suffices to inspect only those shifts [k,l] with 1\u2264 k \u2264 M/2, 1\u2264 l \u2264 N/2, thus cutting the computational complexity by a factor of 4. Figure 5 Test image \u201cLenna\u201d and its circular shift. For each shift [k,l], the differences \u2206xij = | xij \u2013 xi+k mod",
        "publication_year": "2004",
        "authors": [
            "J. Fridrich"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "1,237",
        "reference_count": "3",
        "references": [
            "/paper/Copy-Move-Attack-Forgery-Detection-by-Using-SIFT-Kudke-Gawande/90b5127d4becbeffa54ffc345aa6233d4fd1d4ec",
            "/paper/A-REVIEW-PAPER-ON-COPY-MOVE-FORGERY-DETECTION-Kaur/6104c946b2788d739d00142d2daf6879d1ce9f93",
            "/paper/Image-Forgery-and-Detection-of-Copy-Move-Forgery-in-Kaur/cacf3d30d56310b9e10a3089a20d6ee19556ce78",
            "/paper/Detection-of-Copy-Move-Forgery-in-Images-Using-and-Manu-Mehtre/d7547fd8db63a8f81bdf683f90602a4e5b103803",
            "/paper/Passive-Image-Forensic-Method-to-detect-Copy-Move-Amtullah-Koul/371c52f6e4226476cffb111913a45038799e02b1",
            "/paper/Copy-Move-Forgery-Detection-Based-on-Discrete-and-Alazrak-Elsharkawy/ed4d266ea9ad5359af43f48902f73fa7c8418fc1",
            "/paper/Copy-Move-Forgery-Detection-Based-on-Discrete-and-Azrak-Elsharkawy/39fb9bd6342ec6380cb1a7e167555c607601acc6",
            "/paper/A-Scale-Invariant-Digital-Image-Copy-Paste-Forgery-Warbhe-Dharaskar/9e54819d026c5baeac952cf66d426fe378becfea",
            "/paper/Detection-of-Region-Duplication-in-Digital-Images%3AA-Wadhwa-Ahemad/3c7967db40af444242f8a8c42d7623fab5abe820",
            "/paper/Copy-Move-Image-Forgery-Detection-using-Block-and-Kohale/867833564bc8ed394d80b6235443f7e1a470f26a"
        ]
    },
    {
        "id": "2e613574b6c3a3422728d4478440d922162a4fa0",
        "title": "D2C: Diffusion-Denoising Models for Few-shot Conditional Generation",
        "abstract": "D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality, and achieves superior performance over state-of-the-art VAEs and diffusion models. Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire. This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models. On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% - 60% of the human evaluators in a double-blind study.",
        "publication_year": "2021",
        "authors": [
            "Abhishek Sinha",
            "Jiaming Song",
            "Chenlin Meng",
            "S. Ermon"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "66",
        "reference_count": "104",
        "references": [
            "/paper/Few-Shot-Diffusion-Models-Giannone-Nielsen/677979b8bdad4b2b694e459dee1fb7e8b843eadd",
            "/paper/Conditional-Generation-from-Unconditional-Diffusion-Graikos-Yellapragada/b14a3b26aa5884501ec0f66133d5c591ad29f8e2",
            "/paper/Few-shot-Image-Generation-with-Diffusion-Models-Zhu-Ma/2c525c0a0e058b0f0d0a351c1fd43fd92929433a",
            "/paper/Diffusion-Autoencoders%3A-Toward-a-Meaningful-and-Preechakul-Chatthee/b582edb16f5425642767cb6c26839111f867f4dc",
            "/paper/High-Resolution-Image-Synthesis-with-Latent-Models-Rombach-Blattmann/c10075b3746a9f3dd5811970e93c8ca3ad39b39d",
            "/paper/DiffuseVAE%3A-Efficient%2C-Controllable-and-Generation-Pandey-Mukherjee/ce8e3fa6fa6d45b8b92169a2e181dafb20749a2f",
            "/paper/Unsupervised-Representation-Learning-from-Diffusion-Zhang-Zhao/57655bdf9bfaf326ca73e5843edeb3e9f2c4d315",
            "/paper/GlueGen%3A-Plug-and-Play-Multi-modal-Encoders-for-Qin-Yu/32ff6ef526283742c0daafaa1fe454097ce88237",
            "/paper/Retrieval-Augmented-Diffusion-Models-Blattmann-Rombach/33f3f31f871070f19b0c3e967a24e322bfc178f2",
            "/paper/Optimizing-Hierarchical-Image-VAEs-for-Sample-Luhman-Luhman/cc795386ffe1402c9543c78752b6db99a1e9995c",
            "/paper/Generating-Diverse-High-Fidelity-Images-with-Razavi-Oord/6be216d93421bf19c1659e7721241ae73d483baf",
            "/paper/Plug-%26-Play-Generative-Networks%3A-Conditional-of-in-Nguyen-Clune/1b40fe1a9d25d5694c7ea40a57d0aaa2e2cd5dd1",
            "/paper/NCP-VAE%3A-Variational-Autoencoders-with-Noise-Priors-Aneja-Schwing/c97dfad7a023fbec97a901dae02b73e2e8e0fff1",
            "/paper/Variational-Lossy-Autoencoder-Chen-Kingma/590c9a1ff422b03477f7830b20609f212c85aa13",
            "/paper/Implicit-Generation-and-Generalization-in-Models-Du-Mordatch/6b66d111d2d6bcb35c546f6cffd4404990227d07",
            "/paper/FIGR%3A-Few-shot-Image-Generation-with-Reptile-Clou%C3%A2tre-Demers/4accba659540e97ffc8d8a3cf72fc894863069f6",
            "/paper/VAE-with-a-VampPrior-Tomczak-Welling/5ea2cdab68c69d7aef5a004495783ae7628193f2",
            "/paper/Large-Scale-GAN-Training-for-High-Fidelity-Natural-Brock-Donahue/22aab110058ebbd198edb1f1e7b4f69fb13c0613",
            "/paper/NVAE%3A-A-Deep-Hierarchical-Variational-Autoencoder-Vahdat-Kautz/853de0e00ac5ac257a622ae678ed373b8e086404",
            "/paper/Denoising-Diffusion-Implicit-Models-Song-Meng/014576b866078524286802b1d0e18628520aa886"
        ]
    },
    {
        "id": "46a6794b8888a2cf81b2aa16740d406cba4ad075",
        "title": "Detecting Dataset Drift and Non-IID Sampling via k-Nearest Neighbors",
        "abstract": "This work presents a straightforward statistical test to detect certain violations of the assumption that the data are Independent and Identically Distributed, and based on a k-Nearest Neighbors estimate, this approach can be used to audit any multivariate numeric data as well as other data types that can be numerically represented. We present a straightforward statistical test to detect certain violations of the assumption that the data are Independent and Identically Distributed (IID). The specific form of violation considered is common across real-world applications: whether the examples are ordered in the dataset such that almost adjacent examples tend to have more similar feature values (e.g. due to distributional drift, or attractive interactions between datapoints). Based on a k-Nearest Neighbors estimate, our approach can be used to audit any multivariate numeric data as well as other data types (image, text, audio, etc.) that can be numerically represented, perhaps with model embeddings. Compared with existing methods to detect drift or auto-correlation, our approach is both applicable to more types of data and also able to detect a wider variety of IID violations in practice. Code: https://github.com/cleanlab/cleanlab",
        "publication_year": "2023",
        "authors": [
            "Jesse Cummings",
            "El'ias Snorrason",
            "Jonas W. Mueller"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "31",
        "references": [
            "/paper/Machine-Learning-with-Interdependent-and-Data-Darrell-Kloft/186370ecc1f05ef8d3f611873a039fcde3af68b5",
            "/paper/Online-and-Non-Parametric-Drift-Detection-Methods-Blanco-Campo-%C3%81vila/5595903937bc3840c52aa0c5e201f8e68674c733",
            "/paper/The-Non-IID-Data-Quagmire-of-Decentralized-Machine-Hsieh-Phanishayee/206261db1196e4e391ca42077f6fca6b3ece34d0",
            "/paper/Accumulating-regional-density-dissimilarity-for-in-Liu-Lu/8a7ac3722e997dd1e86d79516f19df828a2c86d3",
            "/paper/KNN-Classifier-with-Self-Adjusting-Memory-for-Drift-Losing-Hammer/cfa845d67bace16179ad7d03bb62ce0692071298",
            "/paper/Concept-learning-using-one-class-classifiers-for-in-G%C3%B6z%C3%BCa%C3%A7ik-Can/ff1e8d3895641ce8157f6cc5d312e3a2837c997a",
            "/paper/Detecting-concept-drift-in-data-streams-using-model-Demsar-Bosni%C4%87/0ab3ff640041fa2aef5db911cf192a061905f8cd",
            "/paper/Multivariate-Two-Sample-Tests-Based-on-Nearest-Schilling/94c0012f7462ead19f310d26956b5cc5a68137b8",
            "/paper/RDDM%3A-Reactive-drift-detection-method-Barros-Cabral/ed591150ef3d34f0f790d6b545e3feb235e14d45",
            "/paper/Diversity-measure-as-a-new-drift-detection-method-Mahdi-Pardede/ec28cad6b19ee5fcdbb4192b25e6b0e879e70b92"
        ]
    },
    {
        "id": "6847d5107a4ea78c351a03fbb4d956285bd84f2d",
        "title": "Optimal Deep Transfer Learning Model for Automated Tuberculosis Classification on Chest Radiographs",
        "abstract": "An Optimal Deep Transfer Learning Model for Automated Tuberculosis Classification (ODTLATC) model is presented, which majorly concentrates on the identification of TB on chest radiographs and the improved performance of the ODTLatC model over other DL models is reported. Tuberculosis (TB) is the fifth leading cause of mortality rates across the world, adding nearly 10 million new cases and 1.5 million deaths annually. As TB caused by the bacteria that mostly affect the lungs is prevented and cured, the World Health Organization (WHO) reported a systematic and broad screening for eradicating the disease. Despite its interpretational difficulty and low specificity, poster anterior (PA) chest radiography becomes one of the preferred TB screening techniques. TB is majorly a disease in poor nations; thus, medical practitioners trained to interpret such CXRs were rare. Numerous computer-aided diagnosis (CAD) researches which deal with CXR abnormalities do not give more attention to other diseases (i.e., non-TB). This article devises an Optimal Deep Transfer Learning Model for Automated Tuberculosis Classification (ODTLATC) model. The presented ODTLATC model majorly concentrates on the identification of TB on chest radiographs. To attain this, the ODTLATC model follows a three-stage process such as pre-processing, feature extraction, and classification. At the initial stage, the ODTLATC model employs Weiner filtering (WF) approach for image denoising process. For feature extraction, deep convolutional neural network based residual network (ResNet50) model is utilized. At last, whale optimization algorithm (WOA) with bidirectional recurrent neural network (BiRNN) model is exploited for TB classification purposes. To demonstrate the better performance of the ODTLATC model, a extensive variety of simulations are conducted and the outcomes were inspected on chest radiographs. The comparative study reported the improved performance of the ODTLATC model over other DL models.",
        "publication_year": "2022",
        "authors": [
            "K. Manivannan",
            "S. Sathiamoorthy"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "23",
        "references": [
            "/paper/EfficientNet-Based-Convolutional-Neural-Networks-Ravi-Narasimhan/ac1ff733028f9cb175d9ccece59e851fd18c96bd",
            "/paper/Reliable-Tuberculosis-Detection-Using-Chest-X-Ray-Rahman-Khandakar/b00de2e18011659a26ffcc51c8e5f24d76efa8dd",
            "/paper/Benchmarking-Machine-Learning-Models-to-Assist-in-Barros-Alves/f8300891588a0a5808868af209bb3f4108548acb",
            "/paper/An-Efficient-Mixture-of-Deep-and-Machine-Learning-Al-timemy-Khushaba/d40d80d979971d20d4d05d53b909d4c55cad2eb4",
            "/paper/Artificial-Intelligence-in-Diagnosing-Tuberculosis%3A-Meraj-Yaakob/cf82c7d0f9b66c112dbce347a2855c9dc87d06c8",
            "/paper/Differentiating-between-drug-sensitive-and-with-for-Yang-Yu/837b7204754fef035a87e86540a2deeb53087553",
            "/paper/Automatic-cough-classification-for-tuberculosis-in-Pahar-Klopper/61890eb3bea4c813e0df2fa4c2e3f5259700b2e0",
            "/paper/Tuberculosis-detection-using-deep-learning-and-edge-Hwa-Bade/c8c68e3bee594341816c326fee6049aafb3d3378",
            "/paper/An-Ensemble-Algorithm-Based-on-Deep-Learning-for-Hern%C3%A1ndez-Panizo/2c67421fa7765108c6f90c3d5cfb3995df92accd",
            "/paper/Application-of-machine-learning-techniques-to-drug-Kouchaki-Yang/d204e2b55aa3f7e90b4e99f86177141715774c67"
        ]
    },
    {
        "id": "5e768672936864225e4cae6f1b8fb45e9f9b478d",
        "title": "Recent Progress in Transformer-based Medical Image Analysis",
        "abstract": "A review of the transformer in medical image analysis with the latest contents, detailed information, and task-modality organization mode that may greatly benefit the broad MIA community. The transformer is primarily used in the field of natural language processing. Recently, it has been adopted and shows promise in the computer vision (CV) field. Medical image analysis (MIA), as a critical branch of CV, also greatly benefits from this state-of-the-art technique. In this review, we first recap the core component of the transformer, the attention mechanism, and the detailed structures of the transformer. After that, we depict the recent progress of the transformer in the field of MIA. We organize the applications in a sequence of different tasks, including classification, segmentation, captioning, registration, detection, reconstruction, denoising, localization, and synthesis. The mainstream classification and segmentation tasks are further divided into eleven medical image modalities. Finally, We discuss the open challenges and future opportunities in this field. This review with the latest contents, detailed information, and task-modality organization mode may greatly benefit the broad MIA community.",
        "publication_year": "2022",
        "authors": [
            "Zhao-cheng Liu",
            "Qiujie Lv",
            "Ziduo Yang",
            "Yifan Li",
            "Chau Hung Lee",
            "Leizhao Shen"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "416",
        "references": [
            "/paper/Transformers-in-Medical-Image-Analysis%3A-A-Review-He-Gan/42bad1b72259aa1ff70d7ce2539220a83f1af9a4",
            "/paper/Transforming-medical-imaging-with-Transformers-A-of-Li-Chen/1cb5a1fce0b65b616e69cc5ffd4e43e03d259e97",
            "/paper/Transformer-with-progressive-sampling-for-medical-Jiang-Li/e2e8e145c81fb362b33de045fd41de50da441bc6",
            "/paper/Automated-multi-modal-Transformer-network-(AMTNet)-Zheng-Tan/7a9c1dc303afce73da1795963b879a4b5843376d",
            "/paper/Symmetric-Transformer-based-Network-for-Image-Ma-Song/9a85ca639891d7fb45160cff4b94bf6440003dc6",
            "/paper/TFCNs%3A-A-CNN-Transformer-Hybrid-Network-for-Medical-Li-Li/5c7742c5d005868f5f9b340b6aadf19bae758983",
            "/paper/Semantic-segmentation-in-medical-images-through-and-Dhamija-Gupta/076eb09a8a255e8f1360e195e0cbd27eb97f0a3b",
            "/paper/Multi-scale-Hierarchical-Transformer-structure-for-Wang-Wang/0051cb2efa21d2ab2f021156c78963ae2be7db29",
            "/paper/ITUnet%3A-Integration-Of-Transformers-And-Unet-For-Kan-Shi/3359d5505db96a054acc3e67e55c6568a0a72888",
            "/paper/TransMed%3A-Transformers-Advance-Multi-Modal-Medical-Dai-Gao/03e13ef9192206fecdb227366b298c992dbf7061"
        ]
    },
    {
        "id": "df8bcc74868724d33f85e4b5de3212eb394b6390",
        "title": "Magnetic resonance\u2010guided radiation therapy: A review",
        "abstract": "MRgRT can potentially transform radiation oncology by improving tumour control and quality of life after radiation therapy and increasing convenience of treatment by shortening treatment courses for patients, according to a review of current implementations, studies, potential benefits and challenges. Magnetic resonance\u2010guided radiation therapy (MRgRT) is a promising approach to improving clinical outcomes for patients treated with radiation therapy. The roles of image guidance, adaptive planning and magnetic resonance imaging in radiation therapy have been increasing over the last two decades. Technical advances have led to the feasible combination of magnetic resonance imaging and radiation therapy technologies, leading to improved soft\u2010tissue visualisation, assessment of inter\u2010 and intrafraction motion, motion management, online adaptive radiation therapy and the incorporation of functional information into treatment. MRgRT can potentially transform radiation oncology by improving tumour control and quality of life after radiation therapy and increasing convenience of treatment by shortening treatment courses for patients. Multiple groups have developed clinical implementations of MRgRT predominantly in the abdomen and pelvis, with patients having been treated since 2014. While studies of MRgRT have primarily been dosimetric so far, an increasing number of trials are underway examining the potential clinical benefits of MRgRT, with coordinated efforts to rigorously evaluate the benefits of the promising technology. This review discusses the current implementations, studies, potential benefits and challenges of MRgRT.",
        "publication_year": "2020",
        "authors": [
            "S. Chin",
            "C. Eccles",
            "A. McWilliam",
            "R. Chuter",
            "Emma Y. X. Walker",
            "P. Whitehurst",
            "J. Berresford",
            "M. V. van Herk",
            "P. Hoskin",
            "A. Choudhury"
        ],
        "related_topics": [
            "Medicine"
        ],
        "citation_count": "74",
        "reference_count": "190",
        "references": [
            "/paper/Magnetic-Resonance-Guided-Radiation-Therapy-for-and-Lavigne-Ng/f838d431533d2ab84f599a5054872409ee1484e3",
            "/paper/%5BMagnetic-resonance-guided-radiotherapy-%3A-The-of-a-Hoegen-Spindeldreier/fcdefadd8a9befb1ef783e9c2a344f69af4fe12e",
            "/paper/Towards-Accurate-and-Precise-Image-Guided-Clinical-Randall-Rammohan/b08da6fea644d255f543e87bf0ab20c7d71ef5d1",
            "/paper/Clinical-application-of-MR-Linac-in-tumor-a-review-Liu-Li/87e77c6e94ae01b584912153b702c0bc5744968e",
            "/paper/History-of-Technological-Advancements-towards-The-Rammohan-Randall/b54e609b6feda6562630b120d52c6f7e8461d8ed",
            "/paper/Nanoparticles-for-MRI-guided-radiation-therapy%3A-a-Smith-Kuncic/f7b2bb2adab9f28e4ff1ec08b0fd127bf4e0e7ad",
            "/paper/MR-Guided-Radiotherapy-for-Head-and-Neck-Cancer%3A-Boeke-M%C3%B6nnich/bc17d2e62e264bed84994439cafc6faf89491bc6",
            "/paper/Patient-Reported-Tolerance-of-Magnetic-Radiation-Sayan-Serbez/693eee0f8df5cd01888108a8a57732628247ad9c",
            "/paper/MR-Guided-Radiotherapy-for-Brain-and-Spine-Tumors-Maziero-Straza/440fa2f90fdd8c8b2b3097a71facebb15df222db",
            "/paper/Advances-in-MRI%E2%80%90guided-precision-radiotherapy-Liu-Li/7858f2fafb4a6d54dd73e40352c11ca7c77ecab0",
            "/paper/The-Role-of-Magnetic-Resonance-Imaging-in-Tharmalingam-Alonzi/2c69b46b27ac404f088cd798490b86308902855b",
            "/paper/Magnetic-resonance-imaging-in-precision-radiation-Bainbridge-Salem/c2683f27ba55b2046848ffdff62b64b31241c07c",
            "/paper/MRI-guidance-for-motion-management-in-external-beam-Paganelli-Whelan/5cd35f8269dadb695d142ccaf8630406148a9b8e",
            "/paper/Clinical-implementation-of-magnetic-resonance-for-Tetar-Bruynzeel/1b637770d04d2d00a4289c881908d283b006c491",
            "/paper/A-New-Era-of-Image-Guidance-with-Magnetic-Radiation-Mittauer-Paliwal/6a6193b00708fbe3f963b19dc098f2db6255c157",
            "/paper/Online-Magnetic-Resonance-Image-Guided-Adaptive-Acharya-Fischer-Valuck/b7a789729740fc38ff93d2a2db9ddd2ae8700bac",
            "/paper/A-Multi-Institutional-Experience-of-MR-Guided-Liver-Rosenberg-Henke/422bf8e43f4e240cfc055af620e8563382e98d91",
            "/paper/Magnetic-Resonance-guided-External-Beam-Radiation-a-Asher-Padgett/bd0c01b0d04bfeb32320f5d47983cd4184d17ccc",
            "/paper/First-Reported-Case-of-Pediatric-Radiation-With-Henke-Green/31c536c2cc43274ac6f19ad4a9cc53fcb94288a8",
            "/paper/Future-of-medical-physics%3A-Real%E2%80%90time-MRI%E2%80%90guided-Oborn-Dowdell/a9553fd35de8c6b12b8b82ef0c238a4e04d50b4f"
        ]
    },
    {
        "id": "466f101a55619f100ffa02c7531fb2ec52ea395c",
        "title": "Uncertainty quantification in skin cancer classification using three-way decision-based Bayesian deep learning",
        "abstract": "Semantic Scholar extracted view of \"Uncertainty quantification in skin cancer classification using three-way decision-based Bayesian deep learning\" by M. Abdar et al.",
        "publication_year": "2021",
        "authors": [
            "M. Abdar",
            "Maryam Samami",
            "Sajjad Dehghani Mahmoodabad",
            "T. Doan",
            "Bogdan Mazoure",
            "R. Hashemifesharaki",
            "Li Liu",
            "A. Khosravi",
            "Usha R. Acharya",
            "V. Makarenkov",
            "S. Nahavandi"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "80",
        "reference_count": "61",
        "references": [
            "/paper/Calibrating-Ensembles-for-Scalable-Uncertainty-in-Buddenkotte-Sanchez/864c4fb8e4cc4adc3e890307eaa8f48ef5a57e59",
            "/paper/Exploring-uncertainty-measures-in-convolutional-for-Song-Li/4efcca204f430fd5fe49bde14f27d343b0544dd0",
            "/paper/Uncertainty-informed-Mutual-Learning-for-Joint-and-Ren-Zou/eb416a1d6f9034c4488d7e525d9103ff87dc21eb",
            "/paper/Hercules%3A-Deep-Hierarchical-Attentive-Multilevel-Abdar-Fahami/6835ce26dced0ab42960f80d9f0ee77ac23773d3",
            "/paper/Uncertainty-aware-skin-cancer-detection%3A-The-of-Tabarisaadi-Khosravi/62cb68e46f583f23d1d0ae4b4ad153b530bafd18",
            "/paper/Uncertainty-Estimation-in-Medical-Image-Systematic-Kurz-Hauser/a442ead524b9eeb261cc560d22c20d387e811aa8",
            "/paper/Bayesian-uncertainty-estimation-for-detection-of-in-Rezaei-N%C3%A4ppi/c4b1f1165e8c380f3a4284e292c218010ee0a0f7",
            "/paper/Trustworthy-clinical-AI-solutions%3A-a-unified-review-Lambert-Forbes/b18cb2b9c1a605c72d9e4c8d045c945a06fab384",
            "/paper/BARF%3A-A-new-direct-and-cross-based-binary-residual-Abdar-Fahami/9daf5fa2abd01b8afaab456d09b6fe5c766d0fbf",
            "/paper/Application-of-simultaneous-uncertainty-for-image-a-Sahlsten-Jaskari/cf3359000887bace9f37d2db52a933883c9381b3",
            "/paper/Uncertainty-Estimation-in-Deep-Neural-Networks-for-Combalia-Hueto/ffa5e53e64df5c3585f11dbedab4b35a99399d1b",
            "/paper/Leveraging-uncertainty-information-from-deep-neural-Leibig-Allken/d46155be59bed4d7d2082b02bcb6734a6d66c428",
            "/paper/Risk-Aware-Machine-Learning-Classifier-for-Skin-Mobiny-Singh/79a8674b05b045b459cf6eeba3b4687da1878fcd",
            "/paper/A-Systematic-Comparison-of-Bayesian-Deep-Learning-Filos-Farquhar/6aad42dfcc59a119269f9765936d9c55911971ab",
            "/paper/Aleatoric-uncertainty-estimation-with-test-time-for-Wang-Li/41c9bfb05a7c3eeafe3f749eae1acf6afd873194",
            "/paper/A-fully-automated-deep-learning-pipeline-for-cancer-Alyafeai-Ghouti/4a680827f3fc53dc170504fb8cb48c460671bad3",
            "/paper/Medical-image-classification-using-synergic-deep-Zhang-Xie/563964d518376309086a8998e31a31ee06676efe",
            "/paper/A-Comparative-Study-of-Neural-Network-Architectures-Rasul-Dey/42d3a34c0bcafde033d1555876cf7ecd0040a4e2",
            "/paper/Pixels-to-Classes%3A-Intelligent-Learning-Framework-Khan-Zhang/41730452cc20a90c1dcb3acaaa0ede449bc01669",
            "/paper/An-Attention-Based-Mechanism-to-Combine-Images-and-Pacheco-Krohling/7e947dded26631755c5a9efe44f8fc438771bf94"
        ]
    },
    {
        "id": "6988cb30337787a8f982b04a4e1d3be39d7573f7",
        "title": "Patent Research in a Period of Industry Transformation: A Focus on Electromobility",
        "abstract": "Patent, as a valuable collection of technical information, is gaining momentum as proxy measures of innovative activities and is ascribed a unique role in tracking the rise of emerging technologies. The last 30 years have seen a dramatic transformation of the world\u2019s manufacturing landscape, for instance, a greening development in the automotive sector. A typical example of this practice is the emergence of electromobility (e-mobility)\u2014an integrated approach addressing issues from sustainable transportation to revolutionary driving behavior adopted to circumvent problems concerning both resources and pollu- tion while meeting mobility demands. Since novel technologies covered by e-mobility are not yet entirely attainable in the market, the only metric particularly is patent data. However, a correspondingly bright light seems not to be shined on e-mobility patent research, even in the area of engineering. This paper employs bibliometric and sentence- by-sentence analysis coupled with visualization tools to illustrate how the patent exam-ines e-mobility-oriented issues in a contextualized and multivalent way. The conclusion reached is that patent research on e-mobility still has more spaces to move up, not only in improving its efficiency in plotting evolution of technologies but with regard to interpret - ing patents across the historical background of the industrial revolution.",
        "publication_year": "2018",
        "authors": [
            "Z. Qu"
        ],
        "related_topics": [
            "Business"
        ],
        "citation_count": 0,
        "reference_count": "66",
        "references": [
            "/paper/Technological-diversity-of-emerging-a-case-study-of-Sierzchula-Bakker/6f445b108742143cfe3939d6f13ecf9c68e69855",
            "/paper/Innovation-in-disruptive-regulatory-environments%3A-A-Pilkington-Dyerson/358516552aae3f808fb4540f99553ec35d9f4b25",
            "/paper/Identifying-trends-in-battery-technologies-with-to-Golembiewski-Stein/acad7a598d55d35b5e1b34eaec1da10aab9d3262",
            "/paper/Sectoral-systems-of-environmental-innovation%3A-An-to-Oltra-Jean/cba3b3b646643a3a693d619e561bb283a8e8b209",
            "/paper/Variety-of-technological-trajectories-in-low-(LEVs)-Oltra-Jean/105e99318c1a695f81d75dc595a1fe4f98213081",
            "/paper/On-the-relation-between-communication-and-A-of-and-Budde-Alkemade/36e04b03034b4a8a124d4b047a3a4dd6c9416cab",
            "/paper/Environmental-finance%3A-A-research-agenda-for-Linnenluecke-Smith/a50b13312f62818b68991517a7dee6fb3d903bb8",
            "/paper/Using-patents-and-prototypes-for-preliminary-of-%3A-Sierzchula-Nemet/f613bb779004689debbe8a850e6f44564b842e52",
            "/paper/Paving-the-Road-to-Electric-Vehicles-%E2%80%93-A-Patent-of-Borgstedt-Neyer/70447fb5e07661a6dd252f8cabe8c657b89bfe4d",
            "/paper/Business-strategies-of-incumbents-in-the-market-for-Wesseling-Niesten/459a568803f5cea64b05f475d626853b25e83e7c"
        ]
    },
    {
        "id": "c6360bc7b7e1e34debd6a233a5f11edb9ad7a10a",
        "title": "Image Classification Learning Method Incorporating Zero-Sample Learning and Small-Sample Learning",
        "abstract": "The methods of zero-sample and small-sample learning are fused, the design is introduced and analyzed, and the future research directions are prospected according to the current research problems. At present, artificial intelligence algorithms based on deep learning have achieved good results in image classification, biometric recognition, medical diagnosis, and other fields. However, in practice, many times researchers are unable to obtain a large number of samples due to many limitations or high sampling costs. Therefore, image sorting zero-sampling order research algorithms have become the central engine of intelligent processing and a hot spot for current research. Because of the need for the development of deep learning prediction capability, coupled with the emergence of time and technical-level drawbacks, the advantages of zero-sample and small-sample are gradually emerging, so this paper chooses to fuse the learning methods of both for image recognition research. This paper mainly introduces the current situation of zero-sample and small-sample learning and summarizes the learning of zero-sample and small-sample. And the meaning of zero-sample learning and small-sample learning and the classification of the main learning methods are introduced and compared and outlined, respectively. Finally, the methods of zero-sample and small-sample learning are fused, the design is introduced and analyzed, and the future research directions are prospected according to the current research problems.",
        "publication_year": "2022",
        "authors": [
            "Fanglei Sun",
            "Zhifeng Diao"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "19",
        "references": [
            "/paper/A-multi-level-feature-fusion-based-approach-to-Ding-Zhu/61766d9c5dcb3df66a18f99849e7e163fe48f6a0",
            "/paper/Brain-tumor-magnetic-resonance-image-a-deep-Lakshmi-Rao/4529e4c2e5e0fa2a897378686cfc6c0331275bfc",
            "/paper/Fast-and-Efficient-Method-for-Optical-Coherence-Ara-Matiolanski/7308007012179184cb56cd9419ab28d3c11effdc",
            "/paper/An-Improved-U-Net-Image-Segmentation-Method-and-Its-Shi-Duan/130e6745c3f2037b795e91f42d454f57d0f947ed",
            "/paper/Experimental-Studies-on-Rock-Thin-Section-Image-by-Li-Zhao/f2323fcefbec4d039584dd6131e7796d19d4785a",
            "/paper/Land-use-land-cover-classification-of-remote-images-Digra-Dhir/25881d5c53315b14c9678672f651436a9c591a6e",
            "/paper/Feature-Extraction-and-Classification-of-Digital-A-Biradar-Akkasaligar/5ffd348971662306cbb0d7d293966a22539aba8c",
            "/paper/A-robust-hybrid-fusion-segmentation-approach-for-in-Devi-Perumal/dfca6edd6eacf3f45cc0fa3d69c648f4c0764095",
            "/paper/An-efficient-approach-for-detection-and-of-cancer-Elayaraja-Sengottaiyan/dbcf9ed76cea2d1b5cb49fc9d395f32c69e2c577",
            "/paper/Weedy-Rice-Classification-Using-Image-Processing-a-Ruslan-Khairunniza-Bejo/1393f17810aa4b0c59e6f82a5f8e9a591cf2582e"
        ]
    },
    {
        "id": "bd5e6d3381992ec221e719ca0513a8a248a75aaf",
        "title": "Weakly Supervised Image Annotation and Segmentation",
        "abstract": "This paper proposed a Bayesian Non-parametric (BN) approach to solve the complex visual tasks using the non- Parametric property to regulate the model's constraint. The various aspects in the processing of an image include object recognition, object classification, image segmentation, and attribute learning, are closely related to each other. In this paper, we proposed a Bayesian Non-parametric (BN) approach to solve the complex visual tasks using the non-parametric property to regulate the model's constraint. A Chinese Restaurant Process Stacked with Weakly Supervised Markov Random Field (WS-MRF-CRP) is developed, which uses Markov Random Field (MRF) for low-level and Chinese Restaurant Process (CRP) for high-level. The proposed approach learns and incorporates association between various object and attribute classes. The input image is clustered into individual components using the MRF, and then the CRP is used for merging the components and generating the image-attribute association. Experiments performed on the Berkeley Segmentation dataset demonstrated that the proposed model performs better than other existing weakly supervised models.",
        "publication_year": "2021",
        "authors": [
            "D. Naik",
            "C. Jaidhar"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "21",
        "references": [
            "/paper/The-Indian-Buffet-Process%3A-An-Introduction-and-Griffiths-Ghahramani/808073f5802d627439ec981c1e7f361e67e624b9",
            "/paper/SURF%3A-Speeded-Up-Robust-Features-Hunt/6c7cf406a47048730c1a08d46cb0166b16566524",
            "/paper/Distinctive-Image-Features-from-Scale-Invariant-LoweDavid/a088830972dace89fcf1fa745546405650253ea8",
            "/paper/Comparison-of-Image-Encoder-Architectures-for-Image-Maru-Chandana/694b58db149f167f9742dbc9f7729fc51c4d52ac",
            "/paper/En-De-Cap%3A-An-Encoder-Decoder-model-for-Image-Patwari-Naik/76775403c7d37d0d94e4ee3823c083ef5b8ffe0c",
            "/paper/Weakly-Supervised-Learning-of-a-Deep-Convolutional-Feng-Wang/8b5e2a1cf32e65f283fcee865e9168459348cfd4",
            "/paper/Weakly-Supervised-Biomedical-Image-Segmentation-by-Liang-Nan/ff0e15a4c7fd7609ba1e4d2d992457cee9124316",
            "/paper/Weakly-Supervised-Semantic-Segmentation-of-Images-Nivaggioli-Randrianarivo/64cfc2822ebeeb16664eb35b2fa7dca9f05514cf",
            "/paper/Image-Segmentation-based-on-Markov-Random-Field-V.A.-Ganesh/065fb4221b64dcf51c7b9e790c05a38c44c77197",
            "/paper/Weakly-Supervised-Semantic-Segmentation-by-Multiple-Luo-Meng/c882f2b8fe375dfb65f34fa3b6786a54b825afa7"
        ]
    },
    {
        "id": "283d9b2d3cd72c5e96d9f39d25da56de66911285",
        "title": "Free Vibration Analysis of Spinning Sandwich Annular Plates with Functionally Graded Graphene Nanoplatelet Reinforced Porous Core",
        "abstract": "This paper conducted the free vibration analysis of a sandwich annular thin plate with whirl motion. The upper and lower faces of the annular plate are made of uniform solid metal, while its core is porous foamed metal reinforced by graphene nanoplatelets (GPLs). Both uniform and non-uniform distributions of GPLs and porosity along the direction of plate thickness which leads to a functionally graded (FG) core are taken into account. The effective material properties including Young\u2019s modulus, Poisson\u2019s ratio and mass density are calculated by employing the Halpin\u2013Tsai model and the rule of mixture, respectively. Based on the Kirchhoff plate theory, the differential equations of motion are derived by applying the Lagrange\u2019s equation. Then, the assumed mode method is utilized to obtain free vibration behaviors of the sandwich annular plate. The finite element method is adopted to verify the present model and vibration analysis. The effects of porosity coefficient, porosity distribution, graphene nanoplatelet (GPL) distribution, graphene nanoplatelet (GPL) weight fraction, graphene nanoplatelet length-to-thickness ratio (GPL-LTR), graphene nanoplatelet length-to-width ratio (GPL-LWR), spinning speed, outer radius-to-thickness ratio and inner radius-to-thickness ratio of the plate, are examined in detail.",
        "publication_year": "2022",
        "authors": [
            "Tianhao Huang",
            "Yu Ma",
            "Tianyu Zhao",
            "Jie Yang",
            "Xin Wang"
        ],
        "related_topics": [
            "Engineering"
        ],
        "citation_count": 0,
        "reference_count": "37",
        "references": [
            "/paper/Symmetric-and-asymmetric-vibrations-of-rotating-Yang-Li/5ed1dc77478222daf8ce17bf33ba6521dbc708ef",
            "/paper/On-vibration-and-stability-analysis-of-porous-by-Saidi-Bahaadini/3b2bb9fc0443b7c7febcd60e3a89991b7ee2b367",
            "/paper/Nonlinear-free-vibration-of-functionally-graded-on-Gao-Gao/2af94572ed20523f3c3d1157c26f33452bd836b2",
            "/paper/Vibration-characteristics-of-functionally-graded-Dong-Li/bbe275b8bcd55bf861039f2582570db43053a83f",
            "/paper/Free-vibration-analysis-of-a-functionally-graded-Zhao-Cui/b6277c7240f818568aaad18caa5347e3a96b5996",
            "/paper/Free-vibration-and-buckling-behavior-of-graded-by-Anamagh-Bediz/1947358cadb7b89e9e38c1b6adb6315ba05e169e",
            "/paper/Free-vibration-and-stability-of-graphene-platelet-Twinkle-Pitchaimani/e2e858f557c3dd78f1c6705d64fc41eb95f3003b",
            "/paper/Nonlinear-vibration-and-dynamic-buckling-analyses-Li-Wu/966239f713e2d05702e0de1731d71ea91e616a9d",
            "/paper/Free-vibration-and-elastic-buckling-of-functionally-Kitipornchai-Chen/017928ae80bda5891199f4be6a8efa1e8f7b9c1e",
            "/paper/A-comprehensive-analysis-of-porous-curved-beams-by-Anirudh-Ganapathi/67746765435e915de7d49bd798b5a8683a59cafa",
            "/paper/Vibration-characteristics-of-graphene-nanoplatelet-Zhao-Cui/a82544fcb37768f540412eb9c6b5f483bbb38999"
        ]
    },
    {
        "id": "a3272f80526e0ea32f5985e640fb5a388fbc02e6",
        "title": "Cross-Dimensional Refined Learning for Real-Time 3D Visual Perception from Monocular Video",
        "abstract": "A novel real-time capable learning method that jointly perceives a 3D scene's geometry structure and semantic labels and proposes an end-to-end cross-dimensional refinement neural network (CDRNet) to extract both 3D mesh and 3D semantic labeling in real time. We present a novel real-time capable learning method that jointly perceives a 3D scene's geometry structure and semantic labels. Recent approaches to real-time 3D scene reconstruction mostly adopt a volumetric scheme, where a truncated signed distance function (TSDF) is directly regressed. However, these volumetric approaches tend to focus on the global coherence of their reconstructions, which leads to a lack of local geometrical detail. To overcome this issue, we propose to leverage the latent geometrical prior knowledge in 2D image features by explicit depth prediction and anchored feature generation, to refine the occupancy learning in TSDF volume. Besides, we find that this cross-dimensional feature refinement methodology can also be adopted for the semantic segmentation task. Hence, we proposed an end-to-end cross-dimensional refinement neural network (CDRNet) to extract both 3D mesh and 3D semantic labeling in real time. The experiment results show that the proposed method achieves state-of-the-art 3D perception efficiency on multiple datasets, which indicates the great potential of our method for industrial applications.",
        "publication_year": "2023",
        "authors": [
            "Ziyang Hong",
            "C. Yue"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "48",
        "references": [
            "/paper/Atlas%3A-End-to-End-3D-Scene-Reconstruction-from-Murez-As/35e2056e29d5295b2669fe696ae0c27007c94625",
            "/paper/A-Real-Time-Online-Learning-Framework-for-Joint-3D-Menini-Kumar/0e9f4bd04ae660e84572650e683c6b170988afac",
            "/paper/NeuralRecon%3A-Real-Time-Coherent-3D-Reconstruction-Sun-Xie/8db0bf0fa406254d4cd57eb413443a0b96bd12b8",
            "/paper/3DMV%3A-Joint-3D-Multi-View-Prediction-for-3D-Scene-Dai-Nie%C3%9Fner/fafe7c7aa0a19d40bbbf08b9e87d650438b01b67",
            "/paper/3DVNet%3A-Multi-View-Depth-Prediction-and-Volumetric-Rich-Stier/534ff47a5bd665ae38398053a90f7295a0e0b30b",
            "/paper/SimpleRecon%3A-3D-Reconstruction-Without-3D-Sayed-Gibson/caf96058774d96dd08df799319ef0d5430121518",
            "/paper/VolumeFusion%3A-Deep-Depth-Fusion-for-3D-Scene-Choe-Im/0ba9140dad37b1b3d8cfcdb6d54389edd701c58d",
            "/paper/MVSNet%3A-Depth-Inference-for-Unstructured-Multi-view-Yao-Luo/87ca28235555f7e70cf1edc2a63cda4aef7fee42",
            "/paper/Point-Based-Multi-View-Stereo-Network-Chen-Han/58a309e2ab84c66450480806e09b8a55f455c218",
            "/paper/Boundary-Aware-3D-Building-Reconstruction-From-a-Mahmud-Price/ed18b6abdad0d14925da6e902c8439f9202d24e5"
        ]
    },
    {
        "id": "928b6bc4492740e8eba83366e9b21eb47ba59067",
        "title": "Recognizing human faces under varying degree of Illumination: A comprehensive survey",
        "abstract": "This paper aims to give a detailed survey of various face recognition algorithms and covers all the approaches that aim to solve all the problems that are created due to varying illumination. Illumination variation is one of the well-known and challenging problems known for face recognition. A lot of studies have been explored to reduce the effect caused by varying illumination. We have analysed the latest state of art technique and have divided into two categories. One based on Singular Value Decomposition (SVD), Resonance and Local Binary Pattern (LBP) techniques. And the second based on the Self Quotient Image (SQI) and Histogram Based techniques. In the first category SVD technique gives 99.53% of recognition rate on Yale B face database, Principle Component Analysis (PCA) technique gives 100% recognition rate on Yale B database, LBP and Circle Threshold (CT)-LBP techniques give 94.8% and 98.12% of recognition rate using Yale B database. In the second category SQI technique gives 98.3% of recognition rate on Yale B database and Histogram technique gives 100% recognition rate using CMUPIE database. This paper aims to give a detailed survey of various face recognition algorithms. The review covers all the approaches that aim to solve all the problems that are created due to varying illumination.",
        "publication_year": "2015",
        "authors": [
            "A. G. Pai",
            "S. Fernandes",
            "Keerthan Nayak",
            "Nagesha",
            "K. K. Accamma",
            "K. Sushmitha",
            "Kushala Kumari"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "8",
        "reference_count": "27",
        "references": [
            "/paper/Face-illumination-processing-using-nonlinear-range-Yang-Nie/4491efbbf6e6964f5790408c14cfd431f0c85bcb",
            "/paper/PERFORMANCE-EVALUATION-OF-ILLUMINATION-INVARIANT-Manhotra-Sharma/10a98632ed618c23c58af93e17d90ef654b1845f",
            "/paper/Face-Illumination-Processing-Using-Discrete-Cosine-Zhijun-Xue/8a418d2dbf42f411e27551f033fc6c47ed020d99",
            "/paper/Recovering-variations-in-facial-albedo-from-low-Chen-Zhang/8919a83be135007cda72875c777bb648a71cf6ce",
            "/paper/A-Comprehensive-Review-on-Various-State-Of-The-Art-SannidhanM-AnanthPrabhu/3c5726eeb4c14bdbc48919b06d15724d647d2ab1",
            "/paper/Score-Level-Fusion-Based-Approach-for-Water-Leak-in-Rao/7c2bd18f0623c0a815c9e26077028c5e8f2c62ba",
            "/paper/%E5%88%A9%E7%94%A8%E7%A6%BB%E6%95%A3%E4%BD%99%E5%BC%A6%E5%8F%98%E6%8D%A2%E4%B8%8EWallis%E7%9A%84%E4%BA%BA%E8%84%B8%E5%85%89%E7%85%A7%E5%A4%84%E7%90%86%E7%AE%97%E6%B3%95-Face-Illumination-Using-%E6%9D%A8%E5%BF%97%E5%86%9B-%E4%BD%95%E9%9B%AA/436a29190e0b107b6f3fafca5f2279162e64c1dd",
            "/paper/Face-Recognition-under-Varying-Illuminations-Using-Manhotra-Sharma/5ca14fa73da37855bfa880b549483ee2aba26669",
            "/paper/Illumination-invariant-face-recognition-using-SQI-Biglari-Mirzaei/335b26f82390ff531dd040aaf48c48c30eddf4a1",
            "/paper/Face-Recognition-under-Varying-Illumination-Based-Xu-Li/47abb090848b06b5ca045a8f9582a7fb0d729dfa",
            "/paper/A-Novel-Method-for-Illumination-Normalization-for-Patil-Vasudha/8682e9df3a49599094eeff58f8373f63b0239daf",
            "/paper/A-comparative-study-on-ICA-and-LPP-based-Face-under-Fernandes-Bala/2486459ca4b06f0e88ec83f03ca60fd8c668a5e9",
            "/paper/A-Comparative-Study-on-Score-Level-Fusion-and-MACE-Fernandes-Bala/19b03e7f37e4bc2f049ba57056f6e687e52e1f5c",
            "/paper/Rescaling-of-low-frequency-DCT-coefficients-with-Goel-Nehra/f9d84a4a559b303a672f113abdd7d333ed94ab21",
            "/paper/Low-cost-illumination-invariant-face-recognition-by-Lin-Chiu/30b6811205b42e92d7a82c606d4521319764250b",
            "/paper/Illumination-invariant-human-face-recognition%3A-or-Baradarani-Wu/315a90543d60a5b6c5d1716fe9076736f0e90d24",
            "/paper/Face-recognition-using-illumination-invariant-local-Shafie-Hafiz/5599ac2cd569ed83ecab8449d2f245e13034da06",
            "/paper/SVD-Face%3A-Illumination-Invariant-Face-Kim-Suh/dac6e9d708a9757f848409f25df99c5a561c863c"
        ]
    },
    {
        "id": "a9d28021588042becb0ab76d0bc74c2d7aaf075a",
        "title": "Multi-Grid Redundant Bounding Box Annotation for Accurate Object Detection",
        "abstract": "This paper presents a new mathematical approach that assigns multiple grids per object for accurately tight-fit bounding box prediction and proposes an effective offline copy-paste data augmentation for object detection. Modern leading object detectors are either two-stage or one-stage networks repurposed from a deep CNN-based backbone classifier network. YOLOv3 is one such very-well known state-of-the-art one-shot detector that takes in an input image and divides it into an equal-sized grid matrix. The grid cell having the center of an object is the one responsible for detecting the particular object. This paper presents a new mathematical approach that assigns multiple grids per object for accurately tight-fit bounding box prediction. We also propose an effective offline copy-paste data augmentation for object detection. Our proposed method significantly outperforms some current state-of-the-art object detectors with a prospect for further better performance.",
        "publication_year": "2021",
        "authors": [
            "Solomon Negussie Tesema",
            "E. Bourennane"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "21",
        "references": [
            "/paper/Resource-and-Power-Efficient-High-Performance-Using-Tesema-Bourennane/3ed92c6061ed85fc8e57b84ddd9e4e2d19e2857e",
            "/paper/DenseBox%3A-Unifying-Landmark-Localization-with-End-Huang-Yang/aeb86c216e32b39c8716fe5a832aa7b135164a87",
            "/paper/Towards-General-Purpose-Object-Detection%3A-Deep-Grid-Tesema-Bourennane/3a24360c0566bfc3cfe301a91aba8a47f13b2756",
            "/paper/Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue/2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "/paper/FCOS%3A-Fully-Convolutional-One-Stage-Object-Tian-Shen/e2751a898867ce6687e08a5cc7bdb562e999b841",
            "/paper/SSD%3A-Single-Shot-MultiBox-Detector-Liu-Anguelov/4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
            "/paper/CenterNet%3A-Keypoint-Triplets-for-Object-Detection-Duan-Bai/52e7190540745960333ab483623099edf1641257",
            "/paper/You-Only-Look-Once%3A-Unified%2C-Real-Time-Object-Redmon-Divvala/f8e79ac0ea341056ef20f2616628b3e964764cfd",
            "/paper/Faster-R-CNN%3A-Towards-Real-Time-Object-Detection-Ren-He/424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "/paper/Feature-Pyramid-Networks-for-Object-Detection-Lin-Doll%C3%A1r/b9b4e05faa194e5022edd9eb9dd07e3d675c2b36",
            "/paper/CornerNet%3A-Detecting-Objects-as-Paired-Keypoints-Law-Deng/5091118278b0697d20c74304f28afe8c785cc80f"
        ]
    },
    {
        "id": "e500eb7c2e630f7faae7d39f4cede2bc6ffd1497",
        "title": "Recognition and retrieval of objects in diverse applications",
        "abstract": "This work addresses the classification of boar spermatozoa according to acrosome integrity using several methods based on invariant local features and proposes a new descriptor, named colour COSFIRE, in the scope of the European project Advisory System Against Sexual Exploitation of Children. This work proposes and evaluates object description and retrieval techniques in different real applications. It addresses the classification of boar spermatozoa according to acrosome integrity using several methods based on invariant local features. In addition, it provides two new methods for insert localisation and an automatic solution for the recognition of broken inserts in edge profile milling heads that can be set up in-process without delaying any machining operations. Finally, it evaluates different clusterings of keypoints for object retrieval and proposes a new descriptor, named colour COSFIRE , in the scope of the European project Advisory System Against Sexual Exploitation of Children.",
        "publication_year": "2018",
        "authors": [
            "Laura Fern\u00e1ndez-Robles"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": 0,
        "reference_count": "8",
        "references": [
            "/paper/Object-recognition-techniques-in-real-applications-Robles/346f61e32eb0434a5ec70a2d3b1a5ecacee29249",
            "/paper/Evaluation-of-Clustering-Configurations-for-Object-Fern%C3%A1ndez-Robles-Castej%C3%B3n-Limas/10a19896e6c524535a75b4c940a4e0cb6692920c",
            "/paper/Machine-vision-based-identification-of-broken-in-Fern%C3%A1ndez-Robles-Azzopardi/e4a2f90739434df36da41b4629c0c76c6d1b49cc",
            "/paper/Acrosome-integrity-assessment-of-boar-spermatozoa-Garc%C3%ADa-Olalla-Alegre/c4c977cccb7425e4707a5714f60de3079b2e0582",
            "/paper/Cutting-Edge-Localisation-in-an-Edge-Profile-Head-Fern%C3%A1ndez-Robles-Azzopardi/ec9174b7383c7a0e78828f77f84b05893e1ebc06",
            "/paper/Trainable-COSFIRE-Filters-for-Keypoint-Detection-Azzopardi-Azzopardi/8718a0177b3259b52078d47914911d7c7f4144d1",
            "/paper/On-line-tool-wear-measurement-for-ball-end-milling-Zhang-Zhang/c2e296266d2adfeb2a04e25dc2033653575ba313"
        ]
    },
    {
        "id": "3ea54da8ba5deba902e34199b63956edf68906ff",
        "title": "Fast Keypoint Recognition Using Random Ferns",
        "abstract": "This paper shows that formulating the problem in a naive Bayesian classification framework makes such preprocessing unnecessary and produces an algorithm that is simple, efficient, and robust, and it scales well as the number of classes grows. While feature point recognition is a key component of modern approaches to object detection, existing approaches require computationally expensive patch preprocessing to handle perspective distortion. In this paper, we show that formulating the problem in a naive Bayesian classification framework makes such preprocessing unnecessary and produces an algorithm that is simple, efficient, and robust. Furthermore, it scales well as the number of classes grows. To recognize the patches surrounding keypoints, our classifier uses hundreds of simple binary features and models class posterior probabilities. We make the problem computationally tractable by assuming independence between arbitrary sets of features. Even though this is not strictly true, we demonstrate that our classifier nevertheless performs remarkably well on image data sets containing very significant perspective changes.",
        "publication_year": "2010",
        "authors": [
            "Mustafa \u00d6zuysal",
            "Michael Calonder",
            "Vincent Lepetit",
            "P. Fua"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "716",
        "reference_count": "38",
        "references": [
            "/paper/Keypoint-Recognition-using-Random-Forests-and-Ferns-Lepetit-Fua/5686df7f538412179d02c5a60c6de63e439c1c19",
            "/paper/A-Random-Fern-based-Feature-Approach-for-Image-Khoo-Keun/0a6e8022a8620478fe3038faf3b914410fbe9935",
            "/paper/A-Convolutional-Treelets-Binary-Feature-Approach-to-Wu-Zhu/481824a8e4a0f63af0ea924758a359bf2702cb60",
            "/paper/Stable-Keypoint-Recognition-using-Viewpoint-Yoshida-Saito/1bbe8d58890968dd771a1a3a1e2f99e10d301ea4",
            "/paper/A-General-Framework-for-Fast-3D-Object-Detection-an-Montero-Lang/2b5f9a622406458549322d30d9fee46aba385e68",
            "/paper/Treelets-Binary-Feature-Retrieval-for-Fast-Keypoint-Zhu-Wu/c886a252d512fd715ec2c6d5367ef72daabe1ef1",
            "/paper/Fast-Localization-in-Large-Scale-Environments-Using-Feng-Fan/2e4feaa6c10694e1b1b0766363c814ea57015433",
            "/paper/Robust-vision-tracking-by-online-random-ferns-and-Ji-Zhang/4593f061b655131dfc51d27320020d43f902627b",
            "/paper/EVALUATION-OF-CLUSTERING-CONFIGURATIONS-FOR-OBJECT-Robles-Limas/e21f24dc1c5b3d2b1cb279d8e0340b149094ea1d",
            "/paper/Discriminative-Ferns-Ensemble-for-Hand-Pose-Krupka-Vinnikov/3839e653ee908f8e6939a72f7f32cbf21bf4fcf4",
            "/paper/Fast-Keypoint-Recognition-in-Ten-Lines-of-Code-%C3%96zuysal-Fua/8747f8a3ceaa0802922631ddb7f503674a3f6ca3",
            "/paper/Keypoint-recognition-using-randomized-trees-Lepetit-Fua/aa6557c658aed10ff303aa90fe8d0952332a43c0",
            "/paper/Keypoint-Signatures-for-Fast-Learning-and-Calonder-Lepetit/a5b1a6b90cb2f0437c24feb5113a34dc94c1b606",
            "/paper/Image-Classification-using-Random-Forests-and-Ferns-Bosch-Zisserman/d175a196816e44c08928ad05e30fd774468d69aa",
            "/paper/2D-Object-Detection-and-Recognition%3A-Models%2C-and-Amit/1c99a396bc7eab74482c439dac3d30b629aedbff",
            "/paper/Shape-Quantization-and-Recognition-with-Randomized-Amit-Geman/de5e95325e139fd0a46df1dd28aabecd0273b772",
            "/paper/Object-based-image-retrieval-using-the-statistical-Hoiem-Sukthankar/4d86a6366a66a5b6e84142c7edb8f2d255f267bb",
            "/paper/Evaluation-of-Features-Detectors-and-Descriptors-on-Moreels-Perona/0481840f39b8c1560b7137ad3ad0eae0346fe11e",
            "/paper/Scalable-Recognition-with-a-Vocabulary-Tree-Nist%C3%A9r-Stew%C3%A9nius/b3e7d3e37e67af7f4546b46051063bea1b62dbae",
            "/paper/Shape-indexing-using-approximate-nearest-neighbour-Beis-Lowe/ef05b72c9e5b267811d4a069fb1d8038f0e36bbd"
        ]
    },
    {
        "id": "bf979b536cbc8c87e58e52ddb0653b241ef8afd3",
        "title": "Learning Vision-based Reactive Policies for Obstacle Avoidance",
        "abstract": "The ability of the proposed method to efficiently learn stable obstacle avoidance strategies at a high success rate, while maintaining closed-loop responsiveness required for critical applications like human-robot interaction is shown. In this paper, we address the problem of vision-based obstacle avoidance for robotic manipulators. This topic poses challenges for both perception and motion generation. While most work in the field aims at improving one of those aspects, we provide a unified framework for approaching this problem. The main goal of this framework is to connect perception and motion by identifying the relationship between the visual input and the corresponding motion representation. To this end, we propose a method for learning reactive obstacle avoidance policies. We evaluate our method on goal-reaching tasks for single and multiple obstacles scenarios. We show the ability of the proposed method to efficiently learn stable obstacle avoidance strategies at a high success rate, while maintaining closed-loop responsiveness required for critical applications like human-robot interaction.",
        "publication_year": "2020",
        "authors": [
            "Elie Aljalbout",
            "Ji Chen",
            "Konstantin Ritt",
            "Maximilian Ulmer",
            "S. Haddadin"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "11",
        "reference_count": "46",
        "references": [
            "/paper/Obstacle-Avoidance-for-Robotic-Manipulator-in-Joint-Wang-Kasaei/c74ba54cc1b29a99619f8d3cf13b8a35e3307396",
            "/paper/CLAMGen%3A-Closed-Loop-Arm-Motion-Generation-via-RL-Akinola-Wang/bb213772d6323464ad08ec07c4555f56c57d4a39",
            "/paper/Towards-Coordinated-Robot-Motions%3A-End-to-End-of-on-Rana-Li/5bbe4035ab4fb1e24d32ff7e96e62f63747ebcd4",
            "/paper/Learning-To-Find-Shortest-Collision-Free-Paths-From-P'andy-Lenton/693440a1300e8ec6958dfc41ebc371cd715a3ccc",
            "/paper/pu-ta-%C3%A7%C3%A3-o-End-to-End-Visual-Obstacle-Avoidance-for-Sanches/eae3a8c4d8201489ec8bfca397a2c9b2b6a3128d",
            "/paper/Reactive-Motion-Generation-on-Learned-Riemannian-Beik-Mohammadi-Hauberg/b0359bfed2214f54776e800f191c2166ea8f9666",
            "/paper/Dual-Arm-Adversarial-Robot-Learning-Aljalbout/f87a4a2649b55998af55e045f3f3c2519dea92c2",
            "/paper/CLAS%3A-Coordinating-Multi-Robot-Manipulation-with-Aljalbout-Karl/b8a3a030a36fbb74deea909aec5a959c94477fd2",
            "/paper/RMP2%3A-A-Structured-Composable-Policy-Class-for-Li-Cheng/9005089be5c27c31a01790142d916cfef84d0c8c",
            "/paper/Seeking-Visual-Discomfort%3A-Curiosity-driven-for-Aljalbout-Ulmer/e55840992c735daa4b18fe80af7ab19d94bbbaad",
            "/paper/Deep-Reinforcement-Learning-for-Collision-Avoidance-Sangiovanni-Rendiniello/0d414a26e8581cc445848740e10a839de8250a6c",
            "/paper/Robust-Vision-based-Obstacle-Avoidance-for-Micro-in-Lin-Zhu/f021adfcf0d6f6a2e5c0e548244e0a9e820e1cfb",
            "/paper/Uncertainty-Aware-Reinforcement-Learning-for-Kahn-Villaflor/f2c20cb6ebd2ad704c5bcae4eb8b942d3c62f8e0",
            "/paper/Real-Time-Obstacle-Avoidance-for-Manipulators-and-Khatib/c8a04d0cbb9f70e86800b11b594c9a05d7b6bac0",
            "/paper/Learning-feedback-terms-for-reactive-planning-and-Rai-Sutanto/0e8364e61acae884edc8c1cee5df1f6ee2c6e8c4",
            "/paper/Vision-Based-Obstacle-Avoidance-Techniques-Guzel-Bicker/c5dc7ab83818f0278dd87be8c0693beff5fccc50",
            "/paper/Real-Time-Perception-Meets-Reactive-Motion-Kappler-Meier/3f0e3c09eeab2a66201cb2819740871fd7e4bd54",
            "/paper/Safe-Exploration-Techniques-for-Reinforcement-An-Pecka-Svoboda/477a3dc8c6cc9d7be19c52d9cb0d13aa649ee535",
            "/paper/Elastic-Strips-%3A-A-Framework-for-Motion-Generation-Brock/b740fec2db9066f8faab08aedfec4a1b55837d6b",
            "/paper/End-to-End-Training-of-Deep-Visuomotor-Policies-Levine-Finn/b6b8a1b80891c96c28cc6340267b58186157e536"
        ]
    },
    {
        "id": "034cfd74bf49688decde034c81fa3f8b71b5b424",
        "title": "Class-Discriminative Feature Embedding For Meta-Learning based Few-Shot Classification",
        "abstract": "A few-shot learning framework based on structured margin loss which takes into account the global structure of the support set in order to generate a highly discriminative feature space where the features from distinct classes are well separated in clusters is proposed. Although deep learning-based approaches have been very effective in solving problems with plenty of labeled data, they suffer in tackling problems for which labeled data are scarce. In few-shot classification, the objective is to train a classifier from only a handful of labeled examples in a support set. In this paper, we propose a few-shot learning framework based on structured margin loss which takes into account the global structure of the support set in order to generate a highly discriminative feature space where the features from distinct classes are well separated in clusters. Moreover, in our meta-learning-based framework, we propose a context-aware query embedding encoder for incorporating support set context into query embedding and generating more discriminative and task-dependent query embeddings. The task-dependent features help the metalearner to learn a distribution over tasks more effectively. Extensive experiments based on few-shot, zero-shot and semi-supervised learning on three benchmarks show the advantages of the proposed model compared to state-of-the- art.",
        "publication_year": "2020",
        "authors": [
            "Alireza Rahimpour",
            "H. Qi"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "6",
        "reference_count": "43",
        "references": [
            "/paper/Improving-Meta-Learning-Classification-with-Pan-Li/3e69c3181fbbea3978fe555abed9ee642bdcda91",
            "/paper/Class-wise-Attention-Reinforcement-for-Pan-Li/91896dd0aa4bac0284c2a9b29c2a4db0580819e9",
            "/paper/PNPDet%3A-Efficient-Few-shot-Detection-without-via-Zhang-Cui/c4d8a3086ef7f098b10601f775a5a2dee6fe5944",
            "/paper/Fighting-fire-with-fire%3A-A-spatial%E2%80%93frequency-with-Zheng-Yan/6bf306366811f4495e7d0ee85de235498da2f317",
            "/paper/Vita-Ashford-Michael/60da9b6b7be8ac8871af2080e26f87809883a3ba",
            "/paper/Meta-learning-meets-the-Internet-of-Things%3A-Graph-Zheng-Yan/990be11df3e31eff2875661cee62482921274ded",
            "/paper/Meta-Learning-for-Semi-Supervised-Few-Shot-Ren-Triantafillou/df093d69cd98cf4b26542f53614a79754754eb78",
            "/paper/Learning-to-Compare%3A-Relation-Network-for-Few-Shot-Sung-Yang/bfe284e4338e62f0a61bb33398353efd687f206f",
            "/paper/Zero-Shot-Learning-via-Joint-Latent-Similarity-Zhang-Saligrama/b2a5c3744eea40c76d0359e517026e8ed6c922ff",
            "/paper/TADAM%3A-Task-dependent-adaptive-metric-for-improved-Oreshkin-L%C3%B3pez/e6a83abec5cffb0bf1669f2f2c1efdf2b15cb171",
            "/paper/Meta-Learning-With-Differentiable-Convex-Lee-Maji/fc437af6204008647ea49f81058d5fdaddf75ead",
            "/paper/Zero-Shot-Learning-via-Semantic-Similarity-Zhang-Saligrama/ac98259064e86f643f2cd11e5417b43bf28daa91",
            "/paper/Synthesized-Classifiers-for-Zero-Shot-Learning-Changpinyo-Chao/846946cd21413211a4701f309c3927d67363cd30",
            "/paper/Generating-Classification-Weights-With-GNN-for-Gidaris-Komodakis/4732ef442a5d0f2ec77600ddd44aaa288a043ace",
            "/paper/Optimization-as-a-Model-for-Few-Shot-Learning-Ravi-Larochelle/29c887794eed2ca9462638ff853e6fe1ab91d5d8",
            "/paper/Matching-Networks-for-One-Shot-Learning-Vinyals-Blundell/be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6"
        ]
    },
    {
        "id": "f5ddbb4c316cab504edbe2306cb8753ec6efaa6f",
        "title": "Unsupervised Video Summarization With Cycle-Consistent Adversarial LSTM Networks",
        "abstract": "This paper addresses unsupervised video summarization by developing a novel Cycle-consistent Adversarial LSTM architecture to effectively reduce the information loss in the summary video, and establishes the relation between mutual information maximization and such cycle learning procedure and further introduce cycle-cons consistent loss to regularize the summarization. Video summarization is an important technique to browse, manage and retrieve a large amount of videos efficiently. The main objective of video summarization is to minimize the information loss when selecting a subset of video frames from the original video, hence the summary video can faithfully represent the overall story of the original video. Recently developed unsupervised video summarization approaches are free of requiring tedious annotation on important frames to train a video summarization model and thus are practically attractive. However, their performance is still limited due to the difficulty of minimizing information loss between the summary and original videos. In this paper, we address unsupervised video summarization by developing a novel Cycle-consistent Adversarial LSTM architecture to effectively reduce the information loss in the summary video. The proposed model, named Cycle-SUM, consists of a frame selector and a cycle-consistent learning based evaluator. The selector is a bi-directional LSTM network to capture the long-range relationship between video frames. To overcome the difficulty of specifying a suitable information preserving metric between original video and summary video, the evaluator is introduced to \u201csupervise\u201d selector to improve the video summarization quality. Specifically, the evaluator is composed of two generative adversarial networks (GANs), in which the forward GAN component is learned to reconstruct the original video from summary video, while the backward GAN learns to invert the process. We establish the relation between mutual information maximization and such cycle learning procedure and further introduce cycle-consistent loss to regularize the summarization. Extensive experiments on three video summarization benchmark datasets demonstrate a state-of-the-art performance, and show the superiority of the Cycle-SUM model compared with other unsupervised approaches.",
        "publication_year": "2020",
        "authors": [
            "Li Yuan",
            "F. E. Tay",
            "P. Li",
            "Jiashi Feng"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "23",
        "reference_count": "48",
        "references": [
            "/paper/Unsupervised-Video-Summarization-with-a-Attentive-Liang-Lv/4881c8c48881f28b7d29226f1a54f38b71481048",
            "/paper/Estimation-of-concise-video-summaries-from-long-via-Kaur-Mishra/ad21272d355b6e0bd74a88a86269bf0462d480c5",
            "/paper/Spatiotemporal-two-stream-LSTM-network-for-video-Hu-Hu/f27c8877f5b493af0be24fe131dd83b1f5667344",
            "/paper/Visual-and-Semantic-Feature-Coordinated-Bi-Lstm-for-Hong-Zhong/376afe8dc30ffea356c76d112ba87015d538ef9c",
            "/paper/Video-summarization-using-deep-learning-techniques%3A-Saini-Kumar/4b5e595806cdfcef0ca07316e6f5c44152f9edd6",
            "/paper/SHTVS%3A-Shot-level-based-Hierarchical-Transformer-An-Zhao/427615203023adf00de2704d80f810d48ba0aee5",
            "/paper/Graph-Attention-Networks-Adjusted-Bi-LSTM-for-Video-Zhong-Wang/fe1c760ef204bd8f3f7106d0b76a478330eda292",
            "/paper/Towards-Practical-and-Efficient-Long-Video-Summary-Ke-Chang/dbc71b4046ebb7bd742d3c41e7de307dd14ffbad",
            "/paper/TLDW%3A-Extreme-Multimodal-Summarisation-of-News-Tang-Hu/bef0efe8201b5620d35495fa7a857dea745abb09",
            "/paper/DTVNet%2B%3A-A-High-Resolution-Scenic-Dataset-for-Video-Zhang-Xu/4c722aa398bdbd3a26a7c56b6442f2ede1332a14",
            "/paper/Cycle-SUM%3A-Cycle-consistent-Adversarial-LSTM-for-Yuan-Tay/cef078462df7ca5f022fd654ae7d5764e7936eb0",
            "/paper/Unsupervised-Video-Summarization-with-Adversarial-Mahasseni-Lam/620fe6c786d15efca7f553ad70f295e2b693b391",
            "/paper/Video-Summarization-With-Attention-Based-Networks-Ji-Xiong/88a8baa1be5292e62622f1cb8e627fbf759bf741",
            "/paper/Discriminative-Feature-Learning-for-Unsupervised-Jung-Cho/69b3b29c6fabaea88d382922346d9157395a3226",
            "/paper/Towards-Scalable-Summarization-of-Consumer-Videos-Cong-Yuan/c6f27e57cfcbd71aa01e2f91a314f84dd093b4bc",
            "/paper/Video-summarization-via-minimum-sparse-Mei-Guan/4621a4bfd302e7cd7d9a49d8da4b7c3a99d5e0fc",
            "/paper/Diverse-Sequential-Subset-Selection-for-Supervised-Gong-Chao/25da1b119ba1e0bb602be6ce8492d1e33dbac9ff",
            "/paper/Deep-Reinforcement-Learning-for-Unsupervised-Video-Zhou-Qiao/9e9e5f0c36548cfe2855aae46b519b146aa8c9ae",
            "/paper/Summary-Transfer%3A-Exemplar-Based-Subset-Selection-Zhang-Chao/e3db6c8086082acd55f6c95070ad309ecb834517",
            "/paper/A-General-Framework-for-Edited-Video-and-Raw-Video-Li-Zhao/18a6a7edfbce1cc11fb7447a133c0ac150b2785c"
        ]
    },
    {
        "id": "81d092ae2a3a0eb1ea62ef101abc164dc6c84612",
        "title": "BSCF: Learning background suppressed correlation filter tracker for wireless multimedia sensor networks",
        "abstract": "Semantic Scholar extracted view of \"BSCF: Learning background suppressed correlation filter tracker for wireless multimedia sensor networks\" by Bo Huang et al.",
        "publication_year": "2021",
        "authors": [
            "Bo Huang",
            "Tingfa Xu",
            "Ziyi Shen",
            "Shenwang Jiang",
            "Jianan Li"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "4",
        "reference_count": "20",
        "references": [
            "/paper/Design-of-optimized-compressed-sensing-routing-for-Ramesh-Yaashuwanth/8ecab2b987e1ec579ba9fb306c0df50ba167430a",
            "/paper/Learning-Response-Consistent-and-Correlation-for-Zhang-Li/ea4b35a2a4b70a7011cb2e81224ea574d80c25a4",
            "/paper/Learning-Augmented-Memory-Joint-Aberrance-Repressed-Ji-He/ecdee88e40c5a6d6a25ee880f1b3a4e733aa123c",
            "/paper/Multimedia-sensor-image-detection-based-on-equation-Xu-Zhang/1dfaa8fef6188c86945b710285d5e558e8eab659",
            "/paper/Transfer-learning-based-discriminative-correlation-Huang-Xu/775f0b77ef332912abf40ade26a8923de125f0ea",
            "/paper/Robust-Visual-Tracking-via-Constrained-Multi-Kernel-Huang-Xu/cad2fb5036f2de25defd13158d05349a382152f1",
            "/paper/Energy-efficient-tracking-in-uncertain-sensor-Ren-Li/4ba45604fe48a55b04db36d51ea6677e9bf8f9b3",
            "/paper/Analysis-of-compressive-sensing-and-energy-for-Tekin-Gungor/8185270d0de96173b9dba8dd971bfb760b9b5fa0",
            "/paper/An-accurate-prediction-method-for-moving-target-and-Ahmadi-Viani/2738cd27281f156ede7422701c383eba0d10a0fc",
            "/paper/A-deep-learning-method-based-on-an-attention-for-Li-Wang/f66c919e872430d3987e4448012487a448c6847b",
            "/paper/Anomaly-detection-in-ad-hoc-networks-based-on-deep-Feng-Liu/e227fcdd7998cc398ef01fc1526008c1edeb047e",
            "/paper/Context-constraint-and-pattern-memory-for-long-term-Huang-Xu/5b153d49db0d5458abda051451b51e1d4c8e3d8a",
            "/paper/High-Speed-Tracking-with-Kernelized-Correlation-Henriques-Caseiro/65c9b4b1d49f46b3f8f64a5f617acfc14f85d031",
            "/paper/GRNN-and-KF-framework-based-real-time-target-using-Jondhale-Deshpande/60aaf0c120e8545636dbb04701c2e70f5729e6c2"
        ]
    },
    {
        "id": "1d317427f8008adfab927be909f5a1836f687c9b",
        "title": "Joint Channel Reliability and Correlation Filters Learning for Visual Tracking",
        "abstract": "A new formulation for jointly learning the channel reliability and the correlation filters is proposed and it can be combined with existing techniques in the DCF framework to further improve the performance. Multi-channel discriminative correlation filter (DCF) tracking methods have exhibited superior performance on several benchmarks. However, existing methods usually treat each channel of the features equally, whereas they pay less attention to the contribution of different channels. Different channels exhibit variant properties in the tracking process. A DCF learned with equally important channels is likely to be contaminated by the unreliable ones, which results in model degradation. To address this problem, we propose a new formulation for jointly learning the channel reliability and the correlation filters. The formulation is generic, and it can be combined with existing techniques in the DCF framework to further improve the performance. Our method can adaptively increase the impact of reliable channels and down-weight the corrupted ones. To solve the joint learning problem, we propose an optimization strategy that alternates between the correlation filters and the channel weights. Further, we prove the upper bound of the objective function and solve the channel weights efficiently. The joint learning strategy makes the correlation filters more discriminative and the channel weights more accurate. To verify the joint formulation, we propose a tracker based on the proposed formulation and the techniques used in the ECO tracker. We conduct extensive experiments to evaluate the proposed tracker on three benchmarks. The experimental results show that our formulation is effective and efficient, and that it performs favorably against other state-of-the-art trackers.",
        "publication_year": "2020",
        "authors": [
            "Fei Du",
            "Peng Liu",
            "Wei Zhao",
            "Xianglong Tang"
        ],
        "related_topics": [
            "Computer Science"
        ],
        "citation_count": "12",
        "reference_count": "67",
        "references": [
            "/paper/Robust-visual-tracking-via-spatio-temporal-adaptive-Liang-Liu/5dc3b582ebd95f029b1daa8622cf56bb192ef018",
            "/paper/Learning-discriminative-correlation-filters-via-for-Ma-Zhao/21375e2f1dec58df23d4b721b9cb2f31564a76fd",
            "/paper/Complementary-Discriminative-Correlation-Filters-on-Zhu-Wu/c824e01622546f8d56e5af4f319293c7b46bbc36",
            "/paper/Bayesian-Correlation-Filter-Learning-With-Gaussian-Cao-Shi/a8ec018284c385de4c5154165e256b4506d6903e",
            "/paper/Spatiotemporal-regularization-correlation-filter-Zhu-Shu/a893b7ef12dbac67bfd18b91c9e28e8c8f8c8b3d",
            "/paper/Subspace-reconstruction-based-correlation-filter-Tai-Tan/2d6f62535dfab9d811f6ba0e290752d38c3894dd",
            "/paper/Learning-Temporary-Block-Based-Bidirectional-for-Lin-Fu/efd510a4c92606f8713673a9da593a0507b60405",
            "/paper/Deep-Convolutional-Correlation-Iterative-Particle-Mozhdehi-Medeiros/e85e8c5851ab1cc44ea6dd2d3d4b5af9789db9af",
            "/paper/Hierarchical-Convolutional-Features-Fusion-for-Zhang-Chang/18c4a7b508039fb7ad99e6aca9bcae043d94c0d7",
            "/paper/Accurate-and-robust-visual-tracking-using-bounding-Yang-Gu/49f3a4dc3ec86f1b958f9573b71ab54589bae38c",
            "/paper/Correlation-Tracking-via-Joint-Discrimination-and-Sun-Wang/3f0da079ac950a4dfb699c41a90c087000e6ac38",
            "/paper/Discriminative-Correlation-Filter-with-Channel-and-Luke%C5%BEi%C4%8D-Voj%C3%ADr/b16a583ee173f222c690242aaff7925838893fe8",
            "/paper/Learning-Spatially-Regularized-Correlation-Filters-Danelljan-H%C3%A4ger/09769e80cdf027db32a1fcb695a1aa0937214763",
            "/paper/Learning-Spatial-Temporal-Regularized-Correlation-Li-Tian/9f45b55af027503fab557f55f70e81e43c6c1db7",
            "/paper/ECO%3A-Efficient-Convolution-Operators-for-Tracking-Danelljan-Bhat/a87cc499cf101b3697cacc65094b4b6590e0d061",
            "/paper/Learning-Background-Aware-Correlation-Filters-for-Galoogahi-Fagg/01c40508dcb6f8e9efcdefe49e22bc0ccaf8881c",
            "/paper/Correlation-Filters-with-Weighted-Convolution-He-Fan/a102c524b2bdc3c1f9377a5ea079d47783d636c6",
            "/paper/Convolutional-Features-for-Correlation-Filter-Based-Danelljan-H%C3%A4ger/311bc4e48838d8e5ef619df3ce0bc598aba788a1",
            "/paper/Multi-channel-Correlation-Filters-Galoogahi-Sim/21765df4c0224afcc25eb780bef654cbe6f0bc3a",
            "/paper/High-Speed-Tracking-with-Multi-kernel-Correlation-Tang-Yu/0dccbfe5a91e1d5610c46585270af0c648d2aa25"
        ]
    }
]